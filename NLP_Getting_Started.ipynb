{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Getting Started\n",
    "Sean O'Neil, Quinn Knudsen, Chris Bryla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "from sklearn import naive_bayes\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Dropout, Flatten, MaxPooling2D\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the approaches that will be tested:\n",
    "\n",
    "Preprocessing applied to both training and test data:\n",
    "\n",
    "* Tokenize with a special tokenizer designed for tweets\n",
    "* Make all text lowercase\n",
    "* Remove tokens that only occur in one tweet\n",
    "* For the test data: Remove tokens that don't occur in the training data (this could be a problem\n",
    "\n",
    "Additional preprocessing optionally applied:\n",
    "\n",
    "* Remove tokens that only contain punctuation or numbers\n",
    "* Remove mentions and URLs\n",
    "* Remove tokens with certain special characters\n",
    "* Remove stopwords\n",
    "* Lemmatize tokens\n",
    "\n",
    "Options that were not pursued:\n",
    "\n",
    "* Add in the location and keyword data, or maybe just whether or not they are present\n",
    "* Add in the sentiment data\n",
    "\n",
    "Minimal model: Take the set intersection between the unique tokens from the training and test datasets, and use those as the variables. This leaves 2618 variables.\n",
    "\n",
    "Secondary preprocessing options:\n",
    "\n",
    "* Use the raw counts of each word in each document\n",
    "* Use the document-length normalized frequencies\n",
    "* Use the TF-IDF transformed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the train and test data\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "# Tokenizer for Twitter derived tweetmotif from the ARK, developed at CMU\n",
    "tweetMotif = r''' (?x)\t# set flag to allow verbose regexps\n",
    "      (?:https?://|www)\\S+      # simple URLs\n",
    "      | (?::-\\)|;-\\))\t\t# small list of emoticons\n",
    "      | &(?:amp|lt|gt|quot);    # XML or HTML entity\n",
    "      | \\#\\w+                 # hashtags\n",
    "      | @\\w+                  # mentions   \n",
    "      | \\d+:\\d+               # timelike pattern\n",
    "      | \\d+\\.\\d+              # number with a decimal\n",
    "      | (?:\\d+,)+?\\d{3}(?=(?:[^,]|$))   # number with a comma\n",
    "      | (?:[A-Z]\\.)+                    # simple abbreviations\n",
    "      | (?:--+)               # multiple dashes\n",
    "      | \\w+(?:-\\w+)*          # words with internal hyphens or apostrophes\n",
    "      | ['\\\".?!,:;/]+         # special characters\n",
    "      '''\n",
    "\n",
    "# Read in stopwords and add in some special ones\n",
    "fstop = open('smart.english.stop', 'r')\n",
    "stoptext = fstop.read()\n",
    "fstop.close()\n",
    "stopwords = nltk.word_tokenize(stoptext)\n",
    "stopwords.extend(['&amp;', '&lt;', '&gt;', 'as', 'ur', 'isn', 'don', 'wa'])\n",
    "\n",
    "# Initialize the lemmatizer\n",
    "wnl = nltk.WordNetLemmatizer()\n",
    "\n",
    "# Filters out tokens consisting only of punctuation and/or numbers\n",
    "def alpha_filter(w):\n",
    "    pattern = re.compile('^[^a-z]+$')\n",
    "    return pattern.match(w)\n",
    "\n",
    "# Applies the previous filter, and also removes mentions, URLs, tokens with special characters, and stopwords\n",
    "def word_filter(word):\n",
    "    return not (alpha_filter(word) or '@' in word or '//' in word or 'รฅ' in word or 'รป' in word or word in stopwords)\n",
    "\n",
    "# Applies tokenizaion, lemmatization, and filters\n",
    "def document_analyze(document):\n",
    "    return list(filter(word_filter, map(wnl.lemmatize, nltk.regexp_tokenize(document, pattern=tweetMotif))))\n",
    "\n",
    "def POS_process(word):\n",
    "    if word == 'i':\n",
    "        return 'I'\n",
    "    # In case hashtags are actual words, this may help\n",
    "    # If they are made from multiple words, that could be a problem\n",
    "    if word.startswith('#'):\n",
    "        return word[1:]\n",
    "    # Leave mentions unchanged\n",
    "    # Replace links with a placeholder\n",
    "    if '//' in word:\n",
    "        return 'LINK' # Maybe?\n",
    "    return word\n",
    "\n",
    "def POS_translate(document):\n",
    "    return [word + '/' + tag for (word, tag) in nltk.pos_tag(document)]\n",
    "\n",
    "def POS_analyze(document):\n",
    "    return POS_translate(list(map(POS_process, nltk.regexp_tokenize(document, pattern=tweetMotif))))\n",
    "\n",
    "# Creates the term-document matrix\n",
    "def vectorize_data(data, prepro=True):\n",
    "    # Make all the text lowercase\n",
    "    documents = [text.lower() for text in data['text']]\n",
    "\n",
    "    # Run the vectorization process with the custom analyzer\n",
    "    vec = None\n",
    "    if prepro:\n",
    "        vec = CountVectorizer(analyzer = document_analyze)\n",
    "    else:\n",
    "        # No preprocessing\n",
    "        vec = CountVectorizer(analyzer = partial(nltk.regexp_tokenize, pattern=tweetMotif))\n",
    "    cv = vec.fit_transform(documents)\n",
    "    df = pd.DataFrame(cv.toarray(), columns=vec.get_feature_names())\n",
    "\n",
    "    if prepro:\n",
    "        # Remove words that only occur once\n",
    "        toDrop = []\n",
    "        for i in range(len(df.columns)):\n",
    "            if np.sum(df[df.columns[i]] > 0) < 2: # Threshold for minimum number of documents with this word\n",
    "                toDrop.append(df.columns[i])\n",
    "        df = df.drop(columns=toDrop)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def POS_vectorize_data(data):\n",
    "    documents = [text.lower() for text in data['text']]\n",
    "    vec = CountVectorizer(analyzer = POS_analyze)\n",
    "    cv = vec.fit_transform(documents)\n",
    "    df = pd.DataFrame(cv.toarray(), columns = vec.get_feature_names())\n",
    "     # Remove word-tag pairs that only occur once\n",
    "    toDrop = []\n",
    "    for i in range(len(df.columns)):\n",
    "        if np.sum(df[df.columns[i]] > 0) < 2: # Threshold for minimum number of documents with this word\n",
    "            toDrop.append(df.columns[i])\n",
    "    df = df.drop(columns=toDrop)\n",
    "    return df\n",
    "\n",
    "def ngram_vectorize_data(data, n=2):\n",
    "    documents = [text.lower() for text in data['text']]\n",
    "    vec = CountVectorizer(ngram_range = (1,n), analyzer = partial(nltk.regexp_tokenize, pattern=tweetMotif))\n",
    "    cv = vec.fit_transform(documents)\n",
    "    df = pd.DataFrame(cv.toarray(), columns = vec.get_feature_names())\n",
    "    toDrop = []\n",
    "    for i in range(len(df.columns)):\n",
    "        if np.sum(df[df.columns[i]] > 0) < 2: # Threshold for minimum number of documents with this word\n",
    "            toDrop.append(df.columns[i])\n",
    "    df = df.drop(columns=toDrop)\n",
    "    return df\n",
    "\n",
    "# Vectorize training set\n",
    "dftrain = vectorize_data(train)\n",
    "dftrain_noprep = vectorize_data(train, prepro=False)\n",
    "# Add back in the target variable\n",
    "dftrain['_target'] = train['target']\n",
    "dftrain_noprep['_target'] = train['target']\n",
    "\n",
    "# Vectorize test set\n",
    "dftest = vectorize_data(test)\n",
    "dftest_noprep = vectorize_data(test, prepro=False)\n",
    "\n",
    "# Vectorize with POS tagging\n",
    "dftrain_pos = POS_vectorize_data(train)\n",
    "dftest_pos = POS_vectorize_data(test)\n",
    "\n",
    "# Vectorize with bigrams, no preprocessing (since that seems to work better in other situations)\n",
    "dftrain_bi = ngram_vectorize_data(train)\n",
    "dftest_bi = ngram_vectorize_data(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_frequency_transformations(dftrain, dftest):\n",
    "    # Find the tokens in both training and testing sets\n",
    "    combined = set.intersection(set(dftrain.columns), set(dftest.columns))\n",
    "\n",
    "    # minimal: only include features which are in both\n",
    "    dftrain_minimal = dftrain.copy()\n",
    "    dftest_minimal = dftest.copy()\n",
    "\n",
    "    # Drop the columns not in the intersection (including _target, which will be added back in at the end)\n",
    "    toDrop = []\n",
    "    for col in dftrain.columns:\n",
    "        if col not in combined:\n",
    "            toDrop.append(col)\n",
    "    dftrain_minimal = dftrain_minimal.drop(columns=toDrop)\n",
    "\n",
    "    toDrop = []\n",
    "    for col in dftest.columns:\n",
    "        if col not in combined:\n",
    "            toDrop.append(col)\n",
    "    dftest_minimal = dftest_minimal.drop(columns=toDrop)\n",
    "\n",
    "    # Frequency normalization (normalize by the total length in terms of tokens remaining, so excluding things like stopwords)\n",
    "\n",
    "    # Count the number of tokens left in each document, and prevent division by 0\n",
    "    train_document_lengths = np.array(np.sum(dftrain_minimal, axis=1)).reshape(-1,1)\n",
    "    train_document_lengths[train_document_lengths == 0] = 1\n",
    "    test_document_lengths = np.array(np.sum(dftest_minimal, axis=1)).reshape(-1,1)\n",
    "    test_document_lengths[test_document_lengths == 0] = 1\n",
    "\n",
    "    # Make normalized frequencies\n",
    "    dftrain_min_norm = dftrain_minimal.copy()\n",
    "    dftest_min_norm = dftest_minimal.copy()\n",
    "\n",
    "    # Divide by the document lengths, row-wise\n",
    "    dftrain_min_norm /= train_document_lengths\n",
    "    dftest_min_norm /= test_document_lengths\n",
    "\n",
    "    # TF-IDF transform\n",
    "\n",
    "    # Note: The training and testing dataframes have the same columns.\n",
    "    # When computing inverse document frequency, use the combined data from both training and test sets.\n",
    "\n",
    "    # Find the inverse document frequency\n",
    "    #log(train.shape[0]/count)\n",
    "    # Because we removed columns with only one occurrence for both the train and test sets, these counts will always be 4 or more.\n",
    "    counts = np.array(np.sum(dftrain_minimal > 0, axis=0)) + np.array(np.sum(dftest_minimal > 0, axis=0))\n",
    "\n",
    "    # Take the log of the inverse document frequency\n",
    "    idf_vals = np.log10((train.shape[0]+test.shape[0])/counts)\n",
    "\n",
    "    dftrain_min_tfidf = dftrain_min_norm.copy()\n",
    "    dftest_min_tfidf = dftest_min_norm.copy()\n",
    "\n",
    "    # Multiply by the IDF values, column-wise\n",
    "    dftrain_min_tfidf *= idf_vals\n",
    "    dftest_min_tfidf *= idf_vals\n",
    "\n",
    "    # Add back in the _target variable to these three preprocessing variants\n",
    "    dftrain_minimal['_target'] = train['target']\n",
    "    dftrain_min_norm['_target'] = train['target']\n",
    "    dftrain_min_tfidf['_target'] = train['target']\n",
    "    \n",
    "    return dftrain_minimal, dftest_minimal, dftrain_min_norm, dftest_min_norm, dftrain_min_tfidf, dftest_min_tfidf\n",
    "\n",
    "# 12 train-test pairs are here\n",
    "# Preprocessing is {no prep, prep, POS tagging, bigrams} * {raw counts, normalized, TF-IDF}\n",
    "dftrain_minimal_noprep, dftest_minimal_noprep, dftrain_min_norm_noprep, dftest_min_norm_noprep, dftrain_min_tfidf_noprep, dftest_min_tfidf_noprep = apply_frequency_transformations(dftrain_noprep, dftest_noprep)\n",
    "dftrain_minimal, dftest_minimal, dftrain_min_norm, dftest_min_norm, dftrain_min_tfidf, dftest_min_tfidf = apply_frequency_transformations(dftrain, dftest)\n",
    "dftrain_pos_minimal, dftest_pos_minimal, dftrain_pos_min_norm, dftest_pos_min_norm, dftrain_pos_min_tfidf, dftest_pos_min_tfidf = apply_frequency_transformations(dftrain_pos, dftest_pos)\n",
    "dftrain_bi_minimal, dftest_bi_minimal, dftrain_bi_min_norm, dftest_bi_min_norm, dftrain_bi_min_tfidf, dftest_bi_min_tfidf = apply_frequency_transformations(dftrain_bi, dftest_bi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for cross-validation\n",
    "# Critical t value for n = 5 and 95% confidence is 2.78\n",
    "# ddof=1 gives the sample standard deviation\n",
    "# Assumes len(scores) = 5\n",
    "def make_CI(name, scores):\n",
    "    mean_score = np.mean(scores)\n",
    "    ci_val = 2.78*np.std(scores, ddof=1)/np.sqrt(5)\n",
    "    print('{}: {:.2f}% +/- {:.2f}%'.format(name, 100*mean_score, 100*ci_val))\n",
    "\n",
    "def all_metrics(scores):\n",
    "    make_CI('Accuracy', scores[0])\n",
    "    make_CI('Precision for positive class', scores[1])\n",
    "    make_CI('Precision for negative class', scores[2])\n",
    "    make_CI('Recall for positive class', scores[3])\n",
    "    make_CI('Recall for negative class', scores[4])\n",
    "    make_CI('F for positive class', scores[5])\n",
    "    make_CI('F for negative class', scores[6])\n",
    "    make_CI('Mean F score', scores[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame(test['id'], columns=['id', 'target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# e.g. split=0.3, trees=500, quick evaluation\n",
    "def run_randomForest(dataset, split, trees):\n",
    "    t0 = time.time()\n",
    "    np.random.seed(1)\n",
    "    train, test = train_test_split(dataset, test_size=split)\n",
    "    train_x = train.drop('_target', axis=1)\n",
    "    train_y = train['_target']\n",
    "    test_x = test.drop('_target', axis=1)\n",
    "    test_y = test['_target']\n",
    "    print('Training...')\n",
    "    rft = RandomForestClassifier(n_estimators=trees)\n",
    "    rft.fit(train_x, train_y)\n",
    "    print('Predicting...')\n",
    "    preds = rft.predict(test_x)\n",
    "    cm = confusion_matrix(test_y, preds)\n",
    "    print(cm)\n",
    "    tp, fn, fp, tn = cm.ravel()\n",
    "    accuracy = (tp+tn)/(tp+fn+fp+tn)\n",
    "    precision = tp/(tp+fp)\n",
    "    recall = tp/(tp+fn)\n",
    "    F = 2*precision*recall/(precision+recall)\n",
    "    print('Accuracy: {:.2f}%\\nPrecision: {:.2f}%\\nRecall: {:.2f}%\\nF: {:.2f}'.format(100*accuracy, 100*precision, 100*recall, 100*F))\n",
    "    t1 = time.time()\n",
    "    print('(Took {:.3f} sec)'.format(t1-t0))\n",
    "    return rft\n",
    "\n",
    "# 5-fold cross-validation, all metrics\n",
    "def cross_validate_random_forest(dataset, trees):\n",
    "    t_0 = time.time()\n",
    "    # Fixed at 5-fold cross-validation for now\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "    X = np.array(dataset.drop('_target', axis=1))\n",
    "    y = np.array(dataset['_target'])\n",
    "    accs = []\n",
    "    precs_p = []\n",
    "    precs_n = []\n",
    "    recs_p = []\n",
    "    recs_n = []\n",
    "    Fs_p = []\n",
    "    Fs_n = []\n",
    "    mFs = [] # Mean F score, this is the actual competition metric\n",
    "    cm = np.zeros((2,2))\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        t0 = time.time()\n",
    "        train_x, test_x = X[train_index], X[test_index]\n",
    "        train_y, test_y = y[train_index], y[test_index]\n",
    "        #print('Training...')\n",
    "        rft = RandomForestClassifier(n_estimators=trees)\n",
    "        rft.fit(train_x, train_y)\n",
    "        #print('Predicting...')\n",
    "        preds = rft.predict(test_x)\n",
    "        cm_batch = confusion_matrix(test_y, preds)\n",
    "        cm += np.array(cm_batch)\n",
    "        tp, fn, fp, tn = cm_batch.ravel()\n",
    "        acc = (tp+tn)/(tp+fn+fp+tn)\n",
    "        accs.append( acc )\n",
    "        prec_p = tp/(tp+fp)\n",
    "        precs_p.append( prec_p )\n",
    "        prec_n = tn/(tn+fn)\n",
    "        precs_n.append( prec_n )\n",
    "        rec_p = tp/(tp+fn)\n",
    "        recs_p.append( rec_p )\n",
    "        rec_n = tn/(tn+fp)\n",
    "        recs_n.append( rec_n )\n",
    "        F_p = 2*prec_p*rec_p/(prec_p+rec_p)\n",
    "        Fs_p.append( F_p )\n",
    "        F_n = 2*prec_n*rec_n/(prec_n+rec_n)\n",
    "        Fs_n.append( F_n )\n",
    "        mF = (F_p + F_n)/2.0\n",
    "        mFs.append( mF )\n",
    "        t1 = time.time()\n",
    "        print('(Took {:.3f} sec)'.format(t1-t0))\n",
    "    t_1 = time.time()\n",
    "    print('Combined confusion matrix:')\n",
    "    print(cm)\n",
    "    print('(Overall, took {:.3f} sec)'.format(t_1-t_0))\n",
    "    return [accs, precs_p, precs_n, recs_p, recs_n, Fs_p, Fs_n, mFs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Predicting...\n",
      "[[204  15]\n",
      " [ 66  96]]\n",
      "Accuracy: 78.74%\n",
      "Precision: 75.56%\n",
      "Recall: 93.15%\n",
      "F: 83.44\n",
      "(Took 134.297 sec)\n"
     ]
    }
   ],
   "source": [
    "# Test model to see feature importances\n",
    "rf = run_randomForest(dftrain_minimal_noprep, 0.05, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x1152 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "features = {dftrain_minimal_noprep.columns[i]:rf.feature_importances_[i] for i in range(len(rf.feature_importances_))}\n",
    "top_features = sorted(features.items(), key=lambda x: x[1], reverse=True)[0:100]\n",
    "topfeatures = pd.DataFrame(columns = ['feature', 'importance'])\n",
    "topfeatures['feature'] = [top_features[i][0] for i in range(100)]\n",
    "topfeatures['importance'] = [top_features[i][1] for i in range(100)]\n",
    "\n",
    "plt.figure(figsize=(16,16))\n",
    "plt.barh([i for i in range(20)], topfeatures['importance'][0:20], align='center', alpha=0.5)\n",
    "plt.yticks([i for i in range(20)], topfeatures['feature'][0:20])\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Top feature importances')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Took 175.586 sec)\n",
      "(Took 166.540 sec)\n",
      "(Took 178.447 sec)\n",
      "(Took 172.954 sec)\n",
      "(Took 175.714 sec)\n",
      "Combined confusion matrix:\n",
      "[[3967.  375.]\n",
      " [1252. 2019.]]\n",
      "(Overall, took 870.133 sec)\n",
      "Accuracy: 78.63% +/- 0.32%\n",
      "Precision for positive class: 76.01% +/- 1.34%\n",
      "Precision for negative class: 84.37% +/- 2.26%\n",
      "Recall for positive class: 91.37% +/- 1.48%\n",
      "Recall for negative class: 61.72% +/- 1.95%\n",
      "F for positive class: 82.98% +/- 0.62%\n",
      "F for negative class: 71.26% +/- 0.86%\n",
      "Mean F score: 77.12% +/- 0.24%\n",
      "(Took 167.798 sec)\n",
      "(Took 161.820 sec)\n",
      "(Took 176.910 sec)\n",
      "(Took 177.370 sec)\n",
      "(Took 168.292 sec)\n",
      "Combined confusion matrix:\n",
      "[[3976.  366.]\n",
      " [1284. 1987.]]\n",
      "(Overall, took 852.725 sec)\n",
      "Accuracy: 78.33% +/- 0.99%\n",
      "Precision for positive class: 75.59% +/- 1.84%\n",
      "Precision for negative class: 84.48% +/- 2.08%\n",
      "Recall for positive class: 91.57% +/- 1.39%\n",
      "Recall for negative class: 60.77% +/- 1.70%\n",
      "F for positive class: 82.81% +/- 1.09%\n",
      "F for negative class: 70.67% +/- 0.56%\n",
      "Mean F score: 76.74% +/- 0.78%\n",
      "(Took 170.507 sec)\n",
      "(Took 165.736 sec)\n",
      "(Took 166.746 sec)\n",
      "(Took 165.870 sec)\n",
      "(Took 165.094 sec)\n",
      "Combined confusion matrix:\n",
      "[[3978.  364.]\n",
      " [1287. 1984.]]\n",
      "(Overall, took 834.439 sec)\n",
      "Accuracy: 78.31% +/- 0.73%\n",
      "Precision for positive class: 75.55% +/- 1.46%\n",
      "Precision for negative class: 84.53% +/- 2.24%\n",
      "Recall for positive class: 91.62% +/- 1.49%\n",
      "Recall for negative class: 60.66% +/- 1.35%\n",
      "F for positive class: 82.80% +/- 0.94%\n",
      "F for negative class: 70.61% +/- 0.40%\n",
      "Mean F score: 76.71% +/- 0.46%\n",
      "(Took 232.670 sec)\n",
      "(Took 244.208 sec)\n",
      "(Took 236.544 sec)\n",
      "(Took 236.417 sec)\n",
      "(Took 233.768 sec)\n",
      "Combined confusion matrix:\n",
      "[[3578.  764.]\n",
      " [1045. 2226.]]\n",
      "(Overall, took 1183.808 sec)\n",
      "Accuracy: 76.24% +/- 0.66%\n",
      "Precision for positive class: 77.43% +/- 2.11%\n",
      "Precision for negative class: 74.50% +/- 2.68%\n",
      "Recall for positive class: 82.42% +/- 2.38%\n",
      "Recall for negative class: 68.08% +/- 3.31%\n",
      "F for positive class: 79.82% +/- 0.68%\n",
      "F for negative class: 71.09% +/- 1.34%\n",
      "Mean F score: 75.45% +/- 0.75%\n",
      "(Took 272.014 sec)\n",
      "(Took 272.352 sec)\n",
      "(Took 274.971 sec)\n",
      "(Took 292.892 sec)\n",
      "(Took 453.295 sec)\n",
      "Combined confusion matrix:\n",
      "[[3685.  657.]\n",
      " [1024. 2247.]]\n",
      "(Overall, took 1565.736 sec)\n",
      "Accuracy: 77.92% +/- 1.01%\n",
      "Precision for positive class: 78.28% +/- 2.24%\n",
      "Precision for negative class: 77.41% +/- 2.59%\n",
      "Recall for positive class: 84.88% +/- 2.15%\n",
      "Recall for negative class: 68.71% +/- 3.60%\n",
      "F for positive class: 81.42% +/- 0.88%\n",
      "F for negative class: 72.75% +/- 1.75%\n",
      "Mean F score: 77.09% +/- 1.13%\n",
      "(Took 471.777 sec)\n",
      "(Took 289.937 sec)\n",
      "(Took 287.464 sec)\n",
      "(Took 286.283 sec)\n",
      "(Took 278.269 sec)\n",
      "Combined confusion matrix:\n",
      "[[3675.  667.]\n",
      " [1020. 2251.]]\n",
      "(Overall, took 1614.060 sec)\n",
      "Accuracy: 77.84% +/- 1.06%\n",
      "Precision for positive class: 78.31% +/- 2.15%\n",
      "Precision for negative class: 77.19% +/- 2.89%\n",
      "Recall for positive class: 84.65% +/- 2.49%\n",
      "Recall for negative class: 68.83% +/- 3.61%\n",
      "F for positive class: 81.33% +/- 0.96%\n",
      "F for negative class: 72.72% +/- 1.78%\n",
      "Mean F score: 77.02% +/- 1.15%\n"
     ]
    }
   ],
   "source": [
    "# Cross-validated random forest models (all have 500 trees)\n",
    "# No prep, raw counts\n",
    "all_metrics(cross_validate_random_forest(dftrain_minimal_noprep, 500))\n",
    "# No prep, normalized\n",
    "all_metrics(cross_validate_random_forest(dftrain_min_norm_noprep, 500))\n",
    "# No prep, TF-IDF\n",
    "all_metrics(cross_validate_random_forest(dftrain_min_tfidf_noprep, 500))\n",
    "# Preprocessed, raw counts\n",
    "all_metrics(cross_validate_random_forest(dftrain_minimal, 500))\n",
    "# Preprocessed, normalized\n",
    "all_metrics(cross_validate_random_forest(dftrain_min_norm, 500))\n",
    "# Preprocessed, TF-IDF\n",
    "all_metrics(cross_validate_random_forest(dftrain_min_tfidf, 500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Took 83.597 sec)\n",
      "(Took 84.683 sec)\n",
      "(Took 87.339 sec)\n",
      "(Took 84.173 sec)\n",
      "(Took 83.563 sec)\n",
      "Combined confusion matrix:\n",
      "[[3918.  424.]\n",
      " [1250. 2021.]]\n",
      "(Overall, took 423.794 sec)\n",
      "Accuracy: 78.01% +/- 0.71%\n",
      "Precision for positive class: 75.82% +/- 1.75%\n",
      "Precision for negative class: 82.67% +/- 2.21%\n",
      "Recall for positive class: 90.25% +/- 1.23%\n",
      "Recall for negative class: 61.80% +/- 2.17%\n",
      "F for positive class: 82.39% +/- 0.71%\n",
      "F for negative class: 70.70% +/- 1.13%\n",
      "Mean F score: 76.55% +/- 0.74%\n",
      "(Took 84.540 sec)\n",
      "(Took 86.329 sec)\n",
      "(Took 88.001 sec)\n",
      "(Took 84.978 sec)\n",
      "(Took 81.386 sec)\n",
      "Combined confusion matrix:\n",
      "[[3940.  402.]\n",
      " [1315. 1956.]]\n",
      "(Overall, took 425.446 sec)\n",
      "Accuracy: 77.45% +/- 1.06%\n",
      "Precision for positive class: 74.98% +/- 1.96%\n",
      "Precision for negative class: 82.96% +/- 1.40%\n",
      "Recall for positive class: 90.74% +/- 0.94%\n",
      "Recall for negative class: 59.82% +/- 2.31%\n",
      "F for positive class: 82.10% +/- 1.06%\n",
      "F for negative class: 69.49% +/- 1.23%\n",
      "Mean F score: 75.80% +/- 1.00%\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'dftrain_pos_min_tdidf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-c45c3bac6f4c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mall_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcross_validate_random_forest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdftrain_pos_min_norm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m500\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# POS tagging, TF-IDF\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mall_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcross_validate_random_forest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdftrain_pos_min_tdidf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m500\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'dftrain_pos_min_tdidf' is not defined"
     ]
    }
   ],
   "source": [
    "# POS tagging, raw counts\n",
    "all_metrics(cross_validate_random_forest(dftrain_pos_minimal, 500))\n",
    "# POS tagging, normalized\n",
    "all_metrics(cross_validate_random_forest(dftrain_pos_min_norm, 500))\n",
    "# Typo, woops. Don't want to rerun these"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Took 81.309 sec)\n",
      "(Took 78.752 sec)\n",
      "(Took 85.863 sec)\n",
      "(Took 83.712 sec)\n",
      "(Took 84.529 sec)\n",
      "Combined confusion matrix:\n",
      "[[3929.  413.]\n",
      " [1311. 1960.]]\n",
      "(Overall, took 414.357 sec)\n",
      "Accuracy: 77.35% +/- 0.71%\n",
      "Precision for positive class: 74.98% +/- 1.71%\n",
      "Precision for negative class: 82.61% +/- 1.81%\n",
      "Recall for positive class: 90.50% +/- 1.02%\n",
      "Recall for negative class: 59.94% +/- 1.88%\n",
      "F for positive class: 82.00% +/- 0.80%\n",
      "F for negative class: 69.45% +/- 0.90%\n",
      "Mean F score: 75.72% +/- 0.64%\n"
     ]
    }
   ],
   "source": [
    "# POS tagging, TF-IDF\n",
    "all_metrics(cross_validate_random_forest(dftrain_pos_min_tfidf, 500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Took 94.395 sec)\n",
      "(Took 94.816 sec)\n",
      "(Took 95.213 sec)\n",
      "(Took 95.183 sec)\n",
      "(Took 91.817 sec)\n",
      "Combined confusion matrix:\n",
      "[[3925.  417.]\n",
      " [1215. 2056.]]\n",
      "(Overall, took 471.786 sec)\n",
      "Accuracy: 78.56% +/- 0.81%\n",
      "Precision for positive class: 76.36% +/- 1.67%\n",
      "Precision for negative class: 83.16% +/- 1.90%\n",
      "Recall for positive class: 90.40% +/- 1.34%\n",
      "Recall for negative class: 62.87% +/- 1.86%\n",
      "F for positive class: 82.78% +/- 0.95%\n",
      "F for negative class: 71.58% +/- 0.79%\n",
      "Mean F score: 77.18% +/- 0.66%\n"
     ]
    }
   ],
   "source": [
    "# Bigrams, no preprocessing\n",
    "all_metrics(cross_validate_random_forest(dftrain_bi_minimal, 500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Took 103.832 sec)\n",
      "(Took 99.711 sec)\n",
      "(Took 100.042 sec)\n",
      "(Took 99.642 sec)\n",
      "(Took 104.247 sec)\n",
      "Combined confusion matrix:\n",
      "[[3934.  408.]\n",
      " [1267. 2004.]]\n",
      "(Overall, took 507.804 sec)\n",
      "Accuracy: 78.00% +/- 0.93%\n",
      "Precision for positive class: 75.64% +/- 1.91%\n",
      "Precision for negative class: 83.11% +/- 2.07%\n",
      "Recall for positive class: 90.61% +/- 1.32%\n",
      "Recall for negative class: 61.29% +/- 1.97%\n",
      "F for positive class: 82.44% +/- 0.99%\n",
      "F for negative class: 70.53% +/- 0.77%\n",
      "Mean F score: 76.48% +/- 0.79%\n",
      "(Took 102.143 sec)\n",
      "(Took 98.834 sec)\n",
      "(Took 103.584 sec)\n",
      "(Took 101.021 sec)\n",
      "(Took 102.712 sec)\n",
      "Combined confusion matrix:\n",
      "[[3929.  413.]\n",
      " [1251. 2020.]]\n",
      "(Overall, took 508.609 sec)\n",
      "Accuracy: 78.14% +/- 0.82%\n",
      "Precision for positive class: 75.85% +/- 1.66%\n",
      "Precision for negative class: 83.05% +/- 2.22%\n",
      "Recall for positive class: 90.49% +/- 1.43%\n",
      "Recall for negative class: 61.77% +/- 1.55%\n",
      "F for positive class: 82.52% +/- 0.94%\n",
      "F for negative class: 70.83% +/- 0.65%\n",
      "Mean F score: 76.67% +/- 0.66%\n"
     ]
    }
   ],
   "source": [
    "# Bigrams, normalized\n",
    "all_metrics(cross_validate_random_forest(dftrain_bi_min_norm, 500))\n",
    "# Bigrams, TF-IDF\n",
    "all_metrics(cross_validate_random_forest(dftrain_bi_min_tfidf, 500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_random_forest_predictions(trees, train, test):\n",
    "    print('Training...')\n",
    "    rft = RandomForestClassifier(n_estimators=trees)\n",
    "    rft.fit(train.drop('_target', axis=1), train['_target'])\n",
    "    print('Predicting...')\n",
    "    preds = rft.predict(test)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Predicting...\n"
     ]
    }
   ],
   "source": [
    "submission['target'] = make_random_forest_predictions(500, dftrain_minimal_noprep, dftest_minimal_noprep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission_rf_noprep.csv', index=False)\n",
    "# Score: 79.55%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Predicting...\n"
     ]
    }
   ],
   "source": [
    "submission['target'] = make_random_forest_predictions(500, dftrain_min_tfidf, dftest_min_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission_rf_prep.csv', index=False)\n",
    "# Score: 78.732%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other random forest models were tried before doing cross-validation, but these were not left in the file.\n",
    "Adjusting the number of trees, 500 was found to work decently well.\n",
    "Modifying the criterion between entropy and Gini did not produce better models.\n",
    "Raising the minimum document threshold for a word decreased scores.\n",
    "Overall, the best models ended up being the ones that didn't do any real preprocessing, which is kind of ironic given that I tried all these preprocessing options."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regularization did not help at all, sadly\n",
    "def run_logistic(dataset, split, epochs, batch_size=32, reg='', patience=1):\n",
    "    t0 = time.time()\n",
    "    np.random.seed(1)\n",
    "    train, test = train_test_split(dataset, test_size=split)\n",
    "    train_x = np.array(train.drop('_target', axis=1))\n",
    "    train_y = np.array(train['_target'])\n",
    "    test_x = np.array(test.drop('_target', axis=1))\n",
    "    test_y = np.array(test['_target'])\n",
    "    print('Training...')\n",
    "    nn = Sequential()\n",
    "    if reg == '':\n",
    "        nn.add(Dense(1, activation='sigmoid', input_shape=(train_x.shape[1],)))\n",
    "    else:\n",
    "        nn.add(Dense(1, activation='sigmoid', kernel_regularizer='l1', input_shape=(train_x.shape[1],)))\n",
    "    nn.compile(optimizer='adam', loss='binary_crossentropy', metrics=[])\n",
    "    es = EarlyStopping(monitor='val_loss', patience=patience)\n",
    "    h = nn.fit(train_x, train_y, batch_size = batch_size, epochs=epochs, validation_data=(test_x, test_y), callbacks=[es])\n",
    "    plt.plot(h.history['val_loss'])\n",
    "    print('Predicting...')\n",
    "    preds = 1*(nn.predict(test_x) > 0.5)\n",
    "    cm = confusion_matrix(test_y, preds)\n",
    "    print(cm)\n",
    "    tp, fn, fp, tn = cm.ravel()\n",
    "    accuracy = (tp+tn)/(tp+fn+fp+tn)\n",
    "    precision = tp/(tp+fp)\n",
    "    recall = tp/(tp+fn)\n",
    "    F = 2*precision*recall/(precision+recall)\n",
    "    print('Accuracy: {:.2f}%\\nPrecision: {:.2f}%\\nRecall: {:.2f}%\\nF: {:.2f}'.format(100*accuracy, 100*precision, 100*recall, 100*F))\n",
    "    t1 = time.time()\n",
    "    print('(Took {:.3f} sec)'.format(t1-t0))\n",
    "    return nn\n",
    "\n",
    "# 5-fold cross-validation, all metrics\n",
    "def cross_validate_logistic(dataset, epochs, batch_size=32):\n",
    "    t_0 = time.time()\n",
    "    # Fixed at 5-fold cross-validation for now\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "    X = np.array(dataset.drop('_target', axis=1))\n",
    "    y = np.array(dataset['_target'])\n",
    "    accs = []\n",
    "    precs_p = []\n",
    "    precs_n = []\n",
    "    recs_p = []\n",
    "    recs_n = []\n",
    "    Fs_p = []\n",
    "    Fs_n = []\n",
    "    mFs = [] # Mean F score, this is the actual competition metric\n",
    "    cm = np.zeros((2,2))\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        t0 = time.time()\n",
    "        train_x, test_x = X[train_index], X[test_index]\n",
    "        train_y, test_y = y[train_index], y[test_index]\n",
    "        #print('Training...')\n",
    "        nn = Sequential()\n",
    "        nn.add(Dense(1, activation='sigmoid', input_shape=(train_x.shape[1],)))\n",
    "        nn.compile(optimizer='adam', loss='binary_crossentropy', metrics=[])\n",
    "        es = EarlyStopping(monitor='val_loss', patience=1)\n",
    "        h = nn.fit(train_x, train_y, batch_size = batch_size, epochs=epochs, validation_data=(test_x, test_y), callbacks=[es])\n",
    "        #plt.plot(h.history['val_loss'])\n",
    "        print('Predicting...')\n",
    "        preds = 1*(nn.predict(test_x) > 0.5)\n",
    "        cm_batch = confusion_matrix(test_y, preds)\n",
    "        cm += np.array(cm_batch)\n",
    "        tp, fn, fp, tn = cm_batch.ravel()\n",
    "        acc = (tp+tn)/(tp+fn+fp+tn)\n",
    "        accs.append( acc )\n",
    "        prec_p = tp/(tp+fp)\n",
    "        precs_p.append( prec_p )\n",
    "        prec_n = tn/(tn+fn)\n",
    "        precs_n.append( prec_n )\n",
    "        rec_p = tp/(tp+fn)\n",
    "        recs_p.append( rec_p )\n",
    "        rec_n = tn/(tn+fp)\n",
    "        recs_n.append( rec_n )\n",
    "        F_p = 2*prec_p*rec_p/(prec_p+rec_p)\n",
    "        Fs_p.append( F_p )\n",
    "        F_n = 2*prec_n*rec_n/(prec_n+rec_n)\n",
    "        Fs_n.append( F_n )\n",
    "        mF = (F_p + F_n)/2.0\n",
    "        mFs.append( mF )\n",
    "        t1 = time.time()\n",
    "        print('(Took {:.3f} sec)'.format(t1-t0))\n",
    "    t_1 = time.time()\n",
    "    print('Combined confusion matrix:')\n",
    "    print(cm)\n",
    "    print('(Overall, took {:.3f} sec)'.format(t_1-t_0))\n",
    "    return [accs, precs_p, precs_n, recs_p, recs_n, Fs_p, Fs_n, mFs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other logistic regression models were tried, without significant improvement.\n",
    "Again, raising the document frequency threshold decreased scores.\n",
    "Regularization (both L1 and L2 were tried) did not seem to improve scores at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No prep, raw counts\n",
      "Training...\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/100\n",
      "6090/6090 [==============================] - 2s 263us/sample - loss: 0.6479 - val_loss: 0.6052\n",
      "Epoch 2/100\n",
      "6090/6090 [==============================] - 1s 135us/sample - loss: 0.5776 - val_loss: 0.5608\n",
      "Epoch 3/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.5337 - val_loss: 0.5324\n",
      "Epoch 4/100\n",
      "6090/6090 [==============================] - 1s 132us/sample - loss: 0.5017 - val_loss: 0.5132\n",
      "Epoch 5/100\n",
      "6090/6090 [==============================] - 1s 148us/sample - loss: 0.4769 - val_loss: 0.4984\n",
      "Epoch 6/100\n",
      "6090/6090 [==============================] - 1s 162us/sample - loss: 0.4563 - val_loss: 0.4871\n",
      "Epoch 7/100\n",
      "6090/6090 [==============================] - 1s 142us/sample - loss: 0.4387 - val_loss: 0.4785\n",
      "Epoch 8/100\n",
      "6090/6090 [==============================] - 1s 136us/sample - loss: 0.4235 - val_loss: 0.4714\n",
      "Epoch 9/100\n",
      "6090/6090 [==============================] - 1s 136us/sample - loss: 0.4099 - val_loss: 0.4655\n",
      "Epoch 10/100\n",
      "6090/6090 [==============================] - 1s 165us/sample - loss: 0.3978 - val_loss: 0.4608\n",
      "Epoch 11/100\n",
      "6090/6090 [==============================] - 1s 132us/sample - loss: 0.3868 - val_loss: 0.4571\n",
      "Epoch 12/100\n",
      "6090/6090 [==============================] - 1s 168us/sample - loss: 0.3769 - val_loss: 0.4538\n",
      "Epoch 13/100\n",
      "6090/6090 [==============================] - 1s 130us/sample - loss: 0.3676 - val_loss: 0.4509\n",
      "Epoch 14/100\n",
      "6090/6090 [==============================] - 1s 132us/sample - loss: 0.3591 - val_loss: 0.4485\n",
      "Epoch 15/100\n",
      "6090/6090 [==============================] - 1s 130us/sample - loss: 0.3513 - val_loss: 0.4467\n",
      "Epoch 16/100\n",
      "6090/6090 [==============================] - 1s 147us/sample - loss: 0.3438 - val_loss: 0.4452\n",
      "Epoch 17/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.3367 - val_loss: 0.4438\n",
      "Epoch 18/100\n",
      "6090/6090 [==============================] - 1s 128us/sample - loss: 0.3303 - val_loss: 0.4431\n",
      "Epoch 19/100\n",
      "6090/6090 [==============================] - 1s 129us/sample - loss: 0.3242 - val_loss: 0.4421\n",
      "Epoch 20/100\n",
      "6090/6090 [==============================] - 1s 132us/sample - loss: 0.3181 - val_loss: 0.4415\n",
      "Epoch 21/100\n",
      "6090/6090 [==============================] - 1s 135us/sample - loss: 0.3126 - val_loss: 0.4412\n",
      "Epoch 22/100\n",
      "6090/6090 [==============================] - 1s 181us/sample - loss: 0.3071 - val_loss: 0.4410\n",
      "Epoch 23/100\n",
      "6090/6090 [==============================] - 1s 134us/sample - loss: 0.3021 - val_loss: 0.4409\n",
      "Epoch 24/100\n",
      "6090/6090 [==============================] - 1s 140us/sample - loss: 0.2972 - val_loss: 0.4410\n",
      "Predicting...\n",
      "[[787  95]\n",
      " [197 444]]\n",
      "Accuracy: 80.83%\n",
      "Precision: 79.98%\n",
      "Recall: 89.23%\n",
      "F: 84.35\n",
      "(Took 23.070 sec)\n",
      "No prep, normalized\n",
      "Training...\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/100\n",
      "6090/6090 [==============================] - 1s 233us/sample - loss: 0.6859 - val_loss: 0.6782\n",
      "Epoch 2/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.6739 - val_loss: 0.6681\n",
      "Epoch 3/100\n",
      "6090/6090 [==============================] - 1s 135us/sample - loss: 0.6647 - val_loss: 0.6602\n",
      "Epoch 4/100\n",
      "6090/6090 [==============================] - 1s 142us/sample - loss: 0.6568 - val_loss: 0.6532\n",
      "Epoch 5/100\n",
      "6090/6090 [==============================] - 1s 142us/sample - loss: 0.6495 - val_loss: 0.6469\n",
      "Epoch 6/100\n",
      "6090/6090 [==============================] - 1s 144us/sample - loss: 0.6425 - val_loss: 0.6408\n",
      "Epoch 7/100\n",
      "6090/6090 [==============================] - 1s 148us/sample - loss: 0.6358 - val_loss: 0.6350\n",
      "Epoch 8/100\n",
      "6090/6090 [==============================] - 1s 140us/sample - loss: 0.6292 - val_loss: 0.6295\n",
      "Epoch 9/100\n",
      "6090/6090 [==============================] - 1s 142us/sample - loss: 0.6230 - val_loss: 0.6241\n",
      "Epoch 10/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.6169 - val_loss: 0.6190\n",
      "Epoch 11/100\n",
      "6090/6090 [==============================] - 1s 140us/sample - loss: 0.6110 - val_loss: 0.6141\n",
      "Epoch 12/100\n",
      "6090/6090 [==============================] - 1s 141us/sample - loss: 0.6053 - val_loss: 0.6093\n",
      "Epoch 13/100\n",
      "6090/6090 [==============================] - 1s 146us/sample - loss: 0.5998 - val_loss: 0.6047\n",
      "Epoch 14/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.5945 - val_loss: 0.6003\n",
      "Epoch 15/100\n",
      "6090/6090 [==============================] - 1s 153us/sample - loss: 0.5893 - val_loss: 0.5961\n",
      "Epoch 16/100\n",
      "6090/6090 [==============================] - 1s 143us/sample - loss: 0.5843 - val_loss: 0.5919\n",
      "Epoch 17/100\n",
      "6090/6090 [==============================] - 1s 147us/sample - loss: 0.5794 - val_loss: 0.5879\n",
      "Epoch 18/100\n",
      "6090/6090 [==============================] - 1s 149us/sample - loss: 0.5747 - val_loss: 0.5840\n",
      "Epoch 19/100\n",
      "6090/6090 [==============================] - 1s 143us/sample - loss: 0.5701 - val_loss: 0.5802\n",
      "Epoch 20/100\n",
      "6090/6090 [==============================] - 1s 141us/sample - loss: 0.5656 - val_loss: 0.5767\n",
      "Epoch 21/100\n",
      "6090/6090 [==============================] - 1s 150us/sample - loss: 0.5613 - val_loss: 0.5731\n",
      "Epoch 22/100\n",
      "6090/6090 [==============================] - 1s 159us/sample - loss: 0.5571 - val_loss: 0.5698\n",
      "Epoch 23/100\n",
      "6090/6090 [==============================] - 1s 142us/sample - loss: 0.5530 - val_loss: 0.5666\n",
      "Epoch 24/100\n",
      "6090/6090 [==============================] - 1s 141us/sample - loss: 0.5490 - val_loss: 0.5634\n",
      "Epoch 25/100\n",
      "6090/6090 [==============================] - 1s 144us/sample - loss: 0.5451 - val_loss: 0.5604\n",
      "Epoch 26/100\n",
      "6090/6090 [==============================] - 1s 152us/sample - loss: 0.5413 - val_loss: 0.5574\n",
      "Epoch 27/100\n",
      "6090/6090 [==============================] - 1s 157us/sample - loss: 0.5376 - val_loss: 0.5545\n",
      "Epoch 28/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.5340 - val_loss: 0.5518\n",
      "Epoch 29/100\n",
      "6090/6090 [==============================] - 1s 142us/sample - loss: 0.5306 - val_loss: 0.5491\n",
      "Epoch 30/100\n",
      "6090/6090 [==============================] - 1s 141us/sample - loss: 0.5271 - val_loss: 0.5465\n",
      "Epoch 31/100\n",
      "6090/6090 [==============================] - 1s 141us/sample - loss: 0.5238 - val_loss: 0.5439\n",
      "Epoch 32/100\n",
      "6090/6090 [==============================] - 1s 142us/sample - loss: 0.5206 - val_loss: 0.5415\n",
      "Epoch 33/100\n",
      "6090/6090 [==============================] - 1s 143us/sample - loss: 0.5174 - val_loss: 0.5391\n",
      "Epoch 34/100\n",
      "6090/6090 [==============================] - 1s 142us/sample - loss: 0.5143 - val_loss: 0.5368\n",
      "Epoch 35/100\n",
      "6090/6090 [==============================] - 1s 140us/sample - loss: 0.5113 - val_loss: 0.5346\n",
      "Epoch 36/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.5084 - val_loss: 0.5325\n",
      "Epoch 37/100\n",
      "6090/6090 [==============================] - 1s 154us/sample - loss: 0.5055 - val_loss: 0.5303\n",
      "Epoch 38/100\n",
      "6090/6090 [==============================] - 1s 150us/sample - loss: 0.5027 - val_loss: 0.5283\n",
      "Epoch 39/100\n",
      "6090/6090 [==============================] - 1s 155us/sample - loss: 0.5000 - val_loss: 0.5263\n",
      "Epoch 40/100\n",
      "6090/6090 [==============================] - 1s 187us/sample - loss: 0.4973 - val_loss: 0.5244\n",
      "Epoch 41/100\n",
      "6090/6090 [==============================] - 1s 148us/sample - loss: 0.4946 - val_loss: 0.5225\n",
      "Epoch 42/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.4920 - val_loss: 0.5208\n",
      "Epoch 43/100\n",
      "6090/6090 [==============================] - 1s 154us/sample - loss: 0.4895 - val_loss: 0.5190\n",
      "Epoch 44/100\n",
      "6090/6090 [==============================] - 1s 171us/sample - loss: 0.4871 - val_loss: 0.5173\n",
      "Epoch 45/100\n",
      "6090/6090 [==============================] - 1s 162us/sample - loss: 0.4846 - val_loss: 0.5156\n",
      "Epoch 46/100\n",
      "6090/6090 [==============================] - 1s 165us/sample - loss: 0.4822 - val_loss: 0.5140\n",
      "Epoch 47/100\n",
      "6090/6090 [==============================] - 1s 166us/sample - loss: 0.4799 - val_loss: 0.5124\n",
      "Epoch 48/100\n",
      "6090/6090 [==============================] - 1s 148us/sample - loss: 0.4776 - val_loss: 0.5109\n",
      "Epoch 49/100\n",
      "6090/6090 [==============================] - 1s 151us/sample - loss: 0.4754 - val_loss: 0.5094\n",
      "Epoch 50/100\n",
      "6090/6090 [==============================] - 1s 192us/sample - loss: 0.4732 - val_loss: 0.5079\n",
      "Epoch 51/100\n",
      "6090/6090 [==============================] - 1s 210us/sample - loss: 0.4710 - val_loss: 0.5065\n",
      "Epoch 52/100\n",
      "6090/6090 [==============================] - 1s 149us/sample - loss: 0.4689 - val_loss: 0.5052\n",
      "Epoch 53/100\n",
      "6090/6090 [==============================] - 1s 147us/sample - loss: 0.4668 - val_loss: 0.5038\n",
      "Epoch 54/100\n",
      "6090/6090 [==============================] - 1s 153us/sample - loss: 0.4647 - val_loss: 0.5025\n",
      "Epoch 55/100\n",
      "6090/6090 [==============================] - 1s 148us/sample - loss: 0.4628 - val_loss: 0.5012\n",
      "Epoch 56/100\n",
      "6090/6090 [==============================] - 1s 149us/sample - loss: 0.4608 - val_loss: 0.5000\n",
      "Epoch 57/100\n",
      "6090/6090 [==============================] - 1s 184us/sample - loss: 0.4588 - val_loss: 0.4988\n",
      "Epoch 58/100\n",
      "6090/6090 [==============================] - 1s 147us/sample - loss: 0.4569 - val_loss: 0.4976\n",
      "Epoch 59/100\n",
      "6090/6090 [==============================] - 1s 146us/sample - loss: 0.4550 - val_loss: 0.4964\n",
      "Epoch 60/100\n",
      "6090/6090 [==============================] - 1s 143us/sample - loss: 0.4531 - val_loss: 0.4953\n",
      "Epoch 61/100\n",
      "6090/6090 [==============================] - 1s 148us/sample - loss: 0.4513 - val_loss: 0.4942\n",
      "Epoch 62/100\n",
      "6090/6090 [==============================] - 1s 179us/sample - loss: 0.4495 - val_loss: 0.4932\n",
      "Epoch 63/100\n",
      "6090/6090 [==============================] - 1s 159us/sample - loss: 0.4477 - val_loss: 0.4921\n",
      "Epoch 64/100\n",
      "6090/6090 [==============================] - 1s 145us/sample - loss: 0.4460 - val_loss: 0.4911\n",
      "Epoch 65/100\n",
      "6090/6090 [==============================] - 1s 142us/sample - loss: 0.4443 - val_loss: 0.4901\n",
      "Epoch 66/100\n",
      "6090/6090 [==============================] - 1s 154us/sample - loss: 0.4426 - val_loss: 0.4892\n",
      "Epoch 67/100\n",
      "6090/6090 [==============================] - 1s 147us/sample - loss: 0.4409 - val_loss: 0.4882\n",
      "Epoch 68/100\n",
      "6090/6090 [==============================] - 1s 147us/sample - loss: 0.4393 - val_loss: 0.4873\n",
      "Epoch 69/100\n",
      "6090/6090 [==============================] - 1s 146us/sample - loss: 0.4376 - val_loss: 0.4864\n",
      "Epoch 70/100\n",
      "6090/6090 [==============================] - 1s 148us/sample - loss: 0.4360 - val_loss: 0.4855\n",
      "Epoch 71/100\n",
      "6090/6090 [==============================] - 1s 144us/sample - loss: 0.4344 - val_loss: 0.4847\n",
      "Epoch 72/100\n",
      "6090/6090 [==============================] - 1s 142us/sample - loss: 0.4329 - val_loss: 0.4838\n",
      "Epoch 73/100\n",
      "6090/6090 [==============================] - 1s 144us/sample - loss: 0.4314 - val_loss: 0.4830\n",
      "Epoch 74/100\n",
      "6090/6090 [==============================] - 1s 143us/sample - loss: 0.4299 - val_loss: 0.4822\n",
      "Epoch 75/100\n",
      "6090/6090 [==============================] - 1s 140us/sample - loss: 0.4284 - val_loss: 0.4814\n",
      "Epoch 76/100\n",
      "6090/6090 [==============================] - 1s 145us/sample - loss: 0.4269 - val_loss: 0.4807\n",
      "Epoch 77/100\n",
      "6090/6090 [==============================] - 1s 151us/sample - loss: 0.4254 - val_loss: 0.4799\n",
      "Epoch 78/100\n",
      "6090/6090 [==============================] - 1s 150us/sample - loss: 0.4240 - val_loss: 0.4792\n",
      "Epoch 79/100\n",
      "6090/6090 [==============================] - 1s 145us/sample - loss: 0.4226 - val_loss: 0.4785\n",
      "Epoch 80/100\n",
      "6090/6090 [==============================] - 1s 141us/sample - loss: 0.4212 - val_loss: 0.4778\n",
      "Epoch 81/100\n",
      "6090/6090 [==============================] - 1s 145us/sample - loss: 0.4198 - val_loss: 0.4771\n",
      "Epoch 82/100\n",
      "6090/6090 [==============================] - 1s 149us/sample - loss: 0.4184 - val_loss: 0.4765\n",
      "Epoch 83/100\n",
      "6090/6090 [==============================] - 1s 149us/sample - loss: 0.4171 - val_loss: 0.4758\n",
      "Epoch 84/100\n",
      "6090/6090 [==============================] - 1s 142us/sample - loss: 0.4157 - val_loss: 0.4752\n",
      "Epoch 85/100\n",
      "6090/6090 [==============================] - 1s 177us/sample - loss: 0.4144 - val_loss: 0.4745\n",
      "Epoch 86/100\n",
      "6090/6090 [==============================] - 1s 167us/sample - loss: 0.4131 - val_loss: 0.4739\n",
      "Epoch 87/100\n",
      "6090/6090 [==============================] - 1s 167us/sample - loss: 0.4118 - val_loss: 0.4733\n",
      "Epoch 88/100\n",
      "6090/6090 [==============================] - 1s 185us/sample - loss: 0.4105 - val_loss: 0.4727\n",
      "Epoch 89/100\n",
      "6090/6090 [==============================] - 1s 150us/sample - loss: 0.4092 - val_loss: 0.4721\n",
      "Epoch 90/100\n",
      "6090/6090 [==============================] - 1s 143us/sample - loss: 0.4080 - val_loss: 0.4716\n",
      "Epoch 91/100\n",
      "6090/6090 [==============================] - 1s 146us/sample - loss: 0.4068 - val_loss: 0.4710\n",
      "Epoch 92/100\n",
      "6090/6090 [==============================] - 1s 150us/sample - loss: 0.4055 - val_loss: 0.4705\n",
      "Epoch 93/100\n",
      "6090/6090 [==============================] - 1s 151us/sample - loss: 0.4043 - val_loss: 0.4700\n",
      "Epoch 94/100\n",
      "6090/6090 [==============================] - 1s 153us/sample - loss: 0.4031 - val_loss: 0.4695\n",
      "Epoch 95/100\n",
      "6090/6090 [==============================] - 1s 149us/sample - loss: 0.4019 - val_loss: 0.4689\n",
      "Epoch 96/100\n",
      "6090/6090 [==============================] - 1s 155us/sample - loss: 0.4008 - val_loss: 0.4685\n",
      "Epoch 97/100\n",
      "6090/6090 [==============================] - 1s 144us/sample - loss: 0.3996 - val_loss: 0.4680\n",
      "Epoch 98/100\n",
      "6090/6090 [==============================] - 1s 153us/sample - loss: 0.3985 - val_loss: 0.4675\n",
      "Epoch 99/100\n",
      "6090/6090 [==============================] - 1s 149us/sample - loss: 0.3973 - val_loss: 0.4670\n",
      "Epoch 100/100\n",
      "6090/6090 [==============================] - 1s 147us/sample - loss: 0.3962 - val_loss: 0.4666\n",
      "Predicting...\n",
      "[[789  93]\n",
      " [217 424]]\n",
      "Accuracy: 79.65%\n",
      "Precision: 78.43%\n",
      "Recall: 89.46%\n",
      "F: 83.58\n",
      "(Took 94.116 sec)\n",
      "No prep, TF-IDF\n",
      "Training...\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/100\n",
      "6090/6090 [==============================] - 2s 261us/sample - loss: 0.6834 - val_loss: 0.6728\n",
      "Epoch 2/100\n",
      "6090/6090 [==============================] - 1s 171us/sample - loss: 0.6646 - val_loss: 0.6582\n",
      "Epoch 3/100\n",
      "6090/6090 [==============================] - 1s 172us/sample - loss: 0.6491 - val_loss: 0.6458\n",
      "Epoch 4/100\n",
      "6090/6090 [==============================] - 1s 167us/sample - loss: 0.6352 - val_loss: 0.6348\n",
      "Epoch 5/100\n",
      "6090/6090 [==============================] - 1s 167us/sample - loss: 0.6223 - val_loss: 0.6248\n",
      "Epoch 6/100\n",
      "6090/6090 [==============================] - 1s 210us/sample - loss: 0.6102 - val_loss: 0.6154\n",
      "Epoch 7/100\n",
      "6090/6090 [==============================] - 1s 186us/sample - loss: 0.5986 - val_loss: 0.6064\n",
      "Epoch 8/100\n",
      "6090/6090 [==============================] - 1s 180us/sample - loss: 0.5876 - val_loss: 0.5982\n",
      "Epoch 9/100\n",
      "6090/6090 [==============================] - 1s 162us/sample - loss: 0.5771 - val_loss: 0.5904\n",
      "Epoch 10/100\n",
      "6090/6090 [==============================] - 1s 184us/sample - loss: 0.5671 - val_loss: 0.5830\n",
      "Epoch 11/100\n",
      "6090/6090 [==============================] - 1s 189us/sample - loss: 0.5576 - val_loss: 0.5759\n",
      "Epoch 12/100\n",
      "6090/6090 [==============================] - 1s 161us/sample - loss: 0.5485 - val_loss: 0.5693\n",
      "Epoch 13/100\n",
      "6090/6090 [==============================] - 1s 144us/sample - loss: 0.5398 - val_loss: 0.5630\n",
      "Epoch 14/100\n",
      "6090/6090 [==============================] - 1s 152us/sample - loss: 0.5316 - val_loss: 0.5571\n",
      "Epoch 15/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.5236 - val_loss: 0.5515\n",
      "Epoch 16/100\n",
      "6090/6090 [==============================] - 1s 154us/sample - loss: 0.5161 - val_loss: 0.5461\n",
      "Epoch 17/100\n",
      "6090/6090 [==============================] - 1s 160us/sample - loss: 0.5088 - val_loss: 0.5411\n",
      "Epoch 18/100\n",
      "6090/6090 [==============================] - 1s 168us/sample - loss: 0.5018 - val_loss: 0.5362\n",
      "Epoch 19/100\n",
      "6090/6090 [==============================] - 1s 180us/sample - loss: 0.4951 - val_loss: 0.5317\n",
      "Epoch 20/100\n",
      "6090/6090 [==============================] - 1s 177us/sample - loss: 0.4887 - val_loss: 0.5274\n",
      "Epoch 21/100\n",
      "6090/6090 [==============================] - 1s 172us/sample - loss: 0.4825 - val_loss: 0.5233\n",
      "Epoch 22/100\n",
      "6090/6090 [==============================] - 1s 155us/sample - loss: 0.4765 - val_loss: 0.5194\n",
      "Epoch 23/100\n",
      "6090/6090 [==============================] - 1s 185us/sample - loss: 0.4708 - val_loss: 0.5157\n",
      "Epoch 24/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6090/6090 [==============================] - 1s 153us/sample - loss: 0.4653 - val_loss: 0.5123\n",
      "Epoch 25/100\n",
      "6090/6090 [==============================] - 1s 173us/sample - loss: 0.4600 - val_loss: 0.5089\n",
      "Epoch 26/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.4549 - val_loss: 0.5058\n",
      "Epoch 27/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.4500 - val_loss: 0.5028\n",
      "Epoch 28/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.4452 - val_loss: 0.5000\n",
      "Epoch 29/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.4406 - val_loss: 0.4973\n",
      "Epoch 30/100\n",
      "6090/6090 [==============================] - 1s 157us/sample - loss: 0.4361 - val_loss: 0.4947\n",
      "Epoch 31/100\n",
      "6090/6090 [==============================] - 1s 148us/sample - loss: 0.4318 - val_loss: 0.4923\n",
      "Epoch 32/100\n",
      "6090/6090 [==============================] - 1s 151us/sample - loss: 0.4275 - val_loss: 0.4900\n",
      "Epoch 33/100\n",
      "6090/6090 [==============================] - 1s 171us/sample - loss: 0.4235 - val_loss: 0.4879\n",
      "Epoch 34/100\n",
      "6090/6090 [==============================] - 1s 180us/sample - loss: 0.4196 - val_loss: 0.4858\n",
      "Epoch 35/100\n",
      "6090/6090 [==============================] - 1s 140us/sample - loss: 0.4157 - val_loss: 0.4838\n",
      "Epoch 36/100\n",
      "6090/6090 [==============================] - 1s 145us/sample - loss: 0.4120 - val_loss: 0.4819\n",
      "Epoch 37/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.4084 - val_loss: 0.4802\n",
      "Epoch 38/100\n",
      "6090/6090 [==============================] - 1s 135us/sample - loss: 0.4049 - val_loss: 0.4784\n",
      "Epoch 39/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.4015 - val_loss: 0.4768\n",
      "Epoch 40/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.3982 - val_loss: 0.4753\n",
      "Epoch 41/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.3949 - val_loss: 0.4738\n",
      "Epoch 42/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.3918 - val_loss: 0.4724\n",
      "Epoch 43/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.3887 - val_loss: 0.4711\n",
      "Epoch 44/100\n",
      "6090/6090 [==============================] - 1s 135us/sample - loss: 0.3857 - val_loss: 0.4699\n",
      "Epoch 45/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.3828 - val_loss: 0.4687\n",
      "Epoch 46/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.3800 - val_loss: 0.4676\n",
      "Epoch 47/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.3772 - val_loss: 0.4666\n",
      "Epoch 48/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.3745 - val_loss: 0.4656\n",
      "Epoch 49/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.3718 - val_loss: 0.4646\n",
      "Epoch 50/100\n",
      "6090/6090 [==============================] - 1s 136us/sample - loss: 0.3692 - val_loss: 0.4638\n",
      "Epoch 51/100\n",
      "6090/6090 [==============================] - 1s 140us/sample - loss: 0.3667 - val_loss: 0.4629\n",
      "Epoch 52/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.3642 - val_loss: 0.4621\n",
      "Epoch 53/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.3618 - val_loss: 0.4614\n",
      "Epoch 54/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.3594 - val_loss: 0.4606\n",
      "Epoch 55/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.3570 - val_loss: 0.4599\n",
      "Epoch 56/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.3548 - val_loss: 0.4593\n",
      "Epoch 57/100\n",
      "6090/6090 [==============================] - 1s 135us/sample - loss: 0.3525 - val_loss: 0.4588\n",
      "Epoch 58/100\n",
      "6090/6090 [==============================] - 1s 140us/sample - loss: 0.3503 - val_loss: 0.4582\n",
      "Epoch 59/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.3482 - val_loss: 0.4577\n",
      "Epoch 60/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.3461 - val_loss: 0.4572\n",
      "Epoch 61/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.3440 - val_loss: 0.4567\n",
      "Epoch 62/100\n",
      "6090/6090 [==============================] - 1s 140us/sample - loss: 0.3420 - val_loss: 0.4563\n",
      "Epoch 63/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.3399 - val_loss: 0.4559\n",
      "Epoch 64/100\n",
      "6090/6090 [==============================] - 1s 135us/sample - loss: 0.3380 - val_loss: 0.4555\n",
      "Epoch 65/100\n",
      "6090/6090 [==============================] - 1s 135us/sample - loss: 0.3361 - val_loss: 0.4552\n",
      "Epoch 66/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.3342 - val_loss: 0.4549\n",
      "Epoch 67/100\n",
      "6090/6090 [==============================] - 1s 140us/sample - loss: 0.3323 - val_loss: 0.4546\n",
      "Epoch 68/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.3305 - val_loss: 0.4543\n",
      "Epoch 69/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.3287 - val_loss: 0.4541\n",
      "Epoch 70/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.3269 - val_loss: 0.4538\n",
      "Epoch 71/100\n",
      "6090/6090 [==============================] - 1s 140us/sample - loss: 0.3252 - val_loss: 0.4537\n",
      "Epoch 72/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.3235 - val_loss: 0.4535\n",
      "Epoch 73/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.3218 - val_loss: 0.4534\n",
      "Epoch 74/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.3201 - val_loss: 0.4532\n",
      "Epoch 75/100\n",
      "6090/6090 [==============================] - 1s 140us/sample - loss: 0.3185 - val_loss: 0.4531\n",
      "Epoch 76/100\n",
      "6090/6090 [==============================] - 1s 136us/sample - loss: 0.3169 - val_loss: 0.4530\n",
      "Epoch 77/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.3153 - val_loss: 0.4529\n",
      "Epoch 78/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.3137 - val_loss: 0.4528\n",
      "Epoch 79/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.3122 - val_loss: 0.4528\n",
      "Epoch 80/100\n",
      "6090/6090 [==============================] - 1s 142us/sample - loss: 0.3107 - val_loss: 0.4527\n",
      "Epoch 81/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.3092 - val_loss: 0.4527\n",
      "Epoch 82/100\n",
      "6090/6090 [==============================] - 1s 134us/sample - loss: 0.3077 - val_loss: 0.4527\n",
      "Predicting...\n",
      "[[789  93]\n",
      " [209 432]]\n",
      "Accuracy: 80.17%\n",
      "Precision: 79.06%\n",
      "Recall: 89.46%\n",
      "F: 83.94\n",
      "(Took 76.731 sec)\n",
      "Preprocessed, raw counts\n",
      "Training...\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/100\n",
      "6090/6090 [==============================] - 1s 155us/sample - loss: 0.6715 - val_loss: 0.6502\n",
      "Epoch 2/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.6239 - val_loss: 0.6148\n",
      "Epoch 3/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.5864 - val_loss: 0.5870\n",
      "Epoch 4/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.5563 - val_loss: 0.5649\n",
      "Epoch 5/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.5314 - val_loss: 0.5469\n",
      "Epoch 6/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.5106 - val_loss: 0.5324\n",
      "Epoch 7/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.4930 - val_loss: 0.5202\n",
      "Epoch 8/100\n",
      "6090/6090 [==============================] - 1s 116us/sample - loss: 0.4778 - val_loss: 0.5103\n",
      "Epoch 9/100\n",
      "6090/6090 [==============================] - 1s 126us/sample - loss: 0.4645 - val_loss: 0.5017\n",
      "Epoch 10/100\n",
      "6090/6090 [==============================] - 1s 124us/sample - loss: 0.4528 - val_loss: 0.4945\n",
      "Epoch 11/100\n",
      "6090/6090 [==============================] - 1s 118us/sample - loss: 0.4423 - val_loss: 0.4883\n",
      "Epoch 12/100\n",
      "6090/6090 [==============================] - 1s 117us/sample - loss: 0.4329 - val_loss: 0.4829\n",
      "Epoch 13/100\n",
      "6090/6090 [==============================] - 1s 132us/sample - loss: 0.4243 - val_loss: 0.4784\n",
      "Epoch 14/100\n",
      "6090/6090 [==============================] - 1s 123us/sample - loss: 0.4165 - val_loss: 0.4744\n",
      "Epoch 15/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4094 - val_loss: 0.4709\n",
      "Epoch 16/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4028 - val_loss: 0.4681\n",
      "Epoch 17/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.3966 - val_loss: 0.4656\n",
      "Epoch 18/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.3909 - val_loss: 0.4635\n",
      "Epoch 19/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.3855 - val_loss: 0.4617\n",
      "Epoch 20/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.3805 - val_loss: 0.4601\n",
      "Epoch 21/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.3758 - val_loss: 0.4587\n",
      "Epoch 22/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.3713 - val_loss: 0.4577\n",
      "Epoch 23/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.3671 - val_loss: 0.4567\n",
      "Epoch 24/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.3631 - val_loss: 0.4560\n",
      "Epoch 25/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.3593 - val_loss: 0.4554\n",
      "Epoch 26/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.3556 - val_loss: 0.4549\n",
      "Epoch 27/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.3521 - val_loss: 0.4546\n",
      "Epoch 28/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.3488 - val_loss: 0.4544\n",
      "Epoch 29/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.3455 - val_loss: 0.4544\n",
      "Epoch 30/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.3425 - val_loss: 0.4544\n",
      "Predicting...\n",
      "[[780 102]\n",
      " [207 434]]\n",
      "Accuracy: 79.71%\n",
      "Precision: 79.03%\n",
      "Recall: 88.44%\n",
      "F: 83.47\n",
      "(Took 21.801 sec)\n",
      "Preprocessed, normalized\n",
      "Training...\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/100\n",
      "6090/6090 [==============================] - 1s 151us/sample - loss: 0.6866 - val_loss: 0.6799\n",
      "Epoch 2/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.6746 - val_loss: 0.6699\n",
      "Epoch 3/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.6649 - val_loss: 0.6621\n",
      "Epoch 4/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.6565 - val_loss: 0.6552\n",
      "Epoch 5/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.6486 - val_loss: 0.6488\n",
      "Epoch 6/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.6411 - val_loss: 0.6428\n",
      "Epoch 7/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.6339 - val_loss: 0.6370\n",
      "Epoch 8/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.6269 - val_loss: 0.6316\n",
      "Epoch 9/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.6201 - val_loss: 0.6263\n",
      "Epoch 10/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.6135 - val_loss: 0.6212\n",
      "Epoch 11/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.6071 - val_loss: 0.6162\n",
      "Epoch 12/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.6008 - val_loss: 0.6114\n",
      "Epoch 13/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.5948 - val_loss: 0.6068\n",
      "Epoch 14/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.5889 - val_loss: 0.6024\n",
      "Epoch 15/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.5831 - val_loss: 0.5980\n",
      "Epoch 16/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.5775 - val_loss: 0.5938\n",
      "Epoch 17/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.5721 - val_loss: 0.5898\n",
      "Epoch 18/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.5668 - val_loss: 0.5858\n",
      "Epoch 19/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.5616 - val_loss: 0.5821\n",
      "Epoch 20/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.5566 - val_loss: 0.5784\n",
      "Epoch 21/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.5517 - val_loss: 0.5749\n",
      "Epoch 22/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.5470 - val_loss: 0.5715\n",
      "Epoch 23/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.5423 - val_loss: 0.5681\n",
      "Epoch 24/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.5378 - val_loss: 0.5650\n",
      "Epoch 25/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.5334 - val_loss: 0.5619\n",
      "Epoch 26/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.5291 - val_loss: 0.5588\n",
      "Epoch 27/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.5249 - val_loss: 0.5560\n",
      "Epoch 28/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.5208 - val_loss: 0.5532\n",
      "Epoch 29/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.5168 - val_loss: 0.5505\n",
      "Epoch 30/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.5129 - val_loss: 0.5479\n",
      "Epoch 31/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.5091 - val_loss: 0.5454\n",
      "Epoch 32/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.5054 - val_loss: 0.5429\n",
      "Epoch 33/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.5019 - val_loss: 0.5405\n",
      "Epoch 34/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4983 - val_loss: 0.5383\n",
      "Epoch 35/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4948 - val_loss: 0.5361\n",
      "Epoch 36/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.4915 - val_loss: 0.5340\n",
      "Epoch 37/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.4882 - val_loss: 0.5319\n",
      "Epoch 38/100\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.4850 - val_loss: 0.5299\n",
      "Epoch 39/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.4820 - val_loss: 0.5280\n",
      "Epoch 40/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.4789 - val_loss: 0.5262\n",
      "Epoch 41/100\n",
      "6090/6090 [==============================] - 1s 118us/sample - loss: 0.4759 - val_loss: 0.5244\n",
      "Epoch 42/100\n",
      "6090/6090 [==============================] - 1s 116us/sample - loss: 0.4730 - val_loss: 0.5227\n",
      "Epoch 43/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.4702 - val_loss: 0.5210\n",
      "Epoch 44/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.4674 - val_loss: 0.5194\n",
      "Epoch 45/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.4647 - val_loss: 0.5179\n",
      "Epoch 46/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.4620 - val_loss: 0.5164\n",
      "Epoch 47/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.4594 - val_loss: 0.5150\n",
      "Epoch 48/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.4569 - val_loss: 0.5136\n",
      "Epoch 49/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.4544 - val_loss: 0.5122\n",
      "Epoch 50/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.4520 - val_loss: 0.5109\n",
      "Epoch 51/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.4496 - val_loss: 0.5097\n",
      "Epoch 52/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.4473 - val_loss: 0.5086\n",
      "Epoch 53/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4450 - val_loss: 0.5074\n",
      "Epoch 54/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.4427 - val_loss: 0.5063\n",
      "Epoch 55/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4406 - val_loss: 0.5052\n",
      "Epoch 56/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4384 - val_loss: 0.5042\n",
      "Epoch 57/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4363 - val_loss: 0.5032\n",
      "Epoch 58/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.4343 - val_loss: 0.5023\n",
      "Epoch 59/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.4323 - val_loss: 0.5013\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.4303 - val_loss: 0.5004\n",
      "Epoch 61/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4284 - val_loss: 0.4996\n",
      "Epoch 62/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.4264 - val_loss: 0.4988\n",
      "Epoch 63/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4246 - val_loss: 0.4980\n",
      "Epoch 64/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4228 - val_loss: 0.4973\n",
      "Epoch 65/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4210 - val_loss: 0.4965\n",
      "Epoch 66/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4192 - val_loss: 0.4958\n",
      "Epoch 67/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.4175 - val_loss: 0.4952\n",
      "Epoch 68/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4158 - val_loss: 0.4945\n",
      "Epoch 69/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4141 - val_loss: 0.4939\n",
      "Epoch 70/100\n",
      "6090/6090 [==============================] - 1s 123us/sample - loss: 0.4125 - val_loss: 0.4933\n",
      "Epoch 71/100\n",
      "6090/6090 [==============================] - 1s 122us/sample - loss: 0.4109 - val_loss: 0.4927\n",
      "Epoch 72/100\n",
      "6090/6090 [==============================] - 1s 119us/sample - loss: 0.4093 - val_loss: 0.4922\n",
      "Epoch 73/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.4078 - val_loss: 0.4917\n",
      "Epoch 74/100\n",
      "6090/6090 [==============================] - 1s 116us/sample - loss: 0.4062 - val_loss: 0.4912\n",
      "Epoch 75/100\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.4048 - val_loss: 0.4907\n",
      "Epoch 76/100\n",
      "6090/6090 [==============================] - 1s 116us/sample - loss: 0.4033 - val_loss: 0.4902\n",
      "Epoch 77/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4018 - val_loss: 0.4898\n",
      "Epoch 78/100\n",
      "6090/6090 [==============================] - 1s 116us/sample - loss: 0.4004 - val_loss: 0.4894\n",
      "Epoch 79/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.3990 - val_loss: 0.4890\n",
      "Epoch 80/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.3976 - val_loss: 0.4887\n",
      "Epoch 81/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.3963 - val_loss: 0.4883\n",
      "Epoch 82/100\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.3950 - val_loss: 0.4880\n",
      "Epoch 83/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.3936 - val_loss: 0.4877\n",
      "Epoch 84/100\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.3924 - val_loss: 0.4874\n",
      "Epoch 85/100\n",
      "6090/6090 [==============================] - 1s 120us/sample - loss: 0.3911 - val_loss: 0.4871\n",
      "Epoch 86/100\n",
      "6090/6090 [==============================] - 1s 151us/sample - loss: 0.3898 - val_loss: 0.4868\n",
      "Epoch 87/100\n",
      "6090/6090 [==============================] - 1s 164us/sample - loss: 0.3886 - val_loss: 0.4866\n",
      "Epoch 88/100\n",
      "6090/6090 [==============================] - 1s 117us/sample - loss: 0.3874 - val_loss: 0.4863\n",
      "Epoch 89/100\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.3862 - val_loss: 0.4861\n",
      "Epoch 90/100\n",
      "6090/6090 [==============================] - 1s 126us/sample - loss: 0.3850 - val_loss: 0.4859\n",
      "Epoch 91/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.3839 - val_loss: 0.4857\n",
      "Epoch 92/100\n",
      "6090/6090 [==============================] - 1s 121us/sample - loss: 0.3827 - val_loss: 0.4856\n",
      "Epoch 93/100\n",
      "6090/6090 [==============================] - 1s 120us/sample - loss: 0.3816 - val_loss: 0.4854\n",
      "Epoch 94/100\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.3805 - val_loss: 0.4852\n",
      "Epoch 95/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.3794 - val_loss: 0.4851\n",
      "Epoch 96/100\n",
      "6090/6090 [==============================] - 1s 133us/sample - loss: 0.3783 - val_loss: 0.4849\n",
      "Epoch 97/100\n",
      "6090/6090 [==============================] - 1s 144us/sample - loss: 0.3772 - val_loss: 0.4848\n",
      "Epoch 98/100\n",
      "6090/6090 [==============================] - 1s 126us/sample - loss: 0.3762 - val_loss: 0.4847\n",
      "Epoch 99/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.3752 - val_loss: 0.4845\n",
      "Epoch 100/100\n",
      "6090/6090 [==============================] - 1s 123us/sample - loss: 0.3741 - val_loss: 0.4844\n",
      "Predicting...\n",
      "[[761 121]\n",
      " [204 437]]\n",
      "Accuracy: 78.66%\n",
      "Precision: 78.86%\n",
      "Recall: 86.28%\n",
      "F: 82.40\n",
      "(Took 73.108 sec)\n",
      "Preprocessed, TF-IDF\n",
      "Training...\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/100\n",
      "6090/6090 [==============================] - 1s 222us/sample - loss: 0.6829 - val_loss: 0.6714\n",
      "Epoch 2/100\n",
      "6090/6090 [==============================] - 1s 122us/sample - loss: 0.6601 - val_loss: 0.6540\n",
      "Epoch 3/100\n",
      "6090/6090 [==============================] - 1s 121us/sample - loss: 0.6408 - val_loss: 0.6392\n",
      "Epoch 4/100\n",
      "6090/6090 [==============================] - 1s 122us/sample - loss: 0.6235 - val_loss: 0.6262\n",
      "Epoch 5/100\n",
      "6090/6090 [==============================] - 1s 121us/sample - loss: 0.6077 - val_loss: 0.6145\n",
      "Epoch 6/100\n",
      "6090/6090 [==============================] - 1s 116us/sample - loss: 0.5930 - val_loss: 0.6038\n",
      "Epoch 7/100\n",
      "6090/6090 [==============================] - 1s 131us/sample - loss: 0.5791 - val_loss: 0.5938\n",
      "Epoch 8/100\n",
      "6090/6090 [==============================] - 1s 135us/sample - loss: 0.5662 - val_loss: 0.5847\n",
      "Epoch 9/100\n",
      "6090/6090 [==============================] - 1s 143us/sample - loss: 0.5541 - val_loss: 0.5763\n",
      "Epoch 10/100\n",
      "6090/6090 [==============================] - 1s 125us/sample - loss: 0.5428 - val_loss: 0.5685\n",
      "Epoch 11/100\n",
      "6090/6090 [==============================] - 1s 141us/sample - loss: 0.5320 - val_loss: 0.5613\n",
      "Epoch 12/100\n",
      "6090/6090 [==============================] - 1s 127us/sample - loss: 0.5219 - val_loss: 0.5546\n",
      "Epoch 13/100\n",
      "6090/6090 [==============================] - 1s 124us/sample - loss: 0.5124 - val_loss: 0.5486\n",
      "Epoch 14/100\n",
      "6090/6090 [==============================] - 1s 130us/sample - loss: 0.5035 - val_loss: 0.5428\n",
      "Epoch 15/100\n",
      "6090/6090 [==============================] - 1s 133us/sample - loss: 0.4950 - val_loss: 0.5375\n",
      "Epoch 16/100\n",
      "6090/6090 [==============================] - 1s 135us/sample - loss: 0.4869 - val_loss: 0.5327\n",
      "Epoch 17/100\n",
      "6090/6090 [==============================] - 1s 129us/sample - loss: 0.4794 - val_loss: 0.5282\n",
      "Epoch 18/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.4722 - val_loss: 0.5242\n",
      "Epoch 19/100\n",
      "6090/6090 [==============================] - 1s 126us/sample - loss: 0.4653 - val_loss: 0.5203\n",
      "Epoch 20/100\n",
      "6090/6090 [==============================] - 1s 159us/sample - loss: 0.4588 - val_loss: 0.5168\n",
      "Epoch 21/100\n",
      "6090/6090 [==============================] - 1s 160us/sample - loss: 0.4526 - val_loss: 0.5136\n",
      "Epoch 22/100\n",
      "6090/6090 [==============================] - 1s 136us/sample - loss: 0.4467 - val_loss: 0.5106\n",
      "Epoch 23/100\n",
      "6090/6090 [==============================] - 1s 120us/sample - loss: 0.4412 - val_loss: 0.5078\n",
      "Epoch 24/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.4358 - val_loss: 0.5053\n",
      "Epoch 25/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.4307 - val_loss: 0.5030\n",
      "Epoch 26/100\n",
      "6090/6090 [==============================] - 1s 118us/sample - loss: 0.4258 - val_loss: 0.5009\n",
      "Epoch 27/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4211 - val_loss: 0.4990\n",
      "Epoch 28/100\n",
      "6090/6090 [==============================] - 1s 117us/sample - loss: 0.4166 - val_loss: 0.4971\n",
      "Epoch 29/100\n",
      "6090/6090 [==============================] - 1s 116us/sample - loss: 0.4123 - val_loss: 0.4955\n",
      "Epoch 30/100\n",
      "6090/6090 [==============================] - 1s 124us/sample - loss: 0.4082 - val_loss: 0.4941\n",
      "Epoch 31/100\n",
      "6090/6090 [==============================] - 1s 116us/sample - loss: 0.4042 - val_loss: 0.4927\n",
      "Epoch 32/100\n",
      "6090/6090 [==============================] - 1s 119us/sample - loss: 0.4004 - val_loss: 0.4916\n",
      "Epoch 33/100\n",
      "6090/6090 [==============================] - 1s 121us/sample - loss: 0.3968 - val_loss: 0.4905\n",
      "Epoch 34/100\n",
      "6090/6090 [==============================] - 1s 118us/sample - loss: 0.3933 - val_loss: 0.4897\n",
      "Epoch 35/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.3899 - val_loss: 0.4888\n",
      "Epoch 36/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.3866 - val_loss: 0.4881\n",
      "Epoch 37/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.3835 - val_loss: 0.4874\n",
      "Epoch 38/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.3804 - val_loss: 0.4869\n",
      "Epoch 39/100\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.3775 - val_loss: 0.4865\n",
      "Epoch 40/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.3747 - val_loss: 0.4861\n",
      "Epoch 41/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.3719 - val_loss: 0.4858\n",
      "Epoch 42/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.3693 - val_loss: 0.4855\n",
      "Epoch 43/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.3667 - val_loss: 0.4853\n",
      "Epoch 44/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.3642 - val_loss: 0.4852\n",
      "Epoch 45/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.3617 - val_loss: 0.4852\n",
      "Epoch 46/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.3594 - val_loss: 0.4852\n",
      "Predicting...\n",
      "[[753 129]\n",
      " [206 435]]\n",
      "Accuracy: 78.00%\n",
      "Precision: 78.52%\n",
      "Recall: 85.37%\n",
      "F: 81.80\n",
      "(Took 35.979 sec)\n",
      "Done with Logistic models\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# No preprocessing\n",
    "# Raw counts\n",
    "print('No prep, raw counts')\n",
    "run_logistic(dftrain_minimal_noprep, 0.2, 100)\n",
    "# Normalized\n",
    "print('No prep, normalized')\n",
    "run_logistic(dftrain_min_norm_noprep, 0.2, 100)\n",
    "# TF-IDF\n",
    "print('No prep, TF-IDF')\n",
    "run_logistic(dftrain_min_tfidf_noprep, 0.2, 100)\n",
    "# With preprocessing\n",
    "# Raw counts\n",
    "print('Preprocessed, raw counts')\n",
    "run_logistic(dftrain_minimal, 0.2, 100)\n",
    "# Normalized\n",
    "print('Preprocessed, normalized')\n",
    "run_logistic(dftrain_min_norm, 0.2, 100)\n",
    "# TF-IDF\n",
    "print('Preprocessed, TF-IDF')\n",
    "run_logistic(dftrain_min_tfidf, 0.2, 100)\n",
    "print('Done with Logistic models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Train on 7232 samples, validate on 381 samples\n",
      "Epoch 1/100\n",
      "7232/7232 [==============================] - 1s 127us/sample - loss: 0.6381 - val_loss: 0.5928\n",
      "Epoch 2/100\n",
      "7232/7232 [==============================] - 1s 98us/sample - loss: 0.5646 - val_loss: 0.5442\n",
      "Epoch 3/100\n",
      "7232/7232 [==============================] - 1s 96us/sample - loss: 0.5202 - val_loss: 0.5143\n",
      "Epoch 4/100\n",
      "7232/7232 [==============================] - 1s 95us/sample - loss: 0.4888 - val_loss: 0.4937\n",
      "Epoch 5/100\n",
      "7232/7232 [==============================] - 1s 98us/sample - loss: 0.4644 - val_loss: 0.4787\n",
      "Epoch 6/100\n",
      "7232/7232 [==============================] - 1s 97us/sample - loss: 0.4444 - val_loss: 0.4676\n",
      "Epoch 7/100\n",
      "7232/7232 [==============================] - 1s 98us/sample - loss: 0.4277 - val_loss: 0.4585\n",
      "Epoch 8/100\n",
      "7232/7232 [==============================] - 1s 98us/sample - loss: 0.4131 - val_loss: 0.4516\n",
      "Epoch 9/100\n",
      "7232/7232 [==============================] - 1s 97us/sample - loss: 0.4003 - val_loss: 0.4452\n",
      "Epoch 10/100\n",
      "7232/7232 [==============================] - 1s 101us/sample - loss: 0.3888 - val_loss: 0.4400\n",
      "Epoch 11/100\n",
      "7232/7232 [==============================] - 1s 98us/sample - loss: 0.3785 - val_loss: 0.4363\n",
      "Epoch 12/100\n",
      "7232/7232 [==============================] - 1s 104us/sample - loss: 0.3690 - val_loss: 0.4326\n",
      "Epoch 13/100\n",
      "7232/7232 [==============================] - 1s 94us/sample - loss: 0.3603 - val_loss: 0.4297\n",
      "Epoch 14/100\n",
      "7232/7232 [==============================] - 1s 105us/sample - loss: 0.3525 - val_loss: 0.4270\n",
      "Epoch 15/100\n",
      "7232/7232 [==============================] - 1s 95us/sample - loss: 0.3450 - val_loss: 0.4245\n",
      "Epoch 16/100\n",
      "7232/7232 [==============================] - 1s 97us/sample - loss: 0.3380 - val_loss: 0.4226\n",
      "Epoch 17/100\n",
      "7232/7232 [==============================] - 1s 97us/sample - loss: 0.3316 - val_loss: 0.4210\n",
      "Epoch 18/100\n",
      "7232/7232 [==============================] - 1s 100us/sample - loss: 0.3254 - val_loss: 0.4194\n",
      "Epoch 19/100\n",
      "7232/7232 [==============================] - 1s 103us/sample - loss: 0.3196 - val_loss: 0.4179\n",
      "Epoch 20/100\n",
      "7232/7232 [==============================] - 1s 103us/sample - loss: 0.3141 - val_loss: 0.4168\n",
      "Epoch 21/100\n",
      "7232/7232 [==============================] - 1s 103us/sample - loss: 0.3089 - val_loss: 0.4161\n",
      "Epoch 22/100\n",
      "7232/7232 [==============================] - 1s 94us/sample - loss: 0.3039 - val_loss: 0.4153\n",
      "Epoch 23/100\n",
      "7232/7232 [==============================] - 1s 93us/sample - loss: 0.2993 - val_loss: 0.4140\n",
      "Epoch 24/100\n",
      "7232/7232 [==============================] - 1s 94us/sample - loss: 0.2947 - val_loss: 0.4139\n",
      "Epoch 25/100\n",
      "7232/7232 [==============================] - 1s 100us/sample - loss: 0.2903 - val_loss: 0.4130\n",
      "Epoch 26/100\n",
      "7232/7232 [==============================] - 1s 95us/sample - loss: 0.2862 - val_loss: 0.4128\n",
      "Epoch 27/100\n",
      "7232/7232 [==============================] - 1s 97us/sample - loss: 0.2822 - val_loss: 0.4129\n",
      "Predicting...\n",
      "[[201  18]\n",
      " [ 48 114]]\n",
      "Accuracy: 82.68%\n",
      "Precision: 80.72%\n",
      "Recall: 91.78%\n",
      "F: 85.90\n",
      "(Took 20.732 sec)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test to see feature importances\n",
    "nn_test = run_logistic(dftrain_minimal_noprep, 0.05, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x1152 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Regularization may not help that much\n",
    "weights = nn_test.get_weights()[0]\n",
    "\n",
    "features = {dftrain_minimal_noprep.columns[i]:weights[i][0] for i in range(len(weights))}\n",
    "top_features = sorted(features.items(), key=lambda x: x[1], reverse=True)[0:100]\n",
    "topfeatures = pd.DataFrame(columns = ['feature', 'importance'])\n",
    "topfeatures['feature'] = [top_features[i][0] for i in range(100)]\n",
    "topfeatures['importance'] = [top_features[i][1] for i in range(100)]\n",
    "\n",
    "plt.figure(figsize=(16,16))\n",
    "plt.barh([i for i in range(20)], topfeatures['importance'][0:20], align='center', alpha=0.9)\n",
    "plt.yticks([i for i in range(20)], topfeatures['feature'][0:20], fontsize=16)\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Top feature importances')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8810530469586786"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train again and make an ROC curve. Use 20% training data to get a better estimate\n",
    "np.random.seed(1)\n",
    "_, test = train_test_split(dftrain_minimal_noprep, test_size=0.05)\n",
    "test_x = np.array(test.drop('_target', axis=1))\n",
    "test_y = np.array(test['_target'])\n",
    "preds = nn_test.predict(test_x)\n",
    "fpr, tpr, _ = roc_curve(y_true=test_y, y_score=preds)\n",
    "plt.figure(1)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC curve')\n",
    "auc = roc_auc_score(y_true=test_y, y_score=preds)\n",
    "auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/100\n",
      "6090/6090 [==============================] - 3s 488us/sample - loss: 0.6500 - val_loss: 0.6065\n",
      "Epoch 2/100\n",
      "6090/6090 [==============================] - 1s 159us/sample - loss: 0.5783 - val_loss: 0.5613\n",
      "Epoch 3/100\n",
      "6090/6090 [==============================] - 1s 155us/sample - loss: 0.5338 - val_loss: 0.5327\n",
      "Epoch 4/100\n",
      "6090/6090 [==============================] - 1s 159us/sample - loss: 0.5017 - val_loss: 0.5133\n",
      "Epoch 5/100\n",
      "6090/6090 [==============================] - 1s 159us/sample - loss: 0.4768 - val_loss: 0.4986\n",
      "Epoch 6/100\n",
      "6090/6090 [==============================] - 1s 148us/sample - loss: 0.4561 - val_loss: 0.4876\n",
      "Epoch 7/100\n",
      "6090/6090 [==============================] - 1s 142us/sample - loss: 0.4386 - val_loss: 0.4788\n",
      "Epoch 8/100\n",
      "6090/6090 [==============================] - 1s 152us/sample - loss: 0.4233 - val_loss: 0.4718\n",
      "Epoch 9/100\n",
      "6090/6090 [==============================] - 1s 132us/sample - loss: 0.4098 - val_loss: 0.4659\n",
      "Epoch 10/100\n",
      "6090/6090 [==============================] - 1s 136us/sample - loss: 0.3977 - val_loss: 0.4613\n",
      "Epoch 11/100\n",
      "6090/6090 [==============================] - 1s 152us/sample - loss: 0.3868 - val_loss: 0.4574\n",
      "Epoch 12/100\n",
      "6090/6090 [==============================] - 1s 150us/sample - loss: 0.3768 - val_loss: 0.4540\n",
      "Epoch 13/100\n",
      "6090/6090 [==============================] - 1s 150us/sample - loss: 0.3675 - val_loss: 0.4513\n",
      "Epoch 14/100\n",
      "6090/6090 [==============================] - 1s 153us/sample - loss: 0.3589 - val_loss: 0.4491\n",
      "Epoch 15/100\n",
      "6090/6090 [==============================] - 1s 132us/sample - loss: 0.3511 - val_loss: 0.4471\n",
      "Epoch 16/100\n",
      "6090/6090 [==============================] - 1s 141us/sample - loss: 0.3437 - val_loss: 0.4456\n",
      "Epoch 17/100\n",
      "6090/6090 [==============================] - 1s 143us/sample - loss: 0.3366 - val_loss: 0.4443\n",
      "Epoch 18/100\n",
      "6090/6090 [==============================] - 1s 153us/sample - loss: 0.3301 - val_loss: 0.4434\n",
      "Epoch 19/100\n",
      "6090/6090 [==============================] - 1s 147us/sample - loss: 0.3238 - val_loss: 0.4426\n",
      "Epoch 20/100\n",
      "6090/6090 [==============================] - 1s 167us/sample - loss: 0.3180 - val_loss: 0.4420\n",
      "Epoch 21/100\n",
      "6090/6090 [==============================] - 1s 147us/sample - loss: 0.3124 - val_loss: 0.4417\n",
      "Epoch 22/100\n",
      "6090/6090 [==============================] - 1s 144us/sample - loss: 0.3070 - val_loss: 0.4413\n",
      "Epoch 23/100\n",
      "6090/6090 [==============================] - 1s 161us/sample - loss: 0.3019 - val_loss: 0.4414\n",
      "Predicting...\n",
      "(Took 26.054 sec)\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/100\n",
      "6090/6090 [==============================] - 1s 208us/sample - loss: 0.6470 - val_loss: 0.6094\n",
      "Epoch 2/100\n",
      "6090/6090 [==============================] - 1s 166us/sample - loss: 0.5754 - val_loss: 0.5686\n",
      "Epoch 3/100\n",
      "6090/6090 [==============================] - 1s 153us/sample - loss: 0.5310 - val_loss: 0.5437\n",
      "Epoch 4/100\n",
      "6090/6090 [==============================] - 1s 158us/sample - loss: 0.4988 - val_loss: 0.5267\n",
      "Epoch 5/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.4737 - val_loss: 0.5140\n",
      "Epoch 6/100\n",
      "6090/6090 [==============================] - 1s 157us/sample - loss: 0.4533 - val_loss: 0.5036\n",
      "Epoch 7/100\n",
      "6090/6090 [==============================] - 1s 156us/sample - loss: 0.4358 - val_loss: 0.4957\n",
      "Epoch 8/100\n",
      "6090/6090 [==============================] - 1s 154us/sample - loss: 0.4206 - val_loss: 0.4893\n",
      "Epoch 9/100\n",
      "6090/6090 [==============================] - 1s 161us/sample - loss: 0.4071 - val_loss: 0.4835\n",
      "Epoch 10/100\n",
      "6090/6090 [==============================] - 1s 157us/sample - loss: 0.3951 - val_loss: 0.4792\n",
      "Epoch 11/100\n",
      "6090/6090 [==============================] - 1s 154us/sample - loss: 0.3844 - val_loss: 0.4755\n",
      "Epoch 12/100\n",
      "6090/6090 [==============================] - 1s 153us/sample - loss: 0.3745 - val_loss: 0.4722\n",
      "Epoch 13/100\n",
      "6090/6090 [==============================] - 1s 148us/sample - loss: 0.3654 - val_loss: 0.4699\n",
      "Epoch 14/100\n",
      "6090/6090 [==============================] - 1s 145us/sample - loss: 0.3570 - val_loss: 0.4677\n",
      "Epoch 15/100\n",
      "6090/6090 [==============================] - 1s 143us/sample - loss: 0.3492 - val_loss: 0.4661\n",
      "Epoch 16/100\n",
      "6090/6090 [==============================] - 1s 147us/sample - loss: 0.3418 - val_loss: 0.4646\n",
      "Epoch 17/100\n",
      "6090/6090 [==============================] - 1s 150us/sample - loss: 0.3349 - val_loss: 0.4634\n",
      "Epoch 18/100\n",
      "6090/6090 [==============================] - 1s 144us/sample - loss: 0.3284 - val_loss: 0.4629\n",
      "Epoch 19/100\n",
      "6090/6090 [==============================] - 1s 152us/sample - loss: 0.3221 - val_loss: 0.4619\n",
      "Epoch 20/100\n",
      "6090/6090 [==============================] - 1s 146us/sample - loss: 0.3165 - val_loss: 0.4616\n",
      "Epoch 21/100\n",
      "6090/6090 [==============================] - 1s 140us/sample - loss: 0.3108 - val_loss: 0.4614\n",
      "Epoch 22/100\n",
      "6090/6090 [==============================] - 1s 148us/sample - loss: 0.3056 - val_loss: 0.4618\n",
      "Predicting...\n",
      "(Took 21.571 sec)\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/100\n",
      "6090/6090 [==============================] - 1s 191us/sample - loss: 0.6437 - val_loss: 0.6130\n",
      "Epoch 2/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.5740 - val_loss: 0.5715\n",
      "Epoch 3/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.5301 - val_loss: 0.5458\n",
      "Epoch 4/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.4982 - val_loss: 0.5273\n",
      "Epoch 5/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.4734 - val_loss: 0.5136\n",
      "Epoch 6/100\n",
      "6090/6090 [==============================] - 1s 146us/sample - loss: 0.4529 - val_loss: 0.5028\n",
      "Epoch 7/100\n",
      "6090/6090 [==============================] - 1s 147us/sample - loss: 0.4356 - val_loss: 0.4947\n",
      "Epoch 8/100\n",
      "6090/6090 [==============================] - 1s 146us/sample - loss: 0.4205 - val_loss: 0.4879\n",
      "Epoch 9/100\n",
      "6090/6090 [==============================] - 1s 144us/sample - loss: 0.4073 - val_loss: 0.4822\n",
      "Epoch 10/100\n",
      "6090/6090 [==============================] - 1s 145us/sample - loss: 0.3954 - val_loss: 0.4776\n",
      "Epoch 11/100\n",
      "6090/6090 [==============================] - 1s 143us/sample - loss: 0.3845 - val_loss: 0.4740\n",
      "Epoch 12/100\n",
      "6090/6090 [==============================] - 1s 140us/sample - loss: 0.3746 - val_loss: 0.4708\n",
      "Epoch 13/100\n",
      "6090/6090 [==============================] - 1s 141us/sample - loss: 0.3654 - val_loss: 0.4682\n",
      "Epoch 14/100\n",
      "6090/6090 [==============================] - 1s 140us/sample - loss: 0.3569 - val_loss: 0.4658\n",
      "Epoch 15/100\n",
      "6090/6090 [==============================] - 1s 141us/sample - loss: 0.3490 - val_loss: 0.4638\n",
      "Epoch 16/100\n",
      "6090/6090 [==============================] - 1s 141us/sample - loss: 0.3417 - val_loss: 0.4626\n",
      "Epoch 17/100\n",
      "6090/6090 [==============================] - 1s 141us/sample - loss: 0.3347 - val_loss: 0.4616\n",
      "Epoch 18/100\n",
      "6090/6090 [==============================] - 1s 146us/sample - loss: 0.3281 - val_loss: 0.4611\n",
      "Epoch 19/100\n",
      "6090/6090 [==============================] - 1s 147us/sample - loss: 0.3219 - val_loss: 0.4601\n",
      "Epoch 20/100\n",
      "6090/6090 [==============================] - 1s 146us/sample - loss: 0.3160 - val_loss: 0.4602\n",
      "Predicting...\n",
      "(Took 18.485 sec)\n",
      "Train on 6091 samples, validate on 1522 samples\n",
      "Epoch 1/100\n",
      "6091/6091 [==============================] - 1s 198us/sample - loss: 0.6496 - val_loss: 0.6124\n",
      "Epoch 2/100\n",
      "6091/6091 [==============================] - 1s 165us/sample - loss: 0.5771 - val_loss: 0.5689\n",
      "Epoch 3/100\n",
      "6091/6091 [==============================] - 1s 168us/sample - loss: 0.5328 - val_loss: 0.5408\n",
      "Epoch 4/100\n",
      "6091/6091 [==============================] - 1s 163us/sample - loss: 0.5009 - val_loss: 0.5214\n",
      "Epoch 5/100\n",
      "6091/6091 [==============================] - 1s 162us/sample - loss: 0.4760 - val_loss: 0.5068\n",
      "Epoch 6/100\n",
      "6091/6091 [==============================] - 1s 164us/sample - loss: 0.4554 - val_loss: 0.4954\n",
      "Epoch 7/100\n",
      "6091/6091 [==============================] - 1s 163us/sample - loss: 0.4380 - val_loss: 0.4860\n",
      "Epoch 8/100\n",
      "6091/6091 [==============================] - 1s 137us/sample - loss: 0.4228 - val_loss: 0.4787\n",
      "Epoch 9/100\n",
      "6091/6091 [==============================] - 1s 137us/sample - loss: 0.4091 - val_loss: 0.4725\n",
      "Epoch 10/100\n",
      "6091/6091 [==============================] - 1s 135us/sample - loss: 0.3970 - val_loss: 0.4673\n",
      "Epoch 11/100\n",
      "6091/6091 [==============================] - 1s 137us/sample - loss: 0.3861 - val_loss: 0.4629\n",
      "Epoch 12/100\n",
      "6091/6091 [==============================] - 1s 151us/sample - loss: 0.3760 - val_loss: 0.4596\n",
      "Epoch 13/100\n",
      "6091/6091 [==============================] - 1s 137us/sample - loss: 0.3667 - val_loss: 0.4563\n",
      "Epoch 14/100\n",
      "6091/6091 [==============================] - 1s 133us/sample - loss: 0.3582 - val_loss: 0.4540\n",
      "Epoch 15/100\n",
      "6091/6091 [==============================] - 1s 139us/sample - loss: 0.3502 - val_loss: 0.4519\n",
      "Epoch 16/100\n",
      "6091/6091 [==============================] - 1s 146us/sample - loss: 0.3427 - val_loss: 0.4500\n",
      "Epoch 17/100\n",
      "6091/6091 [==============================] - 1s 140us/sample - loss: 0.3357 - val_loss: 0.4488\n",
      "Epoch 18/100\n",
      "6091/6091 [==============================] - 1s 137us/sample - loss: 0.3290 - val_loss: 0.4479\n",
      "Epoch 19/100\n",
      "6091/6091 [==============================] - 1s 137us/sample - loss: 0.3228 - val_loss: 0.4473\n",
      "Epoch 20/100\n",
      "6091/6091 [==============================] - 1s 137us/sample - loss: 0.3168 - val_loss: 0.4465\n",
      "Epoch 21/100\n",
      "6091/6091 [==============================] - 1s 137us/sample - loss: 0.3112 - val_loss: 0.4464\n",
      "Epoch 22/100\n",
      "6091/6091 [==============================] - 1s 137us/sample - loss: 0.3057 - val_loss: 0.4460\n",
      "Epoch 23/100\n",
      "6091/6091 [==============================] - 1s 139us/sample - loss: 0.3005 - val_loss: 0.4461\n",
      "Predicting...\n",
      "(Took 21.526 sec)\n",
      "Train on 6091 samples, validate on 1522 samples\n",
      "Epoch 1/100\n",
      "6091/6091 [==============================] - 1s 193us/sample - loss: 0.6443 - val_loss: 0.6106\n",
      "Epoch 2/100\n",
      "6091/6091 [==============================] - 1s 138us/sample - loss: 0.5748 - val_loss: 0.5697\n",
      "Epoch 3/100\n",
      "6091/6091 [==============================] - 1s 138us/sample - loss: 0.5313 - val_loss: 0.5430\n",
      "Epoch 4/100\n",
      "6091/6091 [==============================] - 1s 137us/sample - loss: 0.4997 - val_loss: 0.5248\n",
      "Epoch 5/100\n",
      "6091/6091 [==============================] - 1s 136us/sample - loss: 0.4747 - val_loss: 0.5098\n",
      "Epoch 6/100\n",
      "6091/6091 [==============================] - 1s 134us/sample - loss: 0.4541 - val_loss: 0.4990\n",
      "Epoch 7/100\n",
      "6091/6091 [==============================] - 1s 135us/sample - loss: 0.4367 - val_loss: 0.4902\n",
      "Epoch 8/100\n",
      "6091/6091 [==============================] - 1s 137us/sample - loss: 0.4214 - val_loss: 0.4828\n",
      "Epoch 9/100\n",
      "6091/6091 [==============================] - 1s 135us/sample - loss: 0.4079 - val_loss: 0.4769\n",
      "Epoch 10/100\n",
      "6091/6091 [==============================] - 1s 137us/sample - loss: 0.3958 - val_loss: 0.4721\n",
      "Epoch 11/100\n",
      "6091/6091 [==============================] - 1s 139us/sample - loss: 0.3848 - val_loss: 0.4679\n",
      "Epoch 12/100\n",
      "6091/6091 [==============================] - 1s 137us/sample - loss: 0.3748 - val_loss: 0.4643\n",
      "Epoch 13/100\n",
      "6091/6091 [==============================] - 1s 136us/sample - loss: 0.3656 - val_loss: 0.4614\n",
      "Epoch 14/100\n",
      "6091/6091 [==============================] - 1s 134us/sample - loss: 0.3570 - val_loss: 0.4589\n",
      "Epoch 15/100\n",
      "6091/6091 [==============================] - 1s 138us/sample - loss: 0.3490 - val_loss: 0.4572\n",
      "Epoch 16/100\n",
      "6091/6091 [==============================] - 1s 148us/sample - loss: 0.3417 - val_loss: 0.4553\n",
      "Epoch 17/100\n",
      "6091/6091 [==============================] - 1s 139us/sample - loss: 0.3346 - val_loss: 0.4541\n",
      "Epoch 18/100\n",
      "6091/6091 [==============================] - 1s 138us/sample - loss: 0.3281 - val_loss: 0.4529\n",
      "Epoch 19/100\n",
      "6091/6091 [==============================] - 1s 137us/sample - loss: 0.3219 - val_loss: 0.4520\n",
      "Epoch 20/100\n",
      "6091/6091 [==============================] - 1s 139us/sample - loss: 0.3160 - val_loss: 0.4510\n",
      "Epoch 21/100\n",
      "6091/6091 [==============================] - 1s 136us/sample - loss: 0.3104 - val_loss: 0.4507\n",
      "Epoch 22/100\n",
      "6091/6091 [==============================] - 1s 136us/sample - loss: 0.3050 - val_loss: 0.4505\n",
      "Epoch 23/100\n",
      "6091/6091 [==============================] - 1s 137us/sample - loss: 0.2999 - val_loss: 0.4499\n",
      "Epoch 24/100\n",
      "6091/6091 [==============================] - 1s 137us/sample - loss: 0.2950 - val_loss: 0.4500\n",
      "Predicting...\n",
      "(Took 21.269 sec)\n",
      "Combined confusion matrix:\n",
      "[[3831.  511.]\n",
      " [ 961. 2310.]]\n",
      "(Overall, took 109.351 sec)\n",
      "Accuracy: 80.66% +/- 0.54%\n",
      "Precision for positive class: 79.95% +/- 1.50%\n",
      "Precision for negative class: 81.89% +/- 2.03%\n",
      "Recall for positive class: 88.24% +/- 1.27%\n",
      "Recall for negative class: 70.62% +/- 2.17%\n",
      "F for positive class: 83.88% +/- 0.51%\n",
      "F for negative class: 75.82% +/- 1.20%\n",
      "Mean F score: 79.85% +/- 0.66%\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/100\n",
      "6090/6090 [==============================] - 1s 203us/sample - loss: 0.6863 - val_loss: 0.6784\n",
      "Epoch 2/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.6743 - val_loss: 0.6682\n",
      "Epoch 3/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.6650 - val_loss: 0.6602\n",
      "Epoch 4/100\n",
      "6090/6090 [==============================] - 1s 136us/sample - loss: 0.6570 - val_loss: 0.6532\n",
      "Epoch 5/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.6496 - val_loss: 0.6468\n",
      "Epoch 6/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.6425 - val_loss: 0.6406\n",
      "Epoch 7/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.6358 - val_loss: 0.6349\n",
      "Epoch 8/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.6293 - val_loss: 0.6293\n",
      "Epoch 9/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.6230 - val_loss: 0.6240\n",
      "Epoch 10/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.6169 - val_loss: 0.6188\n",
      "Epoch 11/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.6110 - val_loss: 0.6139\n",
      "Epoch 12/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.6053 - val_loss: 0.6090\n",
      "Epoch 13/100\n",
      "6090/6090 [==============================] - 1s 140us/sample - loss: 0.5998 - val_loss: 0.6045\n",
      "Epoch 14/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.5944 - val_loss: 0.6001\n",
      "Epoch 15/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.5892 - val_loss: 0.5958\n",
      "Epoch 16/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.5842 - val_loss: 0.5916\n",
      "Epoch 17/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.5793 - val_loss: 0.5877\n",
      "Epoch 18/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.5746 - val_loss: 0.5838\n",
      "Epoch 19/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.5700 - val_loss: 0.5800\n",
      "Epoch 20/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.5655 - val_loss: 0.5764\n",
      "Epoch 21/100\n",
      "6090/6090 [==============================] - 1s 143us/sample - loss: 0.5612 - val_loss: 0.5729\n",
      "Epoch 22/100\n",
      "6090/6090 [==============================] - 1s 134us/sample - loss: 0.5570 - val_loss: 0.5696\n",
      "Epoch 23/100\n",
      "6090/6090 [==============================] - 1s 135us/sample - loss: 0.5529 - val_loss: 0.5663\n",
      "Epoch 24/100\n",
      "6090/6090 [==============================] - 1s 134us/sample - loss: 0.5489 - val_loss: 0.5632\n",
      "Epoch 25/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.5450 - val_loss: 0.5602\n",
      "Epoch 26/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.5412 - val_loss: 0.5572\n",
      "Epoch 27/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.5376 - val_loss: 0.5543\n",
      "Epoch 28/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.5340 - val_loss: 0.5516\n",
      "Epoch 29/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.5305 - val_loss: 0.5489\n",
      "Epoch 30/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.5271 - val_loss: 0.5463\n",
      "Epoch 31/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.5238 - val_loss: 0.5438\n",
      "Epoch 32/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.5205 - val_loss: 0.5414\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.5174 - val_loss: 0.5390\n",
      "Epoch 34/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.5143 - val_loss: 0.5367\n",
      "Epoch 35/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.5113 - val_loss: 0.5344\n",
      "Epoch 36/100\n",
      "6090/6090 [==============================] - 1s 136us/sample - loss: 0.5083 - val_loss: 0.5322\n",
      "Epoch 37/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.5054 - val_loss: 0.5301\n",
      "Epoch 38/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.5026 - val_loss: 0.5281\n",
      "Epoch 39/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.4999 - val_loss: 0.5261\n",
      "Epoch 40/100\n",
      "6090/6090 [==============================] - 1s 136us/sample - loss: 0.4972 - val_loss: 0.5242\n",
      "Epoch 41/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.4945 - val_loss: 0.5224\n",
      "Epoch 42/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.4919 - val_loss: 0.5205\n",
      "Epoch 43/100\n",
      "6090/6090 [==============================] - 1s 136us/sample - loss: 0.4894 - val_loss: 0.5188\n",
      "Epoch 44/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.4869 - val_loss: 0.5171\n",
      "Epoch 45/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.4845 - val_loss: 0.5154\n",
      "Epoch 46/100\n",
      "6090/6090 [==============================] - 1s 141us/sample - loss: 0.4821 - val_loss: 0.5138\n",
      "Epoch 47/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.4798 - val_loss: 0.5122\n",
      "Epoch 48/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.4775 - val_loss: 0.5107\n",
      "Epoch 49/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.4753 - val_loss: 0.5092\n",
      "Epoch 50/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.4731 - val_loss: 0.5077\n",
      "Epoch 51/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.4709 - val_loss: 0.5063\n",
      "Epoch 52/100\n",
      "6090/6090 [==============================] - 1s 148us/sample - loss: 0.4688 - val_loss: 0.5049\n",
      "Epoch 53/100\n",
      "6090/6090 [==============================] - 1s 152us/sample - loss: 0.4667 - val_loss: 0.5036\n",
      "Epoch 54/100\n",
      "6090/6090 [==============================] - 1s 136us/sample - loss: 0.4646 - val_loss: 0.5023\n",
      "Epoch 55/100\n",
      "6090/6090 [==============================] - 1s 133us/sample - loss: 0.4626 - val_loss: 0.5010\n",
      "Epoch 56/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.4607 - val_loss: 0.4997\n",
      "Epoch 57/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.4587 - val_loss: 0.4986\n",
      "Epoch 58/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.4568 - val_loss: 0.4974\n",
      "Epoch 59/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.4549 - val_loss: 0.4962\n",
      "Epoch 60/100\n",
      "6090/6090 [==============================] - 1s 136us/sample - loss: 0.4530 - val_loss: 0.4952\n",
      "Epoch 61/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.4512 - val_loss: 0.4940\n",
      "Epoch 62/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.4495 - val_loss: 0.4929\n",
      "Epoch 63/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.4476 - val_loss: 0.4919\n",
      "Epoch 64/100\n",
      "6090/6090 [==============================] - 1s 142us/sample - loss: 0.4459 - val_loss: 0.4909\n",
      "Epoch 65/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.4442 - val_loss: 0.4899\n",
      "Epoch 66/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.4425 - val_loss: 0.4890\n",
      "Epoch 67/100\n",
      "6090/6090 [==============================] - 1s 142us/sample - loss: 0.4408 - val_loss: 0.4880\n",
      "Epoch 68/100\n",
      "6090/6090 [==============================] - 1s 144us/sample - loss: 0.4391 - val_loss: 0.4871\n",
      "Epoch 69/100\n",
      "6090/6090 [==============================] - 1s 140us/sample - loss: 0.4376 - val_loss: 0.4862\n",
      "Epoch 70/100\n",
      "6090/6090 [==============================] - 1s 140us/sample - loss: 0.4359 - val_loss: 0.4853\n",
      "Epoch 71/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.4344 - val_loss: 0.4844\n",
      "Epoch 72/100\n",
      "6090/6090 [==============================] - 1s 142us/sample - loss: 0.4328 - val_loss: 0.4836\n",
      "Epoch 73/100\n",
      "6090/6090 [==============================] - 1s 136us/sample - loss: 0.4313 - val_loss: 0.4828\n",
      "Epoch 74/100\n",
      "6090/6090 [==============================] - 1s 135us/sample - loss: 0.4298 - val_loss: 0.4820\n",
      "Epoch 75/100\n",
      "6090/6090 [==============================] - 1s 136us/sample - loss: 0.4283 - val_loss: 0.4812\n",
      "Epoch 76/100\n",
      "6090/6090 [==============================] - 1s 136us/sample - loss: 0.4268 - val_loss: 0.4804\n",
      "Epoch 77/100\n",
      "6090/6090 [==============================] - 1s 145us/sample - loss: 0.4254 - val_loss: 0.4797\n",
      "Epoch 78/100\n",
      "6090/6090 [==============================] - 1s 148us/sample - loss: 0.4239 - val_loss: 0.4790\n",
      "Epoch 79/100\n",
      "6090/6090 [==============================] - 1s 149us/sample - loss: 0.4225 - val_loss: 0.4783\n",
      "Epoch 80/100\n",
      "6090/6090 [==============================] - 1s 153us/sample - loss: 0.4211 - val_loss: 0.4776\n",
      "Epoch 81/100\n",
      "6090/6090 [==============================] - 1s 148us/sample - loss: 0.4197 - val_loss: 0.4769\n",
      "Epoch 82/100\n",
      "6090/6090 [==============================] - 1s 146us/sample - loss: 0.4183 - val_loss: 0.4762\n",
      "Epoch 83/100\n",
      "6090/6090 [==============================] - 1s 135us/sample - loss: 0.4170 - val_loss: 0.4756\n",
      "Epoch 84/100\n",
      "6090/6090 [==============================] - 1s 140us/sample - loss: 0.4156 - val_loss: 0.4749\n",
      "Epoch 85/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.4143 - val_loss: 0.4743\n",
      "Epoch 86/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.4130 - val_loss: 0.4737\n",
      "Epoch 87/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.4117 - val_loss: 0.4731\n",
      "Epoch 88/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.4105 - val_loss: 0.4725\n",
      "Epoch 89/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.4092 - val_loss: 0.4719\n",
      "Epoch 90/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.4079 - val_loss: 0.4714\n",
      "Epoch 91/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.4067 - val_loss: 0.4708\n",
      "Epoch 92/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.4055 - val_loss: 0.4703\n",
      "Epoch 93/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.4043 - val_loss: 0.4698\n",
      "Epoch 94/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.4031 - val_loss: 0.4693\n",
      "Epoch 95/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.4019 - val_loss: 0.4687\n",
      "Epoch 96/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.4007 - val_loss: 0.4683\n",
      "Epoch 97/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.3995 - val_loss: 0.4678\n",
      "Epoch 98/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.3984 - val_loss: 0.4673\n",
      "Epoch 99/100\n",
      "6090/6090 [==============================] - 1s 136us/sample - loss: 0.3973 - val_loss: 0.4669\n",
      "Epoch 100/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.3961 - val_loss: 0.4664\n",
      "Predicting...\n",
      "(Took 85.644 sec)\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/100\n",
      "6090/6090 [==============================] - 1s 180us/sample - loss: 0.6859 - val_loss: 0.6770\n",
      "Epoch 2/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.6741 - val_loss: 0.6670\n",
      "Epoch 3/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.6652 - val_loss: 0.6589\n",
      "Epoch 4/100\n",
      "6090/6090 [==============================] - 1s 140us/sample - loss: 0.6573 - val_loss: 0.6520\n",
      "Epoch 5/100\n",
      "6090/6090 [==============================] - 1s 140us/sample - loss: 0.6500 - val_loss: 0.6456\n",
      "Epoch 6/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.6429 - val_loss: 0.6396\n",
      "Epoch 7/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.6361 - val_loss: 0.6339\n",
      "Epoch 8/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.6296 - val_loss: 0.6285\n",
      "Epoch 9/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.6233 - val_loss: 0.6233\n",
      "Epoch 10/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.6172 - val_loss: 0.6183\n",
      "Epoch 11/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.6112 - val_loss: 0.6135\n",
      "Epoch 12/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.6055 - val_loss: 0.6088\n",
      "Epoch 13/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.6000 - val_loss: 0.6043\n",
      "Epoch 14/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.5946 - val_loss: 0.6000\n",
      "Epoch 15/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.5894 - val_loss: 0.5959\n",
      "Epoch 16/100\n",
      "6090/6090 [==============================] - 1s 136us/sample - loss: 0.5844 - val_loss: 0.5919\n",
      "Epoch 17/100\n",
      "6090/6090 [==============================] - 1s 136us/sample - loss: 0.5795 - val_loss: 0.5880\n",
      "Epoch 18/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.5747 - val_loss: 0.5842\n",
      "Epoch 19/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.5701 - val_loss: 0.5806\n",
      "Epoch 20/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.5656 - val_loss: 0.5772\n",
      "Epoch 21/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.5613 - val_loss: 0.5737\n",
      "Epoch 22/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.5571 - val_loss: 0.5705\n",
      "Epoch 23/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.5529 - val_loss: 0.5673\n",
      "Epoch 24/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.5490 - val_loss: 0.5641\n",
      "Epoch 25/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.5450 - val_loss: 0.5613\n",
      "Epoch 26/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.5412 - val_loss: 0.5583\n",
      "Epoch 27/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.5375 - val_loss: 0.5555\n",
      "Epoch 28/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.5340 - val_loss: 0.5528\n",
      "Epoch 29/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.5304 - val_loss: 0.5503\n",
      "Epoch 30/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.5270 - val_loss: 0.5478\n",
      "Epoch 31/100\n",
      "6090/6090 [==============================] - 1s 140us/sample - loss: 0.5237 - val_loss: 0.5453\n",
      "Epoch 32/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.5204 - val_loss: 0.5429\n",
      "Epoch 33/100\n",
      "6090/6090 [==============================] - 1s 146us/sample - loss: 0.5173 - val_loss: 0.5407\n",
      "Epoch 34/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.5142 - val_loss: 0.5386\n",
      "Epoch 35/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.5111 - val_loss: 0.5363\n",
      "Epoch 36/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.5082 - val_loss: 0.5342\n",
      "Epoch 37/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.5053 - val_loss: 0.5321\n",
      "Epoch 38/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.5025 - val_loss: 0.5301\n",
      "Epoch 39/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.4997 - val_loss: 0.5281\n",
      "Epoch 40/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.4971 - val_loss: 0.5263\n",
      "Epoch 41/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.4944 - val_loss: 0.5245\n",
      "Epoch 42/100\n",
      "6090/6090 [==============================] - 1s 140us/sample - loss: 0.4918 - val_loss: 0.5228\n",
      "Epoch 43/100\n",
      "6090/6090 [==============================] - 1s 140us/sample - loss: 0.4893 - val_loss: 0.5211\n",
      "Epoch 44/100\n",
      "6090/6090 [==============================] - 1s 149us/sample - loss: 0.4868 - val_loss: 0.5194\n",
      "Epoch 45/100\n",
      "6090/6090 [==============================] - 1s 142us/sample - loss: 0.4843 - val_loss: 0.5178\n",
      "Epoch 46/100\n",
      "6090/6090 [==============================] - 1s 144us/sample - loss: 0.4819 - val_loss: 0.5162\n",
      "Epoch 47/100\n",
      "6090/6090 [==============================] - 1s 141us/sample - loss: 0.4796 - val_loss: 0.5146\n",
      "Epoch 48/100\n",
      "6090/6090 [==============================] - 1s 140us/sample - loss: 0.4773 - val_loss: 0.5132\n",
      "Epoch 49/100\n",
      "6090/6090 [==============================] - 1s 136us/sample - loss: 0.4750 - val_loss: 0.5117\n",
      "Epoch 50/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.4728 - val_loss: 0.5103\n",
      "Epoch 51/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.4706 - val_loss: 0.5089\n",
      "Epoch 52/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.4685 - val_loss: 0.5074\n",
      "Epoch 53/100\n",
      "6090/6090 [==============================] - 1s 140us/sample - loss: 0.4664 - val_loss: 0.5062\n",
      "Epoch 54/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.4643 - val_loss: 0.5048\n",
      "Epoch 55/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.4623 - val_loss: 0.5037\n",
      "Epoch 56/100\n",
      "6090/6090 [==============================] - 1s 140us/sample - loss: 0.4603 - val_loss: 0.5024\n",
      "Epoch 57/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.4584 - val_loss: 0.5014\n",
      "Epoch 58/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.4565 - val_loss: 0.5002\n",
      "Epoch 59/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.4546 - val_loss: 0.4990\n",
      "Epoch 60/100\n",
      "6090/6090 [==============================] - 1s 141us/sample - loss: 0.4527 - val_loss: 0.4980\n",
      "Epoch 61/100\n",
      "6090/6090 [==============================] - 1s 140us/sample - loss: 0.4509 - val_loss: 0.4969\n",
      "Epoch 62/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.4491 - val_loss: 0.4959\n",
      "Epoch 63/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.4473 - val_loss: 0.4948\n",
      "Epoch 64/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.4455 - val_loss: 0.4938\n",
      "Epoch 65/100\n",
      "6090/6090 [==============================] - 1s 141us/sample - loss: 0.4438 - val_loss: 0.4928\n",
      "Epoch 66/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.4421 - val_loss: 0.4920\n",
      "Epoch 67/100\n",
      "6090/6090 [==============================] - 1s 140us/sample - loss: 0.4404 - val_loss: 0.4910\n",
      "Epoch 68/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.4388 - val_loss: 0.4901\n",
      "Epoch 69/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.4372 - val_loss: 0.4891\n",
      "Epoch 70/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.4355 - val_loss: 0.4883\n",
      "Epoch 71/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.4340 - val_loss: 0.4874\n",
      "Epoch 72/100\n",
      "6090/6090 [==============================] - 1s 140us/sample - loss: 0.4324 - val_loss: 0.4866\n",
      "Epoch 73/100\n",
      "6090/6090 [==============================] - 1s 141us/sample - loss: 0.4308 - val_loss: 0.4858\n",
      "Epoch 74/100\n",
      "6090/6090 [==============================] - 1s 140us/sample - loss: 0.4294 - val_loss: 0.4850\n",
      "Epoch 75/100\n",
      "6090/6090 [==============================] - 1s 140us/sample - loss: 0.4279 - val_loss: 0.4842\n",
      "Epoch 76/100\n",
      "6090/6090 [==============================] - 1s 141us/sample - loss: 0.4264 - val_loss: 0.4836\n",
      "Epoch 77/100\n",
      "6090/6090 [==============================] - 1s 141us/sample - loss: 0.4249 - val_loss: 0.4828\n",
      "Epoch 78/100\n",
      "6090/6090 [==============================] - 1s 142us/sample - loss: 0.4235 - val_loss: 0.4820\n",
      "Epoch 79/100\n",
      "6090/6090 [==============================] - 1s 141us/sample - loss: 0.4220 - val_loss: 0.4813\n",
      "Epoch 80/100\n",
      "6090/6090 [==============================] - 1s 140us/sample - loss: 0.4206 - val_loss: 0.4807\n",
      "Epoch 81/100\n",
      "6090/6090 [==============================] - 1s 141us/sample - loss: 0.4193 - val_loss: 0.4800\n",
      "Epoch 82/100\n",
      "6090/6090 [==============================] - 1s 140us/sample - loss: 0.4179 - val_loss: 0.4793\n",
      "Epoch 83/100\n",
      "6090/6090 [==============================] - 1s 140us/sample - loss: 0.4165 - val_loss: 0.4786\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.4152 - val_loss: 0.4779\n",
      "Epoch 85/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.4139 - val_loss: 0.4774\n",
      "Epoch 86/100\n",
      "6090/6090 [==============================] - 1s 146us/sample - loss: 0.4126 - val_loss: 0.4767\n",
      "Epoch 87/100\n",
      "6090/6090 [==============================] - 1s 140us/sample - loss: 0.4113 - val_loss: 0.4762\n",
      "Epoch 88/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.4100 - val_loss: 0.4756\n",
      "Epoch 89/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.4087 - val_loss: 0.4749\n",
      "Epoch 90/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.4075 - val_loss: 0.4744\n",
      "Epoch 91/100\n",
      "6090/6090 [==============================] - 1s 140us/sample - loss: 0.4062 - val_loss: 0.4739\n",
      "Epoch 92/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.4050 - val_loss: 0.4734\n",
      "Epoch 93/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.4038 - val_loss: 0.4729\n",
      "Epoch 94/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.4026 - val_loss: 0.4723\n",
      "Epoch 95/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.4014 - val_loss: 0.4718\n",
      "Epoch 96/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.4002 - val_loss: 0.4713\n",
      "Epoch 97/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.3991 - val_loss: 0.4709\n",
      "Epoch 98/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.3980 - val_loss: 0.4704\n",
      "Epoch 99/100\n",
      "6090/6090 [==============================] - 1s 140us/sample - loss: 0.3968 - val_loss: 0.4699\n",
      "Epoch 100/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.3957 - val_loss: 0.4695\n",
      "Predicting...\n",
      "(Took 85.877 sec)\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/100\n",
      "6090/6090 [==============================] - 1s 190us/sample - loss: 0.6853 - val_loss: 0.6808\n",
      "Epoch 2/100\n",
      "6090/6090 [==============================] - 1s 140us/sample - loss: 0.6728 - val_loss: 0.6727\n",
      "Epoch 3/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.6634 - val_loss: 0.6660\n",
      "Epoch 4/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.6553 - val_loss: 0.6598\n",
      "Epoch 5/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.6479 - val_loss: 0.6539\n",
      "Epoch 6/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.6409 - val_loss: 0.6483\n",
      "Epoch 7/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.6341 - val_loss: 0.6428\n",
      "Epoch 8/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.6276 - val_loss: 0.6373\n",
      "Epoch 9/100\n",
      "6090/6090 [==============================] - 1s 140us/sample - loss: 0.6213 - val_loss: 0.6322\n",
      "Epoch 10/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.6152 - val_loss: 0.6271\n",
      "Epoch 11/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.6092 - val_loss: 0.6224\n",
      "Epoch 12/100\n",
      "6090/6090 [==============================] - 1s 140us/sample - loss: 0.6035 - val_loss: 0.6177\n",
      "Epoch 13/100\n",
      "6090/6090 [==============================] - 1s 140us/sample - loss: 0.5980 - val_loss: 0.6134\n",
      "Epoch 14/100\n",
      "6090/6090 [==============================] - 1s 140us/sample - loss: 0.5926 - val_loss: 0.6090\n",
      "Epoch 15/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.5874 - val_loss: 0.6048\n",
      "Epoch 16/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.5824 - val_loss: 0.6008\n",
      "Epoch 17/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.5775 - val_loss: 0.5971\n",
      "Epoch 18/100\n",
      "6090/6090 [==============================] - 1s 140us/sample - loss: 0.5727 - val_loss: 0.5932\n",
      "Epoch 19/100\n",
      "6090/6090 [==============================] - 1s 143us/sample - loss: 0.5681 - val_loss: 0.5896\n",
      "Epoch 20/100\n",
      "6090/6090 [==============================] - 1s 147us/sample - loss: 0.5636 - val_loss: 0.5862\n",
      "Epoch 21/100\n",
      "6090/6090 [==============================] - 1s 145us/sample - loss: 0.5592 - val_loss: 0.5828\n",
      "Epoch 22/100\n",
      "6090/6090 [==============================] - 1s 140us/sample - loss: 0.5550 - val_loss: 0.5796\n",
      "Epoch 23/100\n",
      "6090/6090 [==============================] - 1s 144us/sample - loss: 0.5508 - val_loss: 0.5766\n",
      "Epoch 24/100\n",
      "6090/6090 [==============================] - 1s 150us/sample - loss: 0.5468 - val_loss: 0.5735\n",
      "Epoch 25/100\n",
      "6090/6090 [==============================] - 1s 140us/sample - loss: 0.5429 - val_loss: 0.5706\n",
      "Epoch 26/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.5391 - val_loss: 0.5679\n",
      "Epoch 27/100\n",
      "6090/6090 [==============================] - 1s 140us/sample - loss: 0.5354 - val_loss: 0.5651\n",
      "Epoch 28/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.5318 - val_loss: 0.5626\n",
      "Epoch 29/100\n",
      "6090/6090 [==============================] - 1s 140us/sample - loss: 0.5283 - val_loss: 0.5600\n",
      "Epoch 30/100\n",
      "6090/6090 [==============================] - 1s 142us/sample - loss: 0.5249 - val_loss: 0.5575\n",
      "Epoch 31/100\n",
      "6090/6090 [==============================] - 1s 140us/sample - loss: 0.5215 - val_loss: 0.5552\n",
      "Epoch 32/100\n",
      "6090/6090 [==============================] - 1s 141us/sample - loss: 0.5183 - val_loss: 0.5529\n",
      "Epoch 33/100\n",
      "6090/6090 [==============================] - 1s 141us/sample - loss: 0.5151 - val_loss: 0.5507\n",
      "Epoch 34/100\n",
      "6090/6090 [==============================] - 1s 141us/sample - loss: 0.5120 - val_loss: 0.5485\n",
      "Epoch 35/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.5090 - val_loss: 0.5463\n",
      "Epoch 36/100\n",
      "6090/6090 [==============================] - 1s 140us/sample - loss: 0.5060 - val_loss: 0.5443\n",
      "Epoch 37/100\n",
      "6090/6090 [==============================] - 1s 140us/sample - loss: 0.5031 - val_loss: 0.5424\n",
      "Epoch 38/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.5003 - val_loss: 0.5404\n",
      "Epoch 39/100\n",
      "6090/6090 [==============================] - 1s 141us/sample - loss: 0.4975 - val_loss: 0.5386\n",
      "Epoch 40/100\n",
      "6090/6090 [==============================] - 1s 141us/sample - loss: 0.4948 - val_loss: 0.5367\n",
      "Epoch 41/100\n",
      "6090/6090 [==============================] - 1s 141us/sample - loss: 0.4921 - val_loss: 0.5349\n",
      "Epoch 42/100\n",
      "6090/6090 [==============================] - 1s 141us/sample - loss: 0.4895 - val_loss: 0.5333\n",
      "Epoch 43/100\n",
      "6090/6090 [==============================] - 1s 140us/sample - loss: 0.4870 - val_loss: 0.5316\n",
      "Epoch 44/100\n",
      "6090/6090 [==============================] - 1s 142us/sample - loss: 0.4845 - val_loss: 0.5300\n",
      "Epoch 45/100\n",
      "6090/6090 [==============================] - 1s 140us/sample - loss: 0.4821 - val_loss: 0.5283\n",
      "Epoch 46/100\n",
      "6090/6090 [==============================] - 1s 141us/sample - loss: 0.4797 - val_loss: 0.5269\n",
      "Epoch 47/100\n",
      "6090/6090 [==============================] - 1s 141us/sample - loss: 0.4773 - val_loss: 0.5254\n",
      "Epoch 48/100\n",
      "6090/6090 [==============================] - 1s 143us/sample - loss: 0.4750 - val_loss: 0.5239\n",
      "Epoch 49/100\n",
      "6090/6090 [==============================] - 1s 142us/sample - loss: 0.4728 - val_loss: 0.5224\n",
      "Epoch 50/100\n",
      "6090/6090 [==============================] - 1s 142us/sample - loss: 0.4705 - val_loss: 0.5211\n",
      "Epoch 51/100\n",
      "6090/6090 [==============================] - 1s 140us/sample - loss: 0.4684 - val_loss: 0.5198\n",
      "Epoch 52/100\n",
      "6090/6090 [==============================] - 1s 141us/sample - loss: 0.4662 - val_loss: 0.5186\n",
      "Epoch 53/100\n",
      "6090/6090 [==============================] - 1s 141us/sample - loss: 0.4641 - val_loss: 0.5173\n",
      "Epoch 54/100\n",
      "6090/6090 [==============================] - 1s 141us/sample - loss: 0.4621 - val_loss: 0.5160\n",
      "Epoch 55/100\n",
      "6090/6090 [==============================] - 1s 141us/sample - loss: 0.4600 - val_loss: 0.5148\n",
      "Epoch 56/100\n",
      "6090/6090 [==============================] - 1s 140us/sample - loss: 0.4580 - val_loss: 0.5136\n",
      "Epoch 57/100\n",
      "6090/6090 [==============================] - 1s 141us/sample - loss: 0.4561 - val_loss: 0.5125\n",
      "Epoch 58/100\n",
      "6090/6090 [==============================] - 1s 149us/sample - loss: 0.4542 - val_loss: 0.5113\n",
      "Epoch 59/100\n",
      "6090/6090 [==============================] - 1s 164us/sample - loss: 0.4523 - val_loss: 0.5103\n",
      "Epoch 60/100\n",
      "6090/6090 [==============================] - 1s 161us/sample - loss: 0.4504 - val_loss: 0.5092\n",
      "Epoch 61/100\n",
      "6090/6090 [==============================] - 1s 164us/sample - loss: 0.4486 - val_loss: 0.5082\n",
      "Epoch 62/100\n",
      "6090/6090 [==============================] - 1s 162us/sample - loss: 0.4468 - val_loss: 0.5072\n",
      "Epoch 63/100\n",
      "6090/6090 [==============================] - 1s 162us/sample - loss: 0.4450 - val_loss: 0.5062\n",
      "Epoch 64/100\n",
      "6090/6090 [==============================] - 1s 160us/sample - loss: 0.4433 - val_loss: 0.5053\n",
      "Epoch 65/100\n",
      "6090/6090 [==============================] - 1s 145us/sample - loss: 0.4415 - val_loss: 0.5042\n",
      "Epoch 66/100\n",
      "6090/6090 [==============================] - 1s 140us/sample - loss: 0.4399 - val_loss: 0.5034\n",
      "Epoch 67/100\n",
      "6090/6090 [==============================] - 1s 140us/sample - loss: 0.4382 - val_loss: 0.5025\n",
      "Epoch 68/100\n",
      "6090/6090 [==============================] - 1s 140us/sample - loss: 0.4365 - val_loss: 0.5015\n",
      "Epoch 69/100\n",
      "6090/6090 [==============================] - 1s 143us/sample - loss: 0.4349 - val_loss: 0.5007\n",
      "Epoch 70/100\n",
      "6090/6090 [==============================] - 1s 148us/sample - loss: 0.4333 - val_loss: 0.4998\n",
      "Epoch 71/100\n",
      "6090/6090 [==============================] - 1s 146us/sample - loss: 0.4317 - val_loss: 0.4990\n",
      "Epoch 72/100\n",
      "6090/6090 [==============================] - 1s 172us/sample - loss: 0.4301 - val_loss: 0.4982\n",
      "Epoch 73/100\n",
      "6090/6090 [==============================] - 1s 165us/sample - loss: 0.4286 - val_loss: 0.4974\n",
      "Epoch 74/100\n",
      "6090/6090 [==============================] - 1s 172us/sample - loss: 0.4271 - val_loss: 0.4966\n",
      "Epoch 75/100\n",
      "6090/6090 [==============================] - 1s 173us/sample - loss: 0.4256 - val_loss: 0.4959\n",
      "Epoch 76/100\n",
      "6090/6090 [==============================] - 1s 169us/sample - loss: 0.4241 - val_loss: 0.4952\n",
      "Epoch 77/100\n",
      "6090/6090 [==============================] - 1s 170us/sample - loss: 0.4227 - val_loss: 0.4945\n",
      "Epoch 78/100\n",
      "6090/6090 [==============================] - 1s 168us/sample - loss: 0.4212 - val_loss: 0.4937\n",
      "Epoch 79/100\n",
      "6090/6090 [==============================] - 1s 147us/sample - loss: 0.4198 - val_loss: 0.4931\n",
      "Epoch 80/100\n",
      "6090/6090 [==============================] - 1s 161us/sample - loss: 0.4184 - val_loss: 0.4923\n",
      "Epoch 81/100\n",
      "6090/6090 [==============================] - 1s 156us/sample - loss: 0.4170 - val_loss: 0.4917\n",
      "Epoch 82/100\n",
      "6090/6090 [==============================] - 1s 155us/sample - loss: 0.4157 - val_loss: 0.4910\n",
      "Epoch 83/100\n",
      "6090/6090 [==============================] - 1s 148us/sample - loss: 0.4143 - val_loss: 0.4904\n",
      "Epoch 84/100\n",
      "6090/6090 [==============================] - 1s 145us/sample - loss: 0.4130 - val_loss: 0.4898\n",
      "Epoch 85/100\n",
      "6090/6090 [==============================] - 1s 141us/sample - loss: 0.4116 - val_loss: 0.4891\n",
      "Epoch 86/100\n",
      "6090/6090 [==============================] - 1s 141us/sample - loss: 0.4103 - val_loss: 0.4885\n",
      "Epoch 87/100\n",
      "6090/6090 [==============================] - 1s 145us/sample - loss: 0.4091 - val_loss: 0.4879\n",
      "Epoch 88/100\n",
      "6090/6090 [==============================] - 1s 146us/sample - loss: 0.4078 - val_loss: 0.4873\n",
      "Epoch 89/100\n",
      "6090/6090 [==============================] - 1s 152us/sample - loss: 0.4065 - val_loss: 0.4867\n",
      "Epoch 90/100\n",
      "6090/6090 [==============================] - 1s 141us/sample - loss: 0.4053 - val_loss: 0.4863\n",
      "Epoch 91/100\n",
      "6090/6090 [==============================] - 1s 148us/sample - loss: 0.4041 - val_loss: 0.4858\n",
      "Epoch 92/100\n",
      "6090/6090 [==============================] - 1s 149us/sample - loss: 0.4028 - val_loss: 0.4851\n",
      "Epoch 93/100\n",
      "6090/6090 [==============================] - 1s 154us/sample - loss: 0.4016 - val_loss: 0.4846\n",
      "Epoch 94/100\n",
      "6090/6090 [==============================] - 1s 159us/sample - loss: 0.4004 - val_loss: 0.4841\n",
      "Epoch 95/100\n",
      "6090/6090 [==============================] - 1s 150us/sample - loss: 0.3993 - val_loss: 0.4836\n",
      "Epoch 96/100\n",
      "6090/6090 [==============================] - 1s 152us/sample - loss: 0.3981 - val_loss: 0.4831\n",
      "Epoch 97/100\n",
      "6090/6090 [==============================] - 1s 149us/sample - loss: 0.3969 - val_loss: 0.4825\n",
      "Epoch 98/100\n",
      "6090/6090 [==============================] - 1s 147us/sample - loss: 0.3958 - val_loss: 0.4821\n",
      "Epoch 99/100\n",
      "6090/6090 [==============================] - 1s 143us/sample - loss: 0.3946 - val_loss: 0.4816\n",
      "Epoch 100/100\n",
      "6090/6090 [==============================] - 1s 140us/sample - loss: 0.3935 - val_loss: 0.4812\n",
      "Predicting...\n",
      "(Took 90.130 sec)\n",
      "Train on 6091 samples, validate on 1522 samples\n",
      "Epoch 1/100\n",
      "6091/6091 [==============================] - 1s 184us/sample - loss: 0.6859 - val_loss: 0.6792\n",
      "Epoch 2/100\n",
      "6091/6091 [==============================] - 1s 142us/sample - loss: 0.6736 - val_loss: 0.6697\n",
      "Epoch 3/100\n",
      "6091/6091 [==============================] - 1s 141us/sample - loss: 0.6641 - val_loss: 0.6620\n",
      "Epoch 4/100\n",
      "6091/6091 [==============================] - 1s 141us/sample - loss: 0.6561 - val_loss: 0.6554\n",
      "Epoch 5/100\n",
      "6091/6091 [==============================] - 1s 142us/sample - loss: 0.6487 - val_loss: 0.6493\n",
      "Epoch 6/100\n",
      "6091/6091 [==============================] - 1s 142us/sample - loss: 0.6416 - val_loss: 0.6434\n",
      "Epoch 7/100\n",
      "6091/6091 [==============================] - 1s 142us/sample - loss: 0.6348 - val_loss: 0.6377\n",
      "Epoch 8/100\n",
      "6091/6091 [==============================] - 1s 144us/sample - loss: 0.6282 - val_loss: 0.6324\n",
      "Epoch 9/100\n",
      "6091/6091 [==============================] - 1s 143us/sample - loss: 0.6219 - val_loss: 0.6271\n",
      "Epoch 10/100\n",
      "6091/6091 [==============================] - 1s 143us/sample - loss: 0.6158 - val_loss: 0.6221\n",
      "Epoch 11/100\n",
      "6091/6091 [==============================] - 1s 143us/sample - loss: 0.6099 - val_loss: 0.6173\n",
      "Epoch 12/100\n",
      "6091/6091 [==============================] - 1s 143us/sample - loss: 0.6042 - val_loss: 0.6127\n",
      "Epoch 13/100\n",
      "6091/6091 [==============================] - 1s 144us/sample - loss: 0.5986 - val_loss: 0.6081\n",
      "Epoch 14/100\n",
      "6091/6091 [==============================] - 1s 141us/sample - loss: 0.5933 - val_loss: 0.6039\n",
      "Epoch 15/100\n",
      "6091/6091 [==============================] - 1s 142us/sample - loss: 0.5881 - val_loss: 0.5997\n",
      "Epoch 16/100\n",
      "6091/6091 [==============================] - 1s 146us/sample - loss: 0.5831 - val_loss: 0.5957\n",
      "Epoch 17/100\n",
      "6091/6091 [==============================] - 1s 145us/sample - loss: 0.5782 - val_loss: 0.5918\n",
      "Epoch 18/100\n",
      "6091/6091 [==============================] - 1s 143us/sample - loss: 0.5735 - val_loss: 0.5881\n",
      "Epoch 19/100\n",
      "6091/6091 [==============================] - 1s 146us/sample - loss: 0.5689 - val_loss: 0.5845\n",
      "Epoch 20/100\n",
      "6091/6091 [==============================] - 1s 147us/sample - loss: 0.5644 - val_loss: 0.5810\n",
      "Epoch 21/100\n",
      "6091/6091 [==============================] - 1s 141us/sample - loss: 0.5601 - val_loss: 0.5776\n",
      "Epoch 22/100\n",
      "6091/6091 [==============================] - 1s 146us/sample - loss: 0.5559 - val_loss: 0.5743\n",
      "Epoch 23/100\n",
      "6091/6091 [==============================] - 1s 143us/sample - loss: 0.5518 - val_loss: 0.5711\n",
      "Epoch 24/100\n",
      "6091/6091 [==============================] - 1s 144us/sample - loss: 0.5478 - val_loss: 0.5681\n",
      "Epoch 25/100\n",
      "6091/6091 [==============================] - 1s 143us/sample - loss: 0.5439 - val_loss: 0.5651\n",
      "Epoch 26/100\n",
      "6091/6091 [==============================] - 1s 144us/sample - loss: 0.5402 - val_loss: 0.5623\n",
      "Epoch 27/100\n",
      "6091/6091 [==============================] - 1s 144us/sample - loss: 0.5365 - val_loss: 0.5595\n",
      "Epoch 28/100\n",
      "6091/6091 [==============================] - 1s 144us/sample - loss: 0.5329 - val_loss: 0.5568\n",
      "Epoch 29/100\n",
      "6091/6091 [==============================] - 1s 146us/sample - loss: 0.5295 - val_loss: 0.5542\n",
      "Epoch 30/100\n",
      "6091/6091 [==============================] - 1s 143us/sample - loss: 0.5260 - val_loss: 0.5516\n",
      "Epoch 31/100\n",
      "6091/6091 [==============================] - 1s 143us/sample - loss: 0.5227 - val_loss: 0.5492\n",
      "Epoch 32/100\n",
      "6091/6091 [==============================] - 1s 145us/sample - loss: 0.5195 - val_loss: 0.5468\n",
      "Epoch 33/100\n",
      "6091/6091 [==============================] - 1s 144us/sample - loss: 0.5163 - val_loss: 0.5445\n",
      "Epoch 34/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6091/6091 [==============================] - 1s 144us/sample - loss: 0.5132 - val_loss: 0.5422\n",
      "Epoch 35/100\n",
      "6091/6091 [==============================] - 1s 142us/sample - loss: 0.5103 - val_loss: 0.5400\n",
      "Epoch 36/100\n",
      "6091/6091 [==============================] - 1s 141us/sample - loss: 0.5073 - val_loss: 0.5379\n",
      "Epoch 37/100\n",
      "6091/6091 [==============================] - 1s 142us/sample - loss: 0.5045 - val_loss: 0.5358\n",
      "Epoch 38/100\n",
      "6091/6091 [==============================] - 1s 141us/sample - loss: 0.5017 - val_loss: 0.5338\n",
      "Epoch 39/100\n",
      "6091/6091 [==============================] - 1s 142us/sample - loss: 0.4989 - val_loss: 0.5319\n",
      "Epoch 40/100\n",
      "6091/6091 [==============================] - 1s 144us/sample - loss: 0.4962 - val_loss: 0.5300\n",
      "Epoch 41/100\n",
      "6091/6091 [==============================] - 1s 141us/sample - loss: 0.4936 - val_loss: 0.5282\n",
      "Epoch 42/100\n",
      "6091/6091 [==============================] - 1s 142us/sample - loss: 0.4910 - val_loss: 0.5264\n",
      "Epoch 43/100\n",
      "6091/6091 [==============================] - 1s 142us/sample - loss: 0.4886 - val_loss: 0.5246\n",
      "Epoch 44/100\n",
      "6091/6091 [==============================] - 1s 143us/sample - loss: 0.4861 - val_loss: 0.5229\n",
      "Epoch 45/100\n",
      "6091/6091 [==============================] - 1s 144us/sample - loss: 0.4837 - val_loss: 0.5213\n",
      "Epoch 46/100\n",
      "6091/6091 [==============================] - 1s 143us/sample - loss: 0.4813 - val_loss: 0.5197\n",
      "Epoch 47/100\n",
      "6091/6091 [==============================] - 1s 143us/sample - loss: 0.4790 - val_loss: 0.5181\n",
      "Epoch 48/100\n",
      "6091/6091 [==============================] - 1s 141us/sample - loss: 0.4767 - val_loss: 0.5165\n",
      "Epoch 49/100\n",
      "6091/6091 [==============================] - 1s 141us/sample - loss: 0.4745 - val_loss: 0.5150\n",
      "Epoch 50/100\n",
      "6091/6091 [==============================] - 1s 141us/sample - loss: 0.4723 - val_loss: 0.5137\n",
      "Epoch 51/100\n",
      "6091/6091 [==============================] - 1s 140us/sample - loss: 0.4702 - val_loss: 0.5122\n",
      "Epoch 52/100\n",
      "6091/6091 [==============================] - 1s 142us/sample - loss: 0.4681 - val_loss: 0.5108\n",
      "Epoch 53/100\n",
      "6091/6091 [==============================] - 1s 142us/sample - loss: 0.4660 - val_loss: 0.5095\n",
      "Epoch 54/100\n",
      "6091/6091 [==============================] - 1s 142us/sample - loss: 0.4639 - val_loss: 0.5081\n",
      "Epoch 55/100\n",
      "6091/6091 [==============================] - 1s 142us/sample - loss: 0.4619 - val_loss: 0.5068\n",
      "Epoch 56/100\n",
      "6091/6091 [==============================] - 1s 143us/sample - loss: 0.4599 - val_loss: 0.5056\n",
      "Epoch 57/100\n",
      "6091/6091 [==============================] - 1s 143us/sample - loss: 0.4580 - val_loss: 0.5044\n",
      "Epoch 58/100\n",
      "6091/6091 [==============================] - 1s 143us/sample - loss: 0.4561 - val_loss: 0.5032\n",
      "Epoch 59/100\n",
      "6091/6091 [==============================] - 1s 143us/sample - loss: 0.4542 - val_loss: 0.5020\n",
      "Epoch 60/100\n",
      "6091/6091 [==============================] - 1s 142us/sample - loss: 0.4524 - val_loss: 0.5008\n",
      "Epoch 61/100\n",
      "6091/6091 [==============================] - 1s 143us/sample - loss: 0.4506 - val_loss: 0.4997\n",
      "Epoch 62/100\n",
      "6091/6091 [==============================] - 1s 142us/sample - loss: 0.4488 - val_loss: 0.4986\n",
      "Epoch 63/100\n",
      "6091/6091 [==============================] - 1s 143us/sample - loss: 0.4470 - val_loss: 0.4975\n",
      "Epoch 64/100\n",
      "6091/6091 [==============================] - 1s 144us/sample - loss: 0.4453 - val_loss: 0.4965\n",
      "Epoch 65/100\n",
      "6091/6091 [==============================] - 1s 142us/sample - loss: 0.4436 - val_loss: 0.4954\n",
      "Epoch 66/100\n",
      "6091/6091 [==============================] - 1s 147us/sample - loss: 0.4419 - val_loss: 0.4944\n",
      "Epoch 67/100\n",
      "6091/6091 [==============================] - 1s 158us/sample - loss: 0.4402 - val_loss: 0.4934\n",
      "Epoch 68/100\n",
      "6091/6091 [==============================] - 1s 149us/sample - loss: 0.4386 - val_loss: 0.4925\n",
      "Epoch 69/100\n",
      "6091/6091 [==============================] - 1s 146us/sample - loss: 0.4370 - val_loss: 0.4915\n",
      "Epoch 70/100\n",
      "6091/6091 [==============================] - 1s 147us/sample - loss: 0.4354 - val_loss: 0.4905\n",
      "Epoch 71/100\n",
      "6091/6091 [==============================] - 1s 145us/sample - loss: 0.4338 - val_loss: 0.4897\n",
      "Epoch 72/100\n",
      "6091/6091 [==============================] - 1s 143us/sample - loss: 0.4323 - val_loss: 0.4887\n",
      "Epoch 73/100\n",
      "6091/6091 [==============================] - 1s 144us/sample - loss: 0.4307 - val_loss: 0.4879\n",
      "Epoch 74/100\n",
      "6091/6091 [==============================] - 1s 143us/sample - loss: 0.4292 - val_loss: 0.4870\n",
      "Epoch 75/100\n",
      "6091/6091 [==============================] - 1s 143us/sample - loss: 0.4277 - val_loss: 0.4862\n",
      "Epoch 76/100\n",
      "6091/6091 [==============================] - 1s 143us/sample - loss: 0.4263 - val_loss: 0.4854\n",
      "Epoch 77/100\n",
      "6091/6091 [==============================] - 1s 144us/sample - loss: 0.4248 - val_loss: 0.4846\n",
      "Epoch 78/100\n",
      "6091/6091 [==============================] - 1s 143us/sample - loss: 0.4234 - val_loss: 0.4838\n",
      "Epoch 79/100\n",
      "6091/6091 [==============================] - 1s 143us/sample - loss: 0.4220 - val_loss: 0.4830\n",
      "Epoch 80/100\n",
      "6091/6091 [==============================] - 1s 143us/sample - loss: 0.4206 - val_loss: 0.4823\n",
      "Epoch 81/100\n",
      "6091/6091 [==============================] - 1s 145us/sample - loss: 0.4192 - val_loss: 0.4815\n",
      "Epoch 82/100\n",
      "6091/6091 [==============================] - 1s 142us/sample - loss: 0.4178 - val_loss: 0.4808\n",
      "Epoch 83/100\n",
      "6091/6091 [==============================] - 1s 144us/sample - loss: 0.4165 - val_loss: 0.4801\n",
      "Epoch 84/100\n",
      "6091/6091 [==============================] - 1s 143us/sample - loss: 0.4152 - val_loss: 0.4794\n",
      "Epoch 85/100\n",
      "6091/6091 [==============================] - 1s 142us/sample - loss: 0.4139 - val_loss: 0.4786\n",
      "Epoch 86/100\n",
      "6091/6091 [==============================] - 1s 143us/sample - loss: 0.4125 - val_loss: 0.4780\n",
      "Epoch 87/100\n",
      "6091/6091 [==============================] - 1s 144us/sample - loss: 0.4113 - val_loss: 0.4773\n",
      "Epoch 88/100\n",
      "6091/6091 [==============================] - 1s 143us/sample - loss: 0.4100 - val_loss: 0.4767\n",
      "Epoch 89/100\n",
      "6091/6091 [==============================] - 1s 142us/sample - loss: 0.4087 - val_loss: 0.4760\n",
      "Epoch 90/100\n",
      "6091/6091 [==============================] - 1s 145us/sample - loss: 0.4075 - val_loss: 0.4754\n",
      "Epoch 91/100\n",
      "6091/6091 [==============================] - 1s 145us/sample - loss: 0.4062 - val_loss: 0.4748\n",
      "Epoch 92/100\n",
      "6091/6091 [==============================] - 1s 144us/sample - loss: 0.4050 - val_loss: 0.4742\n",
      "Epoch 93/100\n",
      "6091/6091 [==============================] - 1s 143us/sample - loss: 0.4038 - val_loss: 0.4736\n",
      "Epoch 94/100\n",
      "6091/6091 [==============================] - 1s 143us/sample - loss: 0.4026 - val_loss: 0.4730\n",
      "Epoch 95/100\n",
      "6091/6091 [==============================] - 1s 144us/sample - loss: 0.4015 - val_loss: 0.4724\n",
      "Epoch 96/100\n",
      "6091/6091 [==============================] - 1s 143us/sample - loss: 0.4003 - val_loss: 0.4718\n",
      "Epoch 97/100\n",
      "6091/6091 [==============================] - 1s 143us/sample - loss: 0.3991 - val_loss: 0.4712\n",
      "Epoch 98/100\n",
      "6091/6091 [==============================] - 1s 145us/sample - loss: 0.3980 - val_loss: 0.4706\n",
      "Epoch 99/100\n",
      "6091/6091 [==============================] - 1s 143us/sample - loss: 0.3969 - val_loss: 0.4701\n",
      "Epoch 100/100\n",
      "6091/6091 [==============================] - 1s 145us/sample - loss: 0.3958 - val_loss: 0.4696\n",
      "Predicting...\n",
      "(Took 88.620 sec)\n",
      "Train on 6091 samples, validate on 1522 samples\n",
      "Epoch 1/100\n",
      "6091/6091 [==============================] - 1s 196us/sample - loss: 0.6853 - val_loss: 0.6805\n",
      "Epoch 2/100\n",
      "6091/6091 [==============================] - 1s 150us/sample - loss: 0.6726 - val_loss: 0.6724\n",
      "Epoch 3/100\n",
      "6091/6091 [==============================] - 1s 143us/sample - loss: 0.6632 - val_loss: 0.6656\n",
      "Epoch 4/100\n",
      "6091/6091 [==============================] - 1s 147us/sample - loss: 0.6551 - val_loss: 0.6595\n",
      "Epoch 5/100\n",
      "6091/6091 [==============================] - 1s 144us/sample - loss: 0.6477 - val_loss: 0.6535\n",
      "Epoch 6/100\n",
      "6091/6091 [==============================] - 1s 142us/sample - loss: 0.6406 - val_loss: 0.6477\n",
      "Epoch 7/100\n",
      "6091/6091 [==============================] - 1s 144us/sample - loss: 0.6338 - val_loss: 0.6422\n",
      "Epoch 8/100\n",
      "6091/6091 [==============================] - 1s 144us/sample - loss: 0.6272 - val_loss: 0.6368\n",
      "Epoch 9/100\n",
      "6091/6091 [==============================] - 1s 142us/sample - loss: 0.6209 - val_loss: 0.6316\n",
      "Epoch 10/100\n",
      "6091/6091 [==============================] - 1s 143us/sample - loss: 0.6148 - val_loss: 0.6266\n",
      "Epoch 11/100\n",
      "6091/6091 [==============================] - 1s 142us/sample - loss: 0.6088 - val_loss: 0.6218\n",
      "Epoch 12/100\n",
      "6091/6091 [==============================] - 1s 142us/sample - loss: 0.6031 - val_loss: 0.6172\n",
      "Epoch 13/100\n",
      "6091/6091 [==============================] - 1s 142us/sample - loss: 0.5975 - val_loss: 0.6127\n",
      "Epoch 14/100\n",
      "6091/6091 [==============================] - 1s 144us/sample - loss: 0.5922 - val_loss: 0.6083\n",
      "Epoch 15/100\n",
      "6091/6091 [==============================] - 1s 141us/sample - loss: 0.5870 - val_loss: 0.6042\n",
      "Epoch 16/100\n",
      "6091/6091 [==============================] - 1s 143us/sample - loss: 0.5819 - val_loss: 0.6002\n",
      "Epoch 17/100\n",
      "6091/6091 [==============================] - 1s 142us/sample - loss: 0.5770 - val_loss: 0.5962\n",
      "Epoch 18/100\n",
      "6091/6091 [==============================] - 1s 142us/sample - loss: 0.5723 - val_loss: 0.5925\n",
      "Epoch 19/100\n",
      "6091/6091 [==============================] - 1s 145us/sample - loss: 0.5677 - val_loss: 0.5890\n",
      "Epoch 20/100\n",
      "6091/6091 [==============================] - 1s 143us/sample - loss: 0.5632 - val_loss: 0.5854\n",
      "Epoch 21/100\n",
      "6091/6091 [==============================] - 1s 144us/sample - loss: 0.5588 - val_loss: 0.5821\n",
      "Epoch 22/100\n",
      "6091/6091 [==============================] - 1s 143us/sample - loss: 0.5546 - val_loss: 0.5788\n",
      "Epoch 23/100\n",
      "6091/6091 [==============================] - 1s 154us/sample - loss: 0.5505 - val_loss: 0.5756\n",
      "Epoch 24/100\n",
      "6091/6091 [==============================] - 1s 168us/sample - loss: 0.5465 - val_loss: 0.5726\n",
      "Epoch 25/100\n",
      "6091/6091 [==============================] - 1s 142us/sample - loss: 0.5426 - val_loss: 0.5696\n",
      "Epoch 26/100\n",
      "6091/6091 [==============================] - 1s 140us/sample - loss: 0.5389 - val_loss: 0.5668\n",
      "Epoch 27/100\n",
      "6091/6091 [==============================] - 1s 140us/sample - loss: 0.5352 - val_loss: 0.5640\n",
      "Epoch 28/100\n",
      "6091/6091 [==============================] - 1s 147us/sample - loss: 0.5316 - val_loss: 0.5613\n",
      "Epoch 29/100\n",
      "6091/6091 [==============================] - 1s 148us/sample - loss: 0.5281 - val_loss: 0.5587\n",
      "Epoch 30/100\n",
      "6091/6091 [==============================] - 1s 145us/sample - loss: 0.5247 - val_loss: 0.5561\n",
      "Epoch 31/100\n",
      "6091/6091 [==============================] - 1s 148us/sample - loss: 0.5214 - val_loss: 0.5537\n",
      "Epoch 32/100\n",
      "6091/6091 [==============================] - 1s 142us/sample - loss: 0.5181 - val_loss: 0.5513\n",
      "Epoch 33/100\n",
      "6091/6091 [==============================] - 1s 144us/sample - loss: 0.5150 - val_loss: 0.5490\n",
      "Epoch 34/100\n",
      "6091/6091 [==============================] - 1s 144us/sample - loss: 0.5119 - val_loss: 0.5468\n",
      "Epoch 35/100\n",
      "6091/6091 [==============================] - 1s 146us/sample - loss: 0.5089 - val_loss: 0.5447\n",
      "Epoch 36/100\n",
      "6091/6091 [==============================] - 1s 146us/sample - loss: 0.5059 - val_loss: 0.5425\n",
      "Epoch 37/100\n",
      "6091/6091 [==============================] - 1s 146us/sample - loss: 0.5031 - val_loss: 0.5405\n",
      "Epoch 38/100\n",
      "6091/6091 [==============================] - 1s 144us/sample - loss: 0.5003 - val_loss: 0.5385\n",
      "Epoch 39/100\n",
      "6091/6091 [==============================] - 1s 144us/sample - loss: 0.4975 - val_loss: 0.5366\n",
      "Epoch 40/100\n",
      "6091/6091 [==============================] - 1s 156us/sample - loss: 0.4948 - val_loss: 0.5347\n",
      "Epoch 41/100\n",
      "6091/6091 [==============================] - 1s 153us/sample - loss: 0.4922 - val_loss: 0.5329\n",
      "Epoch 42/100\n",
      "6091/6091 [==============================] - 1s 150us/sample - loss: 0.4896 - val_loss: 0.5311\n",
      "Epoch 43/100\n",
      "6091/6091 [==============================] - 1s 153us/sample - loss: 0.4871 - val_loss: 0.5294\n",
      "Epoch 44/100\n",
      "6091/6091 [==============================] - 1s 146us/sample - loss: 0.4846 - val_loss: 0.5277\n",
      "Epoch 45/100\n",
      "6091/6091 [==============================] - 1s 144us/sample - loss: 0.4822 - val_loss: 0.5262\n",
      "Epoch 46/100\n",
      "6091/6091 [==============================] - 1s 145us/sample - loss: 0.4799 - val_loss: 0.5246\n",
      "Epoch 47/100\n",
      "6091/6091 [==============================] - 1s 144us/sample - loss: 0.4776 - val_loss: 0.5230\n",
      "Epoch 48/100\n",
      "6091/6091 [==============================] - 1s 144us/sample - loss: 0.4752 - val_loss: 0.5215\n",
      "Epoch 49/100\n",
      "6091/6091 [==============================] - 1s 145us/sample - loss: 0.4730 - val_loss: 0.5200\n",
      "Epoch 50/100\n",
      "6091/6091 [==============================] - 1s 144us/sample - loss: 0.4708 - val_loss: 0.5186\n",
      "Epoch 51/100\n",
      "6091/6091 [==============================] - 1s 144us/sample - loss: 0.4687 - val_loss: 0.5172\n",
      "Epoch 52/100\n",
      "6091/6091 [==============================] - 1s 143us/sample - loss: 0.4666 - val_loss: 0.5159\n",
      "Epoch 53/100\n",
      "6091/6091 [==============================] - 1s 147us/sample - loss: 0.4645 - val_loss: 0.5146\n",
      "Epoch 54/100\n",
      "6091/6091 [==============================] - 1s 150us/sample - loss: 0.4624 - val_loss: 0.5133\n",
      "Epoch 55/100\n",
      "6091/6091 [==============================] - 1s 146us/sample - loss: 0.4604 - val_loss: 0.5120\n",
      "Epoch 56/100\n",
      "6091/6091 [==============================] - 1s 145us/sample - loss: 0.4585 - val_loss: 0.5108\n",
      "Epoch 57/100\n",
      "6091/6091 [==============================] - 1s 150us/sample - loss: 0.4565 - val_loss: 0.5096\n",
      "Epoch 58/100\n",
      "6091/6091 [==============================] - 1s 146us/sample - loss: 0.4546 - val_loss: 0.5085\n",
      "Epoch 59/100\n",
      "6091/6091 [==============================] - 1s 145us/sample - loss: 0.4528 - val_loss: 0.5073\n",
      "Epoch 60/100\n",
      "6091/6091 [==============================] - 1s 144us/sample - loss: 0.4509 - val_loss: 0.5062\n",
      "Epoch 61/100\n",
      "6091/6091 [==============================] - 1s 146us/sample - loss: 0.4491 - val_loss: 0.5051\n",
      "Epoch 62/100\n",
      "6091/6091 [==============================] - 1s 146us/sample - loss: 0.4473 - val_loss: 0.5041\n",
      "Epoch 63/100\n",
      "6091/6091 [==============================] - 1s 143us/sample - loss: 0.4455 - val_loss: 0.5030\n",
      "Epoch 64/100\n",
      "6091/6091 [==============================] - 1s 145us/sample - loss: 0.4438 - val_loss: 0.5020\n",
      "Epoch 65/100\n",
      "6091/6091 [==============================] - 1s 146us/sample - loss: 0.4421 - val_loss: 0.5010\n",
      "Epoch 66/100\n",
      "6091/6091 [==============================] - 1s 146us/sample - loss: 0.4404 - val_loss: 0.5001\n",
      "Epoch 67/100\n",
      "6091/6091 [==============================] - 1s 147us/sample - loss: 0.4388 - val_loss: 0.4991\n",
      "Epoch 68/100\n",
      "6091/6091 [==============================] - 1s 146us/sample - loss: 0.4371 - val_loss: 0.4982\n",
      "Epoch 69/100\n",
      "6091/6091 [==============================] - 1s 147us/sample - loss: 0.4355 - val_loss: 0.4973\n",
      "Epoch 70/100\n",
      "6091/6091 [==============================] - 1s 146us/sample - loss: 0.4339 - val_loss: 0.4964\n",
      "Epoch 71/100\n",
      "6091/6091 [==============================] - 1s 145us/sample - loss: 0.4324 - val_loss: 0.4955\n",
      "Epoch 72/100\n",
      "6091/6091 [==============================] - 1s 145us/sample - loss: 0.4308 - val_loss: 0.4947\n",
      "Epoch 73/100\n",
      "6091/6091 [==============================] - 1s 146us/sample - loss: 0.4293 - val_loss: 0.4938\n",
      "Epoch 74/100\n",
      "6091/6091 [==============================] - 1s 146us/sample - loss: 0.4278 - val_loss: 0.4931\n",
      "Epoch 75/100\n",
      "6091/6091 [==============================] - 1s 142us/sample - loss: 0.4263 - val_loss: 0.4923\n",
      "Epoch 76/100\n",
      "6091/6091 [==============================] - 1s 143us/sample - loss: 0.4248 - val_loss: 0.4915\n",
      "Epoch 77/100\n",
      "6091/6091 [==============================] - 1s 149us/sample - loss: 0.4234 - val_loss: 0.4907\n",
      "Epoch 78/100\n",
      "6091/6091 [==============================] - 1s 145us/sample - loss: 0.4220 - val_loss: 0.4900\n",
      "Epoch 79/100\n",
      "6091/6091 [==============================] - 1s 146us/sample - loss: 0.4205 - val_loss: 0.4893\n",
      "Epoch 80/100\n",
      "6091/6091 [==============================] - 1s 147us/sample - loss: 0.4192 - val_loss: 0.4886\n",
      "Epoch 81/100\n",
      "6091/6091 [==============================] - 1s 146us/sample - loss: 0.4178 - val_loss: 0.4879\n",
      "Epoch 82/100\n",
      "6091/6091 [==============================] - 1s 146us/sample - loss: 0.4164 - val_loss: 0.4872\n",
      "Epoch 83/100\n",
      "6091/6091 [==============================] - 1s 144us/sample - loss: 0.4151 - val_loss: 0.4865\n",
      "Epoch 84/100\n",
      "6091/6091 [==============================] - 1s 145us/sample - loss: 0.4137 - val_loss: 0.4859\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85/100\n",
      "6091/6091 [==============================] - 1s 143us/sample - loss: 0.4124 - val_loss: 0.4852\n",
      "Epoch 86/100\n",
      "6091/6091 [==============================] - 1s 143us/sample - loss: 0.4111 - val_loss: 0.4846\n",
      "Epoch 87/100\n",
      "6091/6091 [==============================] - 1s 143us/sample - loss: 0.4099 - val_loss: 0.4840\n",
      "Epoch 88/100\n",
      "6091/6091 [==============================] - 1s 144us/sample - loss: 0.4086 - val_loss: 0.4834\n",
      "Epoch 89/100\n",
      "6091/6091 [==============================] - 1s 146us/sample - loss: 0.4073 - val_loss: 0.4828\n",
      "Epoch 90/100\n",
      "6091/6091 [==============================] - 1s 145us/sample - loss: 0.4061 - val_loss: 0.4822\n",
      "Epoch 91/100\n",
      "6091/6091 [==============================] - 1s 146us/sample - loss: 0.4049 - val_loss: 0.4816\n",
      "Epoch 92/100\n",
      "6091/6091 [==============================] - 1s 143us/sample - loss: 0.4037 - val_loss: 0.4811\n",
      "Epoch 93/100\n",
      "6091/6091 [==============================] - 1s 146us/sample - loss: 0.4025 - val_loss: 0.4806\n",
      "Epoch 94/100\n",
      "6091/6091 [==============================] - 1s 143us/sample - loss: 0.4013 - val_loss: 0.4800\n",
      "Epoch 95/100\n",
      "6091/6091 [==============================] - 1s 143us/sample - loss: 0.4001 - val_loss: 0.4795\n",
      "Epoch 96/100\n",
      "6091/6091 [==============================] - 1s 143us/sample - loss: 0.3989 - val_loss: 0.4790\n",
      "Epoch 97/100\n",
      "6091/6091 [==============================] - 1s 143us/sample - loss: 0.3978 - val_loss: 0.4785\n",
      "Epoch 98/100\n",
      "6091/6091 [==============================] - 1s 145us/sample - loss: 0.3967 - val_loss: 0.4780\n",
      "Epoch 99/100\n",
      "6091/6091 [==============================] - 1s 143us/sample - loss: 0.3955 - val_loss: 0.4775\n",
      "Epoch 100/100\n",
      "6091/6091 [==============================] - 1s 142us/sample - loss: 0.3944 - val_loss: 0.4770\n",
      "Predicting...\n",
      "(Took 89.677 sec)\n",
      "Combined confusion matrix:\n",
      "[[3854.  488.]\n",
      " [1118. 2153.]]\n",
      "(Overall, took 440.454 sec)\n",
      "Accuracy: 78.90% +/- 0.85%\n",
      "Precision for positive class: 77.51% +/- 1.60%\n",
      "Precision for negative class: 81.53% +/- 1.28%\n",
      "Recall for positive class: 88.76% +/- 0.93%\n",
      "Recall for negative class: 65.83% +/- 0.87%\n",
      "F for positive class: 82.75% +/- 1.06%\n",
      "F for negative class: 72.84% +/- 0.31%\n",
      "Mean F score: 77.79% +/- 0.64%\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/100\n",
      "6090/6090 [==============================] - 1s 188us/sample - loss: 0.6833 - val_loss: 0.6725\n",
      "Epoch 2/100\n",
      "6090/6090 [==============================] - 1s 143us/sample - loss: 0.6646 - val_loss: 0.6578\n",
      "Epoch 3/100\n",
      "6090/6090 [==============================] - 1s 148us/sample - loss: 0.6492 - val_loss: 0.6458\n",
      "Epoch 4/100\n",
      "6090/6090 [==============================] - 1s 145us/sample - loss: 0.6353 - val_loss: 0.6348\n",
      "Epoch 5/100\n",
      "6090/6090 [==============================] - 1s 146us/sample - loss: 0.6223 - val_loss: 0.6246\n",
      "Epoch 6/100\n",
      "6090/6090 [==============================] - 1s 144us/sample - loss: 0.6101 - val_loss: 0.6152\n",
      "Epoch 7/100\n",
      "6090/6090 [==============================] - 1s 146us/sample - loss: 0.5985 - val_loss: 0.6064\n",
      "Epoch 8/100\n",
      "6090/6090 [==============================] - 1s 147us/sample - loss: 0.5875 - val_loss: 0.5980\n",
      "Epoch 9/100\n",
      "6090/6090 [==============================] - 1s 148us/sample - loss: 0.5770 - val_loss: 0.5901\n",
      "Epoch 10/100\n",
      "6090/6090 [==============================] - 1s 144us/sample - loss: 0.5670 - val_loss: 0.5827\n",
      "Epoch 11/100\n",
      "6090/6090 [==============================] - 1s 146us/sample - loss: 0.5575 - val_loss: 0.5757\n",
      "Epoch 12/100\n",
      "6090/6090 [==============================] - 1s 154us/sample - loss: 0.5484 - val_loss: 0.5690\n",
      "Epoch 13/100\n",
      "6090/6090 [==============================] - 1s 151us/sample - loss: 0.5397 - val_loss: 0.5627\n",
      "Epoch 14/100\n",
      "6090/6090 [==============================] - 1s 150us/sample - loss: 0.5314 - val_loss: 0.5568\n",
      "Epoch 15/100\n",
      "6090/6090 [==============================] - 1s 155us/sample - loss: 0.5235 - val_loss: 0.5512\n",
      "Epoch 16/100\n",
      "6090/6090 [==============================] - 1s 149us/sample - loss: 0.5159 - val_loss: 0.5458\n",
      "Epoch 17/100\n",
      "6090/6090 [==============================] - 1s 153us/sample - loss: 0.5087 - val_loss: 0.5409\n",
      "Epoch 18/100\n",
      "6090/6090 [==============================] - 1s 151us/sample - loss: 0.5017 - val_loss: 0.5360\n",
      "Epoch 19/100\n",
      "6090/6090 [==============================] - 1s 151us/sample - loss: 0.4950 - val_loss: 0.5315\n",
      "Epoch 20/100\n",
      "6090/6090 [==============================] - 1s 150us/sample - loss: 0.4886 - val_loss: 0.5272\n",
      "Epoch 21/100\n",
      "6090/6090 [==============================] - 1s 147us/sample - loss: 0.4824 - val_loss: 0.5231\n",
      "Epoch 22/100\n",
      "6090/6090 [==============================] - 1s 147us/sample - loss: 0.4765 - val_loss: 0.5193\n",
      "Epoch 23/100\n",
      "6090/6090 [==============================] - 1s 149us/sample - loss: 0.4707 - val_loss: 0.5156\n",
      "Epoch 24/100\n",
      "6090/6090 [==============================] - 1s 146us/sample - loss: 0.4653 - val_loss: 0.5121\n",
      "Epoch 25/100\n",
      "6090/6090 [==============================] - 1s 147us/sample - loss: 0.4600 - val_loss: 0.5088\n",
      "Epoch 26/100\n",
      "6090/6090 [==============================] - 1s 145us/sample - loss: 0.4548 - val_loss: 0.5057\n",
      "Epoch 27/100\n",
      "6090/6090 [==============================] - 1s 145us/sample - loss: 0.4499 - val_loss: 0.5027\n",
      "Epoch 28/100\n",
      "6090/6090 [==============================] - 1s 145us/sample - loss: 0.4451 - val_loss: 0.4999\n",
      "Epoch 29/100\n",
      "6090/6090 [==============================] - 1s 145us/sample - loss: 0.4405 - val_loss: 0.4972\n",
      "Epoch 30/100\n",
      "6090/6090 [==============================] - 1s 146us/sample - loss: 0.4360 - val_loss: 0.4946\n",
      "Epoch 31/100\n",
      "6090/6090 [==============================] - 1s 145us/sample - loss: 0.4317 - val_loss: 0.4922\n",
      "Epoch 32/100\n",
      "6090/6090 [==============================] - 1s 146us/sample - loss: 0.4275 - val_loss: 0.4899\n",
      "Epoch 33/100\n",
      "6090/6090 [==============================] - 1s 145us/sample - loss: 0.4234 - val_loss: 0.4877\n",
      "Epoch 34/100\n",
      "6090/6090 [==============================] - 1s 148us/sample - loss: 0.4195 - val_loss: 0.4857\n",
      "Epoch 35/100\n",
      "6090/6090 [==============================] - 1s 147us/sample - loss: 0.4157 - val_loss: 0.4837\n",
      "Epoch 36/100\n",
      "6090/6090 [==============================] - 1s 146us/sample - loss: 0.4119 - val_loss: 0.4818\n",
      "Epoch 37/100\n",
      "6090/6090 [==============================] - 1s 146us/sample - loss: 0.4083 - val_loss: 0.4801\n",
      "Epoch 38/100\n",
      "6090/6090 [==============================] - 1s 147us/sample - loss: 0.4048 - val_loss: 0.4784\n",
      "Epoch 39/100\n",
      "6090/6090 [==============================] - 1s 146us/sample - loss: 0.4014 - val_loss: 0.4768\n",
      "Epoch 40/100\n",
      "6090/6090 [==============================] - 1s 146us/sample - loss: 0.3981 - val_loss: 0.4753\n",
      "Epoch 41/100\n",
      "6090/6090 [==============================] - 1s 147us/sample - loss: 0.3949 - val_loss: 0.4738\n",
      "Epoch 42/100\n",
      "6090/6090 [==============================] - 1s 145us/sample - loss: 0.3917 - val_loss: 0.4724\n",
      "Epoch 43/100\n",
      "6090/6090 [==============================] - 1s 149us/sample - loss: 0.3887 - val_loss: 0.4711\n",
      "Epoch 44/100\n",
      "6090/6090 [==============================] - 1s 145us/sample - loss: 0.3857 - val_loss: 0.4699\n",
      "Epoch 45/100\n",
      "6090/6090 [==============================] - 1s 146us/sample - loss: 0.3828 - val_loss: 0.4687\n",
      "Epoch 46/100\n",
      "6090/6090 [==============================] - 1s 146us/sample - loss: 0.3799 - val_loss: 0.4677\n",
      "Epoch 47/100\n",
      "6090/6090 [==============================] - 1s 147us/sample - loss: 0.3772 - val_loss: 0.4666\n",
      "Epoch 48/100\n",
      "6090/6090 [==============================] - 1s 144us/sample - loss: 0.3744 - val_loss: 0.4655\n",
      "Epoch 49/100\n",
      "6090/6090 [==============================] - 1s 145us/sample - loss: 0.3718 - val_loss: 0.4646\n",
      "Epoch 50/100\n",
      "6090/6090 [==============================] - 1s 144us/sample - loss: 0.3692 - val_loss: 0.4637\n",
      "Epoch 51/100\n",
      "6090/6090 [==============================] - 1s 144us/sample - loss: 0.3666 - val_loss: 0.4628\n",
      "Epoch 52/100\n",
      "6090/6090 [==============================] - 1s 148us/sample - loss: 0.3642 - val_loss: 0.4621\n",
      "Epoch 53/100\n",
      "6090/6090 [==============================] - 1s 146us/sample - loss: 0.3617 - val_loss: 0.4613\n",
      "Epoch 54/100\n",
      "6090/6090 [==============================] - 1s 147us/sample - loss: 0.3594 - val_loss: 0.4606\n",
      "Epoch 55/100\n",
      "6090/6090 [==============================] - 1s 148us/sample - loss: 0.3570 - val_loss: 0.4599\n",
      "Epoch 56/100\n",
      "6090/6090 [==============================] - 1s 145us/sample - loss: 0.3547 - val_loss: 0.4593\n",
      "Epoch 57/100\n",
      "6090/6090 [==============================] - 1s 146us/sample - loss: 0.3525 - val_loss: 0.4587\n",
      "Epoch 58/100\n",
      "6090/6090 [==============================] - 1s 145us/sample - loss: 0.3503 - val_loss: 0.4581\n",
      "Epoch 59/100\n",
      "6090/6090 [==============================] - 1s 144us/sample - loss: 0.3482 - val_loss: 0.4576\n",
      "Epoch 60/100\n",
      "6090/6090 [==============================] - 1s 143us/sample - loss: 0.3461 - val_loss: 0.4571\n",
      "Epoch 61/100\n",
      "6090/6090 [==============================] - 1s 143us/sample - loss: 0.3440 - val_loss: 0.4567\n",
      "Epoch 62/100\n",
      "6090/6090 [==============================] - 1s 143us/sample - loss: 0.3420 - val_loss: 0.4563\n",
      "Epoch 63/100\n",
      "6090/6090 [==============================] - 1s 145us/sample - loss: 0.3400 - val_loss: 0.4558\n",
      "Epoch 64/100\n",
      "6090/6090 [==============================] - 1s 145us/sample - loss: 0.3380 - val_loss: 0.4555\n",
      "Epoch 65/100\n",
      "6090/6090 [==============================] - 1s 143us/sample - loss: 0.3361 - val_loss: 0.4551\n",
      "Epoch 66/100\n",
      "6090/6090 [==============================] - 1s 144us/sample - loss: 0.3342 - val_loss: 0.4549\n",
      "Epoch 67/100\n",
      "6090/6090 [==============================] - 1s 143us/sample - loss: 0.3323 - val_loss: 0.4545\n",
      "Epoch 68/100\n",
      "6090/6090 [==============================] - 1s 142us/sample - loss: 0.3305 - val_loss: 0.4543\n",
      "Epoch 69/100\n",
      "6090/6090 [==============================] - 1s 144us/sample - loss: 0.3287 - val_loss: 0.4540\n",
      "Epoch 70/100\n",
      "6090/6090 [==============================] - 1s 147us/sample - loss: 0.3269 - val_loss: 0.4538\n",
      "Epoch 71/100\n",
      "6090/6090 [==============================] - 1s 146us/sample - loss: 0.3252 - val_loss: 0.4536\n",
      "Epoch 72/100\n",
      "6090/6090 [==============================] - 1s 145us/sample - loss: 0.3235 - val_loss: 0.4534\n",
      "Epoch 73/100\n",
      "6090/6090 [==============================] - 1s 145us/sample - loss: 0.3218 - val_loss: 0.4533\n",
      "Epoch 74/100\n",
      "6090/6090 [==============================] - 1s 145us/sample - loss: 0.3201 - val_loss: 0.4531\n",
      "Epoch 75/100\n",
      "6090/6090 [==============================] - 1s 144us/sample - loss: 0.3185 - val_loss: 0.4531\n",
      "Epoch 76/100\n",
      "6090/6090 [==============================] - 1s 145us/sample - loss: 0.3169 - val_loss: 0.4529\n",
      "Epoch 77/100\n",
      "6090/6090 [==============================] - 1s 146us/sample - loss: 0.3153 - val_loss: 0.4529\n",
      "Epoch 78/100\n",
      "6090/6090 [==============================] - 1s 146us/sample - loss: 0.3137 - val_loss: 0.4528\n",
      "Epoch 79/100\n",
      "6090/6090 [==============================] - 1s 146us/sample - loss: 0.3122 - val_loss: 0.4528\n",
      "Epoch 80/100\n",
      "6090/6090 [==============================] - 1s 147us/sample - loss: 0.3107 - val_loss: 0.4527\n",
      "Epoch 81/100\n",
      "6090/6090 [==============================] - 1s 146us/sample - loss: 0.3092 - val_loss: 0.4527\n",
      "Predicting...\n",
      "(Took 73.346 sec)\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/100\n",
      "6090/6090 [==============================] - 1s 192us/sample - loss: 0.6842 - val_loss: 0.6728\n",
      "Epoch 2/100\n",
      "6090/6090 [==============================] - 1s 144us/sample - loss: 0.6655 - val_loss: 0.6570\n",
      "Epoch 3/100\n",
      "6090/6090 [==============================] - 1s 155us/sample - loss: 0.6500 - val_loss: 0.6445\n",
      "Epoch 4/100\n",
      "6090/6090 [==============================] - 1s 150us/sample - loss: 0.6360 - val_loss: 0.6332\n",
      "Epoch 5/100\n",
      "6090/6090 [==============================] - 1s 154us/sample - loss: 0.6230 - val_loss: 0.6230\n",
      "Epoch 6/100\n",
      "6090/6090 [==============================] - 1s 151us/sample - loss: 0.6107 - val_loss: 0.6135\n",
      "Epoch 7/100\n",
      "6090/6090 [==============================] - 1s 149us/sample - loss: 0.5991 - val_loss: 0.6046\n",
      "Epoch 8/100\n",
      "6090/6090 [==============================] - 1s 146us/sample - loss: 0.5881 - val_loss: 0.5964\n",
      "Epoch 9/100\n",
      "6090/6090 [==============================] - 1s 142us/sample - loss: 0.5776 - val_loss: 0.5886\n",
      "Epoch 10/100\n",
      "6090/6090 [==============================] - 1s 141us/sample - loss: 0.5676 - val_loss: 0.5813\n",
      "Epoch 11/100\n",
      "6090/6090 [==============================] - 1s 147us/sample - loss: 0.5580 - val_loss: 0.5744\n",
      "Epoch 12/100\n",
      "6090/6090 [==============================] - 1s 144us/sample - loss: 0.5489 - val_loss: 0.5679\n",
      "Epoch 13/100\n",
      "6090/6090 [==============================] - 1s 146us/sample - loss: 0.5402 - val_loss: 0.5617\n",
      "Epoch 14/100\n",
      "6090/6090 [==============================] - 1s 147us/sample - loss: 0.5319 - val_loss: 0.5558\n",
      "Epoch 15/100\n",
      "6090/6090 [==============================] - 1s 145us/sample - loss: 0.5240 - val_loss: 0.5504\n",
      "Epoch 16/100\n",
      "6090/6090 [==============================] - 1s 144us/sample - loss: 0.5163 - val_loss: 0.5450\n",
      "Epoch 17/100\n",
      "6090/6090 [==============================] - 1s 143us/sample - loss: 0.5090 - val_loss: 0.5400\n",
      "Epoch 18/100\n",
      "6090/6090 [==============================] - 1s 143us/sample - loss: 0.5020 - val_loss: 0.5354\n",
      "Epoch 19/100\n",
      "6090/6090 [==============================] - 1s 146us/sample - loss: 0.4953 - val_loss: 0.5309\n",
      "Epoch 20/100\n",
      "6090/6090 [==============================] - 1s 148us/sample - loss: 0.4889 - val_loss: 0.5267\n",
      "Epoch 21/100\n",
      "6090/6090 [==============================] - 1s 149us/sample - loss: 0.4827 - val_loss: 0.5228\n",
      "Epoch 22/100\n",
      "6090/6090 [==============================] - 1s 147us/sample - loss: 0.4767 - val_loss: 0.5190\n",
      "Epoch 23/100\n",
      "6090/6090 [==============================] - 1s 147us/sample - loss: 0.4710 - val_loss: 0.5154\n",
      "Epoch 24/100\n",
      "6090/6090 [==============================] - 1s 146us/sample - loss: 0.4655 - val_loss: 0.5120\n",
      "Epoch 25/100\n",
      "6090/6090 [==============================] - 1s 147us/sample - loss: 0.4602 - val_loss: 0.5087\n",
      "Epoch 26/100\n",
      "6090/6090 [==============================] - 1s 147us/sample - loss: 0.4550 - val_loss: 0.5057\n",
      "Epoch 27/100\n",
      "6090/6090 [==============================] - 1s 152us/sample - loss: 0.4501 - val_loss: 0.5028\n",
      "Epoch 28/100\n",
      "6090/6090 [==============================] - 1s 146us/sample - loss: 0.4453 - val_loss: 0.5001\n",
      "Epoch 29/100\n",
      "6090/6090 [==============================] - 1s 147us/sample - loss: 0.4406 - val_loss: 0.4974\n",
      "Epoch 30/100\n",
      "6090/6090 [==============================] - 1s 147us/sample - loss: 0.4362 - val_loss: 0.4949\n",
      "Epoch 31/100\n",
      "6090/6090 [==============================] - 1s 148us/sample - loss: 0.4319 - val_loss: 0.4925\n",
      "Epoch 32/100\n",
      "6090/6090 [==============================] - 1s 147us/sample - loss: 0.4277 - val_loss: 0.4903\n",
      "Epoch 33/100\n",
      "6090/6090 [==============================] - 1s 146us/sample - loss: 0.4235 - val_loss: 0.4882\n",
      "Epoch 34/100\n",
      "6090/6090 [==============================] - 1s 145us/sample - loss: 0.4196 - val_loss: 0.4860\n",
      "Epoch 35/100\n",
      "6090/6090 [==============================] - 1s 147us/sample - loss: 0.4158 - val_loss: 0.4841\n",
      "Epoch 36/100\n",
      "6090/6090 [==============================] - 1s 144us/sample - loss: 0.4121 - val_loss: 0.4823\n",
      "Epoch 37/100\n",
      "6090/6090 [==============================] - 1s 147us/sample - loss: 0.4085 - val_loss: 0.4806\n",
      "Epoch 38/100\n",
      "6090/6090 [==============================] - 1s 148us/sample - loss: 0.4049 - val_loss: 0.4789\n",
      "Epoch 39/100\n",
      "6090/6090 [==============================] - 1s 147us/sample - loss: 0.4015 - val_loss: 0.4774\n",
      "Epoch 40/100\n",
      "6090/6090 [==============================] - 1s 148us/sample - loss: 0.3982 - val_loss: 0.4760\n",
      "Epoch 41/100\n",
      "6090/6090 [==============================] - 1s 146us/sample - loss: 0.3950 - val_loss: 0.4746\n",
      "Epoch 42/100\n",
      "6090/6090 [==============================] - 1s 145us/sample - loss: 0.3918 - val_loss: 0.4732\n",
      "Epoch 43/100\n",
      "6090/6090 [==============================] - 1s 148us/sample - loss: 0.3888 - val_loss: 0.4719\n",
      "Epoch 44/100\n",
      "6090/6090 [==============================] - 1s 149us/sample - loss: 0.3857 - val_loss: 0.4707\n",
      "Epoch 45/100\n",
      "6090/6090 [==============================] - 1s 160us/sample - loss: 0.3828 - val_loss: 0.4695\n",
      "Epoch 46/100\n",
      "6090/6090 [==============================] - 1s 150us/sample - loss: 0.3800 - val_loss: 0.4685\n",
      "Epoch 47/100\n",
      "6090/6090 [==============================] - 1s 145us/sample - loss: 0.3772 - val_loss: 0.4674\n",
      "Epoch 48/100\n",
      "6090/6090 [==============================] - 1s 161us/sample - loss: 0.3745 - val_loss: 0.4665\n",
      "Epoch 49/100\n",
      "6090/6090 [==============================] - 1s 182us/sample - loss: 0.3718 - val_loss: 0.4656\n",
      "Epoch 50/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6090/6090 [==============================] - 1s 193us/sample - loss: 0.3692 - val_loss: 0.4647\n",
      "Epoch 51/100\n",
      "6090/6090 [==============================] - 1s 246us/sample - loss: 0.3667 - val_loss: 0.4639\n",
      "Epoch 52/100\n",
      "6090/6090 [==============================] - 1s 186us/sample - loss: 0.3642 - val_loss: 0.4631\n",
      "Epoch 53/100\n",
      "6090/6090 [==============================] - 1s 176us/sample - loss: 0.3618 - val_loss: 0.4624\n",
      "Epoch 54/100\n",
      "6090/6090 [==============================] - 1s 165us/sample - loss: 0.3594 - val_loss: 0.4617\n",
      "Epoch 55/100\n",
      "6090/6090 [==============================] - 1s 169us/sample - loss: 0.3570 - val_loss: 0.4610\n",
      "Epoch 56/100\n",
      "6090/6090 [==============================] - 1s 163us/sample - loss: 0.3548 - val_loss: 0.4604\n",
      "Epoch 57/100\n",
      "6090/6090 [==============================] - 1s 161us/sample - loss: 0.3525 - val_loss: 0.4599\n",
      "Epoch 58/100\n",
      "6090/6090 [==============================] - 1s 166us/sample - loss: 0.3503 - val_loss: 0.4594\n",
      "Epoch 59/100\n",
      "6090/6090 [==============================] - 1s 173us/sample - loss: 0.3482 - val_loss: 0.4589\n",
      "Epoch 60/100\n",
      "6090/6090 [==============================] - 1s 141us/sample - loss: 0.3461 - val_loss: 0.4585\n",
      "Epoch 61/100\n",
      "6090/6090 [==============================] - 1s 145us/sample - loss: 0.3440 - val_loss: 0.4580\n",
      "Epoch 62/100\n",
      "6090/6090 [==============================] - 1s 153us/sample - loss: 0.3420 - val_loss: 0.4576\n",
      "Epoch 63/100\n",
      "6090/6090 [==============================] - 1s 148us/sample - loss: 0.3400 - val_loss: 0.4572\n",
      "Epoch 64/100\n",
      "6090/6090 [==============================] - 1s 180us/sample - loss: 0.3380 - val_loss: 0.4568\n",
      "Epoch 65/100\n",
      "6090/6090 [==============================] - 1s 141us/sample - loss: 0.3361 - val_loss: 0.4565\n",
      "Epoch 66/100\n",
      "6090/6090 [==============================] - 1s 154us/sample - loss: 0.3342 - val_loss: 0.4563\n",
      "Epoch 67/100\n",
      "6090/6090 [==============================] - 1s 136us/sample - loss: 0.3323 - val_loss: 0.4559\n",
      "Epoch 68/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.3305 - val_loss: 0.4557\n",
      "Epoch 69/100\n",
      "6090/6090 [==============================] - 1s 141us/sample - loss: 0.3287 - val_loss: 0.4555\n",
      "Epoch 70/100\n",
      "6090/6090 [==============================] - 1s 155us/sample - loss: 0.3269 - val_loss: 0.4553\n",
      "Epoch 71/100\n",
      "6090/6090 [==============================] - 1s 141us/sample - loss: 0.3251 - val_loss: 0.4552\n",
      "Epoch 72/100\n",
      "6090/6090 [==============================] - 1s 144us/sample - loss: 0.3235 - val_loss: 0.4550\n",
      "Epoch 73/100\n",
      "6090/6090 [==============================] - 1s 152us/sample - loss: 0.3218 - val_loss: 0.4549\n",
      "Epoch 74/100\n",
      "6090/6090 [==============================] - 1s 159us/sample - loss: 0.3201 - val_loss: 0.4548\n",
      "Epoch 75/100\n",
      "6090/6090 [==============================] - 1s 151us/sample - loss: 0.3185 - val_loss: 0.4547\n",
      "Epoch 76/100\n",
      "6090/6090 [==============================] - 1s 145us/sample - loss: 0.3168 - val_loss: 0.4546\n",
      "Epoch 77/100\n",
      "6090/6090 [==============================] - 1s 146us/sample - loss: 0.3153 - val_loss: 0.4545\n",
      "Epoch 78/100\n",
      "6090/6090 [==============================] - 1s 150us/sample - loss: 0.3137 - val_loss: 0.4545\n",
      "Predicting...\n",
      "(Took 73.555 sec)\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/100\n",
      "6090/6090 [==============================] - 1s 199us/sample - loss: 0.6830 - val_loss: 0.6759\n",
      "Epoch 2/100\n",
      "6090/6090 [==============================] - 1s 155us/sample - loss: 0.6635 - val_loss: 0.6629\n",
      "Epoch 3/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.6478 - val_loss: 0.6518\n",
      "Epoch 4/100\n",
      "6090/6090 [==============================] - 1s 150us/sample - loss: 0.6338 - val_loss: 0.6417\n",
      "Epoch 5/100\n",
      "6090/6090 [==============================] - 1s 149us/sample - loss: 0.6209 - val_loss: 0.6321\n",
      "Epoch 6/100\n",
      "6090/6090 [==============================] - 1s 146us/sample - loss: 0.6087 - val_loss: 0.6231\n",
      "Epoch 7/100\n",
      "6090/6090 [==============================] - 1s 153us/sample - loss: 0.5972 - val_loss: 0.6145\n",
      "Epoch 8/100\n",
      "6090/6090 [==============================] - 1s 162us/sample - loss: 0.5862 - val_loss: 0.6064\n",
      "Epoch 9/100\n",
      "6090/6090 [==============================] - 1s 158us/sample - loss: 0.5757 - val_loss: 0.5987\n",
      "Epoch 10/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.5657 - val_loss: 0.5915\n",
      "Epoch 11/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.5562 - val_loss: 0.5846\n",
      "Epoch 12/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.5471 - val_loss: 0.5781\n",
      "Epoch 13/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.5385 - val_loss: 0.5720\n",
      "Epoch 14/100\n",
      "6090/6090 [==============================] - 1s 150us/sample - loss: 0.5301 - val_loss: 0.5662\n",
      "Epoch 15/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.5222 - val_loss: 0.5606\n",
      "Epoch 16/100\n",
      "6090/6090 [==============================] - 1s 157us/sample - loss: 0.5146 - val_loss: 0.5555\n",
      "Epoch 17/100\n",
      "6090/6090 [==============================] - 1s 148us/sample - loss: 0.5073 - val_loss: 0.5505\n",
      "Epoch 18/100\n",
      "6090/6090 [==============================] - 1s 154us/sample - loss: 0.5003 - val_loss: 0.5458\n",
      "Epoch 19/100\n",
      "6090/6090 [==============================] - 1s 148us/sample - loss: 0.4936 - val_loss: 0.5414\n",
      "Epoch 20/100\n",
      "6090/6090 [==============================] - 1s 141us/sample - loss: 0.4872 - val_loss: 0.5373\n",
      "Epoch 21/100\n",
      "6090/6090 [==============================] - 1s 145us/sample - loss: 0.4810 - val_loss: 0.5333\n",
      "Epoch 22/100\n",
      "6090/6090 [==============================] - 1s 172us/sample - loss: 0.4751 - val_loss: 0.5295\n",
      "Epoch 23/100\n",
      "6090/6090 [==============================] - 1s 142us/sample - loss: 0.4693 - val_loss: 0.5259\n",
      "Epoch 24/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.4638 - val_loss: 0.5225\n",
      "Epoch 25/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.4585 - val_loss: 0.5193\n",
      "Epoch 26/100\n",
      "6090/6090 [==============================] - 1s 152us/sample - loss: 0.4534 - val_loss: 0.5163\n",
      "Epoch 27/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.4484 - val_loss: 0.5133\n",
      "Epoch 28/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.4436 - val_loss: 0.5105\n",
      "Epoch 29/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.4390 - val_loss: 0.5079\n",
      "Epoch 30/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.4345 - val_loss: 0.5054\n",
      "Epoch 31/100\n",
      "6090/6090 [==============================] - 1s 136us/sample - loss: 0.4302 - val_loss: 0.5029\n",
      "Epoch 32/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.4260 - val_loss: 0.5006\n",
      "Epoch 33/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.4219 - val_loss: 0.4985\n",
      "Epoch 34/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.4179 - val_loss: 0.4965\n",
      "Epoch 35/100\n",
      "6090/6090 [==============================] - 1s 136us/sample - loss: 0.4141 - val_loss: 0.4945\n",
      "Epoch 36/100\n",
      "6090/6090 [==============================] - 1s 136us/sample - loss: 0.4104 - val_loss: 0.4926\n",
      "Epoch 37/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.4067 - val_loss: 0.4908\n",
      "Epoch 38/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.4032 - val_loss: 0.4892\n",
      "Epoch 39/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.3998 - val_loss: 0.4876\n",
      "Epoch 40/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.3964 - val_loss: 0.4860\n",
      "Epoch 41/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.3932 - val_loss: 0.4845\n",
      "Epoch 42/100\n",
      "6090/6090 [==============================] - 1s 136us/sample - loss: 0.3901 - val_loss: 0.4832\n",
      "Epoch 43/100\n",
      "6090/6090 [==============================] - 1s 135us/sample - loss: 0.3870 - val_loss: 0.4819\n",
      "Epoch 44/100\n",
      "6090/6090 [==============================] - 1s 136us/sample - loss: 0.3840 - val_loss: 0.4806\n",
      "Epoch 45/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.3810 - val_loss: 0.4794\n",
      "Epoch 46/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.3782 - val_loss: 0.4783\n",
      "Epoch 47/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.3754 - val_loss: 0.4772\n",
      "Epoch 48/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.3726 - val_loss: 0.4762\n",
      "Epoch 49/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.3700 - val_loss: 0.4752\n",
      "Epoch 50/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.3674 - val_loss: 0.4743\n",
      "Epoch 51/100\n",
      "6090/6090 [==============================] - 1s 136us/sample - loss: 0.3648 - val_loss: 0.4733\n",
      "Epoch 52/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.3623 - val_loss: 0.4725\n",
      "Epoch 53/100\n",
      "6090/6090 [==============================] - 1s 135us/sample - loss: 0.3599 - val_loss: 0.4718\n",
      "Epoch 54/100\n",
      "6090/6090 [==============================] - 1s 136us/sample - loss: 0.3575 - val_loss: 0.4710\n",
      "Epoch 55/100\n",
      "6090/6090 [==============================] - 1s 136us/sample - loss: 0.3551 - val_loss: 0.4703\n",
      "Epoch 56/100\n",
      "6090/6090 [==============================] - 1s 136us/sample - loss: 0.3529 - val_loss: 0.4697\n",
      "Epoch 57/100\n",
      "6090/6090 [==============================] - 1s 136us/sample - loss: 0.3506 - val_loss: 0.4690\n",
      "Epoch 58/100\n",
      "6090/6090 [==============================] - 1s 136us/sample - loss: 0.3484 - val_loss: 0.4684\n",
      "Epoch 59/100\n",
      "6090/6090 [==============================] - 1s 141us/sample - loss: 0.3462 - val_loss: 0.4679\n",
      "Epoch 60/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.3441 - val_loss: 0.4673\n",
      "Epoch 61/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.3420 - val_loss: 0.4668\n",
      "Epoch 62/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.3400 - val_loss: 0.4663\n",
      "Epoch 63/100\n",
      "6090/6090 [==============================] - 1s 136us/sample - loss: 0.3380 - val_loss: 0.4659\n",
      "Epoch 64/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.3360 - val_loss: 0.4655\n",
      "Epoch 65/100\n",
      "6090/6090 [==============================] - 1s 136us/sample - loss: 0.3341 - val_loss: 0.4651\n",
      "Epoch 66/100\n",
      "6090/6090 [==============================] - 1s 136us/sample - loss: 0.3322 - val_loss: 0.4647\n",
      "Epoch 67/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.3303 - val_loss: 0.4643\n",
      "Epoch 68/100\n",
      "6090/6090 [==============================] - 1s 136us/sample - loss: 0.3285 - val_loss: 0.4640\n",
      "Epoch 69/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.3267 - val_loss: 0.4637\n",
      "Epoch 70/100\n",
      "6090/6090 [==============================] - 1s 150us/sample - loss: 0.3249 - val_loss: 0.4635\n",
      "Epoch 71/100\n",
      "6090/6090 [==============================] - 1s 144us/sample - loss: 0.3231 - val_loss: 0.4632\n",
      "Epoch 72/100\n",
      "6090/6090 [==============================] - 1s 144us/sample - loss: 0.3214 - val_loss: 0.4630\n",
      "Epoch 73/100\n",
      "6090/6090 [==============================] - 1s 141us/sample - loss: 0.3197 - val_loss: 0.4627\n",
      "Epoch 74/100\n",
      "6090/6090 [==============================] - 1s 152us/sample - loss: 0.3181 - val_loss: 0.4626\n",
      "Epoch 75/100\n",
      "6090/6090 [==============================] - 1s 145us/sample - loss: 0.3164 - val_loss: 0.4624\n",
      "Epoch 76/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.3148 - val_loss: 0.4623\n",
      "Epoch 77/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.3132 - val_loss: 0.4621\n",
      "Epoch 78/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.3116 - val_loss: 0.4620\n",
      "Epoch 79/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.3101 - val_loss: 0.4619\n",
      "Epoch 80/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.3085 - val_loss: 0.4618\n",
      "Epoch 81/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.3070 - val_loss: 0.4617\n",
      "Epoch 82/100\n",
      "6090/6090 [==============================] - 1s 135us/sample - loss: 0.3055 - val_loss: 0.4616\n",
      "Epoch 83/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.3041 - val_loss: 0.4616\n",
      "Epoch 84/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.3026 - val_loss: 0.4616\n",
      "Epoch 85/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.3012 - val_loss: 0.4616\n",
      "Predicting...\n",
      "(Took 74.180 sec)\n",
      "Train on 6091 samples, validate on 1522 samples\n",
      "Epoch 1/100\n",
      "6091/6091 [==============================] - 1s 184us/sample - loss: 0.6832 - val_loss: 0.6739\n",
      "Epoch 2/100\n",
      "6091/6091 [==============================] - 1s 138us/sample - loss: 0.6643 - val_loss: 0.6598\n",
      "Epoch 3/100\n",
      "6091/6091 [==============================] - 1s 137us/sample - loss: 0.6486 - val_loss: 0.6480\n",
      "Epoch 4/100\n",
      "6091/6091 [==============================] - 1s 138us/sample - loss: 0.6346 - val_loss: 0.6374\n",
      "Epoch 5/100\n",
      "6091/6091 [==============================] - 1s 139us/sample - loss: 0.6216 - val_loss: 0.6275\n",
      "Epoch 6/100\n",
      "6091/6091 [==============================] - 1s 138us/sample - loss: 0.6094 - val_loss: 0.6184\n",
      "Epoch 7/100\n",
      "6091/6091 [==============================] - 1s 138us/sample - loss: 0.5978 - val_loss: 0.6097\n",
      "Epoch 8/100\n",
      "6091/6091 [==============================] - 1s 138us/sample - loss: 0.5867 - val_loss: 0.6015\n",
      "Epoch 9/100\n",
      "6091/6091 [==============================] - 1s 138us/sample - loss: 0.5762 - val_loss: 0.5937\n",
      "Epoch 10/100\n",
      "6091/6091 [==============================] - 1s 154us/sample - loss: 0.5662 - val_loss: 0.5864\n",
      "Epoch 11/100\n",
      "6091/6091 [==============================] - 1s 154us/sample - loss: 0.5567 - val_loss: 0.5795\n",
      "Epoch 12/100\n",
      "6091/6091 [==============================] - 1s 152us/sample - loss: 0.5476 - val_loss: 0.5729\n",
      "Epoch 13/100\n",
      "6091/6091 [==============================] - 1s 155us/sample - loss: 0.5389 - val_loss: 0.5668\n",
      "Epoch 14/100\n",
      "6091/6091 [==============================] - 1s 153us/sample - loss: 0.5306 - val_loss: 0.5609\n",
      "Epoch 15/100\n",
      "6091/6091 [==============================] - 1s 153us/sample - loss: 0.5227 - val_loss: 0.5554\n",
      "Epoch 16/100\n",
      "6091/6091 [==============================] - 1s 156us/sample - loss: 0.5151 - val_loss: 0.5501\n",
      "Epoch 17/100\n",
      "6091/6091 [==============================] - 1s 153us/sample - loss: 0.5078 - val_loss: 0.5451\n",
      "Epoch 18/100\n",
      "6091/6091 [==============================] - 1s 142us/sample - loss: 0.5008 - val_loss: 0.5403\n",
      "Epoch 19/100\n",
      "6091/6091 [==============================] - 1s 139us/sample - loss: 0.4941 - val_loss: 0.5358\n",
      "Epoch 20/100\n",
      "6091/6091 [==============================] - 1s 139us/sample - loss: 0.4877 - val_loss: 0.5315\n",
      "Epoch 21/100\n",
      "6091/6091 [==============================] - 1s 140us/sample - loss: 0.4816 - val_loss: 0.5274\n",
      "Epoch 22/100\n",
      "6091/6091 [==============================] - 1s 139us/sample - loss: 0.4756 - val_loss: 0.5236\n",
      "Epoch 23/100\n",
      "6091/6091 [==============================] - 1s 140us/sample - loss: 0.4699 - val_loss: 0.5199\n",
      "Epoch 24/100\n",
      "6091/6091 [==============================] - 1s 141us/sample - loss: 0.4645 - val_loss: 0.5164\n",
      "Epoch 25/100\n",
      "6091/6091 [==============================] - 1s 137us/sample - loss: 0.4592 - val_loss: 0.5131\n",
      "Epoch 26/100\n",
      "6091/6091 [==============================] - 1s 138us/sample - loss: 0.4541 - val_loss: 0.5099\n",
      "Epoch 27/100\n",
      "6091/6091 [==============================] - 1s 137us/sample - loss: 0.4491 - val_loss: 0.5068\n",
      "Epoch 28/100\n",
      "6091/6091 [==============================] - 1s 137us/sample - loss: 0.4444 - val_loss: 0.5039\n",
      "Epoch 29/100\n",
      "6091/6091 [==============================] - 1s 138us/sample - loss: 0.4398 - val_loss: 0.5012\n",
      "Epoch 30/100\n",
      "6091/6091 [==============================] - 1s 139us/sample - loss: 0.4353 - val_loss: 0.4986\n",
      "Epoch 31/100\n",
      "6091/6091 [==============================] - 1s 138us/sample - loss: 0.4310 - val_loss: 0.4961\n",
      "Epoch 32/100\n",
      "6091/6091 [==============================] - 1s 138us/sample - loss: 0.4268 - val_loss: 0.4937\n",
      "Epoch 33/100\n",
      "6091/6091 [==============================] - 1s 137us/sample - loss: 0.4227 - val_loss: 0.4914\n",
      "Epoch 34/100\n",
      "6091/6091 [==============================] - 1s 138us/sample - loss: 0.4188 - val_loss: 0.4893\n",
      "Epoch 35/100\n",
      "6091/6091 [==============================] - 1s 139us/sample - loss: 0.4150 - val_loss: 0.4872\n",
      "Epoch 36/100\n",
      "6091/6091 [==============================] - 1s 139us/sample - loss: 0.4113 - val_loss: 0.4853\n",
      "Epoch 37/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6091/6091 [==============================] - 1s 137us/sample - loss: 0.4077 - val_loss: 0.4834\n",
      "Epoch 38/100\n",
      "6091/6091 [==============================] - 1s 137us/sample - loss: 0.4042 - val_loss: 0.4815\n",
      "Epoch 39/100\n",
      "6091/6091 [==============================] - 1s 138us/sample - loss: 0.4008 - val_loss: 0.4798\n",
      "Epoch 40/100\n",
      "6091/6091 [==============================] - 1s 138us/sample - loss: 0.3975 - val_loss: 0.4782\n",
      "Epoch 41/100\n",
      "6091/6091 [==============================] - 1s 137us/sample - loss: 0.3943 - val_loss: 0.4767\n",
      "Epoch 42/100\n",
      "6091/6091 [==============================] - 1s 138us/sample - loss: 0.3911 - val_loss: 0.4751\n",
      "Epoch 43/100\n",
      "6091/6091 [==============================] - 1s 138us/sample - loss: 0.3881 - val_loss: 0.4736\n",
      "Epoch 44/100\n",
      "6091/6091 [==============================] - 1s 139us/sample - loss: 0.3851 - val_loss: 0.4723\n",
      "Epoch 45/100\n",
      "6091/6091 [==============================] - 1s 138us/sample - loss: 0.3822 - val_loss: 0.4710\n",
      "Epoch 46/100\n",
      "6091/6091 [==============================] - 1s 138us/sample - loss: 0.3793 - val_loss: 0.4697\n",
      "Epoch 47/100\n",
      "6091/6091 [==============================] - 1s 137us/sample - loss: 0.3766 - val_loss: 0.4685\n",
      "Epoch 48/100\n",
      "6091/6091 [==============================] - 1s 138us/sample - loss: 0.3738 - val_loss: 0.4673\n",
      "Epoch 49/100\n",
      "6091/6091 [==============================] - 1s 140us/sample - loss: 0.3712 - val_loss: 0.4662\n",
      "Epoch 50/100\n",
      "6091/6091 [==============================] - 1s 140us/sample - loss: 0.3686 - val_loss: 0.4652\n",
      "Epoch 51/100\n",
      "6091/6091 [==============================] - 1s 137us/sample - loss: 0.3661 - val_loss: 0.4642\n",
      "Epoch 52/100\n",
      "6091/6091 [==============================] - 1s 136us/sample - loss: 0.3636 - val_loss: 0.4632\n",
      "Epoch 53/100\n",
      "6091/6091 [==============================] - 1s 138us/sample - loss: 0.3612 - val_loss: 0.4623\n",
      "Epoch 54/100\n",
      "6091/6091 [==============================] - 1s 139us/sample - loss: 0.3588 - val_loss: 0.4615\n",
      "Epoch 55/100\n",
      "6091/6091 [==============================] - 1s 139us/sample - loss: 0.3565 - val_loss: 0.4606\n",
      "Epoch 56/100\n",
      "6091/6091 [==============================] - 1s 140us/sample - loss: 0.3542 - val_loss: 0.4599\n",
      "Epoch 57/100\n",
      "6091/6091 [==============================] - 1s 140us/sample - loss: 0.3519 - val_loss: 0.4591\n",
      "Epoch 58/100\n",
      "6091/6091 [==============================] - 1s 140us/sample - loss: 0.3498 - val_loss: 0.4584\n",
      "Epoch 59/100\n",
      "6091/6091 [==============================] - 1s 139us/sample - loss: 0.3476 - val_loss: 0.4577\n",
      "Epoch 60/100\n",
      "6091/6091 [==============================] - 1s 151us/sample - loss: 0.3455 - val_loss: 0.4571\n",
      "Epoch 61/100\n",
      "6091/6091 [==============================] - 1s 146us/sample - loss: 0.3434 - val_loss: 0.4565\n",
      "Epoch 62/100\n",
      "6091/6091 [==============================] - 1s 151us/sample - loss: 0.3414 - val_loss: 0.4559\n",
      "Epoch 63/100\n",
      "6091/6091 [==============================] - 1s 141us/sample - loss: 0.3394 - val_loss: 0.4553\n",
      "Epoch 64/100\n",
      "6091/6091 [==============================] - 1s 146us/sample - loss: 0.3374 - val_loss: 0.4548\n",
      "Epoch 65/100\n",
      "6091/6091 [==============================] - 1s 142us/sample - loss: 0.3355 - val_loss: 0.4543\n",
      "Epoch 66/100\n",
      "6091/6091 [==============================] - 1s 137us/sample - loss: 0.3336 - val_loss: 0.4538\n",
      "Epoch 67/100\n",
      "6091/6091 [==============================] - 1s 137us/sample - loss: 0.3318 - val_loss: 0.4533\n",
      "Epoch 68/100\n",
      "6091/6091 [==============================] - 1s 138us/sample - loss: 0.3299 - val_loss: 0.4530\n",
      "Epoch 69/100\n",
      "6091/6091 [==============================] - 1s 139us/sample - loss: 0.3281 - val_loss: 0.4525\n",
      "Epoch 70/100\n",
      "6091/6091 [==============================] - 1s 139us/sample - loss: 0.3264 - val_loss: 0.4522\n",
      "Epoch 71/100\n",
      "6091/6091 [==============================] - 1s 139us/sample - loss: 0.3246 - val_loss: 0.4518\n",
      "Epoch 72/100\n",
      "6091/6091 [==============================] - 1s 143us/sample - loss: 0.3229 - val_loss: 0.4515\n",
      "Epoch 73/100\n",
      "6091/6091 [==============================] - 1s 138us/sample - loss: 0.3212 - val_loss: 0.4512\n",
      "Epoch 74/100\n",
      "6091/6091 [==============================] - 1s 137us/sample - loss: 0.3196 - val_loss: 0.4510\n",
      "Epoch 75/100\n",
      "6091/6091 [==============================] - 1s 138us/sample - loss: 0.3179 - val_loss: 0.4507\n",
      "Epoch 76/100\n",
      "6091/6091 [==============================] - 1s 138us/sample - loss: 0.3163 - val_loss: 0.4504\n",
      "Epoch 77/100\n",
      "6091/6091 [==============================] - 1s 138us/sample - loss: 0.3147 - val_loss: 0.4501\n",
      "Epoch 78/100\n",
      "6091/6091 [==============================] - 1s 138us/sample - loss: 0.3132 - val_loss: 0.4500\n",
      "Epoch 79/100\n",
      "6091/6091 [==============================] - 1s 137us/sample - loss: 0.3116 - val_loss: 0.4498\n",
      "Epoch 80/100\n",
      "6091/6091 [==============================] - 1s 138us/sample - loss: 0.3101 - val_loss: 0.4496\n",
      "Epoch 81/100\n",
      "6091/6091 [==============================] - 1s 139us/sample - loss: 0.3086 - val_loss: 0.4493\n",
      "Epoch 82/100\n",
      "6091/6091 [==============================] - 1s 138us/sample - loss: 0.3071 - val_loss: 0.4493\n",
      "Epoch 83/100\n",
      "6091/6091 [==============================] - 1s 139us/sample - loss: 0.3056 - val_loss: 0.4491\n",
      "Epoch 84/100\n",
      "6091/6091 [==============================] - 1s 137us/sample - loss: 0.3042 - val_loss: 0.4490\n",
      "Epoch 85/100\n",
      "6091/6091 [==============================] - 1s 139us/sample - loss: 0.3028 - val_loss: 0.4489\n",
      "Epoch 86/100\n",
      "6091/6091 [==============================] - 1s 137us/sample - loss: 0.3014 - val_loss: 0.4489\n",
      "Epoch 87/100\n",
      "6091/6091 [==============================] - 1s 136us/sample - loss: 0.3000 - val_loss: 0.4488\n",
      "Epoch 88/100\n",
      "6091/6091 [==============================] - 1s 138us/sample - loss: 0.2986 - val_loss: 0.4487\n",
      "Epoch 89/100\n",
      "6091/6091 [==============================] - 1s 137us/sample - loss: 0.2973 - val_loss: 0.4487\n",
      "Epoch 90/100\n",
      "6091/6091 [==============================] - 1s 139us/sample - loss: 0.2959 - val_loss: 0.4487\n",
      "Predicting...\n",
      "(Took 79.385 sec)\n",
      "Train on 6091 samples, validate on 1522 samples\n",
      "Epoch 1/100\n",
      "6091/6091 [==============================] - 1s 184us/sample - loss: 0.6833 - val_loss: 0.6759\n",
      "Epoch 2/100\n",
      "6091/6091 [==============================] - 1s 138us/sample - loss: 0.6639 - val_loss: 0.6628\n",
      "Epoch 3/100\n",
      "6091/6091 [==============================] - 1s 137us/sample - loss: 0.6481 - val_loss: 0.6515\n",
      "Epoch 4/100\n",
      "6091/6091 [==============================] - 1s 138us/sample - loss: 0.6341 - val_loss: 0.6412\n",
      "Epoch 5/100\n",
      "6091/6091 [==============================] - 1s 138us/sample - loss: 0.6211 - val_loss: 0.6313\n",
      "Epoch 6/100\n",
      "6091/6091 [==============================] - 1s 137us/sample - loss: 0.6088 - val_loss: 0.6221\n",
      "Epoch 7/100\n",
      "6091/6091 [==============================] - 1s 138us/sample - loss: 0.5972 - val_loss: 0.6133\n",
      "Epoch 8/100\n",
      "6091/6091 [==============================] - 1s 138us/sample - loss: 0.5862 - val_loss: 0.6050\n",
      "Epoch 9/100\n",
      "6091/6091 [==============================] - 1s 140us/sample - loss: 0.5757 - val_loss: 0.5972\n",
      "Epoch 10/100\n",
      "6091/6091 [==============================] - 1s 138us/sample - loss: 0.5657 - val_loss: 0.5899\n",
      "Epoch 11/100\n",
      "6091/6091 [==============================] - 1s 137us/sample - loss: 0.5562 - val_loss: 0.5829\n",
      "Epoch 12/100\n",
      "6091/6091 [==============================] - 1s 139us/sample - loss: 0.5471 - val_loss: 0.5764\n",
      "Epoch 13/100\n",
      "6091/6091 [==============================] - 1s 138us/sample - loss: 0.5384 - val_loss: 0.5701\n",
      "Epoch 14/100\n",
      "6091/6091 [==============================] - 1s 138us/sample - loss: 0.5301 - val_loss: 0.5641\n",
      "Epoch 15/100\n",
      "6091/6091 [==============================] - 1s 139us/sample - loss: 0.5222 - val_loss: 0.5585\n",
      "Epoch 16/100\n",
      "6091/6091 [==============================] - 1s 139us/sample - loss: 0.5146 - val_loss: 0.5531\n",
      "Epoch 17/100\n",
      "6091/6091 [==============================] - 1s 139us/sample - loss: 0.5074 - val_loss: 0.5481\n",
      "Epoch 18/100\n",
      "6091/6091 [==============================] - 1s 138us/sample - loss: 0.5004 - val_loss: 0.5434\n",
      "Epoch 19/100\n",
      "6091/6091 [==============================] - 1s 139us/sample - loss: 0.4938 - val_loss: 0.5388\n",
      "Epoch 20/100\n",
      "6091/6091 [==============================] - 1s 140us/sample - loss: 0.4873 - val_loss: 0.5346\n",
      "Epoch 21/100\n",
      "6091/6091 [==============================] - 1s 146us/sample - loss: 0.4812 - val_loss: 0.5306\n",
      "Epoch 22/100\n",
      "6091/6091 [==============================] - 1s 140us/sample - loss: 0.4752 - val_loss: 0.5267\n",
      "Epoch 23/100\n",
      "6091/6091 [==============================] - 1s 140us/sample - loss: 0.4695 - val_loss: 0.5230\n",
      "Epoch 24/100\n",
      "6091/6091 [==============================] - 1s 138us/sample - loss: 0.4640 - val_loss: 0.5196\n",
      "Epoch 25/100\n",
      "6091/6091 [==============================] - 1s 139us/sample - loss: 0.4587 - val_loss: 0.5163\n",
      "Epoch 26/100\n",
      "6091/6091 [==============================] - 1s 138us/sample - loss: 0.4536 - val_loss: 0.5131\n",
      "Epoch 27/100\n",
      "6091/6091 [==============================] - 1s 138us/sample - loss: 0.4487 - val_loss: 0.5102\n",
      "Epoch 28/100\n",
      "6091/6091 [==============================] - 1s 139us/sample - loss: 0.4439 - val_loss: 0.5074\n",
      "Epoch 29/100\n",
      "6091/6091 [==============================] - 1s 139us/sample - loss: 0.4393 - val_loss: 0.5048\n",
      "Epoch 30/100\n",
      "6091/6091 [==============================] - 1s 139us/sample - loss: 0.4348 - val_loss: 0.5022\n",
      "Epoch 31/100\n",
      "6091/6091 [==============================] - 1s 140us/sample - loss: 0.4305 - val_loss: 0.4998\n",
      "Epoch 32/100\n",
      "6091/6091 [==============================] - 1s 139us/sample - loss: 0.4263 - val_loss: 0.4975\n",
      "Epoch 33/100\n",
      "6091/6091 [==============================] - 1s 139us/sample - loss: 0.4222 - val_loss: 0.4953\n",
      "Epoch 34/100\n",
      "6091/6091 [==============================] - 1s 140us/sample - loss: 0.4183 - val_loss: 0.4932\n",
      "Epoch 35/100\n",
      "6091/6091 [==============================] - 1s 139us/sample - loss: 0.4145 - val_loss: 0.4912\n",
      "Epoch 36/100\n",
      "6091/6091 [==============================] - 1s 141us/sample - loss: 0.4108 - val_loss: 0.4893\n",
      "Epoch 37/100\n",
      "6091/6091 [==============================] - 1s 139us/sample - loss: 0.4072 - val_loss: 0.4876\n",
      "Epoch 38/100\n",
      "6091/6091 [==============================] - 1s 138us/sample - loss: 0.4037 - val_loss: 0.4859\n",
      "Epoch 39/100\n",
      "6091/6091 [==============================] - 1s 138us/sample - loss: 0.4003 - val_loss: 0.4843\n",
      "Epoch 40/100\n",
      "6091/6091 [==============================] - 1s 138us/sample - loss: 0.3970 - val_loss: 0.4827\n",
      "Epoch 41/100\n",
      "6091/6091 [==============================] - 1s 139us/sample - loss: 0.3938 - val_loss: 0.4813\n",
      "Epoch 42/100\n",
      "6091/6091 [==============================] - 1s 139us/sample - loss: 0.3906 - val_loss: 0.4799\n",
      "Epoch 43/100\n",
      "6091/6091 [==============================] - 1s 140us/sample - loss: 0.3876 - val_loss: 0.4787\n",
      "Epoch 44/100\n",
      "6091/6091 [==============================] - 1s 148us/sample - loss: 0.3846 - val_loss: 0.4774\n",
      "Epoch 45/100\n",
      "6091/6091 [==============================] - 1s 151us/sample - loss: 0.3817 - val_loss: 0.4762\n",
      "Epoch 46/100\n",
      "6091/6091 [==============================] - 1s 144us/sample - loss: 0.3788 - val_loss: 0.4751\n",
      "Epoch 47/100\n",
      "6091/6091 [==============================] - 1s 147us/sample - loss: 0.3761 - val_loss: 0.4740\n",
      "Epoch 48/100\n",
      "6091/6091 [==============================] - 1s 142us/sample - loss: 0.3734 - val_loss: 0.4730\n",
      "Epoch 49/100\n",
      "6091/6091 [==============================] - 1s 145us/sample - loss: 0.3707 - val_loss: 0.4720\n",
      "Epoch 50/100\n",
      "6091/6091 [==============================] - 1s 138us/sample - loss: 0.3681 - val_loss: 0.4711\n",
      "Epoch 51/100\n",
      "6091/6091 [==============================] - 1s 140us/sample - loss: 0.3656 - val_loss: 0.4703\n",
      "Epoch 52/100\n",
      "6091/6091 [==============================] - 1s 138us/sample - loss: 0.3631 - val_loss: 0.4695\n",
      "Epoch 53/100\n",
      "6091/6091 [==============================] - 1s 139us/sample - loss: 0.3607 - val_loss: 0.4688\n",
      "Epoch 54/100\n",
      "6091/6091 [==============================] - 1s 139us/sample - loss: 0.3583 - val_loss: 0.4681\n",
      "Epoch 55/100\n",
      "6091/6091 [==============================] - 1s 138us/sample - loss: 0.3560 - val_loss: 0.4673\n",
      "Epoch 56/100\n",
      "6091/6091 [==============================] - 1s 140us/sample - loss: 0.3537 - val_loss: 0.4667\n",
      "Epoch 57/100\n",
      "6091/6091 [==============================] - 1s 140us/sample - loss: 0.3515 - val_loss: 0.4661\n",
      "Epoch 58/100\n",
      "6091/6091 [==============================] - 1s 145us/sample - loss: 0.3493 - val_loss: 0.4656\n",
      "Epoch 59/100\n",
      "6091/6091 [==============================] - 1s 140us/sample - loss: 0.3472 - val_loss: 0.4651\n",
      "Epoch 60/100\n",
      "6091/6091 [==============================] - 1s 140us/sample - loss: 0.3450 - val_loss: 0.4646\n",
      "Epoch 61/100\n",
      "6091/6091 [==============================] - 1s 140us/sample - loss: 0.3430 - val_loss: 0.4641\n",
      "Epoch 62/100\n",
      "6091/6091 [==============================] - 1s 139us/sample - loss: 0.3410 - val_loss: 0.4637\n",
      "Epoch 63/100\n",
      "6091/6091 [==============================] - 1s 139us/sample - loss: 0.3390 - val_loss: 0.4632\n",
      "Epoch 64/100\n",
      "6091/6091 [==============================] - 1s 138us/sample - loss: 0.3370 - val_loss: 0.4629\n",
      "Epoch 65/100\n",
      "6091/6091 [==============================] - 1s 139us/sample - loss: 0.3351 - val_loss: 0.4625\n",
      "Epoch 66/100\n",
      "6091/6091 [==============================] - 1s 140us/sample - loss: 0.3332 - val_loss: 0.4622\n",
      "Epoch 67/100\n",
      "6091/6091 [==============================] - 1s 139us/sample - loss: 0.3314 - val_loss: 0.4619\n",
      "Epoch 68/100\n",
      "6091/6091 [==============================] - 1s 140us/sample - loss: 0.3295 - val_loss: 0.4616\n",
      "Epoch 69/100\n",
      "6091/6091 [==============================] - 1s 139us/sample - loss: 0.3277 - val_loss: 0.4614\n",
      "Epoch 70/100\n",
      "6091/6091 [==============================] - 1s 140us/sample - loss: 0.3260 - val_loss: 0.4612\n",
      "Epoch 71/100\n",
      "6091/6091 [==============================] - 1s 140us/sample - loss: 0.3243 - val_loss: 0.4610\n",
      "Epoch 72/100\n",
      "6091/6091 [==============================] - 1s 141us/sample - loss: 0.3225 - val_loss: 0.4607\n",
      "Epoch 73/100\n",
      "6091/6091 [==============================] - 1s 138us/sample - loss: 0.3209 - val_loss: 0.4606\n",
      "Epoch 74/100\n",
      "6091/6091 [==============================] - 1s 138us/sample - loss: 0.3192 - val_loss: 0.4605\n",
      "Epoch 75/100\n",
      "6091/6091 [==============================] - 1s 140us/sample - loss: 0.3176 - val_loss: 0.4604\n",
      "Epoch 76/100\n",
      "6091/6091 [==============================] - 1s 140us/sample - loss: 0.3160 - val_loss: 0.4602\n",
      "Epoch 77/100\n",
      "6091/6091 [==============================] - 1s 139us/sample - loss: 0.3144 - val_loss: 0.4601\n",
      "Epoch 78/100\n",
      "6091/6091 [==============================] - 1s 139us/sample - loss: 0.3128 - val_loss: 0.4601\n",
      "Epoch 79/100\n",
      "6091/6091 [==============================] - 1s 140us/sample - loss: 0.3113 - val_loss: 0.4601\n",
      "Predicting...\n",
      "(Took 68.500 sec)\n",
      "Combined confusion matrix:\n",
      "[[3841.  501.]\n",
      " [1046. 2225.]]\n",
      "(Overall, took 369.565 sec)\n",
      "Accuracy: 79.68% +/- 1.18%\n",
      "Precision for positive class: 78.61% +/- 2.20%\n",
      "Precision for negative class: 81.63% +/- 1.26%\n",
      "Recall for positive class: 88.46% +/- 1.14%\n",
      "Recall for negative class: 68.04% +/- 3.10%\n",
      "F for positive class: 83.23% +/- 1.13%\n",
      "F for negative class: 74.19% +/- 1.59%\n",
      "Mean F score: 78.71% +/- 1.20%\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/100\n",
      "6090/6090 [==============================] - 1s 152us/sample - loss: 0.6694 - val_loss: 0.6474\n",
      "Epoch 2/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.6219 - val_loss: 0.6117\n",
      "Epoch 3/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.5847 - val_loss: 0.5843\n",
      "Epoch 4/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.5548 - val_loss: 0.5624\n",
      "Epoch 5/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.5302 - val_loss: 0.5447\n",
      "Epoch 6/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.5096 - val_loss: 0.5302\n",
      "Epoch 7/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4922 - val_loss: 0.5182\n",
      "Epoch 8/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4772 - val_loss: 0.5081\n",
      "Epoch 9/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4640 - val_loss: 0.4997\n",
      "Epoch 10/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.4524 - val_loss: 0.4925\n",
      "Epoch 11/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4421 - val_loss: 0.4863\n",
      "Epoch 12/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4327 - val_loss: 0.4810\n",
      "Epoch 13/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4242 - val_loss: 0.4764\n",
      "Epoch 14/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4165 - val_loss: 0.4725\n",
      "Epoch 15/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4095 - val_loss: 0.4690\n",
      "Epoch 16/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4029 - val_loss: 0.4664\n",
      "Epoch 17/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.3968 - val_loss: 0.4638\n",
      "Epoch 18/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.3912 - val_loss: 0.4614\n",
      "Epoch 19/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.3858 - val_loss: 0.4597\n",
      "Epoch 20/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.3809 - val_loss: 0.4580\n",
      "Epoch 21/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.3762 - val_loss: 0.4567\n",
      "Epoch 22/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.3717 - val_loss: 0.4557\n",
      "Epoch 23/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.3675 - val_loss: 0.4547\n",
      "Epoch 24/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.3635 - val_loss: 0.4539\n",
      "Epoch 25/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.3598 - val_loss: 0.4533\n",
      "Epoch 26/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.3561 - val_loss: 0.4528\n",
      "Epoch 27/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.3527 - val_loss: 0.4526\n",
      "Epoch 28/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.3494 - val_loss: 0.4525\n",
      "Epoch 29/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.3462 - val_loss: 0.4523\n",
      "Epoch 30/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.3431 - val_loss: 0.4524\n",
      "Predicting...\n",
      "(Took 20.358 sec)\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/100\n",
      "6090/6090 [==============================] - 1s 158us/sample - loss: 0.6706 - val_loss: 0.6450\n",
      "Epoch 2/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.6235 - val_loss: 0.6098\n",
      "Epoch 3/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.5862 - val_loss: 0.5822\n",
      "Epoch 4/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.5563 - val_loss: 0.5610\n",
      "Epoch 5/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.5317 - val_loss: 0.5434\n",
      "Epoch 6/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.5111 - val_loss: 0.5294\n",
      "Epoch 7/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.4936 - val_loss: 0.5177\n",
      "Epoch 8/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4785 - val_loss: 0.5076\n",
      "Epoch 9/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4654 - val_loss: 0.4994\n",
      "Epoch 10/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4537 - val_loss: 0.4924\n",
      "Epoch 11/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4434 - val_loss: 0.4863\n",
      "Epoch 12/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4341 - val_loss: 0.4812\n",
      "Epoch 13/100\n",
      "6090/6090 [==============================] - 1s 122us/sample - loss: 0.4257 - val_loss: 0.4766\n",
      "Epoch 14/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.4180 - val_loss: 0.4728\n",
      "Epoch 15/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.4109 - val_loss: 0.4694\n",
      "Epoch 16/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.4045 - val_loss: 0.4665\n",
      "Epoch 17/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.3984 - val_loss: 0.4639\n",
      "Epoch 18/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.3929 - val_loss: 0.4618\n",
      "Epoch 19/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.3877 - val_loss: 0.4599\n",
      "Epoch 20/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.3828 - val_loss: 0.4585\n",
      "Epoch 21/100\n",
      "6090/6090 [==============================] - 1s 122us/sample - loss: 0.3781 - val_loss: 0.4570\n",
      "Epoch 22/100\n",
      "6090/6090 [==============================] - 1s 118us/sample - loss: 0.3738 - val_loss: 0.4558\n",
      "Epoch 23/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.3697 - val_loss: 0.4548\n",
      "Epoch 24/100\n",
      "6090/6090 [==============================] - 1s 125us/sample - loss: 0.3658 - val_loss: 0.4542\n",
      "Epoch 25/100\n",
      "6090/6090 [==============================] - 1s 118us/sample - loss: 0.3621 - val_loss: 0.4536\n",
      "Epoch 26/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.3585 - val_loss: 0.4532\n",
      "Epoch 27/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.3551 - val_loss: 0.4529\n",
      "Epoch 28/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.3519 - val_loss: 0.4527\n",
      "Epoch 29/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.3488 - val_loss: 0.4527\n",
      "Epoch 30/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.3458 - val_loss: 0.4527\n",
      "Epoch 31/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.3430 - val_loss: 0.4526\n",
      "Epoch 32/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.3402 - val_loss: 0.4527\n",
      "Predicting...\n",
      "(Took 22.407 sec)\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/100\n",
      "6090/6090 [==============================] - 1s 152us/sample - loss: 0.6708 - val_loss: 0.6488\n",
      "Epoch 2/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.6229 - val_loss: 0.6142\n",
      "Epoch 3/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.5853 - val_loss: 0.5871\n",
      "Epoch 4/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.5552 - val_loss: 0.5657\n",
      "Epoch 5/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.5305 - val_loss: 0.5484\n",
      "Epoch 6/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.5098 - val_loss: 0.5344\n",
      "Epoch 7/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4923 - val_loss: 0.5226\n",
      "Epoch 8/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.4772 - val_loss: 0.5129\n",
      "Epoch 9/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.4640 - val_loss: 0.5048\n",
      "Epoch 10/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4523 - val_loss: 0.4977\n",
      "Epoch 11/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4419 - val_loss: 0.4917\n",
      "Epoch 12/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.4325 - val_loss: 0.4866\n",
      "Epoch 13/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4240 - val_loss: 0.4825\n",
      "Epoch 14/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4163 - val_loss: 0.4788\n",
      "Epoch 15/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4091 - val_loss: 0.4756\n",
      "Epoch 16/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4026 - val_loss: 0.4727\n",
      "Epoch 17/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.3965 - val_loss: 0.4705\n",
      "Epoch 18/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.3908 - val_loss: 0.4685\n",
      "Epoch 19/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.3855 - val_loss: 0.4667\n",
      "Epoch 20/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.3805 - val_loss: 0.4655\n",
      "Epoch 21/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.3757 - val_loss: 0.4642\n",
      "Epoch 22/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.3714 - val_loss: 0.4633\n",
      "Epoch 23/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.3672 - val_loss: 0.4626\n",
      "Epoch 24/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.3631 - val_loss: 0.4619\n",
      "Epoch 25/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.3594 - val_loss: 0.4617\n",
      "Epoch 26/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.3557 - val_loss: 0.4612\n",
      "Epoch 27/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.3523 - val_loss: 0.4612\n",
      "Epoch 28/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.3489 - val_loss: 0.4611\n",
      "Epoch 29/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.3458 - val_loss: 0.4613\n",
      "Predicting...\n",
      "(Took 19.680 sec)\n",
      "Train on 6091 samples, validate on 1522 samples\n",
      "Epoch 1/100\n",
      "6091/6091 [==============================] - 1s 152us/sample - loss: 0.6694 - val_loss: 0.6463\n",
      "Epoch 2/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.6226 - val_loss: 0.6098\n",
      "Epoch 3/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.5859 - val_loss: 0.5816\n",
      "Epoch 4/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.5563 - val_loss: 0.5593\n",
      "Epoch 5/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.5318 - val_loss: 0.5411\n",
      "Epoch 6/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.5113 - val_loss: 0.5264\n",
      "Epoch 7/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.4940 - val_loss: 0.5141\n",
      "Epoch 8/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.4789 - val_loss: 0.5040\n",
      "Epoch 9/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4658 - val_loss: 0.4956\n",
      "Epoch 10/100\n",
      "6091/6091 [==============================] - 1s 114us/sample - loss: 0.4542 - val_loss: 0.4883\n",
      "Epoch 11/100\n",
      "6091/6091 [==============================] - 1s 110us/sample - loss: 0.4439 - val_loss: 0.4821\n",
      "Epoch 12/100\n",
      "6091/6091 [==============================] - 1s 110us/sample - loss: 0.4346 - val_loss: 0.4767\n",
      "Epoch 13/100\n",
      "6091/6091 [==============================] - 1s 113us/sample - loss: 0.4262 - val_loss: 0.4721\n",
      "Epoch 14/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.4185 - val_loss: 0.4683\n",
      "Epoch 15/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4115 - val_loss: 0.4650\n",
      "Epoch 16/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4049 - val_loss: 0.4621\n",
      "Epoch 17/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.3989 - val_loss: 0.4595\n",
      "Epoch 18/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.3933 - val_loss: 0.4571\n",
      "Epoch 19/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.3880 - val_loss: 0.4554\n",
      "Epoch 20/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.3831 - val_loss: 0.4539\n",
      "Epoch 21/100\n",
      "6091/6091 [==============================] - 1s 106us/sample - loss: 0.3784 - val_loss: 0.4527\n",
      "Epoch 22/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.3740 - val_loss: 0.4514\n",
      "Epoch 23/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.3699 - val_loss: 0.4505\n",
      "Epoch 24/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.3660 - val_loss: 0.4497\n",
      "Epoch 25/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.3621 - val_loss: 0.4491\n",
      "Epoch 26/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.3586 - val_loss: 0.4485\n",
      "Epoch 27/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.3551 - val_loss: 0.4481\n",
      "Epoch 28/100\n",
      "6091/6091 [==============================] - 1s 110us/sample - loss: 0.3519 - val_loss: 0.4478\n",
      "Epoch 29/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.3487 - val_loss: 0.4477\n",
      "Epoch 30/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.3457 - val_loss: 0.4476\n",
      "Epoch 31/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.3428 - val_loss: 0.4477\n",
      "Predicting...\n",
      "(Took 21.127 sec)\n",
      "Train on 6091 samples, validate on 1522 samples\n",
      "Epoch 1/100\n",
      "6091/6091 [==============================] - 1s 151us/sample - loss: 0.6692 - val_loss: 0.6481\n",
      "Epoch 2/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.6211 - val_loss: 0.6142\n",
      "Epoch 3/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.5836 - val_loss: 0.5877\n",
      "Epoch 4/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.5535 - val_loss: 0.5664\n",
      "Epoch 5/100\n",
      "6091/6091 [==============================] - 1s 106us/sample - loss: 0.5290 - val_loss: 0.5496\n",
      "Epoch 6/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.5083 - val_loss: 0.5358\n",
      "Epoch 7/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4908 - val_loss: 0.5241\n",
      "Epoch 8/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4757 - val_loss: 0.5144\n",
      "Epoch 9/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4625 - val_loss: 0.5064\n",
      "Epoch 10/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4508 - val_loss: 0.4994\n",
      "Epoch 11/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4404 - val_loss: 0.4935\n",
      "Epoch 12/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.4311 - val_loss: 0.4885\n",
      "Epoch 13/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4226 - val_loss: 0.4843\n",
      "Epoch 14/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.4149 - val_loss: 0.4805\n",
      "Epoch 15/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.4078 - val_loss: 0.4773\n",
      "Epoch 16/100\n",
      "6091/6091 [==============================] - 1s 106us/sample - loss: 0.4012 - val_loss: 0.4744\n",
      "Epoch 17/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.3951 - val_loss: 0.4721\n",
      "Epoch 18/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.3895 - val_loss: 0.4701\n",
      "Epoch 19/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.3842 - val_loss: 0.4682\n",
      "Epoch 20/100\n",
      "6091/6091 [==============================] - 1s 106us/sample - loss: 0.3793 - val_loss: 0.4666\n",
      "Epoch 21/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.3746 - val_loss: 0.4652\n",
      "Epoch 22/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.3702 - val_loss: 0.4642\n",
      "Epoch 23/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.3661 - val_loss: 0.4634\n",
      "Epoch 24/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.3621 - val_loss: 0.4627\n",
      "Epoch 25/100\n",
      "6091/6091 [==============================] - 1s 106us/sample - loss: 0.3584 - val_loss: 0.4620\n",
      "Epoch 26/100\n",
      "6091/6091 [==============================] - 1s 121us/sample - loss: 0.3548 - val_loss: 0.4616\n",
      "Epoch 27/100\n",
      "6091/6091 [==============================] - 1s 115us/sample - loss: 0.3514 - val_loss: 0.4613\n",
      "Epoch 28/100\n",
      "6091/6091 [==============================] - 1s 119us/sample - loss: 0.3482 - val_loss: 0.4608\n",
      "Epoch 29/100\n",
      "6091/6091 [==============================] - 1s 120us/sample - loss: 0.3450 - val_loss: 0.4607\n",
      "Epoch 30/100\n",
      "6091/6091 [==============================] - 1s 119us/sample - loss: 0.3420 - val_loss: 0.4609\n",
      "Predicting...\n",
      "(Took 20.695 sec)\n",
      "Combined confusion matrix:\n",
      "[[3846.  496.]\n",
      " [1037. 2234.]]\n",
      "(Overall, took 104.480 sec)\n",
      "Accuracy: 79.86% +/- 0.56%\n",
      "Precision for positive class: 78.76% +/- 1.35%\n",
      "Precision for negative class: 81.83% +/- 1.27%\n",
      "Recall for positive class: 88.57% +/- 0.97%\n",
      "Recall for negative class: 68.30% +/- 1.22%\n",
      "F for positive class: 83.37% +/- 0.83%\n",
      "F for negative class: 74.45% +/- 0.52%\n",
      "Mean F score: 78.91% +/- 0.38%\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/100\n",
      "6090/6090 [==============================] - 1s 164us/sample - loss: 0.6865 - val_loss: 0.6794\n",
      "Epoch 2/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.6745 - val_loss: 0.6699\n",
      "Epoch 3/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.6650 - val_loss: 0.6620\n",
      "Epoch 4/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.6566 - val_loss: 0.6551\n",
      "Epoch 5/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.6487 - val_loss: 0.6489\n",
      "Epoch 6/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.6413 - val_loss: 0.6429\n",
      "Epoch 7/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.6341 - val_loss: 0.6372\n",
      "Epoch 8/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.6271 - val_loss: 0.6317\n",
      "Epoch 9/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.6203 - val_loss: 0.6264\n",
      "Epoch 10/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.6137 - val_loss: 0.6212\n",
      "Epoch 11/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.6072 - val_loss: 0.6163\n",
      "Epoch 12/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.6010 - val_loss: 0.6115\n",
      "Epoch 13/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.5949 - val_loss: 0.6069\n",
      "Epoch 14/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.5890 - val_loss: 0.6025\n",
      "Epoch 15/100\n",
      "6090/6090 [==============================] - 1s 118us/sample - loss: 0.5833 - val_loss: 0.5981\n",
      "Epoch 16/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.5777 - val_loss: 0.5939\n",
      "Epoch 17/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.5723 - val_loss: 0.5899\n",
      "Epoch 18/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.5670 - val_loss: 0.5860\n",
      "Epoch 19/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.5618 - val_loss: 0.5822\n",
      "Epoch 20/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.5568 - val_loss: 0.5786\n",
      "Epoch 21/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.5519 - val_loss: 0.5750\n",
      "Epoch 22/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.5472 - val_loss: 0.5716\n",
      "Epoch 23/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.5425 - val_loss: 0.5683\n",
      "Epoch 24/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.5380 - val_loss: 0.5651\n",
      "Epoch 25/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.5336 - val_loss: 0.5620\n",
      "Epoch 26/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.5293 - val_loss: 0.5590\n",
      "Epoch 27/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.5251 - val_loss: 0.5560\n",
      "Epoch 28/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.5210 - val_loss: 0.5533\n",
      "Epoch 29/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.5170 - val_loss: 0.5505\n",
      "Epoch 30/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.5132 - val_loss: 0.5479\n",
      "Epoch 31/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.5094 - val_loss: 0.5454\n",
      "Epoch 32/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.5057 - val_loss: 0.5430\n",
      "Epoch 33/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.5021 - val_loss: 0.5406\n",
      "Epoch 34/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4985 - val_loss: 0.5383\n",
      "Epoch 35/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4951 - val_loss: 0.5361\n",
      "Epoch 36/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4918 - val_loss: 0.5340\n",
      "Epoch 37/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.4885 - val_loss: 0.5319\n",
      "Epoch 38/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4853 - val_loss: 0.5300\n",
      "Epoch 39/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.4822 - val_loss: 0.5281\n",
      "Epoch 40/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4791 - val_loss: 0.5263\n",
      "Epoch 41/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4762 - val_loss: 0.5244\n",
      "Epoch 42/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4733 - val_loss: 0.5227\n",
      "Epoch 43/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4704 - val_loss: 0.5210\n",
      "Epoch 44/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4676 - val_loss: 0.5194\n",
      "Epoch 45/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.4649 - val_loss: 0.5179\n",
      "Epoch 46/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.4623 - val_loss: 0.5164\n",
      "Epoch 47/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.4597 - val_loss: 0.5150\n",
      "Epoch 48/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.4572 - val_loss: 0.5136\n",
      "Epoch 49/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.4547 - val_loss: 0.5123\n",
      "Epoch 50/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.4523 - val_loss: 0.5110\n",
      "Epoch 51/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4499 - val_loss: 0.5097\n",
      "Epoch 52/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4476 - val_loss: 0.5086\n",
      "Epoch 53/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4453 - val_loss: 0.5074\n",
      "Epoch 54/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4431 - val_loss: 0.5063\n",
      "Epoch 55/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.4409 - val_loss: 0.5053\n",
      "Epoch 56/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.4387 - val_loss: 0.5042\n",
      "Epoch 57/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.4367 - val_loss: 0.5033\n",
      "Epoch 58/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.4346 - val_loss: 0.5023\n",
      "Epoch 59/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.4326 - val_loss: 0.5014\n",
      "Epoch 60/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4306 - val_loss: 0.5005\n",
      "Epoch 61/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4287 - val_loss: 0.4997\n",
      "Epoch 62/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4268 - val_loss: 0.4989\n",
      "Epoch 63/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.4249 - val_loss: 0.4981\n",
      "Epoch 64/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4231 - val_loss: 0.4973\n",
      "Epoch 65/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4214 - val_loss: 0.4966\n",
      "Epoch 66/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4196 - val_loss: 0.4959\n",
      "Epoch 67/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.4179 - val_loss: 0.4952\n",
      "Epoch 68/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.4162 - val_loss: 0.4946\n",
      "Epoch 69/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4145 - val_loss: 0.4940\n",
      "Epoch 70/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.4129 - val_loss: 0.4934\n",
      "Epoch 71/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.4113 - val_loss: 0.4929\n",
      "Epoch 72/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.4097 - val_loss: 0.4923\n",
      "Epoch 73/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.4082 - val_loss: 0.4918\n",
      "Epoch 74/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.4067 - val_loss: 0.4913\n",
      "Epoch 75/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.4052 - val_loss: 0.4908\n",
      "Epoch 76/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.4037 - val_loss: 0.4903\n",
      "Epoch 77/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.4023 - val_loss: 0.4900\n",
      "Epoch 78/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.4008 - val_loss: 0.4895\n",
      "Epoch 79/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.3994 - val_loss: 0.4891\n",
      "Epoch 80/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.3981 - val_loss: 0.4887\n",
      "Epoch 81/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.3967 - val_loss: 0.4884\n",
      "Epoch 82/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.3954 - val_loss: 0.4880\n",
      "Epoch 83/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.3941 - val_loss: 0.4877\n",
      "Epoch 84/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.3928 - val_loss: 0.4873\n",
      "Epoch 85/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.3915 - val_loss: 0.4871\n",
      "Epoch 86/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.3903 - val_loss: 0.4868\n",
      "Epoch 87/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.3891 - val_loss: 0.4866\n",
      "Epoch 88/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.3879 - val_loss: 0.4863\n",
      "Epoch 89/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.3867 - val_loss: 0.4861\n",
      "Epoch 90/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.3855 - val_loss: 0.4859\n",
      "Epoch 91/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.3843 - val_loss: 0.4857\n",
      "Epoch 92/100\n",
      "6090/6090 [==============================] - 1s 124us/sample - loss: 0.3832 - val_loss: 0.4855\n",
      "Epoch 93/100\n",
      "6090/6090 [==============================] - 1s 129us/sample - loss: 0.3821 - val_loss: 0.4853\n",
      "Epoch 94/100\n",
      "6090/6090 [==============================] - 1s 122us/sample - loss: 0.3810 - val_loss: 0.4852\n",
      "Epoch 95/100\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.3799 - val_loss: 0.4851\n",
      "Epoch 96/100\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.3788 - val_loss: 0.4849\n",
      "Epoch 97/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.3777 - val_loss: 0.4848\n",
      "Epoch 98/100\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.3767 - val_loss: 0.4846\n",
      "Epoch 99/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.3756 - val_loss: 0.4845\n",
      "Epoch 100/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.3746 - val_loss: 0.4845\n",
      "Predicting...\n",
      "(Took 67.868 sec)\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/100\n",
      "6090/6090 [==============================] - 1s 188us/sample - loss: 0.6869 - val_loss: 0.6785\n",
      "Epoch 2/100\n",
      "6090/6090 [==============================] - 1s 135us/sample - loss: 0.6753 - val_loss: 0.6682\n",
      "Epoch 3/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.6661 - val_loss: 0.6600\n",
      "Epoch 4/100\n",
      "6090/6090 [==============================] - 1s 130us/sample - loss: 0.6579 - val_loss: 0.6529\n",
      "Epoch 5/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.6502 - val_loss: 0.6463\n",
      "Epoch 6/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.6429 - val_loss: 0.6401\n",
      "Epoch 7/100\n",
      "6090/6090 [==============================] - 1s 135us/sample - loss: 0.6358 - val_loss: 0.6342\n",
      "Epoch 8/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.6288 - val_loss: 0.6284\n",
      "Epoch 9/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.6222 - val_loss: 0.6229\n",
      "Epoch 10/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.6156 - val_loss: 0.6176\n",
      "Epoch 11/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.6093 - val_loss: 0.6125\n",
      "Epoch 12/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.6031 - val_loss: 0.6075\n",
      "Epoch 13/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.5972 - val_loss: 0.6027\n",
      "Epoch 14/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.5913 - val_loss: 0.5981\n",
      "Epoch 15/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.5857 - val_loss: 0.5936\n",
      "Epoch 16/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.5802 - val_loss: 0.5893\n",
      "Epoch 17/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.5748 - val_loss: 0.5851\n",
      "Epoch 18/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.5695 - val_loss: 0.5809\n",
      "Epoch 19/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.5644 - val_loss: 0.5770\n",
      "Epoch 20/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.5595 - val_loss: 0.5732\n",
      "Epoch 21/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.5547 - val_loss: 0.5695\n",
      "Epoch 22/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.5500 - val_loss: 0.5660\n",
      "Epoch 23/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.5454 - val_loss: 0.5625\n",
      "Epoch 24/100\n",
      "6090/6090 [==============================] - 1s 117us/sample - loss: 0.5409 - val_loss: 0.5592\n",
      "Epoch 25/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.5365 - val_loss: 0.5558\n",
      "Epoch 26/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.5323 - val_loss: 0.5527\n",
      "Epoch 27/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.5282 - val_loss: 0.5497\n",
      "Epoch 28/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.5241 - val_loss: 0.5467\n",
      "Epoch 29/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.5202 - val_loss: 0.5439\n",
      "Epoch 30/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.5164 - val_loss: 0.5412\n",
      "Epoch 31/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.5126 - val_loss: 0.5384\n",
      "Epoch 32/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.5089 - val_loss: 0.5359\n",
      "Epoch 33/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.5054 - val_loss: 0.5334\n",
      "Epoch 34/100\n",
      "6090/6090 [==============================] - ETA: 0s - loss: 0.501 - 1s 109us/sample - loss: 0.5019 - val_loss: 0.5310\n",
      "Epoch 35/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4985 - val_loss: 0.5286\n",
      "Epoch 36/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4952 - val_loss: 0.5263\n",
      "Epoch 37/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.4920 - val_loss: 0.5242\n",
      "Epoch 38/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4888 - val_loss: 0.5222\n",
      "Epoch 39/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4857 - val_loss: 0.5200\n",
      "Epoch 40/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4827 - val_loss: 0.5181\n",
      "Epoch 41/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4797 - val_loss: 0.5162\n",
      "Epoch 42/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.4768 - val_loss: 0.5143\n",
      "Epoch 43/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.4740 - val_loss: 0.5125\n",
      "Epoch 44/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4713 - val_loss: 0.5108\n",
      "Epoch 45/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4686 - val_loss: 0.5091\n",
      "Epoch 46/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4660 - val_loss: 0.5075\n",
      "Epoch 47/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4634 - val_loss: 0.5060\n",
      "Epoch 48/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4609 - val_loss: 0.5045\n",
      "Epoch 49/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4584 - val_loss: 0.5030\n",
      "Epoch 50/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4560 - val_loss: 0.5016\n",
      "Epoch 51/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4536 - val_loss: 0.5003\n",
      "Epoch 52/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4513 - val_loss: 0.4990\n",
      "Epoch 53/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4491 - val_loss: 0.4977\n",
      "Epoch 54/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.4469 - val_loss: 0.4964\n",
      "Epoch 55/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.4447 - val_loss: 0.4953\n",
      "Epoch 56/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4426 - val_loss: 0.4942\n",
      "Epoch 57/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4405 - val_loss: 0.4931\n",
      "Epoch 58/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4385 - val_loss: 0.4920\n",
      "Epoch 59/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4365 - val_loss: 0.4910\n",
      "Epoch 60/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4346 - val_loss: 0.4900\n",
      "Epoch 61/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.4326 - val_loss: 0.4891\n",
      "Epoch 62/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4308 - val_loss: 0.4882\n",
      "Epoch 63/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.4289 - val_loss: 0.4873\n",
      "Epoch 64/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4271 - val_loss: 0.4864\n",
      "Epoch 65/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4253 - val_loss: 0.4856\n",
      "Epoch 66/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4236 - val_loss: 0.4848\n",
      "Epoch 67/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4219 - val_loss: 0.4840\n",
      "Epoch 68/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4202 - val_loss: 0.4833\n",
      "Epoch 69/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4186 - val_loss: 0.4826\n",
      "Epoch 70/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4170 - val_loss: 0.4820\n",
      "Epoch 71/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4154 - val_loss: 0.4814\n",
      "Epoch 72/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4138 - val_loss: 0.4807\n",
      "Epoch 73/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4123 - val_loss: 0.4801\n",
      "Epoch 74/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4108 - val_loss: 0.4795\n",
      "Epoch 75/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4093 - val_loss: 0.4790\n",
      "Epoch 76/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4078 - val_loss: 0.4785\n",
      "Epoch 77/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4064 - val_loss: 0.4780\n",
      "Epoch 78/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4050 - val_loss: 0.4775\n",
      "Epoch 79/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4036 - val_loss: 0.4770\n",
      "Epoch 80/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4022 - val_loss: 0.4766\n",
      "Epoch 81/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4009 - val_loss: 0.4761\n",
      "Epoch 82/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.3996 - val_loss: 0.4757\n",
      "Epoch 83/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.3983 - val_loss: 0.4753\n",
      "Epoch 84/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.3970 - val_loss: 0.4748\n",
      "Epoch 85/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.3957 - val_loss: 0.4746\n",
      "Epoch 86/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.3945 - val_loss: 0.4742\n",
      "Epoch 87/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.3933 - val_loss: 0.4738\n",
      "Epoch 88/100\n",
      "6090/6090 [==============================] - 1s 121us/sample - loss: 0.3921 - val_loss: 0.4736\n",
      "Epoch 89/100\n",
      "6090/6090 [==============================] - 1s 120us/sample - loss: 0.3909 - val_loss: 0.4732\n",
      "Epoch 90/100\n",
      "6090/6090 [==============================] - 1s 116us/sample - loss: 0.3897 - val_loss: 0.4730\n",
      "Epoch 91/100\n",
      "6090/6090 [==============================] - 1s 119us/sample - loss: 0.3886 - val_loss: 0.4727\n",
      "Epoch 92/100\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.3874 - val_loss: 0.4724\n",
      "Epoch 93/100\n",
      "6090/6090 [==============================] - 1s 116us/sample - loss: 0.3863 - val_loss: 0.4722\n",
      "Epoch 94/100\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.3852 - val_loss: 0.4719\n",
      "Epoch 95/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.3842 - val_loss: 0.4717\n",
      "Epoch 96/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.3831 - val_loss: 0.4715\n",
      "Epoch 97/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.3820 - val_loss: 0.4713\n",
      "Epoch 98/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.3810 - val_loss: 0.4711\n",
      "Epoch 99/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.3799 - val_loss: 0.4710\n",
      "Epoch 100/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.3789 - val_loss: 0.4709\n",
      "Predicting...\n",
      "(Took 68.269 sec)\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/100\n",
      "6090/6090 [==============================] - 1s 155us/sample - loss: 0.6857 - val_loss: 0.6819\n",
      "Epoch 2/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.6731 - val_loss: 0.6741\n",
      "Epoch 3/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.6634 - val_loss: 0.6677\n",
      "Epoch 4/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.6550 - val_loss: 0.6617\n",
      "Epoch 5/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.6471 - val_loss: 0.6560\n",
      "Epoch 6/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.6396 - val_loss: 0.6504\n",
      "Epoch 7/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.6323 - val_loss: 0.6450\n",
      "Epoch 8/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.6253 - val_loss: 0.6397\n",
      "Epoch 9/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.6185 - val_loss: 0.6346\n",
      "Epoch 10/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.6119 - val_loss: 0.6296\n",
      "Epoch 11/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.6055 - val_loss: 0.6248\n",
      "Epoch 12/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.5993 - val_loss: 0.6201\n",
      "Epoch 13/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.5932 - val_loss: 0.6156\n",
      "Epoch 14/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.5873 - val_loss: 0.6113\n",
      "Epoch 15/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.5815 - val_loss: 0.6070\n",
      "Epoch 16/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.5760 - val_loss: 0.6029\n",
      "Epoch 17/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.5705 - val_loss: 0.5990\n",
      "Epoch 18/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.5653 - val_loss: 0.5951\n",
      "Epoch 19/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.5601 - val_loss: 0.5914\n",
      "Epoch 20/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.5551 - val_loss: 0.5878\n",
      "Epoch 21/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.5503 - val_loss: 0.5842\n",
      "Epoch 22/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.5455 - val_loss: 0.5809\n",
      "Epoch 23/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.5409 - val_loss: 0.5776\n",
      "Epoch 24/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.5364 - val_loss: 0.5745\n",
      "Epoch 25/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.5320 - val_loss: 0.5714\n",
      "Epoch 26/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.5277 - val_loss: 0.5684\n",
      "Epoch 27/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.5236 - val_loss: 0.5656\n",
      "Epoch 28/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.5195 - val_loss: 0.5628\n",
      "Epoch 29/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.5156 - val_loss: 0.5601\n",
      "Epoch 30/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.5117 - val_loss: 0.5575\n",
      "Epoch 31/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.5080 - val_loss: 0.5550\n",
      "Epoch 32/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.5043 - val_loss: 0.5526\n",
      "Epoch 33/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.5007 - val_loss: 0.5502\n",
      "Epoch 34/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.4973 - val_loss: 0.5480\n",
      "Epoch 35/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.4939 - val_loss: 0.5457\n",
      "Epoch 36/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.4906 - val_loss: 0.5436\n",
      "Epoch 37/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.4873 - val_loss: 0.5415\n",
      "Epoch 38/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4841 - val_loss: 0.5395\n",
      "Epoch 39/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.4811 - val_loss: 0.5375\n",
      "Epoch 40/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.4781 - val_loss: 0.5357\n",
      "Epoch 41/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.4751 - val_loss: 0.5340\n",
      "Epoch 42/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.4722 - val_loss: 0.5322\n",
      "Epoch 43/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.4695 - val_loss: 0.5305\n",
      "Epoch 44/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.4667 - val_loss: 0.5289\n",
      "Epoch 45/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.4640 - val_loss: 0.5272\n",
      "Epoch 46/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4614 - val_loss: 0.5257\n",
      "Epoch 47/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4588 - val_loss: 0.5243\n",
      "Epoch 48/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4563 - val_loss: 0.5228\n",
      "Epoch 49/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4539 - val_loss: 0.5214\n",
      "Epoch 50/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4515 - val_loss: 0.5201\n",
      "Epoch 51/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.4492 - val_loss: 0.5188\n",
      "Epoch 52/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.4469 - val_loss: 0.5176\n",
      "Epoch 53/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4446 - val_loss: 0.5164\n",
      "Epoch 54/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4424 - val_loss: 0.5152\n",
      "Epoch 55/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.4402 - val_loss: 0.5140\n",
      "Epoch 56/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.4381 - val_loss: 0.5129\n",
      "Epoch 57/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4361 - val_loss: 0.5119\n",
      "Epoch 58/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.4340 - val_loss: 0.5109\n",
      "Epoch 59/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4321 - val_loss: 0.5099\n",
      "Epoch 60/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4301 - val_loss: 0.5090\n",
      "Epoch 61/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4282 - val_loss: 0.5081\n",
      "Epoch 62/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4263 - val_loss: 0.5071\n",
      "Epoch 63/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4245 - val_loss: 0.5063\n",
      "Epoch 64/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4227 - val_loss: 0.5055\n",
      "Epoch 65/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4210 - val_loss: 0.5047\n",
      "Epoch 66/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4192 - val_loss: 0.5040\n",
      "Epoch 67/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4175 - val_loss: 0.5033\n",
      "Epoch 68/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4159 - val_loss: 0.5025\n",
      "Epoch 69/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4142 - val_loss: 0.5018\n",
      "Epoch 70/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4126 - val_loss: 0.5011\n",
      "Epoch 71/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.4110 - val_loss: 0.5005\n",
      "Epoch 72/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4095 - val_loss: 0.4998\n",
      "Epoch 73/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.4080 - val_loss: 0.4993\n",
      "Epoch 74/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4065 - val_loss: 0.4987\n",
      "Epoch 75/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4050 - val_loss: 0.4981\n",
      "Epoch 76/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4035 - val_loss: 0.4976\n",
      "Epoch 77/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4021 - val_loss: 0.4971\n",
      "Epoch 78/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4007 - val_loss: 0.4966\n",
      "Epoch 79/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.3994 - val_loss: 0.4962\n",
      "Epoch 80/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.3980 - val_loss: 0.4957\n",
      "Epoch 81/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.3966 - val_loss: 0.4953\n",
      "Epoch 82/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.3953 - val_loss: 0.4949\n",
      "Epoch 83/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.3941 - val_loss: 0.4945\n",
      "Epoch 84/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.3928 - val_loss: 0.4941\n",
      "Epoch 85/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.3915 - val_loss: 0.4937\n",
      "Epoch 86/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.3903 - val_loss: 0.4933\n",
      "Epoch 87/100\n",
      "6090/6090 [==============================] - 1s 118us/sample - loss: 0.3891 - val_loss: 0.4930\n",
      "Epoch 88/100\n",
      "6090/6090 [==============================] - 1s 120us/sample - loss: 0.3879 - val_loss: 0.4927\n",
      "Epoch 89/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.3867 - val_loss: 0.4924\n",
      "Epoch 90/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.3855 - val_loss: 0.4921\n",
      "Epoch 91/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.3844 - val_loss: 0.4918\n",
      "Epoch 92/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.3833 - val_loss: 0.4915\n",
      "Epoch 93/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.3822 - val_loss: 0.4913\n",
      "Epoch 94/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.3811 - val_loss: 0.4911\n",
      "Epoch 95/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.3800 - val_loss: 0.4909\n",
      "Epoch 96/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.3789 - val_loss: 0.4906\n",
      "Epoch 97/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.3778 - val_loss: 0.4904\n",
      "Epoch 98/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.3769 - val_loss: 0.4903\n",
      "Epoch 99/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.3758 - val_loss: 0.4901\n",
      "Epoch 100/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.3748 - val_loss: 0.4899\n",
      "Predicting...\n",
      "(Took 66.810 sec)\n",
      "Train on 6091 samples, validate on 1522 samples\n",
      "Epoch 1/100\n",
      "6091/6091 [==============================] - 1s 151us/sample - loss: 0.6861 - val_loss: 0.6796\n",
      "Epoch 2/100\n",
      "6091/6091 [==============================] - 1s 115us/sample - loss: 0.6741 - val_loss: 0.6706\n",
      "Epoch 3/100\n",
      "6091/6091 [==============================] - 1s 117us/sample - loss: 0.6648 - val_loss: 0.6631\n",
      "Epoch 4/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.6565 - val_loss: 0.6564\n",
      "Epoch 5/100\n",
      "6091/6091 [==============================] - 1s 110us/sample - loss: 0.6488 - val_loss: 0.6501\n",
      "Epoch 6/100\n",
      "6091/6091 [==============================] - 1s 111us/sample - loss: 0.6414 - val_loss: 0.6441\n",
      "Epoch 7/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6091/6091 [==============================] - 1s 111us/sample - loss: 0.6343 - val_loss: 0.6383\n",
      "Epoch 8/100\n",
      "6091/6091 [==============================] - 1s 110us/sample - loss: 0.6274 - val_loss: 0.6328\n",
      "Epoch 9/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.6206 - val_loss: 0.6275\n",
      "Epoch 10/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.6141 - val_loss: 0.6223\n",
      "Epoch 11/100\n",
      "6091/6091 [==============================] - 1s 106us/sample - loss: 0.6078 - val_loss: 0.6172\n",
      "Epoch 12/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.6016 - val_loss: 0.6123\n",
      "Epoch 13/100\n",
      "6091/6091 [==============================] - 1s 106us/sample - loss: 0.5956 - val_loss: 0.6076\n",
      "Epoch 14/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.5898 - val_loss: 0.6030\n",
      "Epoch 15/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.5842 - val_loss: 0.5986\n",
      "Epoch 16/100\n",
      "6091/6091 [==============================] - 1s 110us/sample - loss: 0.5787 - val_loss: 0.5943\n",
      "Epoch 17/100\n",
      "6091/6091 [==============================] - 1s 132us/sample - loss: 0.5733 - val_loss: 0.5902\n",
      "Epoch 18/100\n",
      "6091/6091 [==============================] - 1s 132us/sample - loss: 0.5680 - val_loss: 0.5861\n",
      "Epoch 19/100\n",
      "6091/6091 [==============================] - 1s 133us/sample - loss: 0.5630 - val_loss: 0.5822\n",
      "Epoch 20/100\n",
      "6091/6091 [==============================] - 1s 136us/sample - loss: 0.5580 - val_loss: 0.5784\n",
      "Epoch 21/100\n",
      "6091/6091 [==============================] - 1s 133us/sample - loss: 0.5532 - val_loss: 0.5747\n",
      "Epoch 22/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.5485 - val_loss: 0.5711\n",
      "Epoch 23/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.5440 - val_loss: 0.5677\n",
      "Epoch 24/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.5395 - val_loss: 0.5643\n",
      "Epoch 25/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.5352 - val_loss: 0.5611\n",
      "Epoch 26/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.5310 - val_loss: 0.5580\n",
      "Epoch 27/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.5268 - val_loss: 0.5549\n",
      "Epoch 28/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.5228 - val_loss: 0.5520\n",
      "Epoch 29/100\n",
      "6091/6091 [==============================] - 1s 113us/sample - loss: 0.5189 - val_loss: 0.5492\n",
      "Epoch 30/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.5151 - val_loss: 0.5464\n",
      "Epoch 31/100\n",
      "6091/6091 [==============================] - 1s 110us/sample - loss: 0.5114 - val_loss: 0.5437\n",
      "Epoch 32/100\n",
      "6091/6091 [==============================] - 1s 116us/sample - loss: 0.5078 - val_loss: 0.5411\n",
      "Epoch 33/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.5042 - val_loss: 0.5386\n",
      "Epoch 34/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.5008 - val_loss: 0.5361\n",
      "Epoch 35/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.4974 - val_loss: 0.5338\n",
      "Epoch 36/100\n",
      "6091/6091 [==============================] - 1s 110us/sample - loss: 0.4942 - val_loss: 0.5315\n",
      "Epoch 37/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4909 - val_loss: 0.5293\n",
      "Epoch 38/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.4878 - val_loss: 0.5271\n",
      "Epoch 39/100\n",
      "6091/6091 [==============================] - 1s 110us/sample - loss: 0.4848 - val_loss: 0.5250\n",
      "Epoch 40/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4818 - val_loss: 0.5230\n",
      "Epoch 41/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4789 - val_loss: 0.5210\n",
      "Epoch 42/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4760 - val_loss: 0.5191\n",
      "Epoch 43/100\n",
      "6091/6091 [==============================] - 1s 106us/sample - loss: 0.4732 - val_loss: 0.5173\n",
      "Epoch 44/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4705 - val_loss: 0.5155\n",
      "Epoch 45/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.4679 - val_loss: 0.5138\n",
      "Epoch 46/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.4653 - val_loss: 0.5121\n",
      "Epoch 47/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.4627 - val_loss: 0.5104\n",
      "Epoch 48/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.4603 - val_loss: 0.5089\n",
      "Epoch 49/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.4578 - val_loss: 0.5074\n",
      "Epoch 50/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.4555 - val_loss: 0.5059\n",
      "Epoch 51/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4531 - val_loss: 0.5045\n",
      "Epoch 52/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.4509 - val_loss: 0.5031\n",
      "Epoch 53/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4486 - val_loss: 0.5018\n",
      "Epoch 54/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.4464 - val_loss: 0.5005\n",
      "Epoch 55/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.4443 - val_loss: 0.4992\n",
      "Epoch 56/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.4422 - val_loss: 0.4980\n",
      "Epoch 57/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.4402 - val_loss: 0.4968\n",
      "Epoch 58/100\n",
      "6091/6091 [==============================] - 1s 106us/sample - loss: 0.4382 - val_loss: 0.4957\n",
      "Epoch 59/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4362 - val_loss: 0.4946\n",
      "Epoch 60/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.4342 - val_loss: 0.4935\n",
      "Epoch 61/100\n",
      "6091/6091 [==============================] - 1s 106us/sample - loss: 0.4323 - val_loss: 0.4925\n",
      "Epoch 62/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.4305 - val_loss: 0.4915\n",
      "Epoch 63/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.4287 - val_loss: 0.4906\n",
      "Epoch 64/100\n",
      "6091/6091 [==============================] - 1s 106us/sample - loss: 0.4269 - val_loss: 0.4895\n",
      "Epoch 65/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4252 - val_loss: 0.4887\n",
      "Epoch 66/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.4234 - val_loss: 0.4878\n",
      "Epoch 67/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.4217 - val_loss: 0.4870\n",
      "Epoch 68/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.4201 - val_loss: 0.4861\n",
      "Epoch 69/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.4184 - val_loss: 0.4854\n",
      "Epoch 70/100\n",
      "6091/6091 [==============================] - 1s 106us/sample - loss: 0.4169 - val_loss: 0.4845\n",
      "Epoch 71/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.4153 - val_loss: 0.4838\n",
      "Epoch 72/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4137 - val_loss: 0.4831\n",
      "Epoch 73/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.4123 - val_loss: 0.4824\n",
      "Epoch 74/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.4107 - val_loss: 0.4817\n",
      "Epoch 75/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4093 - val_loss: 0.4811\n",
      "Epoch 76/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.4078 - val_loss: 0.4805\n",
      "Epoch 77/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.4064 - val_loss: 0.4798\n",
      "Epoch 78/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4050 - val_loss: 0.4792\n",
      "Epoch 79/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4037 - val_loss: 0.4787\n",
      "Epoch 80/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4023 - val_loss: 0.4781\n",
      "Epoch 81/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.4010 - val_loss: 0.4775\n",
      "Epoch 82/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.3997 - val_loss: 0.4771\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.3984 - val_loss: 0.4766\n",
      "Epoch 84/100\n",
      "6091/6091 [==============================] - 1s 120us/sample - loss: 0.3971 - val_loss: 0.4761\n",
      "Epoch 85/100\n",
      "6091/6091 [==============================] - 1s 119us/sample - loss: 0.3959 - val_loss: 0.4757\n",
      "Epoch 86/100\n",
      "6091/6091 [==============================] - 1s 116us/sample - loss: 0.3947 - val_loss: 0.4752\n",
      "Epoch 87/100\n",
      "6091/6091 [==============================] - 1s 113us/sample - loss: 0.3935 - val_loss: 0.4747\n",
      "Epoch 88/100\n",
      "6091/6091 [==============================] - 1s 114us/sample - loss: 0.3923 - val_loss: 0.4743\n",
      "Epoch 89/100\n",
      "6091/6091 [==============================] - 1s 114us/sample - loss: 0.3911 - val_loss: 0.4739\n",
      "Epoch 90/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.3899 - val_loss: 0.4735\n",
      "Epoch 91/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.3888 - val_loss: 0.4732\n",
      "Epoch 92/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.3877 - val_loss: 0.4728\n",
      "Epoch 93/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.3866 - val_loss: 0.4725\n",
      "Epoch 94/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.3855 - val_loss: 0.4721\n",
      "Epoch 95/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.3844 - val_loss: 0.4718\n",
      "Epoch 96/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.3833 - val_loss: 0.4715\n",
      "Epoch 97/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.3823 - val_loss: 0.4712\n",
      "Epoch 98/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.3813 - val_loss: 0.4710\n",
      "Epoch 99/100\n",
      "6091/6091 [==============================] - ETA: 0s - loss: 0.381 - 1s 107us/sample - loss: 0.3802 - val_loss: 0.4708\n",
      "Epoch 100/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.3792 - val_loss: 0.4704\n",
      "Predicting...\n",
      "(Took 67.763 sec)\n",
      "Train on 6091 samples, validate on 1522 samples\n",
      "Epoch 1/100\n",
      "6091/6091 [==============================] - 1s 153us/sample - loss: 0.6860 - val_loss: 0.6816\n",
      "Epoch 2/100\n",
      "6091/6091 [==============================] - 1s 111us/sample - loss: 0.6735 - val_loss: 0.6734\n",
      "Epoch 3/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.6637 - val_loss: 0.6666\n",
      "Epoch 4/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.6553 - val_loss: 0.6604\n",
      "Epoch 5/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.6475 - val_loss: 0.6545\n",
      "Epoch 6/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.6400 - val_loss: 0.6487\n",
      "Epoch 7/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.6328 - val_loss: 0.6431\n",
      "Epoch 8/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.6259 - val_loss: 0.6375\n",
      "Epoch 9/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.6192 - val_loss: 0.6322\n",
      "Epoch 10/100\n",
      "6091/6091 [==============================] - 1s 111us/sample - loss: 0.6126 - val_loss: 0.6271\n",
      "Epoch 11/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.6062 - val_loss: 0.6221\n",
      "Epoch 12/100\n",
      "6091/6091 [==============================] - 1s 110us/sample - loss: 0.6000 - val_loss: 0.6172\n",
      "Epoch 13/100\n",
      "6091/6091 [==============================] - 1s 110us/sample - loss: 0.5940 - val_loss: 0.6126\n",
      "Epoch 14/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.5881 - val_loss: 0.6081\n",
      "Epoch 15/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.5824 - val_loss: 0.6037\n",
      "Epoch 16/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.5769 - val_loss: 0.5995\n",
      "Epoch 17/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.5715 - val_loss: 0.5954\n",
      "Epoch 18/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.5662 - val_loss: 0.5915\n",
      "Epoch 19/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.5611 - val_loss: 0.5876\n",
      "Epoch 20/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.5561 - val_loss: 0.5840\n",
      "Epoch 21/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.5513 - val_loss: 0.5803\n",
      "Epoch 22/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.5466 - val_loss: 0.5768\n",
      "Epoch 23/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.5420 - val_loss: 0.5735\n",
      "Epoch 24/100\n",
      "6091/6091 [==============================] - 1s 111us/sample - loss: 0.5375 - val_loss: 0.5702\n",
      "Epoch 25/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.5331 - val_loss: 0.5671\n",
      "Epoch 26/100\n",
      "6091/6091 [==============================] - 1s 110us/sample - loss: 0.5289 - val_loss: 0.5640\n",
      "Epoch 27/100\n",
      "6091/6091 [==============================] - 1s 111us/sample - loss: 0.5247 - val_loss: 0.5610\n",
      "Epoch 28/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.5207 - val_loss: 0.5582\n",
      "Epoch 29/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.5168 - val_loss: 0.5554\n",
      "Epoch 30/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.5129 - val_loss: 0.5527\n",
      "Epoch 31/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.5091 - val_loss: 0.5501\n",
      "Epoch 32/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.5055 - val_loss: 0.5476\n",
      "Epoch 33/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.5019 - val_loss: 0.5452\n",
      "Epoch 34/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4984 - val_loss: 0.5428\n",
      "Epoch 35/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.4950 - val_loss: 0.5406\n",
      "Epoch 36/100\n",
      "6091/6091 [==============================] - 1s 110us/sample - loss: 0.4917 - val_loss: 0.5384\n",
      "Epoch 37/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4885 - val_loss: 0.5363\n",
      "Epoch 38/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4853 - val_loss: 0.5342\n",
      "Epoch 39/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4822 - val_loss: 0.5322\n",
      "Epoch 40/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.4792 - val_loss: 0.5303\n",
      "Epoch 41/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4763 - val_loss: 0.5284\n",
      "Epoch 42/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4734 - val_loss: 0.5266\n",
      "Epoch 43/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4706 - val_loss: 0.5248\n",
      "Epoch 44/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4679 - val_loss: 0.5232\n",
      "Epoch 45/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4652 - val_loss: 0.5215\n",
      "Epoch 46/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.4625 - val_loss: 0.5200\n",
      "Epoch 47/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.4600 - val_loss: 0.5185\n",
      "Epoch 48/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4575 - val_loss: 0.5171\n",
      "Epoch 49/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4550 - val_loss: 0.5156\n",
      "Epoch 50/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4526 - val_loss: 0.5142\n",
      "Epoch 51/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.4502 - val_loss: 0.5129\n",
      "Epoch 52/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4479 - val_loss: 0.5116\n",
      "Epoch 53/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4457 - val_loss: 0.5103\n",
      "Epoch 54/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.4435 - val_loss: 0.5092\n",
      "Epoch 55/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.4413 - val_loss: 0.5080\n",
      "Epoch 56/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4392 - val_loss: 0.5069\n",
      "Epoch 57/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.4371 - val_loss: 0.5058\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.4351 - val_loss: 0.5048\n",
      "Epoch 59/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4331 - val_loss: 0.5038\n",
      "Epoch 60/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.4311 - val_loss: 0.5028\n",
      "Epoch 61/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4292 - val_loss: 0.5019\n",
      "Epoch 62/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4274 - val_loss: 0.5010\n",
      "Epoch 63/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4255 - val_loss: 0.5001\n",
      "Epoch 64/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4237 - val_loss: 0.4993\n",
      "Epoch 65/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4219 - val_loss: 0.4985\n",
      "Epoch 66/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4202 - val_loss: 0.4977\n",
      "Epoch 67/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.4185 - val_loss: 0.4969\n",
      "Epoch 68/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4168 - val_loss: 0.4962\n",
      "Epoch 69/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4152 - val_loss: 0.4955\n",
      "Epoch 70/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4136 - val_loss: 0.4948\n",
      "Epoch 71/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4120 - val_loss: 0.4942\n",
      "Epoch 72/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.4104 - val_loss: 0.4936\n",
      "Epoch 73/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.4089 - val_loss: 0.4929\n",
      "Epoch 74/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.4074 - val_loss: 0.4924\n",
      "Epoch 75/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4059 - val_loss: 0.4918\n",
      "Epoch 76/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4044 - val_loss: 0.4913\n",
      "Epoch 77/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.4030 - val_loss: 0.4908\n",
      "Epoch 78/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.4016 - val_loss: 0.4903\n",
      "Epoch 79/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4002 - val_loss: 0.4897\n",
      "Epoch 80/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.3988 - val_loss: 0.4893\n",
      "Epoch 81/100\n",
      "6091/6091 [==============================] - 1s 115us/sample - loss: 0.3975 - val_loss: 0.4889\n",
      "Epoch 82/100\n",
      "6091/6091 [==============================] - 1s 126us/sample - loss: 0.3962 - val_loss: 0.4885\n",
      "Epoch 83/100\n",
      "6091/6091 [==============================] - 1s 128us/sample - loss: 0.3949 - val_loss: 0.4880\n",
      "Epoch 84/100\n",
      "6091/6091 [==============================] - 1s 117us/sample - loss: 0.3936 - val_loss: 0.4877\n",
      "Epoch 85/100\n",
      "6091/6091 [==============================] - 1s 116us/sample - loss: 0.3923 - val_loss: 0.4873\n",
      "Epoch 86/100\n",
      "6091/6091 [==============================] - 1s 126us/sample - loss: 0.3911 - val_loss: 0.4869\n",
      "Epoch 87/100\n",
      "6091/6091 [==============================] - 1s 119us/sample - loss: 0.3899 - val_loss: 0.4866\n",
      "Epoch 88/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.3887 - val_loss: 0.4862\n",
      "Epoch 89/100\n",
      "6091/6091 [==============================] - 1s 106us/sample - loss: 0.3875 - val_loss: 0.4859\n",
      "Epoch 90/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.3863 - val_loss: 0.4856\n",
      "Epoch 91/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.3852 - val_loss: 0.4853\n",
      "Epoch 92/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.3840 - val_loss: 0.4850\n",
      "Epoch 93/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.3829 - val_loss: 0.4848\n",
      "Epoch 94/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.3818 - val_loss: 0.4845\n",
      "Epoch 95/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.3807 - val_loss: 0.4843\n",
      "Epoch 96/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.3797 - val_loss: 0.4841\n",
      "Epoch 97/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.3786 - val_loss: 0.4839\n",
      "Epoch 98/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.3775 - val_loss: 0.4837\n",
      "Epoch 99/100\n",
      "6091/6091 [==============================] - 1s 111us/sample - loss: 0.3765 - val_loss: 0.4835\n",
      "Epoch 100/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.3755 - val_loss: 0.4833\n",
      "Predicting...\n",
      "(Took 67.372 sec)\n",
      "Combined confusion matrix:\n",
      "[[3738.  604.]\n",
      " [1016. 2255.]]\n",
      "(Overall, took 338.339 sec)\n",
      "Accuracy: 78.72% +/- 0.39%\n",
      "Precision for positive class: 78.62% +/- 1.19%\n",
      "Precision for negative class: 78.87% +/- 1.31%\n",
      "Recall for positive class: 86.08% +/- 1.11%\n",
      "Recall for negative class: 68.93% +/- 1.53%\n",
      "F for positive class: 82.18% +/- 0.74%\n",
      "F for negative class: 73.56% +/- 0.82%\n",
      "Mean F score: 77.87% +/- 0.30%\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/100\n",
      "6090/6090 [==============================] - 1s 152us/sample - loss: 0.6815 - val_loss: 0.6699\n",
      "Epoch 2/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.6591 - val_loss: 0.6527\n",
      "Epoch 3/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.6400 - val_loss: 0.6382\n",
      "Epoch 4/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.6228 - val_loss: 0.6253\n",
      "Epoch 5/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.6071 - val_loss: 0.6136\n",
      "Epoch 6/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.5924 - val_loss: 0.6029\n",
      "Epoch 7/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.5787 - val_loss: 0.5931\n",
      "Epoch 8/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.5659 - val_loss: 0.5841\n",
      "Epoch 9/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.5538 - val_loss: 0.5757\n",
      "Epoch 10/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.5425 - val_loss: 0.5680\n",
      "Epoch 11/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.5319 - val_loss: 0.5608\n",
      "Epoch 12/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.5218 - val_loss: 0.5542\n",
      "Epoch 13/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.5124 - val_loss: 0.5481\n",
      "Epoch 14/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.5035 - val_loss: 0.5425\n",
      "Epoch 15/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4950 - val_loss: 0.5372\n",
      "Epoch 16/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4870 - val_loss: 0.5324\n",
      "Epoch 17/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4795 - val_loss: 0.5280\n",
      "Epoch 18/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4723 - val_loss: 0.5239\n",
      "Epoch 19/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4655 - val_loss: 0.5200\n",
      "Epoch 20/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4590 - val_loss: 0.5165\n",
      "Epoch 21/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4529 - val_loss: 0.5134\n",
      "Epoch 22/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.4470 - val_loss: 0.5104\n",
      "Epoch 23/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4414 - val_loss: 0.5077\n",
      "Epoch 24/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4361 - val_loss: 0.5051\n",
      "Epoch 25/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4310 - val_loss: 0.5028\n",
      "Epoch 26/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4261 - val_loss: 0.5007\n",
      "Epoch 27/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4215 - val_loss: 0.4988\n",
      "Epoch 28/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4170 - val_loss: 0.4970\n",
      "Epoch 29/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4127 - val_loss: 0.4954\n",
      "Epoch 30/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4086 - val_loss: 0.4940\n",
      "Epoch 31/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4047 - val_loss: 0.4927\n",
      "Epoch 32/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4009 - val_loss: 0.4915\n",
      "Epoch 33/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.3973 - val_loss: 0.4904\n",
      "Epoch 34/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.3938 - val_loss: 0.4895\n",
      "Epoch 35/100\n",
      "6090/6090 [==============================] - 1s 132us/sample - loss: 0.3904 - val_loss: 0.4886\n",
      "Epoch 36/100\n",
      "6090/6090 [==============================] - 1s 125us/sample - loss: 0.3871 - val_loss: 0.4880\n",
      "Epoch 37/100\n",
      "6090/6090 [==============================] - 1s 124us/sample - loss: 0.3840 - val_loss: 0.4874\n",
      "Epoch 38/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.3810 - val_loss: 0.4868\n",
      "Epoch 39/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.3781 - val_loss: 0.4864\n",
      "Epoch 40/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.3752 - val_loss: 0.4860\n",
      "Epoch 41/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.3725 - val_loss: 0.4857\n",
      "Epoch 42/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.3699 - val_loss: 0.4855\n",
      "Epoch 43/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.3673 - val_loss: 0.4853\n",
      "Epoch 44/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.3648 - val_loss: 0.4852\n",
      "Epoch 45/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.3624 - val_loss: 0.4852\n",
      "Epoch 46/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.3601 - val_loss: 0.4853\n",
      "Predicting...\n",
      "(Took 31.368 sec)\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/100\n",
      "6090/6090 [==============================] - 1s 152us/sample - loss: 0.6824 - val_loss: 0.6689\n",
      "Epoch 2/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.6602 - val_loss: 0.6508\n",
      "Epoch 3/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.6415 - val_loss: 0.6355\n",
      "Epoch 4/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.6245 - val_loss: 0.6220\n",
      "Epoch 5/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.6090 - val_loss: 0.6100\n",
      "Epoch 6/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.5945 - val_loss: 0.5990\n",
      "Epoch 7/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.5809 - val_loss: 0.5888\n",
      "Epoch 8/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.5682 - val_loss: 0.5793\n",
      "Epoch 9/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.5563 - val_loss: 0.5707\n",
      "Epoch 10/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.5451 - val_loss: 0.5626\n",
      "Epoch 11/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.5346 - val_loss: 0.5552\n",
      "Epoch 12/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.5247 - val_loss: 0.5483\n",
      "Epoch 13/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.5153 - val_loss: 0.5420\n",
      "Epoch 14/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.5064 - val_loss: 0.5360\n",
      "Epoch 15/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4981 - val_loss: 0.5306\n",
      "Epoch 16/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4902 - val_loss: 0.5255\n",
      "Epoch 17/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4827 - val_loss: 0.5207\n",
      "Epoch 18/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4756 - val_loss: 0.5163\n",
      "Epoch 19/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4688 - val_loss: 0.5122\n",
      "Epoch 20/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4624 - val_loss: 0.5085\n",
      "Epoch 21/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4564 - val_loss: 0.5050\n",
      "Epoch 22/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4505 - val_loss: 0.5018\n",
      "Epoch 23/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4450 - val_loss: 0.4989\n",
      "Epoch 24/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4397 - val_loss: 0.4962\n",
      "Epoch 25/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.4347 - val_loss: 0.4936\n",
      "Epoch 26/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4299 - val_loss: 0.4914\n",
      "Epoch 27/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4253 - val_loss: 0.4892\n",
      "Epoch 28/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4209 - val_loss: 0.4872\n",
      "Epoch 29/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4167 - val_loss: 0.4854\n",
      "Epoch 30/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4126 - val_loss: 0.4837\n",
      "Epoch 31/100\n",
      "6090/6090 [==============================] - 1s 119us/sample - loss: 0.4087 - val_loss: 0.4823\n",
      "Epoch 32/100\n",
      "6090/6090 [==============================] - 1s 119us/sample - loss: 0.4049 - val_loss: 0.4808\n",
      "Epoch 33/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.4013 - val_loss: 0.4796\n",
      "Epoch 34/100\n",
      "6090/6090 [==============================] - 1s 122us/sample - loss: 0.3979 - val_loss: 0.4786\n",
      "Epoch 35/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.3945 - val_loss: 0.4776\n",
      "Epoch 36/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.3913 - val_loss: 0.4767\n",
      "Epoch 37/100\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.3882 - val_loss: 0.4759\n",
      "Epoch 38/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.3852 - val_loss: 0.4751\n",
      "Epoch 39/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.3823 - val_loss: 0.4746\n",
      "Epoch 40/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.3795 - val_loss: 0.4740\n",
      "Epoch 41/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.3768 - val_loss: 0.4736\n",
      "Epoch 42/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.3742 - val_loss: 0.4732\n",
      "Epoch 43/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.3716 - val_loss: 0.4729\n",
      "Epoch 44/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.3692 - val_loss: 0.4726\n",
      "Epoch 45/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.3668 - val_loss: 0.4724\n",
      "Epoch 46/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.3645 - val_loss: 0.4724\n",
      "Epoch 47/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.3622 - val_loss: 0.4723\n",
      "Epoch 48/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.3600 - val_loss: 0.4723\n",
      "Epoch 49/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.3579 - val_loss: 0.4723\n",
      "Predicting...\n",
      "(Took 33.340 sec)\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/100\n",
      "6090/6090 [==============================] - 1s 154us/sample - loss: 0.6817 - val_loss: 0.6736\n",
      "Epoch 2/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.6582 - val_loss: 0.6582\n",
      "Epoch 3/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.6387 - val_loss: 0.6450\n",
      "Epoch 4/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.6215 - val_loss: 0.6328\n",
      "Epoch 5/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.6058 - val_loss: 0.6216\n",
      "Epoch 6/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.5911 - val_loss: 0.6114\n",
      "Epoch 7/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.5775 - val_loss: 0.6017\n",
      "Epoch 8/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.5646 - val_loss: 0.5927\n",
      "Epoch 9/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.5526 - val_loss: 0.5845\n",
      "Epoch 10/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.5413 - val_loss: 0.5768\n",
      "Epoch 11/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.5307 - val_loss: 0.5696\n",
      "Epoch 12/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.5207 - val_loss: 0.5630\n",
      "Epoch 13/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.5113 - val_loss: 0.5569\n",
      "Epoch 14/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.5025 - val_loss: 0.5512\n",
      "Epoch 15/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4941 - val_loss: 0.5460\n",
      "Epoch 16/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4862 - val_loss: 0.5411\n",
      "Epoch 17/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4787 - val_loss: 0.5366\n",
      "Epoch 18/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4716 - val_loss: 0.5324\n",
      "Epoch 19/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.4649 - val_loss: 0.5284\n",
      "Epoch 20/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4585 - val_loss: 0.5249\n",
      "Epoch 21/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4524 - val_loss: 0.5216\n",
      "Epoch 22/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4467 - val_loss: 0.5186\n",
      "Epoch 23/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4411 - val_loss: 0.5157\n",
      "Epoch 24/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4359 - val_loss: 0.5131\n",
      "Epoch 25/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4308 - val_loss: 0.5105\n",
      "Epoch 26/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4260 - val_loss: 0.5083\n",
      "Epoch 27/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4215 - val_loss: 0.5062\n",
      "Epoch 28/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4170 - val_loss: 0.5043\n",
      "Epoch 29/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4128 - val_loss: 0.5025\n",
      "Epoch 30/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4088 - val_loss: 0.5009\n",
      "Epoch 31/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4049 - val_loss: 0.4995\n",
      "Epoch 32/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4012 - val_loss: 0.4981\n",
      "Epoch 33/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.3976 - val_loss: 0.4969\n",
      "Epoch 34/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.3941 - val_loss: 0.4958\n",
      "Epoch 35/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.3908 - val_loss: 0.4948\n",
      "Epoch 36/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.3876 - val_loss: 0.4939\n",
      "Epoch 37/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.3845 - val_loss: 0.4930\n",
      "Epoch 38/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.3815 - val_loss: 0.4923\n",
      "Epoch 39/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.3786 - val_loss: 0.4917\n",
      "Epoch 40/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.3758 - val_loss: 0.4911\n",
      "Epoch 41/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.3731 - val_loss: 0.4905\n",
      "Epoch 42/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.3705 - val_loss: 0.4902\n",
      "Epoch 43/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.3680 - val_loss: 0.4899\n",
      "Epoch 44/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.3655 - val_loss: 0.4896\n",
      "Epoch 45/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.3631 - val_loss: 0.4894\n",
      "Epoch 46/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.3608 - val_loss: 0.4891\n",
      "Epoch 47/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.3586 - val_loss: 0.4891\n",
      "Epoch 48/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.3564 - val_loss: 0.4890\n",
      "Epoch 49/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.3542 - val_loss: 0.4891\n",
      "Predicting...\n",
      "(Took 33.059 sec)\n",
      "Train on 6091 samples, validate on 1522 samples\n",
      "Epoch 1/100\n",
      "6091/6091 [==============================] - 1s 156us/sample - loss: 0.6826 - val_loss: 0.6717\n",
      "Epoch 2/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.6596 - val_loss: 0.6541\n",
      "Epoch 3/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.6404 - val_loss: 0.6395\n",
      "Epoch 4/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.6235 - val_loss: 0.6264\n",
      "Epoch 5/100\n",
      "6091/6091 [==============================] - 1s 106us/sample - loss: 0.6079 - val_loss: 0.6145\n",
      "Epoch 6/100\n",
      "6091/6091 [==============================] - 1s 106us/sample - loss: 0.5935 - val_loss: 0.6036\n",
      "Epoch 7/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.5799 - val_loss: 0.5934\n",
      "Epoch 8/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.5673 - val_loss: 0.5840\n",
      "Epoch 9/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.5553 - val_loss: 0.5753\n",
      "Epoch 10/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.5442 - val_loss: 0.5671\n",
      "Epoch 11/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.5337 - val_loss: 0.5596\n",
      "Epoch 12/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.5238 - val_loss: 0.5526\n",
      "Epoch 13/100\n",
      "6091/6091 [==============================] - 1s 119us/sample - loss: 0.5145 - val_loss: 0.5461\n",
      "Epoch 14/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.5057 - val_loss: 0.5401\n",
      "Epoch 15/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.4974 - val_loss: 0.5345\n",
      "Epoch 16/100\n",
      "6091/6091 [==============================] - 1s 114us/sample - loss: 0.4895 - val_loss: 0.5293\n",
      "Epoch 17/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.4821 - val_loss: 0.5245\n",
      "Epoch 18/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4751 - val_loss: 0.5200\n",
      "Epoch 19/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.4684 - val_loss: 0.5158\n",
      "Epoch 20/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4620 - val_loss: 0.5119\n",
      "Epoch 21/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.4560 - val_loss: 0.5082\n",
      "Epoch 22/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.4502 - val_loss: 0.5049\n",
      "Epoch 23/100\n",
      "6091/6091 [==============================] - 1s 106us/sample - loss: 0.4447 - val_loss: 0.5018\n",
      "Epoch 24/100\n",
      "6091/6091 [==============================] - 1s 106us/sample - loss: 0.4395 - val_loss: 0.4988\n",
      "Epoch 25/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.4345 - val_loss: 0.4961\n",
      "Epoch 26/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4298 - val_loss: 0.4937\n",
      "Epoch 27/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4252 - val_loss: 0.4914\n",
      "Epoch 28/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4208 - val_loss: 0.4892\n",
      "Epoch 29/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4166 - val_loss: 0.4872\n",
      "Epoch 30/100\n",
      "6091/6091 [==============================] - 1s 123us/sample - loss: 0.4126 - val_loss: 0.4854\n",
      "Epoch 31/100\n",
      "6091/6091 [==============================] - 1s 118us/sample - loss: 0.4087 - val_loss: 0.4836\n",
      "Epoch 32/100\n",
      "6091/6091 [==============================] - 1s 121us/sample - loss: 0.4050 - val_loss: 0.4820\n",
      "Epoch 33/100\n",
      "6091/6091 [==============================] - 1s 115us/sample - loss: 0.4014 - val_loss: 0.4806\n",
      "Epoch 34/100\n",
      "6091/6091 [==============================] - 1s 115us/sample - loss: 0.3980 - val_loss: 0.4792\n",
      "Epoch 35/100\n",
      "6091/6091 [==============================] - 1s 116us/sample - loss: 0.3947 - val_loss: 0.4780\n",
      "Epoch 36/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.3915 - val_loss: 0.4769\n",
      "Epoch 37/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.3884 - val_loss: 0.4758\n",
      "Epoch 38/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.3854 - val_loss: 0.4750\n",
      "Epoch 39/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.3826 - val_loss: 0.4741\n",
      "Epoch 40/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.3798 - val_loss: 0.4734\n",
      "Epoch 41/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.3771 - val_loss: 0.4726\n",
      "Epoch 42/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.3745 - val_loss: 0.4720\n",
      "Epoch 43/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.3720 - val_loss: 0.4715\n",
      "Epoch 44/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.3695 - val_loss: 0.4710\n",
      "Epoch 45/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.3671 - val_loss: 0.4706\n",
      "Epoch 46/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.3648 - val_loss: 0.4703\n",
      "Epoch 47/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.3626 - val_loss: 0.4701\n",
      "Epoch 48/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.3604 - val_loss: 0.4698\n",
      "Epoch 49/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.3583 - val_loss: 0.4695\n",
      "Epoch 50/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.3562 - val_loss: 0.4695\n",
      "Epoch 51/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.3542 - val_loss: 0.4695\n",
      "Epoch 52/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.3523 - val_loss: 0.4693\n",
      "Epoch 53/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.3504 - val_loss: 0.4694\n",
      "Predicting...\n",
      "(Took 35.983 sec)\n",
      "Train on 6091 samples, validate on 1522 samples\n",
      "Epoch 1/100\n",
      "6091/6091 [==============================] - 1s 158us/sample - loss: 0.6828 - val_loss: 0.6733\n",
      "Epoch 2/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.6595 - val_loss: 0.6572\n",
      "Epoch 3/100\n",
      "6091/6091 [==============================] - 1s 106us/sample - loss: 0.6402 - val_loss: 0.6436\n",
      "Epoch 4/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.6232 - val_loss: 0.6311\n",
      "Epoch 5/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.6074 - val_loss: 0.6195\n",
      "Epoch 6/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.5929 - val_loss: 0.6088\n",
      "Epoch 7/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.5792 - val_loss: 0.5989\n",
      "Epoch 8/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.5664 - val_loss: 0.5897\n",
      "Epoch 9/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.5544 - val_loss: 0.5811\n",
      "Epoch 10/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.5431 - val_loss: 0.5732\n",
      "Epoch 11/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.5325 - val_loss: 0.5659\n",
      "Epoch 12/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.5224 - val_loss: 0.5591\n",
      "Epoch 13/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.5130 - val_loss: 0.5528\n",
      "Epoch 14/100\n",
      "6091/6091 [==============================] - 1s 111us/sample - loss: 0.5041 - val_loss: 0.5468\n",
      "Epoch 15/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.4957 - val_loss: 0.5415\n",
      "Epoch 16/100\n",
      "6091/6091 [==============================] - 1s 111us/sample - loss: 0.4877 - val_loss: 0.5364\n",
      "Epoch 17/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.4802 - val_loss: 0.5317\n",
      "Epoch 18/100\n",
      "6091/6091 [==============================] - 1s 110us/sample - loss: 0.4730 - val_loss: 0.5276\n",
      "Epoch 19/100\n",
      "6091/6091 [==============================] - 1s 114us/sample - loss: 0.4663 - val_loss: 0.5236\n",
      "Epoch 20/100\n",
      "6091/6091 [==============================] - 1s 111us/sample - loss: 0.4598 - val_loss: 0.5199\n",
      "Epoch 21/100\n",
      "6091/6091 [==============================] - 1s 110us/sample - loss: 0.4537 - val_loss: 0.5165\n",
      "Epoch 22/100\n",
      "6091/6091 [==============================] - 1s 113us/sample - loss: 0.4478 - val_loss: 0.5134\n",
      "Epoch 23/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.4423 - val_loss: 0.5105\n",
      "Epoch 24/100\n",
      "6091/6091 [==============================] - 1s 111us/sample - loss: 0.4369 - val_loss: 0.5077\n",
      "Epoch 25/100\n",
      "6091/6091 [==============================] - 1s 110us/sample - loss: 0.4319 - val_loss: 0.5052\n",
      "Epoch 26/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.4270 - val_loss: 0.5030\n",
      "Epoch 27/100\n",
      "6091/6091 [==============================] - 1s 110us/sample - loss: 0.4224 - val_loss: 0.5008\n",
      "Epoch 28/100\n",
      "6091/6091 [==============================] - 1s 111us/sample - loss: 0.4180 - val_loss: 0.4990\n",
      "Epoch 29/100\n",
      "6091/6091 [==============================] - 1s 110us/sample - loss: 0.4137 - val_loss: 0.4972\n",
      "Epoch 30/100\n",
      "6091/6091 [==============================] - 1s 110us/sample - loss: 0.4096 - val_loss: 0.4955\n",
      "Epoch 31/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.4057 - val_loss: 0.4940\n",
      "Epoch 32/100\n",
      "6091/6091 [==============================] - 1s 111us/sample - loss: 0.4019 - val_loss: 0.4927\n",
      "Epoch 33/100\n",
      "6091/6091 [==============================] - 1s 110us/sample - loss: 0.3983 - val_loss: 0.4914\n",
      "Epoch 34/100\n",
      "6091/6091 [==============================] - 1s 110us/sample - loss: 0.3948 - val_loss: 0.4903\n",
      "Epoch 35/100\n",
      "6091/6091 [==============================] - 1s 111us/sample - loss: 0.3915 - val_loss: 0.4893\n",
      "Epoch 36/100\n",
      "6091/6091 [==============================] - 1s 110us/sample - loss: 0.3882 - val_loss: 0.4885\n",
      "Epoch 37/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.3851 - val_loss: 0.4876\n",
      "Epoch 38/100\n",
      "6091/6091 [==============================] - 1s 116us/sample - loss: 0.3821 - val_loss: 0.4869\n",
      "Epoch 39/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.3792 - val_loss: 0.4863\n",
      "Epoch 40/100\n",
      "6091/6091 [==============================] - 1s 111us/sample - loss: 0.3764 - val_loss: 0.4858\n",
      "Epoch 41/100\n",
      "6091/6091 [==============================] - 1s 111us/sample - loss: 0.3736 - val_loss: 0.4853\n",
      "Epoch 42/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.3710 - val_loss: 0.4849\n",
      "Epoch 43/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.3684 - val_loss: 0.4846\n",
      "Epoch 44/100\n",
      "6091/6091 [==============================] - 1s 111us/sample - loss: 0.3660 - val_loss: 0.4843\n",
      "Epoch 45/100\n",
      "6091/6091 [==============================] - 1s 111us/sample - loss: 0.3636 - val_loss: 0.4841\n",
      "Epoch 46/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.3613 - val_loss: 0.4839\n",
      "Epoch 47/100\n",
      "6091/6091 [==============================] - 1s 111us/sample - loss: 0.3590 - val_loss: 0.4838\n",
      "Epoch 48/100\n",
      "6091/6091 [==============================] - 1s 106us/sample - loss: 0.3568 - val_loss: 0.4837\n",
      "Epoch 49/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.3546 - val_loss: 0.4838\n",
      "Predicting...\n",
      "(Took 33.591 sec)\n",
      "Combined confusion matrix:\n",
      "[[3694.  648.]\n",
      " [1002. 2269.]]\n",
      "(Overall, took 167.622 sec)\n",
      "Accuracy: 78.33% +/- 0.52%\n",
      "Precision for positive class: 78.67% +/- 0.92%\n",
      "Precision for negative class: 77.79% +/- 2.41%\n",
      "Recall for positive class: 85.08% +/- 1.64%\n",
      "Recall for negative class: 69.35% +/- 1.89%\n",
      "F for positive class: 81.74% +/- 0.51%\n",
      "F for negative class: 73.31% +/- 1.43%\n",
      "Mean F score: 77.52% +/- 0.70%\n"
     ]
    }
   ],
   "source": [
    "# Cross-validated logistic models (all have max epochs of 100, but early stopping if validation loss starts to increase)\n",
    "# No prep, raw counts\n",
    "all_metrics(cross_validate_logistic(dftrain_minimal_noprep, 100))\n",
    "# No prep, normalized\n",
    "all_metrics(cross_validate_logistic(dftrain_min_norm_noprep, 100))\n",
    "# No prep, TF-IDF\n",
    "all_metrics(cross_validate_logistic(dftrain_min_tfidf_noprep, 100))\n",
    "# Preprocessed, raw counts\n",
    "all_metrics(cross_validate_logistic(dftrain_minimal, 100))\n",
    "# Preprocessed, normalized\n",
    "all_metrics(cross_validate_logistic(dftrain_min_norm, 100))\n",
    "# Preprocessed, TF-IDF\n",
    "all_metrics(cross_validate_logistic(dftrain_min_tfidf, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/100\n",
      "6090/6090 [==============================] - 4s 607us/sample - loss: 0.6505 - val_loss: 0.6104\n",
      "Epoch 2/100\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.5842 - val_loss: 0.5677\n",
      "Epoch 3/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.5442 - val_loss: 0.5404\n",
      "Epoch 4/100\n",
      "6090/6090 [==============================] - 1s 96us/sample - loss: 0.5156 - val_loss: 0.5216\n",
      "Epoch 5/100\n",
      "6090/6090 [==============================] - 1s 97us/sample - loss: 0.4932 - val_loss: 0.5073\n",
      "Epoch 6/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.4747 - val_loss: 0.4963\n",
      "Epoch 7/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.4591 - val_loss: 0.4875\n",
      "Epoch 8/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4455 - val_loss: 0.4800\n",
      "Epoch 9/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.4335 - val_loss: 0.4742\n",
      "Epoch 10/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4226 - val_loss: 0.4688\n",
      "Epoch 11/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4129 - val_loss: 0.4647\n",
      "Epoch 12/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4040 - val_loss: 0.4609\n",
      "Epoch 13/100\n",
      "6090/6090 [==============================] - 1s 124us/sample - loss: 0.3958 - val_loss: 0.4577\n",
      "Epoch 14/100\n",
      "6090/6090 [==============================] - 1s 121us/sample - loss: 0.3882 - val_loss: 0.4549\n",
      "Epoch 15/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.3812 - val_loss: 0.4521\n",
      "Epoch 16/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.3746 - val_loss: 0.4501\n",
      "Epoch 17/100\n",
      "6090/6090 [==============================] - 1s 101us/sample - loss: 0.3683 - val_loss: 0.4484\n",
      "Epoch 18/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.3626 - val_loss: 0.4468\n",
      "Epoch 19/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.3570 - val_loss: 0.4457\n",
      "Epoch 20/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.3518 - val_loss: 0.4445\n",
      "Epoch 21/100\n",
      "6090/6090 [==============================] - 1s 99us/sample - loss: 0.3469 - val_loss: 0.4436\n",
      "Epoch 22/100\n",
      "6090/6090 [==============================] - 1s 118us/sample - loss: 0.3422 - val_loss: 0.4426\n",
      "Epoch 23/100\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.3376 - val_loss: 0.4421\n",
      "Epoch 24/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.3333 - val_loss: 0.4416\n",
      "Epoch 25/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.3292 - val_loss: 0.4412\n",
      "Epoch 26/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.3253 - val_loss: 0.4407\n",
      "Epoch 27/100\n",
      "6090/6090 [==============================] - 1s 121us/sample - loss: 0.3214 - val_loss: 0.4404\n",
      "Epoch 28/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.3178 - val_loss: 0.4404\n",
      "Epoch 29/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.3143 - val_loss: 0.4405\n",
      "Predicting...\n",
      "(Took 35.273 sec)\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.6470 - val_loss: 0.6115\n",
      "Epoch 2/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.5824 - val_loss: 0.5737\n",
      "Epoch 3/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.5423 - val_loss: 0.5502\n",
      "Epoch 4/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.5133 - val_loss: 0.5336\n",
      "Epoch 5/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.4909 - val_loss: 0.5210\n",
      "Epoch 6/100\n",
      "6090/6090 [==============================] - 1s 125us/sample - loss: 0.4722 - val_loss: 0.5112\n",
      "Epoch 7/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.4565 - val_loss: 0.5035\n",
      "Epoch 8/100\n",
      "6090/6090 [==============================] - 1s 122us/sample - loss: 0.4429 - val_loss: 0.4969\n",
      "Epoch 9/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.4308 - val_loss: 0.4917\n",
      "Epoch 10/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4200 - val_loss: 0.4873\n",
      "Epoch 11/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.4104 - val_loss: 0.4833\n",
      "Epoch 12/100\n",
      "6090/6090 [==============================] - 1s 117us/sample - loss: 0.4013 - val_loss: 0.4797\n",
      "Epoch 13/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.3932 - val_loss: 0.4770\n",
      "Epoch 14/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.3857 - val_loss: 0.4750\n",
      "Epoch 15/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.3787 - val_loss: 0.4728\n",
      "Epoch 16/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.3721 - val_loss: 0.4712\n",
      "Epoch 17/100\n",
      "6090/6090 [==============================] - 1s 99us/sample - loss: 0.3659 - val_loss: 0.4697\n",
      "Epoch 18/100\n",
      "6090/6090 [==============================] - 1s 101us/sample - loss: 0.3603 - val_loss: 0.4687\n",
      "Epoch 19/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.3547 - val_loss: 0.4678\n",
      "Epoch 20/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.3495 - val_loss: 0.4672\n",
      "Epoch 21/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.3447 - val_loss: 0.4669\n",
      "Epoch 22/100\n",
      "6090/6090 [==============================] - 1s 101us/sample - loss: 0.3399 - val_loss: 0.4665\n",
      "Epoch 23/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.3355 - val_loss: 0.4664\n",
      "Epoch 24/100\n",
      "6090/6090 [==============================] - 1s 124us/sample - loss: 0.3311 - val_loss: 0.4661\n",
      "Epoch 25/100\n",
      "6090/6090 [==============================] - 1s 116us/sample - loss: 0.3271 - val_loss: 0.4665\n",
      "Predicting...\n",
      "(Took 17.318 sec)\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/100\n",
      "6090/6090 [==============================] - 1s 145us/sample - loss: 0.6459 - val_loss: 0.6172\n",
      "Epoch 2/100\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.5808 - val_loss: 0.5788\n",
      "Epoch 3/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.5409 - val_loss: 0.5545\n",
      "Epoch 4/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.5122 - val_loss: 0.5376\n",
      "Epoch 5/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.4897 - val_loss: 0.5252\n",
      "Epoch 6/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4713 - val_loss: 0.5150\n",
      "Epoch 7/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4557 - val_loss: 0.5072\n",
      "Epoch 8/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.4421 - val_loss: 0.5003\n",
      "Epoch 9/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4300 - val_loss: 0.4947\n",
      "Epoch 10/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4193 - val_loss: 0.4904\n",
      "Epoch 11/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.4094 - val_loss: 0.4865\n",
      "Epoch 12/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.4005 - val_loss: 0.4833\n",
      "Epoch 13/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.3924 - val_loss: 0.4806\n",
      "Epoch 14/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.3847 - val_loss: 0.4787\n",
      "Epoch 15/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.3776 - val_loss: 0.4766\n",
      "Epoch 16/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.3709 - val_loss: 0.4750\n",
      "Epoch 17/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.3647 - val_loss: 0.4738\n",
      "Epoch 18/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.3589 - val_loss: 0.4730\n",
      "Epoch 19/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.3532 - val_loss: 0.4724\n",
      "Epoch 20/100\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.3480 - val_loss: 0.4719\n",
      "Epoch 21/100\n",
      "6090/6090 [==============================] - 1s 99us/sample - loss: 0.3429 - val_loss: 0.4712\n",
      "Epoch 22/100\n",
      "6090/6090 [==============================] - 1s 97us/sample - loss: 0.3382 - val_loss: 0.4719\n",
      "Predicting...\n",
      "(Took 15.178 sec)\n",
      "Train on 6091 samples, validate on 1522 samples\n",
      "Epoch 1/100\n",
      "6091/6091 [==============================] - 1s 131us/sample - loss: 0.6465 - val_loss: 0.6133\n",
      "Epoch 2/100\n",
      "6091/6091 [==============================] - 1s 97us/sample - loss: 0.5828 - val_loss: 0.5726\n",
      "Epoch 3/100\n",
      "6091/6091 [==============================] - 1s 95us/sample - loss: 0.5434 - val_loss: 0.5462\n",
      "Epoch 4/100\n",
      "6091/6091 [==============================] - 1s 99us/sample - loss: 0.5151 - val_loss: 0.5276\n",
      "Epoch 5/100\n",
      "6091/6091 [==============================] - 1s 94us/sample - loss: 0.4929 - val_loss: 0.5134\n",
      "Epoch 6/100\n",
      "6091/6091 [==============================] - 1s 93us/sample - loss: 0.4745 - val_loss: 0.5022\n",
      "Epoch 7/100\n",
      "6091/6091 [==============================] - 1s 94us/sample - loss: 0.4590 - val_loss: 0.4931\n",
      "Epoch 8/100\n",
      "6091/6091 [==============================] - 1s 96us/sample - loss: 0.4454 - val_loss: 0.4855\n",
      "Epoch 9/100\n",
      "6091/6091 [==============================] - 1s 92us/sample - loss: 0.4334 - val_loss: 0.4791\n",
      "Epoch 10/100\n",
      "6091/6091 [==============================] - 1s 101us/sample - loss: 0.4228 - val_loss: 0.4737\n",
      "Epoch 11/100\n",
      "6091/6091 [==============================] - 1s 98us/sample - loss: 0.4130 - val_loss: 0.4697\n",
      "Epoch 12/100\n",
      "6091/6091 [==============================] - 1s 93us/sample - loss: 0.4041 - val_loss: 0.4656\n",
      "Epoch 13/100\n",
      "6091/6091 [==============================] - 1s 93us/sample - loss: 0.3959 - val_loss: 0.4624\n",
      "Epoch 14/100\n",
      "6091/6091 [==============================] - 1s 95us/sample - loss: 0.3884 - val_loss: 0.4597\n",
      "Epoch 15/100\n",
      "6091/6091 [==============================] - 1s 95us/sample - loss: 0.3813 - val_loss: 0.4573\n",
      "Epoch 16/100\n",
      "6091/6091 [==============================] - 1s 94us/sample - loss: 0.3748 - val_loss: 0.4552\n",
      "Epoch 17/100\n",
      "6091/6091 [==============================] - 1s 94us/sample - loss: 0.3685 - val_loss: 0.4534\n",
      "Epoch 18/100\n",
      "6091/6091 [==============================] - 1s 95us/sample - loss: 0.3628 - val_loss: 0.4521\n",
      "Epoch 19/100\n",
      "6091/6091 [==============================] - 1s 93us/sample - loss: 0.3573 - val_loss: 0.4505\n",
      "Epoch 20/100\n",
      "6091/6091 [==============================] - 1s 92us/sample - loss: 0.3520 - val_loss: 0.4498\n",
      "Epoch 21/100\n",
      "6091/6091 [==============================] - 1s 96us/sample - loss: 0.3470 - val_loss: 0.4488\n",
      "Epoch 22/100\n",
      "6091/6091 [==============================] - 1s 94us/sample - loss: 0.3424 - val_loss: 0.4483\n",
      "Epoch 23/100\n",
      "6091/6091 [==============================] - 1s 94us/sample - loss: 0.3379 - val_loss: 0.4480\n",
      "Epoch 24/100\n",
      "6091/6091 [==============================] - 1s 95us/sample - loss: 0.3336 - val_loss: 0.4474\n",
      "Epoch 25/100\n",
      "6091/6091 [==============================] - 1s 94us/sample - loss: 0.3294 - val_loss: 0.4473\n",
      "Epoch 26/100\n",
      "6091/6091 [==============================] - 1s 94us/sample - loss: 0.3255 - val_loss: 0.4469\n",
      "Epoch 27/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.3216 - val_loss: 0.4469\n",
      "Epoch 28/100\n",
      "6091/6091 [==============================] - 1s 115us/sample - loss: 0.3180 - val_loss: 0.4469\n",
      "Epoch 29/100\n",
      "6091/6091 [==============================] - 1s 110us/sample - loss: 0.3145 - val_loss: 0.4474\n",
      "Predicting...\n",
      "(Took 17.757 sec)\n",
      "Train on 6091 samples, validate on 1522 samples\n",
      "Epoch 1/100\n",
      "6091/6091 [==============================] - 1s 162us/sample - loss: 0.6499 - val_loss: 0.6167\n",
      "Epoch 2/100\n",
      "6091/6091 [==============================] - 1s 117us/sample - loss: 0.5838 - val_loss: 0.5762\n",
      "Epoch 3/100\n",
      "6091/6091 [==============================] - 1s 131us/sample - loss: 0.5435 - val_loss: 0.5502\n",
      "Epoch 4/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.5145 - val_loss: 0.5323\n",
      "Epoch 5/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4919 - val_loss: 0.5187\n",
      "Epoch 6/100\n",
      "6091/6091 [==============================] - 1s 111us/sample - loss: 0.4731 - val_loss: 0.5080\n",
      "Epoch 7/100\n",
      "6091/6091 [==============================] - 1s 110us/sample - loss: 0.4571 - val_loss: 0.4990\n",
      "Epoch 8/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.4433 - val_loss: 0.4920\n",
      "Epoch 9/100\n",
      "6091/6091 [==============================] - 1s 104us/sample - loss: 0.4311 - val_loss: 0.4862\n",
      "Epoch 10/100\n",
      "6091/6091 [==============================] - 1s 105us/sample - loss: 0.4200 - val_loss: 0.4812\n",
      "Epoch 11/100\n",
      "6091/6091 [==============================] - 1s 99us/sample - loss: 0.4102 - val_loss: 0.4770\n",
      "Epoch 12/100\n",
      "6091/6091 [==============================] - 1s 104us/sample - loss: 0.4012 - val_loss: 0.4735\n",
      "Epoch 13/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.3927 - val_loss: 0.4706\n",
      "Epoch 14/100\n",
      "6091/6091 [==============================] - 1s 106us/sample - loss: 0.3851 - val_loss: 0.4682\n",
      "Epoch 15/100\n",
      "6091/6091 [==============================] - 1s 114us/sample - loss: 0.3779 - val_loss: 0.4661\n",
      "Epoch 16/100\n",
      "6091/6091 [==============================] - 1s 105us/sample - loss: 0.3711 - val_loss: 0.4642\n",
      "Epoch 17/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.3649 - val_loss: 0.4627\n",
      "Epoch 18/100\n",
      "6091/6091 [==============================] - 1s 111us/sample - loss: 0.3590 - val_loss: 0.4616\n",
      "Epoch 19/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.3534 - val_loss: 0.4605\n",
      "Epoch 20/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.3481 - val_loss: 0.4597\n",
      "Epoch 21/100\n",
      "6091/6091 [==============================] - 1s 113us/sample - loss: 0.3430 - val_loss: 0.4591\n",
      "Epoch 22/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.3382 - val_loss: 0.4591\n",
      "Predicting...\n",
      "(Took 15.537 sec)\n",
      "Combined confusion matrix:\n",
      "[[3808.  534.]\n",
      " [ 994. 2277.]]\n",
      "(Overall, took 101.505 sec)\n",
      "Accuracy: 79.93% +/- 0.95%\n",
      "Precision for positive class: 79.31% +/- 2.17%\n",
      "Precision for negative class: 81.02% +/- 1.89%\n",
      "Recall for positive class: 87.71% +/- 1.25%\n",
      "Recall for negative class: 69.64% +/- 2.76%\n",
      "F for positive class: 83.29% +/- 0.89%\n",
      "F for negative class: 74.87% +/- 1.27%\n",
      "Mean F score: 79.08% +/- 0.98%\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/100\n",
      "6090/6090 [==============================] - 1s 146us/sample - loss: 0.6477 - val_loss: 0.6040\n",
      "Epoch 2/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.5817 - val_loss: 0.5614\n",
      "Epoch 3/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.5415 - val_loss: 0.5345\n",
      "Epoch 4/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.5125 - val_loss: 0.5152\n",
      "Epoch 5/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.4899 - val_loss: 0.5012\n",
      "Epoch 6/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4715 - val_loss: 0.4904\n",
      "Epoch 7/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.4557 - val_loss: 0.4815\n",
      "Epoch 8/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.4421 - val_loss: 0.4747\n",
      "Epoch 9/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4302 - val_loss: 0.4686\n",
      "Epoch 10/100\n",
      "6090/6090 [==============================] - 1s 119us/sample - loss: 0.4196 - val_loss: 0.4642\n",
      "Epoch 11/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4099 - val_loss: 0.4604\n",
      "Epoch 12/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.4014 - val_loss: 0.4570\n",
      "Epoch 13/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.3933 - val_loss: 0.4542\n",
      "Epoch 14/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.3859 - val_loss: 0.4518\n",
      "Epoch 15/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.3791 - val_loss: 0.4500\n",
      "Epoch 16/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.3728 - val_loss: 0.4482\n",
      "Epoch 17/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.3669 - val_loss: 0.4467\n",
      "Epoch 18/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6090/6090 [==============================] - 1s 99us/sample - loss: 0.3612 - val_loss: 0.4456\n",
      "Epoch 19/100\n",
      "6090/6090 [==============================] - 1s 101us/sample - loss: 0.3560 - val_loss: 0.4446\n",
      "Epoch 20/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.3510 - val_loss: 0.4438\n",
      "Epoch 21/100\n",
      "6090/6090 [==============================] - 1s 99us/sample - loss: 0.3464 - val_loss: 0.4436\n",
      "Epoch 22/100\n",
      "6090/6090 [==============================] - 1s 98us/sample - loss: 0.3419 - val_loss: 0.4429\n",
      "Epoch 23/100\n",
      "6090/6090 [==============================] - 1s 99us/sample - loss: 0.3377 - val_loss: 0.4428\n",
      "Epoch 24/100\n",
      "6090/6090 [==============================] - 1s 99us/sample - loss: 0.3337 - val_loss: 0.4426\n",
      "Epoch 25/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.3298 - val_loss: 0.4428\n",
      "Predicting...\n",
      "(Took 16.847 sec)\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/100\n",
      "6090/6090 [==============================] - 1s 152us/sample - loss: 0.6486 - val_loss: 0.6105\n",
      "Epoch 2/100\n",
      "6090/6090 [==============================] - 1s 97us/sample - loss: 0.5803 - val_loss: 0.5699\n",
      "Epoch 3/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.5383 - val_loss: 0.5453\n",
      "Epoch 4/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.5084 - val_loss: 0.5284\n",
      "Epoch 5/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.4855 - val_loss: 0.5159\n",
      "Epoch 6/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.4670 - val_loss: 0.5059\n",
      "Epoch 7/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.4514 - val_loss: 0.4980\n",
      "Epoch 8/100\n",
      "6090/6090 [==============================] - 1s 94us/sample - loss: 0.4379 - val_loss: 0.4914\n",
      "Epoch 9/100\n",
      "6090/6090 [==============================] - 1s 101us/sample - loss: 0.4261 - val_loss: 0.4864\n",
      "Epoch 10/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4157 - val_loss: 0.4816\n",
      "Epoch 11/100\n",
      "6090/6090 [==============================] - 1s 120us/sample - loss: 0.4063 - val_loss: 0.4778\n",
      "Epoch 12/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.3978 - val_loss: 0.4745\n",
      "Epoch 13/100\n",
      "6090/6090 [==============================] - 1s 97us/sample - loss: 0.3900 - val_loss: 0.4721\n",
      "Epoch 14/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.3828 - val_loss: 0.4696\n",
      "Epoch 15/100\n",
      "6090/6090 [==============================] - 1s 97us/sample - loss: 0.3761 - val_loss: 0.4679\n",
      "Epoch 16/100\n",
      "6090/6090 [==============================] - 1s 101us/sample - loss: 0.3699 - val_loss: 0.4664\n",
      "Epoch 17/100\n",
      "6090/6090 [==============================] - 1s 98us/sample - loss: 0.3642 - val_loss: 0.4653\n",
      "Epoch 18/100\n",
      "6090/6090 [==============================] - 1s 99us/sample - loss: 0.3587 - val_loss: 0.4642\n",
      "Epoch 19/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.3536 - val_loss: 0.4635\n",
      "Epoch 20/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.3487 - val_loss: 0.4632\n",
      "Epoch 21/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.3441 - val_loss: 0.4628\n",
      "Epoch 22/100\n",
      "6090/6090 [==============================] - 1s 97us/sample - loss: 0.3398 - val_loss: 0.4627\n",
      "Epoch 23/100\n",
      "6090/6090 [==============================] - 1s 96us/sample - loss: 0.3356 - val_loss: 0.4626\n",
      "Epoch 24/100\n",
      "6090/6090 [==============================] - 1s 119us/sample - loss: 0.3316 - val_loss: 0.4627\n",
      "Predicting...\n",
      "(Took 15.857 sec)\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/100\n",
      "6090/6090 [==============================] - 1s 153us/sample - loss: 0.6463 - val_loss: 0.6148\n",
      "Epoch 2/100\n",
      "6090/6090 [==============================] - 1s 116us/sample - loss: 0.5788 - val_loss: 0.5743\n",
      "Epoch 3/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.5376 - val_loss: 0.5476\n",
      "Epoch 4/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.5081 - val_loss: 0.5302\n",
      "Epoch 5/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4854 - val_loss: 0.5171\n",
      "Epoch 6/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.4671 - val_loss: 0.5069\n",
      "Epoch 7/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.4515 - val_loss: 0.4980\n",
      "Epoch 8/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4382 - val_loss: 0.4914\n",
      "Epoch 9/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.4265 - val_loss: 0.4858\n",
      "Epoch 10/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4159 - val_loss: 0.4815\n",
      "Epoch 11/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.4065 - val_loss: 0.4775\n",
      "Epoch 12/100\n",
      "6090/6090 [==============================] - ETA: 0s - loss: 0.397 - 1s 98us/sample - loss: 0.3979 - val_loss: 0.4742\n",
      "Epoch 13/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.3900 - val_loss: 0.4715\n",
      "Epoch 14/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.3827 - val_loss: 0.4693\n",
      "Epoch 15/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.3760 - val_loss: 0.4679\n",
      "Epoch 16/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.3697 - val_loss: 0.4667\n",
      "Epoch 17/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.3638 - val_loss: 0.4650\n",
      "Epoch 18/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.3583 - val_loss: 0.4643\n",
      "Epoch 19/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.3530 - val_loss: 0.4633\n",
      "Epoch 20/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.3481 - val_loss: 0.4628\n",
      "Epoch 21/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.3433 - val_loss: 0.4632\n",
      "Predicting...\n",
      "(Took 14.307 sec)\n",
      "Train on 6091 samples, validate on 1522 samples\n",
      "Epoch 1/100\n",
      "6091/6091 [==============================] - 1s 143us/sample - loss: 0.6495 - val_loss: 0.6133\n",
      "Epoch 2/100\n",
      "6091/6091 [==============================] - 1s 103us/sample - loss: 0.5810 - val_loss: 0.5704\n",
      "Epoch 3/100\n",
      "6091/6091 [==============================] - 1s 100us/sample - loss: 0.5399 - val_loss: 0.5432\n",
      "Epoch 4/100\n",
      "6091/6091 [==============================] - 1s 100us/sample - loss: 0.5108 - val_loss: 0.5243\n",
      "Epoch 5/100\n",
      "6091/6091 [==============================] - 1s 102us/sample - loss: 0.4883 - val_loss: 0.5099\n",
      "Epoch 6/100\n",
      "6091/6091 [==============================] - 1s 99us/sample - loss: 0.4700 - val_loss: 0.4987\n",
      "Epoch 7/100\n",
      "6091/6091 [==============================] - 1s 110us/sample - loss: 0.4546 - val_loss: 0.4895\n",
      "Epoch 8/100\n",
      "6091/6091 [==============================] - 1s 110us/sample - loss: 0.4412 - val_loss: 0.4820\n",
      "Epoch 9/100\n",
      "6091/6091 [==============================] - 1s 111us/sample - loss: 0.4295 - val_loss: 0.4758\n",
      "Epoch 10/100\n",
      "6091/6091 [==============================] - 1s 111us/sample - loss: 0.4190 - val_loss: 0.4707\n",
      "Epoch 11/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.4095 - val_loss: 0.4664\n",
      "Epoch 12/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4008 - val_loss: 0.4627\n",
      "Epoch 13/100\n",
      "6091/6091 [==============================] - 1s 110us/sample - loss: 0.3929 - val_loss: 0.4595\n",
      "Epoch 14/100\n",
      "6091/6091 [==============================] - 1s 106us/sample - loss: 0.3856 - val_loss: 0.4569\n",
      "Epoch 15/100\n",
      "6091/6091 [==============================] - 1s 103us/sample - loss: 0.3789 - val_loss: 0.4547\n",
      "Epoch 16/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.3726 - val_loss: 0.4528\n",
      "Epoch 17/100\n",
      "6091/6091 [==============================] - 1s 105us/sample - loss: 0.3666 - val_loss: 0.4516\n",
      "Epoch 18/100\n",
      "6091/6091 [==============================] - 1s 106us/sample - loss: 0.3611 - val_loss: 0.4504\n",
      "Epoch 19/100\n",
      "6091/6091 [==============================] - 1s 102us/sample - loss: 0.3558 - val_loss: 0.4493\n",
      "Epoch 20/100\n",
      "6091/6091 [==============================] - 1s 98us/sample - loss: 0.3509 - val_loss: 0.4485\n",
      "Epoch 21/100\n",
      "6091/6091 [==============================] - 1s 101us/sample - loss: 0.3462 - val_loss: 0.4480\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/100\n",
      "6091/6091 [==============================] - 1s 96us/sample - loss: 0.3418 - val_loss: 0.4476\n",
      "Epoch 23/100\n",
      "6091/6091 [==============================] - 1s 99us/sample - loss: 0.3375 - val_loss: 0.4472\n",
      "Epoch 24/100\n",
      "6091/6091 [==============================] - 1s 98us/sample - loss: 0.3334 - val_loss: 0.4473\n",
      "Predicting...\n",
      "(Took 15.948 sec)\n",
      "Train on 6091 samples, validate on 1522 samples\n",
      "Epoch 1/100\n",
      "6091/6091 [==============================] - 1s 145us/sample - loss: 0.6419 - val_loss: 0.6130\n",
      "Epoch 2/100\n",
      "6091/6091 [==============================] - 1s 104us/sample - loss: 0.5775 - val_loss: 0.5726\n",
      "Epoch 3/100\n",
      "6091/6091 [==============================] - 1s 98us/sample - loss: 0.5371 - val_loss: 0.5464\n",
      "Epoch 4/100\n",
      "6091/6091 [==============================] - 1s 102us/sample - loss: 0.5083 - val_loss: 0.5283\n",
      "Epoch 5/100\n",
      "6091/6091 [==============================] - 1s 104us/sample - loss: 0.4858 - val_loss: 0.5147\n",
      "Epoch 6/100\n",
      "6091/6091 [==============================] - 1s 101us/sample - loss: 0.4674 - val_loss: 0.5037\n",
      "Epoch 7/100\n",
      "6091/6091 [==============================] - 1s 100us/sample - loss: 0.4519 - val_loss: 0.4947\n",
      "Epoch 8/100\n",
      "6091/6091 [==============================] - 1s 99us/sample - loss: 0.4384 - val_loss: 0.4878\n",
      "Epoch 9/100\n",
      "6091/6091 [==============================] - 1s 99us/sample - loss: 0.4266 - val_loss: 0.4822\n",
      "Epoch 10/100\n",
      "6091/6091 [==============================] - 1s 104us/sample - loss: 0.4161 - val_loss: 0.4772\n",
      "Epoch 11/100\n",
      "6091/6091 [==============================] - 1s 105us/sample - loss: 0.4065 - val_loss: 0.4729\n",
      "Epoch 12/100\n",
      "6091/6091 [==============================] - 1s 99us/sample - loss: 0.3978 - val_loss: 0.4698\n",
      "Epoch 13/100\n",
      "6091/6091 [==============================] - 1s 102us/sample - loss: 0.3898 - val_loss: 0.4664\n",
      "Epoch 14/100\n",
      "6091/6091 [==============================] - 1s 106us/sample - loss: 0.3826 - val_loss: 0.4644\n",
      "Epoch 15/100\n",
      "6091/6091 [==============================] - 1s 99us/sample - loss: 0.3758 - val_loss: 0.4621\n",
      "Epoch 16/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.3694 - val_loss: 0.4607\n",
      "Epoch 17/100\n",
      "6091/6091 [==============================] - 1s 104us/sample - loss: 0.3636 - val_loss: 0.4589\n",
      "Epoch 18/100\n",
      "6091/6091 [==============================] - 1s 106us/sample - loss: 0.3581 - val_loss: 0.4584\n",
      "Epoch 19/100\n",
      "6091/6091 [==============================] - 1s 104us/sample - loss: 0.3528 - val_loss: 0.4575\n",
      "Epoch 20/100\n",
      "6091/6091 [==============================] - 1s 104us/sample - loss: 0.3479 - val_loss: 0.4567\n",
      "Epoch 21/100\n",
      "6091/6091 [==============================] - 1s 100us/sample - loss: 0.3432 - val_loss: 0.4561\n",
      "Epoch 22/100\n",
      "6091/6091 [==============================] - 1s 98us/sample - loss: 0.3388 - val_loss: 0.4559\n",
      "Epoch 23/100\n",
      "6091/6091 [==============================] - 1s 104us/sample - loss: 0.3345 - val_loss: 0.4558\n",
      "Epoch 24/100\n",
      "6091/6091 [==============================] - 1s 100us/sample - loss: 0.3304 - val_loss: 0.4556\n",
      "Epoch 25/100\n",
      "6091/6091 [==============================] - 1s 99us/sample - loss: 0.3266 - val_loss: 0.4558\n",
      "Predicting...\n",
      "(Took 16.299 sec)\n",
      "Combined confusion matrix:\n",
      "[[3820.  522.]\n",
      " [ 968. 2303.]]\n",
      "(Overall, took 79.507 sec)\n",
      "Accuracy: 80.43% +/- 0.90%\n",
      "Precision for positive class: 79.78% +/- 1.77%\n",
      "Precision for negative class: 81.52% +/- 1.70%\n",
      "Recall for positive class: 87.98% +/- 0.98%\n",
      "Recall for negative class: 70.42% +/- 2.02%\n",
      "F for positive class: 83.67% +/- 0.88%\n",
      "F for negative class: 75.55% +/- 1.18%\n",
      "Mean F score: 79.61% +/- 0.90%\n"
     ]
    }
   ],
   "source": [
    "# POS tagging\n",
    "# Raw counts\n",
    "all_metrics(cross_validate_logistic(dftrain_pos_minimal, 100))\n",
    "# Bigram\n",
    "# Raw counts\n",
    "all_metrics(cross_validate_logistic(dftrain_bi_minimal, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/100\n",
      "6090/6090 [==============================] - 1s 148us/sample - loss: 0.6864 - val_loss: 0.6787\n",
      "Epoch 2/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.6744 - val_loss: 0.6686\n",
      "Epoch 3/100\n",
      "6090/6090 [==============================] - 1s 101us/sample - loss: 0.6653 - val_loss: 0.6607\n",
      "Epoch 4/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.6576 - val_loss: 0.6539\n",
      "Epoch 5/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.6505 - val_loss: 0.6477\n",
      "Epoch 6/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.6437 - val_loss: 0.6417\n",
      "Epoch 7/100\n",
      "6090/6090 [==============================] - 1s 118us/sample - loss: 0.6372 - val_loss: 0.6362\n",
      "Epoch 8/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.6310 - val_loss: 0.6308\n",
      "Epoch 9/100\n",
      "6090/6090 [==============================] - 1s 101us/sample - loss: 0.6251 - val_loss: 0.6257\n",
      "Epoch 10/100\n",
      "6090/6090 [==============================] - 1s 122us/sample - loss: 0.6193 - val_loss: 0.6209\n",
      "Epoch 11/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.6138 - val_loss: 0.6162\n",
      "Epoch 12/100\n",
      "6090/6090 [==============================] - 1s 116us/sample - loss: 0.6085 - val_loss: 0.6117\n",
      "Epoch 13/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.6034 - val_loss: 0.6073\n",
      "Epoch 14/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.5984 - val_loss: 0.6032\n",
      "Epoch 15/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.5936 - val_loss: 0.5992\n",
      "Epoch 16/100\n",
      "6090/6090 [==============================] - 1s 118us/sample - loss: 0.5890 - val_loss: 0.5954\n",
      "Epoch 17/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.5845 - val_loss: 0.5917\n",
      "Epoch 18/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.5801 - val_loss: 0.5881\n",
      "Epoch 19/100\n",
      "6090/6090 [==============================] - 1s 116us/sample - loss: 0.5759 - val_loss: 0.5846\n",
      "Epoch 20/100\n",
      "6090/6090 [==============================] - 1s 133us/sample - loss: 0.5719 - val_loss: 0.5812\n",
      "Epoch 21/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.5679 - val_loss: 0.5780\n",
      "Epoch 22/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.5641 - val_loss: 0.5749\n",
      "Epoch 23/100\n",
      "6090/6090 [==============================] - 1s 118us/sample - loss: 0.5603 - val_loss: 0.5718\n",
      "Epoch 24/100\n",
      "6090/6090 [==============================] - 1s 125us/sample - loss: 0.5567 - val_loss: 0.5689\n",
      "Epoch 25/100\n",
      "6090/6090 [==============================] - 1s 122us/sample - loss: 0.5532 - val_loss: 0.5661\n",
      "Epoch 26/100\n",
      "6090/6090 [==============================] - 1s 119us/sample - loss: 0.5497 - val_loss: 0.5634\n",
      "Epoch 27/100\n",
      "6090/6090 [==============================] - 1s 124us/sample - loss: 0.5464 - val_loss: 0.5607\n",
      "Epoch 28/100\n",
      "6090/6090 [==============================] - 1s 117us/sample - loss: 0.5432 - val_loss: 0.5581\n",
      "Epoch 29/100\n",
      "6090/6090 [==============================] - 1s 101us/sample - loss: 0.5400 - val_loss: 0.5556\n",
      "Epoch 30/100\n",
      "6090/6090 [==============================] - 1s 101us/sample - loss: 0.5369 - val_loss: 0.5532\n",
      "Epoch 31/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.5339 - val_loss: 0.5508\n",
      "Epoch 32/100\n",
      "6090/6090 [==============================] - 1s 98us/sample - loss: 0.5309 - val_loss: 0.5485\n",
      "Epoch 33/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.5281 - val_loss: 0.5464\n",
      "Epoch 34/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.5253 - val_loss: 0.5442\n",
      "Epoch 35/100\n",
      "6090/6090 [==============================] - 1s 126us/sample - loss: 0.5225 - val_loss: 0.5421\n",
      "Epoch 36/100\n",
      "6090/6090 [==============================] - 1s 99us/sample - loss: 0.5199 - val_loss: 0.5401\n",
      "Epoch 37/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.5172 - val_loss: 0.5381\n",
      "Epoch 38/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.5147 - val_loss: 0.5362\n",
      "Epoch 39/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.5122 - val_loss: 0.5343\n",
      "Epoch 40/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.5097 - val_loss: 0.5325\n",
      "Epoch 41/100\n",
      "6090/6090 [==============================] - 1s 98us/sample - loss: 0.5073 - val_loss: 0.5307\n",
      "Epoch 42/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.5050 - val_loss: 0.5290\n",
      "Epoch 43/100\n",
      "6090/6090 [==============================] - 1s 121us/sample - loss: 0.5026 - val_loss: 0.5273\n",
      "Epoch 44/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.5004 - val_loss: 0.5257\n",
      "Epoch 45/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4982 - val_loss: 0.5241\n",
      "Epoch 46/100\n",
      "6090/6090 [==============================] - 1s 117us/sample - loss: 0.4960 - val_loss: 0.5226\n",
      "Epoch 47/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4939 - val_loss: 0.5211\n",
      "Epoch 48/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.4918 - val_loss: 0.5196\n",
      "Epoch 49/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4898 - val_loss: 0.5181\n",
      "Epoch 50/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.4877 - val_loss: 0.5167\n",
      "Epoch 51/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.4857 - val_loss: 0.5154\n",
      "Epoch 52/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.4838 - val_loss: 0.5140\n",
      "Epoch 53/100\n",
      "6090/6090 [==============================] - 1s 101us/sample - loss: 0.4818 - val_loss: 0.5127\n",
      "Epoch 54/100\n",
      "6090/6090 [==============================] - 1s 98us/sample - loss: 0.4800 - val_loss: 0.5114\n",
      "Epoch 55/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4781 - val_loss: 0.5102\n",
      "Epoch 56/100\n",
      "6090/6090 [==============================] - 1s 101us/sample - loss: 0.4763 - val_loss: 0.5090\n",
      "Epoch 57/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.4745 - val_loss: 0.5078\n",
      "Epoch 58/100\n",
      "6090/6090 [==============================] - 1s 97us/sample - loss: 0.4727 - val_loss: 0.5066\n",
      "Epoch 59/100\n",
      "6090/6090 [==============================] - 1s 94us/sample - loss: 0.4710 - val_loss: 0.5054\n",
      "Epoch 60/100\n",
      "6090/6090 [==============================] - 1s 95us/sample - loss: 0.4693 - val_loss: 0.5044\n",
      "Epoch 61/100\n",
      "6090/6090 [==============================] - 1s 97us/sample - loss: 0.4676 - val_loss: 0.5032\n",
      "Epoch 62/100\n",
      "6090/6090 [==============================] - 1s 96us/sample - loss: 0.4660 - val_loss: 0.5022\n",
      "Epoch 63/100\n",
      "6090/6090 [==============================] - 1s 98us/sample - loss: 0.4644 - val_loss: 0.5011\n",
      "Epoch 64/100\n",
      "6090/6090 [==============================] - 1s 99us/sample - loss: 0.4627 - val_loss: 0.5001\n",
      "Epoch 65/100\n",
      "6090/6090 [==============================] - 1s 98us/sample - loss: 0.4612 - val_loss: 0.4991\n",
      "Epoch 66/100\n",
      "6090/6090 [==============================] - 1s 122us/sample - loss: 0.4596 - val_loss: 0.4982\n",
      "Epoch 67/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4581 - val_loss: 0.4972\n",
      "Epoch 68/100\n",
      "6090/6090 [==============================] - 1s 116us/sample - loss: 0.4566 - val_loss: 0.4963\n",
      "Epoch 69/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.4551 - val_loss: 0.4954\n",
      "Epoch 70/100\n",
      "6090/6090 [==============================] - 1s 125us/sample - loss: 0.4536 - val_loss: 0.4945\n",
      "Epoch 71/100\n",
      "6090/6090 [==============================] - 1s 136us/sample - loss: 0.4521 - val_loss: 0.4936\n",
      "Epoch 72/100\n",
      "6090/6090 [==============================] - 1s 124us/sample - loss: 0.4507 - val_loss: 0.4928\n",
      "Epoch 73/100\n",
      "6090/6090 [==============================] - 1s 121us/sample - loss: 0.4493 - val_loss: 0.4919\n",
      "Epoch 74/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.4479 - val_loss: 0.4910\n",
      "Epoch 75/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.4465 - val_loss: 0.4903\n",
      "Epoch 76/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.4452 - val_loss: 0.4894\n",
      "Epoch 77/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4438 - val_loss: 0.4887\n",
      "Epoch 78/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.4425 - val_loss: 0.4879\n",
      "Epoch 79/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.4412 - val_loss: 0.4871\n",
      "Epoch 80/100\n",
      "6090/6090 [==============================] - 1s 99us/sample - loss: 0.4399 - val_loss: 0.4864\n",
      "Epoch 81/100\n",
      "6090/6090 [==============================] - 1s 99us/sample - loss: 0.4386 - val_loss: 0.4857\n",
      "Epoch 82/100\n",
      "6090/6090 [==============================] - 1s 98us/sample - loss: 0.4373 - val_loss: 0.4851\n",
      "Epoch 83/100\n",
      "6090/6090 [==============================] - 1s 95us/sample - loss: 0.4361 - val_loss: 0.4843\n",
      "Epoch 84/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.4349 - val_loss: 0.4837\n",
      "Epoch 85/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.4337 - val_loss: 0.4830\n",
      "Epoch 86/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.4325 - val_loss: 0.4824\n",
      "Epoch 87/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.4313 - val_loss: 0.4817\n",
      "Epoch 88/100\n",
      "6090/6090 [==============================] - 1s 120us/sample - loss: 0.4301 - val_loss: 0.4811\n",
      "Epoch 89/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4290 - val_loss: 0.4805\n",
      "Epoch 90/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4278 - val_loss: 0.4799\n",
      "Epoch 91/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.4267 - val_loss: 0.4793\n",
      "Epoch 92/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.4256 - val_loss: 0.4788\n",
      "Epoch 93/100\n",
      "6090/6090 [==============================] - 1s 99us/sample - loss: 0.4245 - val_loss: 0.4782\n",
      "Epoch 94/100\n",
      "6090/6090 [==============================] - 1s 101us/sample - loss: 0.4234 - val_loss: 0.4776\n",
      "Epoch 95/100\n",
      "6090/6090 [==============================] - 1s 99us/sample - loss: 0.4223 - val_loss: 0.4771\n",
      "Epoch 96/100\n",
      "6090/6090 [==============================] - 1s 98us/sample - loss: 0.4212 - val_loss: 0.4766\n",
      "Epoch 97/100\n",
      "6090/6090 [==============================] - 1s 95us/sample - loss: 0.4201 - val_loss: 0.4760\n",
      "Epoch 98/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.4191 - val_loss: 0.4755\n",
      "Epoch 99/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.4181 - val_loss: 0.4750\n",
      "Epoch 100/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.4170 - val_loss: 0.4745\n",
      "Predicting...\n",
      "(Took 66.295 sec)\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/100\n",
      "6090/6090 [==============================] - 1s 172us/sample - loss: 0.6864 - val_loss: 0.6780\n",
      "Epoch 2/100\n",
      "6090/6090 [==============================] - 1s 101us/sample - loss: 0.6747 - val_loss: 0.6675\n",
      "Epoch 3/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.6659 - val_loss: 0.6594\n",
      "Epoch 4/100\n",
      "6090/6090 [==============================] - 1s 96us/sample - loss: 0.6583 - val_loss: 0.6524\n",
      "Epoch 5/100\n",
      "6090/6090 [==============================] - 1s 96us/sample - loss: 0.6512 - val_loss: 0.6461\n",
      "Epoch 6/100\n",
      "6090/6090 [==============================] - 1s 99us/sample - loss: 0.6444 - val_loss: 0.6403\n",
      "Epoch 7/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.6379 - val_loss: 0.6347\n",
      "Epoch 8/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.6317 - val_loss: 0.6293\n",
      "Epoch 9/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.6257 - val_loss: 0.6243\n",
      "Epoch 10/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.6199 - val_loss: 0.6195\n",
      "Epoch 11/100\n",
      "6090/6090 [==============================] - 1s 96us/sample - loss: 0.6144 - val_loss: 0.6147\n",
      "Epoch 12/100\n",
      "6090/6090 [==============================] - 1s 95us/sample - loss: 0.6090 - val_loss: 0.6103\n",
      "Epoch 13/100\n",
      "6090/6090 [==============================] - 1s 98us/sample - loss: 0.6039 - val_loss: 0.6060\n",
      "Epoch 14/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.5989 - val_loss: 0.6019\n",
      "Epoch 15/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.5941 - val_loss: 0.5979\n",
      "Epoch 16/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.5894 - val_loss: 0.5941\n",
      "Epoch 17/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.5849 - val_loss: 0.5904\n",
      "Epoch 18/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.5805 - val_loss: 0.5869\n",
      "Epoch 19/100\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.5763 - val_loss: 0.5835\n",
      "Epoch 20/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.5722 - val_loss: 0.5801\n",
      "Epoch 21/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.5682 - val_loss: 0.5771\n",
      "Epoch 22/100\n",
      "6090/6090 [==============================] - 1s 97us/sample - loss: 0.5644 - val_loss: 0.5740\n",
      "Epoch 23/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.5606 - val_loss: 0.5709\n",
      "Epoch 24/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.5570 - val_loss: 0.5681\n",
      "Epoch 25/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.5534 - val_loss: 0.5653\n",
      "Epoch 26/100\n",
      "6090/6090 [==============================] - 1s 101us/sample - loss: 0.5500 - val_loss: 0.5627\n",
      "Epoch 27/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.5466 - val_loss: 0.5601\n",
      "Epoch 28/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.5434 - val_loss: 0.5575\n",
      "Epoch 29/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.5402 - val_loss: 0.5551\n",
      "Epoch 30/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.5371 - val_loss: 0.5528\n",
      "Epoch 31/100\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.5341 - val_loss: 0.5505\n",
      "Epoch 32/100\n",
      "6090/6090 [==============================] - 1s 99us/sample - loss: 0.5311 - val_loss: 0.5482\n",
      "Epoch 33/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.5282 - val_loss: 0.5461\n",
      "Epoch 34/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.5254 - val_loss: 0.5441\n",
      "Epoch 35/100\n",
      "6090/6090 [==============================] - 1s 98us/sample - loss: 0.5227 - val_loss: 0.5421\n",
      "Epoch 36/100\n",
      "6090/6090 [==============================] - 1s 97us/sample - loss: 0.5200 - val_loss: 0.5401\n",
      "Epoch 37/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.5173 - val_loss: 0.5382\n",
      "Epoch 38/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.5148 - val_loss: 0.5363\n",
      "Epoch 39/100\n",
      "6090/6090 [==============================] - 1s 99us/sample - loss: 0.5122 - val_loss: 0.5345\n",
      "Epoch 40/100\n",
      "6090/6090 [==============================] - 1s 101us/sample - loss: 0.5098 - val_loss: 0.5328\n",
      "Epoch 41/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.5074 - val_loss: 0.5311\n",
      "Epoch 42/100\n",
      "6090/6090 [==============================] - 1s 97us/sample - loss: 0.5050 - val_loss: 0.5293\n",
      "Epoch 43/100\n",
      "6090/6090 [==============================] - 1s 97us/sample - loss: 0.5027 - val_loss: 0.5277\n",
      "Epoch 44/100\n",
      "6090/6090 [==============================] - 1s 98us/sample - loss: 0.5004 - val_loss: 0.5262\n",
      "Epoch 45/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.4982 - val_loss: 0.5247\n",
      "Epoch 46/100\n",
      "6090/6090 [==============================] - 1s 121us/sample - loss: 0.4960 - val_loss: 0.5232\n",
      "Epoch 47/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4939 - val_loss: 0.5217\n",
      "Epoch 48/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.4917 - val_loss: 0.5203\n",
      "Epoch 49/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.4897 - val_loss: 0.5188\n",
      "Epoch 50/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4877 - val_loss: 0.5175\n",
      "Epoch 51/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6090/6090 [==============================] - 1s 123us/sample - loss: 0.4857 - val_loss: 0.5162\n",
      "Epoch 52/100\n",
      "6090/6090 [==============================] - 1s 132us/sample - loss: 0.4837 - val_loss: 0.5149\n",
      "Epoch 53/100\n",
      "6090/6090 [==============================] - 1s 127us/sample - loss: 0.4818 - val_loss: 0.5137\n",
      "Epoch 54/100\n",
      "6090/6090 [==============================] - 1s 126us/sample - loss: 0.4799 - val_loss: 0.5124\n",
      "Epoch 55/100\n",
      "6090/6090 [==============================] - 1s 98us/sample - loss: 0.4780 - val_loss: 0.5111\n",
      "Epoch 56/100\n",
      "6090/6090 [==============================] - 1s 121us/sample - loss: 0.4762 - val_loss: 0.5099\n",
      "Epoch 57/100\n",
      "6090/6090 [==============================] - 1s 120us/sample - loss: 0.4744 - val_loss: 0.5088\n",
      "Epoch 58/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.4726 - val_loss: 0.5077\n",
      "Epoch 59/100\n",
      "6090/6090 [==============================] - 1s 95us/sample - loss: 0.4708 - val_loss: 0.5065\n",
      "Epoch 60/100\n",
      "6090/6090 [==============================] - 1s 121us/sample - loss: 0.4691 - val_loss: 0.5055\n",
      "Epoch 61/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4675 - val_loss: 0.5045\n",
      "Epoch 62/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.4658 - val_loss: 0.5034\n",
      "Epoch 63/100\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.4641 - val_loss: 0.5025\n",
      "Epoch 64/100\n",
      "6090/6090 [==============================] - 1s 97us/sample - loss: 0.4625 - val_loss: 0.5015\n",
      "Epoch 65/100\n",
      "6090/6090 [==============================] - 1s 97us/sample - loss: 0.4609 - val_loss: 0.5006\n",
      "Epoch 66/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.4594 - val_loss: 0.4996\n",
      "Epoch 67/100\n",
      "6090/6090 [==============================] - 1s 122us/sample - loss: 0.4578 - val_loss: 0.4988\n",
      "Epoch 68/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.4563 - val_loss: 0.4978\n",
      "Epoch 69/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.4548 - val_loss: 0.4969\n",
      "Epoch 70/100\n",
      "6090/6090 [==============================] - 1s 118us/sample - loss: 0.4533 - val_loss: 0.4961\n",
      "Epoch 71/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.4519 - val_loss: 0.4952\n",
      "Epoch 72/100\n",
      "6090/6090 [==============================] - 1s 96us/sample - loss: 0.4504 - val_loss: 0.4944\n",
      "Epoch 73/100\n",
      "6090/6090 [==============================] - 1s 95us/sample - loss: 0.4490 - val_loss: 0.4936\n",
      "Epoch 74/100\n",
      "6090/6090 [==============================] - 1s 97us/sample - loss: 0.4476 - val_loss: 0.4928\n",
      "Epoch 75/100\n",
      "6090/6090 [==============================] - 1s 95us/sample - loss: 0.4462 - val_loss: 0.4921\n",
      "Epoch 76/100\n",
      "6090/6090 [==============================] - 1s 95us/sample - loss: 0.4448 - val_loss: 0.4913\n",
      "Epoch 77/100\n",
      "6090/6090 [==============================] - 1s 95us/sample - loss: 0.4435 - val_loss: 0.4906\n",
      "Epoch 78/100\n",
      "6090/6090 [==============================] - 1s 96us/sample - loss: 0.4422 - val_loss: 0.4900\n",
      "Epoch 79/100\n",
      "6090/6090 [==============================] - 1s 96us/sample - loss: 0.4409 - val_loss: 0.4892\n",
      "Epoch 80/100\n",
      "6090/6090 [==============================] - 1s 96us/sample - loss: 0.4396 - val_loss: 0.4885\n",
      "Epoch 81/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.4383 - val_loss: 0.4878\n",
      "Epoch 82/100\n",
      "6090/6090 [==============================] - 1s 96us/sample - loss: 0.4370 - val_loss: 0.4871\n",
      "Epoch 83/100\n",
      "6090/6090 [==============================] - 1s 95us/sample - loss: 0.4358 - val_loss: 0.4865\n",
      "Epoch 84/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4345 - val_loss: 0.4858\n",
      "Epoch 85/100\n",
      "6090/6090 [==============================] - 1s 94us/sample - loss: 0.4333 - val_loss: 0.4852\n",
      "Epoch 86/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.4321 - val_loss: 0.4846\n",
      "Epoch 87/100\n",
      "6090/6090 [==============================] - 1s 101us/sample - loss: 0.4309 - val_loss: 0.4839\n",
      "Epoch 88/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.4298 - val_loss: 0.4834\n",
      "Epoch 89/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.4286 - val_loss: 0.4829\n",
      "Epoch 90/100\n",
      "6090/6090 [==============================] - 1s 118us/sample - loss: 0.4274 - val_loss: 0.4823\n",
      "Epoch 91/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.4263 - val_loss: 0.4817\n",
      "Epoch 92/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.4252 - val_loss: 0.4812\n",
      "Epoch 93/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.4240 - val_loss: 0.4806\n",
      "Epoch 94/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.4230 - val_loss: 0.4801\n",
      "Epoch 95/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.4219 - val_loss: 0.4795\n",
      "Epoch 96/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.4208 - val_loss: 0.4791\n",
      "Epoch 97/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.4197 - val_loss: 0.4786\n",
      "Epoch 98/100\n",
      "6090/6090 [==============================] - 1s 99us/sample - loss: 0.4187 - val_loss: 0.4782\n",
      "Epoch 99/100\n",
      "6090/6090 [==============================] - 1s 97us/sample - loss: 0.4176 - val_loss: 0.4777\n",
      "Epoch 100/100\n",
      "6090/6090 [==============================] - 1s 99us/sample - loss: 0.4166 - val_loss: 0.4772\n",
      "Predicting...\n",
      "(Took 64.932 sec)\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/100\n",
      "6090/6090 [==============================] - 1s 151us/sample - loss: 0.6863 - val_loss: 0.6817\n",
      "Epoch 2/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.6739 - val_loss: 0.6735\n",
      "Epoch 3/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.6646 - val_loss: 0.6669\n",
      "Epoch 4/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.6567 - val_loss: 0.6608\n",
      "Epoch 5/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.6495 - val_loss: 0.6550\n",
      "Epoch 6/100\n",
      "6090/6090 [==============================] - 1s 101us/sample - loss: 0.6427 - val_loss: 0.6496\n",
      "Epoch 7/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.6363 - val_loss: 0.6444\n",
      "Epoch 8/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.6300 - val_loss: 0.6392\n",
      "Epoch 9/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.6241 - val_loss: 0.6342\n",
      "Epoch 10/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.6183 - val_loss: 0.6294\n",
      "Epoch 11/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.6127 - val_loss: 0.6249\n",
      "Epoch 12/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.6074 - val_loss: 0.6206\n",
      "Epoch 13/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.6022 - val_loss: 0.6163\n",
      "Epoch 14/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.5972 - val_loss: 0.6123\n",
      "Epoch 15/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.5924 - val_loss: 0.6085\n",
      "Epoch 16/100\n",
      "6090/6090 [==============================] - 1s 117us/sample - loss: 0.5877 - val_loss: 0.6048\n",
      "Epoch 17/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.5832 - val_loss: 0.6012\n",
      "Epoch 18/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.5789 - val_loss: 0.5977\n",
      "Epoch 19/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.5746 - val_loss: 0.5944\n",
      "Epoch 20/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.5705 - val_loss: 0.5913\n",
      "Epoch 21/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.5665 - val_loss: 0.5882\n",
      "Epoch 22/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.5627 - val_loss: 0.5852\n",
      "Epoch 23/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.5589 - val_loss: 0.5823\n",
      "Epoch 24/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.5553 - val_loss: 0.5796\n",
      "Epoch 25/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.5517 - val_loss: 0.5769\n",
      "Epoch 26/100\n",
      "6090/6090 [==============================] - 1s 96us/sample - loss: 0.5483 - val_loss: 0.5743\n",
      "Epoch 27/100\n",
      "6090/6090 [==============================] - 1s 96us/sample - loss: 0.5449 - val_loss: 0.5718\n",
      "Epoch 28/100\n",
      "6090/6090 [==============================] - 1s 95us/sample - loss: 0.5416 - val_loss: 0.5694\n",
      "Epoch 29/100\n",
      "6090/6090 [==============================] - 1s 99us/sample - loss: 0.5384 - val_loss: 0.5670\n",
      "Epoch 30/100\n",
      "6090/6090 [==============================] - 1s 99us/sample - loss: 0.5353 - val_loss: 0.5647\n",
      "Epoch 31/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.5322 - val_loss: 0.5625\n",
      "Epoch 32/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.5293 - val_loss: 0.5603\n",
      "Epoch 33/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.5264 - val_loss: 0.5582\n",
      "Epoch 34/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.5236 - val_loss: 0.5563\n",
      "Epoch 35/100\n",
      "6090/6090 [==============================] - 1s 99us/sample - loss: 0.5208 - val_loss: 0.5543\n",
      "Epoch 36/100\n",
      "6090/6090 [==============================] - 1s 97us/sample - loss: 0.5181 - val_loss: 0.5524\n",
      "Epoch 37/100\n",
      "6090/6090 [==============================] - 1s 97us/sample - loss: 0.5154 - val_loss: 0.5506\n",
      "Epoch 38/100\n",
      "6090/6090 [==============================] - 1s 97us/sample - loss: 0.5129 - val_loss: 0.5488\n",
      "Epoch 39/100\n",
      "6090/6090 [==============================] - 1s 95us/sample - loss: 0.5103 - val_loss: 0.5471\n",
      "Epoch 40/100\n",
      "6090/6090 [==============================] - 1s 96us/sample - loss: 0.5079 - val_loss: 0.5454\n",
      "Epoch 41/100\n",
      "6090/6090 [==============================] - 1s 95us/sample - loss: 0.5055 - val_loss: 0.5437\n",
      "Epoch 42/100\n",
      "6090/6090 [==============================] - 1s 95us/sample - loss: 0.5031 - val_loss: 0.5421\n",
      "Epoch 43/100\n",
      "6090/6090 [==============================] - 1s 96us/sample - loss: 0.5007 - val_loss: 0.5404\n",
      "Epoch 44/100\n",
      "6090/6090 [==============================] - 1s 96us/sample - loss: 0.4984 - val_loss: 0.5390\n",
      "Epoch 45/100\n",
      "6090/6090 [==============================] - 1s 95us/sample - loss: 0.4962 - val_loss: 0.5376\n",
      "Epoch 46/100\n",
      "6090/6090 [==============================] - ETA: 0s - loss: 0.494 - 1s 97us/sample - loss: 0.4941 - val_loss: 0.5361\n",
      "Epoch 47/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.4919 - val_loss: 0.5347\n",
      "Epoch 48/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.4898 - val_loss: 0.5333\n",
      "Epoch 49/100\n",
      "6090/6090 [==============================] - 1s 120us/sample - loss: 0.4877 - val_loss: 0.5318\n",
      "Epoch 50/100\n",
      "6090/6090 [==============================] - 1s 116us/sample - loss: 0.4856 - val_loss: 0.5305\n",
      "Epoch 51/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4836 - val_loss: 0.5293\n",
      "Epoch 52/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4817 - val_loss: 0.5281\n",
      "Epoch 53/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4798 - val_loss: 0.5268\n",
      "Epoch 54/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.4778 - val_loss: 0.5257\n",
      "Epoch 55/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4760 - val_loss: 0.5245\n",
      "Epoch 56/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.4741 - val_loss: 0.5234\n",
      "Epoch 57/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.4723 - val_loss: 0.5223\n",
      "Epoch 58/100\n",
      "6090/6090 [==============================] - 1s 101us/sample - loss: 0.4706 - val_loss: 0.5212\n",
      "Epoch 59/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4688 - val_loss: 0.5201\n",
      "Epoch 60/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.4671 - val_loss: 0.5192\n",
      "Epoch 61/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.4654 - val_loss: 0.5181\n",
      "Epoch 62/100\n",
      "6090/6090 [==============================] - 1s 101us/sample - loss: 0.4637 - val_loss: 0.5170\n",
      "Epoch 63/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.4621 - val_loss: 0.5161\n",
      "Epoch 64/100\n",
      "6090/6090 [==============================] - 1s 101us/sample - loss: 0.4605 - val_loss: 0.5151\n",
      "Epoch 65/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.4589 - val_loss: 0.5142\n",
      "Epoch 66/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.4573 - val_loss: 0.5133\n",
      "Epoch 67/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.4558 - val_loss: 0.5125\n",
      "Epoch 68/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4542 - val_loss: 0.5116\n",
      "Epoch 69/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.4528 - val_loss: 0.5107\n",
      "Epoch 70/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4513 - val_loss: 0.5100\n",
      "Epoch 71/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.4498 - val_loss: 0.5091\n",
      "Epoch 72/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.4484 - val_loss: 0.5083\n",
      "Epoch 73/100\n",
      "6090/6090 [==============================] - 1s 116us/sample - loss: 0.4469 - val_loss: 0.5075\n",
      "Epoch 74/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.4455 - val_loss: 0.5067\n",
      "Epoch 75/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.4442 - val_loss: 0.5060\n",
      "Epoch 76/100\n",
      "6090/6090 [==============================] - 1s 99us/sample - loss: 0.4428 - val_loss: 0.5053\n",
      "Epoch 77/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.4414 - val_loss: 0.5045\n",
      "Epoch 78/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.4401 - val_loss: 0.5038\n",
      "Epoch 79/100\n",
      "6090/6090 [==============================] - 1s 101us/sample - loss: 0.4388 - val_loss: 0.5031\n",
      "Epoch 80/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4375 - val_loss: 0.5024\n",
      "Epoch 81/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4362 - val_loss: 0.5018\n",
      "Epoch 82/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4349 - val_loss: 0.5011\n",
      "Epoch 83/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.4337 - val_loss: 0.5005\n",
      "Epoch 84/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.4325 - val_loss: 0.4998\n",
      "Epoch 85/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.4313 - val_loss: 0.4992\n",
      "Epoch 86/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.4300 - val_loss: 0.4986\n",
      "Epoch 87/100\n",
      "6090/6090 [==============================] - 1s 101us/sample - loss: 0.4288 - val_loss: 0.4980\n",
      "Epoch 88/100\n",
      "6090/6090 [==============================] - 1s 98us/sample - loss: 0.4277 - val_loss: 0.4975\n",
      "Epoch 89/100\n",
      "6090/6090 [==============================] - 1s 99us/sample - loss: 0.4265 - val_loss: 0.4969\n",
      "Epoch 90/100\n",
      "6090/6090 [==============================] - 1s 99us/sample - loss: 0.4253 - val_loss: 0.4964\n",
      "Epoch 91/100\n",
      "6090/6090 [==============================] - 1s 99us/sample - loss: 0.4242 - val_loss: 0.4959\n",
      "Epoch 92/100\n",
      "6090/6090 [==============================] - 1s 99us/sample - loss: 0.4231 - val_loss: 0.4953\n",
      "Epoch 93/100\n",
      "6090/6090 [==============================] - 1s 99us/sample - loss: 0.4219 - val_loss: 0.4947\n",
      "Epoch 94/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.4208 - val_loss: 0.4942\n",
      "Epoch 95/100\n",
      "6090/6090 [==============================] - 1s 99us/sample - loss: 0.4198 - val_loss: 0.4937\n",
      "Epoch 96/100\n",
      "6090/6090 [==============================] - 1s 98us/sample - loss: 0.4187 - val_loss: 0.4932\n",
      "Epoch 97/100\n",
      "6090/6090 [==============================] - 1s 99us/sample - loss: 0.4176 - val_loss: 0.4927\n",
      "Epoch 98/100\n",
      "6090/6090 [==============================] - 1s 98us/sample - loss: 0.4166 - val_loss: 0.4922\n",
      "Epoch 99/100\n",
      "6090/6090 [==============================] - 1s 99us/sample - loss: 0.4155 - val_loss: 0.4918\n",
      "Epoch 100/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.4145 - val_loss: 0.4913\n",
      "Predicting...\n",
      "(Took 63.935 sec)\n",
      "Train on 6091 samples, validate on 1522 samples\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6091/6091 [==============================] - ETA: 0s - loss: 0.686 - 1s 142us/sample - loss: 0.6855 - val_loss: 0.6789\n",
      "Epoch 2/100\n",
      "6091/6091 [==============================] - 1s 101us/sample - loss: 0.6737 - val_loss: 0.6695\n",
      "Epoch 3/100\n",
      "6091/6091 [==============================] - 1s 99us/sample - loss: 0.6648 - val_loss: 0.6621\n",
      "Epoch 4/100\n",
      "6091/6091 [==============================] - 1s 99us/sample - loss: 0.6571 - val_loss: 0.6555\n",
      "Epoch 5/100\n",
      "6091/6091 [==============================] - 1s 98us/sample - loss: 0.6501 - val_loss: 0.6494\n",
      "Epoch 6/100\n",
      "6091/6091 [==============================] - 1s 101us/sample - loss: 0.6433 - val_loss: 0.6435\n",
      "Epoch 7/100\n",
      "6091/6091 [==============================] - 1s 110us/sample - loss: 0.6369 - val_loss: 0.6380\n",
      "Epoch 8/100\n",
      "6091/6091 [==============================] - 1s 102us/sample - loss: 0.6307 - val_loss: 0.6327\n",
      "Epoch 9/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.6248 - val_loss: 0.6276\n",
      "Epoch 10/100\n",
      "6091/6091 [==============================] - 1s 105us/sample - loss: 0.6190 - val_loss: 0.6227\n",
      "Epoch 11/100\n",
      "6091/6091 [==============================] - 1s 97us/sample - loss: 0.6135 - val_loss: 0.6181\n",
      "Epoch 12/100\n",
      "6091/6091 [==============================] - 1s 98us/sample - loss: 0.6082 - val_loss: 0.6136\n",
      "Epoch 13/100\n",
      "6091/6091 [==============================] - 1s 97us/sample - loss: 0.6031 - val_loss: 0.6093\n",
      "Epoch 14/100\n",
      "6091/6091 [==============================] - 1s 96us/sample - loss: 0.5982 - val_loss: 0.6051\n",
      "Epoch 15/100\n",
      "6091/6091 [==============================] - 1s 98us/sample - loss: 0.5934 - val_loss: 0.6012\n",
      "Epoch 16/100\n",
      "6091/6091 [==============================] - 1s 97us/sample - loss: 0.5888 - val_loss: 0.5973\n",
      "Epoch 17/100\n",
      "6091/6091 [==============================] - 1s 102us/sample - loss: 0.5843 - val_loss: 0.5936\n",
      "Epoch 18/100\n",
      "6091/6091 [==============================] - 1s 97us/sample - loss: 0.5800 - val_loss: 0.5901\n",
      "Epoch 19/100\n",
      "6091/6091 [==============================] - 1s 98us/sample - loss: 0.5758 - val_loss: 0.5866\n",
      "Epoch 20/100\n",
      "6091/6091 [==============================] - 1s 98us/sample - loss: 0.5718 - val_loss: 0.5833\n",
      "Epoch 21/100\n",
      "6091/6091 [==============================] - 1s 111us/sample - loss: 0.5678 - val_loss: 0.5801\n",
      "Epoch 22/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.5640 - val_loss: 0.5769\n",
      "Epoch 23/100\n",
      "6091/6091 [==============================] - 1s 104us/sample - loss: 0.5603 - val_loss: 0.5740\n",
      "Epoch 24/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.5567 - val_loss: 0.5710\n",
      "Epoch 25/100\n",
      "6091/6091 [==============================] - 1s 105us/sample - loss: 0.5532 - val_loss: 0.5683\n",
      "Epoch 26/100\n",
      "6091/6091 [==============================] - 1s 102us/sample - loss: 0.5498 - val_loss: 0.5656\n",
      "Epoch 27/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.5465 - val_loss: 0.5629\n",
      "Epoch 28/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.5432 - val_loss: 0.5604\n",
      "Epoch 29/100\n",
      "6091/6091 [==============================] - 1s 97us/sample - loss: 0.5401 - val_loss: 0.5579\n",
      "Epoch 30/100\n",
      "6091/6091 [==============================] - 1s 97us/sample - loss: 0.5370 - val_loss: 0.5555\n",
      "Epoch 31/100\n",
      "6091/6091 [==============================] - 1s 99us/sample - loss: 0.5340 - val_loss: 0.5532\n",
      "Epoch 32/100\n",
      "6091/6091 [==============================] - 1s 98us/sample - loss: 0.5311 - val_loss: 0.5509\n",
      "Epoch 33/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.5282 - val_loss: 0.5487\n",
      "Epoch 34/100\n",
      "6091/6091 [==============================] - 1s 106us/sample - loss: 0.5255 - val_loss: 0.5466\n",
      "Epoch 35/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.5227 - val_loss: 0.5445\n",
      "Epoch 36/100\n",
      "6091/6091 [==============================] - 1s 103us/sample - loss: 0.5200 - val_loss: 0.5425\n",
      "Epoch 37/100\n",
      "6091/6091 [==============================] - 1s 103us/sample - loss: 0.5175 - val_loss: 0.5405\n",
      "Epoch 38/100\n",
      "6091/6091 [==============================] - 1s 105us/sample - loss: 0.5149 - val_loss: 0.5386\n",
      "Epoch 39/100\n",
      "6091/6091 [==============================] - 1s 103us/sample - loss: 0.5124 - val_loss: 0.5367\n",
      "Epoch 40/100\n",
      "6091/6091 [==============================] - 1s 119us/sample - loss: 0.5100 - val_loss: 0.5349\n",
      "Epoch 41/100\n",
      "6091/6091 [==============================] - 1s 116us/sample - loss: 0.5076 - val_loss: 0.5332\n",
      "Epoch 42/100\n",
      "6091/6091 [==============================] - 1s 111us/sample - loss: 0.5052 - val_loss: 0.5314\n",
      "Epoch 43/100\n",
      "6091/6091 [==============================] - 1s 116us/sample - loss: 0.5029 - val_loss: 0.5298\n",
      "Epoch 44/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.5007 - val_loss: 0.5281\n",
      "Epoch 45/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4985 - val_loss: 0.5265\n",
      "Epoch 46/100\n",
      "6091/6091 [==============================] - 1s 113us/sample - loss: 0.4963 - val_loss: 0.5250\n",
      "Epoch 47/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.4942 - val_loss: 0.5235\n",
      "Epoch 48/100\n",
      "6091/6091 [==============================] - 1s 117us/sample - loss: 0.4921 - val_loss: 0.5220\n",
      "Epoch 49/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.4901 - val_loss: 0.5205\n",
      "Epoch 50/100\n",
      "6091/6091 [==============================] - 1s 117us/sample - loss: 0.4881 - val_loss: 0.5191\n",
      "Epoch 51/100\n",
      "6091/6091 [==============================] - 1s 117us/sample - loss: 0.4861 - val_loss: 0.5178\n",
      "Epoch 52/100\n",
      "6091/6091 [==============================] - 1s 128us/sample - loss: 0.4842 - val_loss: 0.5164\n",
      "Epoch 53/100\n",
      "6091/6091 [==============================] - 1s 124us/sample - loss: 0.4823 - val_loss: 0.5151\n",
      "Epoch 54/100\n",
      "6091/6091 [==============================] - 1s 116us/sample - loss: 0.4804 - val_loss: 0.5138\n",
      "Epoch 55/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.4786 - val_loss: 0.5125\n",
      "Epoch 56/100\n",
      "6091/6091 [==============================] - 1s 110us/sample - loss: 0.4768 - val_loss: 0.5112\n",
      "Epoch 57/100\n",
      "6091/6091 [==============================] - 1s 145us/sample - loss: 0.4750 - val_loss: 0.5101\n",
      "Epoch 58/100\n",
      "6091/6091 [==============================] - 1s 140us/sample - loss: 0.4732 - val_loss: 0.5089\n",
      "Epoch 59/100\n",
      "6091/6091 [==============================] - 1s 146us/sample - loss: 0.4715 - val_loss: 0.5077\n",
      "Epoch 60/100\n",
      "6091/6091 [==============================] - 1s 140us/sample - loss: 0.4698 - val_loss: 0.5066\n",
      "Epoch 61/100\n",
      "6091/6091 [==============================] - 1s 127us/sample - loss: 0.4682 - val_loss: 0.5054\n",
      "Epoch 62/100\n",
      "6091/6091 [==============================] - 1s 139us/sample - loss: 0.4665 - val_loss: 0.5044\n",
      "Epoch 63/100\n",
      "6091/6091 [==============================] - 1s 132us/sample - loss: 0.4649 - val_loss: 0.5033\n",
      "Epoch 64/100\n",
      "6091/6091 [==============================] - 1s 124us/sample - loss: 0.4633 - val_loss: 0.5023\n",
      "Epoch 65/100\n",
      "6091/6091 [==============================] - 1s 132us/sample - loss: 0.4617 - val_loss: 0.5013\n",
      "Epoch 66/100\n",
      "6091/6091 [==============================] - 1s 125us/sample - loss: 0.4602 - val_loss: 0.5002\n",
      "Epoch 67/100\n",
      "6091/6091 [==============================] - 1s 122us/sample - loss: 0.4587 - val_loss: 0.4993\n",
      "Epoch 68/100\n",
      "6091/6091 [==============================] - 1s 124us/sample - loss: 0.4572 - val_loss: 0.4983\n",
      "Epoch 69/100\n",
      "6091/6091 [==============================] - 1s 105us/sample - loss: 0.4557 - val_loss: 0.4973\n",
      "Epoch 70/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.4542 - val_loss: 0.4965\n",
      "Epoch 71/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.4528 - val_loss: 0.4955\n",
      "Epoch 72/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.4513 - val_loss: 0.4946\n",
      "Epoch 73/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4499 - val_loss: 0.4938\n",
      "Epoch 74/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.4486 - val_loss: 0.4929\n",
      "Epoch 75/100\n",
      "6091/6091 [==============================] - 1s 105us/sample - loss: 0.4472 - val_loss: 0.4920\n",
      "Epoch 76/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.4458 - val_loss: 0.4913\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/100\n",
      "6091/6091 [==============================] - 1s 98us/sample - loss: 0.4445 - val_loss: 0.4904\n",
      "Epoch 78/100\n",
      "6091/6091 [==============================] - 1s 98us/sample - loss: 0.4432 - val_loss: 0.4896\n",
      "Epoch 79/100\n",
      "6091/6091 [==============================] - 1s 97us/sample - loss: 0.4419 - val_loss: 0.4889\n",
      "Epoch 80/100\n",
      "6091/6091 [==============================] - 1s 103us/sample - loss: 0.4406 - val_loss: 0.4881\n",
      "Epoch 81/100\n",
      "6091/6091 [==============================] - 1s 102us/sample - loss: 0.4394 - val_loss: 0.4873\n",
      "Epoch 82/100\n",
      "6091/6091 [==============================] - 1s 102us/sample - loss: 0.4381 - val_loss: 0.4865\n",
      "Epoch 83/100\n",
      "6091/6091 [==============================] - 1s 97us/sample - loss: 0.4369 - val_loss: 0.4858\n",
      "Epoch 84/100\n",
      "6091/6091 [==============================] - 1s 97us/sample - loss: 0.4357 - val_loss: 0.4851\n",
      "Epoch 85/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.4345 - val_loss: 0.4844\n",
      "Epoch 86/100\n",
      "6091/6091 [==============================] - 1s 99us/sample - loss: 0.4333 - val_loss: 0.4837\n",
      "Epoch 87/100\n",
      "6091/6091 [==============================] - 1s 97us/sample - loss: 0.4321 - val_loss: 0.4830\n",
      "Epoch 88/100\n",
      "6091/6091 [==============================] - 1s 98us/sample - loss: 0.4309 - val_loss: 0.4823\n",
      "Epoch 89/100\n",
      "6091/6091 [==============================] - 1s 98us/sample - loss: 0.4298 - val_loss: 0.4817\n",
      "Epoch 90/100\n",
      "6091/6091 [==============================] - 1s 99us/sample - loss: 0.4286 - val_loss: 0.4810\n",
      "Epoch 91/100\n",
      "6091/6091 [==============================] - 1s 99us/sample - loss: 0.4276 - val_loss: 0.4804\n",
      "Epoch 92/100\n",
      "6091/6091 [==============================] - 1s 98us/sample - loss: 0.4264 - val_loss: 0.4798\n",
      "Epoch 93/100\n",
      "6091/6091 [==============================] - 1s 98us/sample - loss: 0.4253 - val_loss: 0.4791\n",
      "Epoch 94/100\n",
      "6091/6091 [==============================] - 1s 97us/sample - loss: 0.4242 - val_loss: 0.4785\n",
      "Epoch 95/100\n",
      "6091/6091 [==============================] - ETA: 0s - loss: 0.424 - 1s 98us/sample - loss: 0.4232 - val_loss: 0.4779\n",
      "Epoch 96/100\n",
      "6091/6091 [==============================] - 1s 99us/sample - loss: 0.4221 - val_loss: 0.4774\n",
      "Epoch 97/100\n",
      "6091/6091 [==============================] - 1s 98us/sample - loss: 0.4210 - val_loss: 0.4768\n",
      "Epoch 98/100\n",
      "6091/6091 [==============================] - 1s 97us/sample - loss: 0.4200 - val_loss: 0.4762\n",
      "Epoch 99/100\n",
      "6091/6091 [==============================] - 1s 98us/sample - loss: 0.4190 - val_loss: 0.4756\n",
      "Epoch 100/100\n",
      "6091/6091 [==============================] - 1s 99us/sample - loss: 0.4179 - val_loss: 0.4751\n",
      "Predicting...\n",
      "(Took 66.374 sec)\n",
      "Train on 6091 samples, validate on 1522 samples\n",
      "Epoch 1/100\n",
      "6091/6091 [==============================] - 1s 140us/sample - loss: 0.6848 - val_loss: 0.6806\n",
      "Epoch 2/100\n",
      "6091/6091 [==============================] - 1s 99us/sample - loss: 0.6728 - val_loss: 0.6726\n",
      "Epoch 3/100\n",
      "6091/6091 [==============================] - 1s 99us/sample - loss: 0.6638 - val_loss: 0.6659\n",
      "Epoch 4/100\n",
      "6091/6091 [==============================] - 1s 98us/sample - loss: 0.6561 - val_loss: 0.6597\n",
      "Epoch 5/100\n",
      "6091/6091 [==============================] - 1s 99us/sample - loss: 0.6488 - val_loss: 0.6539\n",
      "Epoch 6/100\n",
      "6091/6091 [==============================] - 1s 98us/sample - loss: 0.6421 - val_loss: 0.6482\n",
      "Epoch 7/100\n",
      "6091/6091 [==============================] - 1s 99us/sample - loss: 0.6356 - val_loss: 0.6426\n",
      "Epoch 8/100\n",
      "6091/6091 [==============================] - 1s 100us/sample - loss: 0.6294 - val_loss: 0.6374\n",
      "Epoch 9/100\n",
      "6091/6091 [==============================] - 1s 99us/sample - loss: 0.6234 - val_loss: 0.6323\n",
      "Epoch 10/100\n",
      "6091/6091 [==============================] - 1s 102us/sample - loss: 0.6176 - val_loss: 0.6274\n",
      "Epoch 11/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.6121 - val_loss: 0.6229\n",
      "Epoch 12/100\n",
      "6091/6091 [==============================] - 1s 111us/sample - loss: 0.6068 - val_loss: 0.6184\n",
      "Epoch 13/100\n",
      "6091/6091 [==============================] - 1s 120us/sample - loss: 0.6016 - val_loss: 0.6142\n",
      "Epoch 14/100\n",
      "6091/6091 [==============================] - 1s 114us/sample - loss: 0.5967 - val_loss: 0.6100\n",
      "Epoch 15/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.5918 - val_loss: 0.6061\n",
      "Epoch 16/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.5872 - val_loss: 0.6023\n",
      "Epoch 17/100\n",
      "6091/6091 [==============================] - 1s 113us/sample - loss: 0.5827 - val_loss: 0.5987\n",
      "Epoch 18/100\n",
      "6091/6091 [==============================] - 1s 111us/sample - loss: 0.5784 - val_loss: 0.5951\n",
      "Epoch 19/100\n",
      "6091/6091 [==============================] - 1s 125us/sample - loss: 0.5741 - val_loss: 0.5917\n",
      "Epoch 20/100\n",
      "6091/6091 [==============================] - 1s 136us/sample - loss: 0.5701 - val_loss: 0.5886\n",
      "Epoch 21/100\n",
      "6091/6091 [==============================] - 1s 131us/sample - loss: 0.5661 - val_loss: 0.5853\n",
      "Epoch 22/100\n",
      "6091/6091 [==============================] - 1s 129us/sample - loss: 0.5623 - val_loss: 0.5823\n",
      "Epoch 23/100\n",
      "6091/6091 [==============================] - 1s 121us/sample - loss: 0.5585 - val_loss: 0.5794\n",
      "Epoch 24/100\n",
      "6091/6091 [==============================] - 1s 117us/sample - loss: 0.5549 - val_loss: 0.5765\n",
      "Epoch 25/100\n",
      "6091/6091 [==============================] - 1s 119us/sample - loss: 0.5514 - val_loss: 0.5737\n",
      "Epoch 26/100\n",
      "6091/6091 [==============================] - 1s 117us/sample - loss: 0.5480 - val_loss: 0.5711\n",
      "Epoch 27/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.5446 - val_loss: 0.5686\n",
      "Epoch 28/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.5413 - val_loss: 0.5661\n",
      "Epoch 29/100\n",
      "6091/6091 [==============================] - 1s 118us/sample - loss: 0.5382 - val_loss: 0.5637\n",
      "Epoch 30/100\n",
      "6091/6091 [==============================] - 1s 113us/sample - loss: 0.5351 - val_loss: 0.5613\n",
      "Epoch 31/100\n",
      "6091/6091 [==============================] - 1s 114us/sample - loss: 0.5321 - val_loss: 0.5590\n",
      "Epoch 32/100\n",
      "6091/6091 [==============================] - 1s 114us/sample - loss: 0.5291 - val_loss: 0.5568\n",
      "Epoch 33/100\n",
      "6091/6091 [==============================] - 1s 116us/sample - loss: 0.5262 - val_loss: 0.5547\n",
      "Epoch 34/100\n",
      "6091/6091 [==============================] - 1s 116us/sample - loss: 0.5235 - val_loss: 0.5526\n",
      "Epoch 35/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.5207 - val_loss: 0.5506\n",
      "Epoch 36/100\n",
      "6091/6091 [==============================] - 1s 113us/sample - loss: 0.5181 - val_loss: 0.5486\n",
      "Epoch 37/100\n",
      "6091/6091 [==============================] - 1s 113us/sample - loss: 0.5154 - val_loss: 0.5467\n",
      "Epoch 38/100\n",
      "6091/6091 [==============================] - 1s 115us/sample - loss: 0.5129 - val_loss: 0.5449\n",
      "Epoch 39/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.5104 - val_loss: 0.5431\n",
      "Epoch 40/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.5079 - val_loss: 0.5413\n",
      "Epoch 41/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.5055 - val_loss: 0.5396\n",
      "Epoch 42/100\n",
      "6091/6091 [==============================] - 1s 106us/sample - loss: 0.5032 - val_loss: 0.5379\n",
      "Epoch 43/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.5009 - val_loss: 0.5363\n",
      "Epoch 44/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.4986 - val_loss: 0.5347\n",
      "Epoch 45/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.4964 - val_loss: 0.5333\n",
      "Epoch 46/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4942 - val_loss: 0.5317\n",
      "Epoch 47/100\n",
      "6091/6091 [==============================] - 1s 111us/sample - loss: 0.4921 - val_loss: 0.5303\n",
      "Epoch 48/100\n",
      "6091/6091 [==============================] - 1s 117us/sample - loss: 0.4900 - val_loss: 0.5288\n",
      "Epoch 49/100\n",
      "6091/6091 [==============================] - 1s 164us/sample - loss: 0.4879 - val_loss: 0.5275\n",
      "Epoch 50/100\n",
      "6091/6091 [==============================] - 1s 129us/sample - loss: 0.4859 - val_loss: 0.5262\n",
      "Epoch 51/100\n",
      "6091/6091 [==============================] - 1s 127us/sample - loss: 0.4840 - val_loss: 0.5248\n",
      "Epoch 52/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.4820 - val_loss: 0.5235\n",
      "Epoch 53/100\n",
      "6091/6091 [==============================] - 1s 106us/sample - loss: 0.4801 - val_loss: 0.5222\n",
      "Epoch 54/100\n",
      "6091/6091 [==============================] - 1s 101us/sample - loss: 0.4782 - val_loss: 0.5210\n",
      "Epoch 55/100\n",
      "6091/6091 [==============================] - 1s 106us/sample - loss: 0.4764 - val_loss: 0.5198\n",
      "Epoch 56/100\n",
      "6091/6091 [==============================] - 1s 102us/sample - loss: 0.4746 - val_loss: 0.5186\n",
      "Epoch 57/100\n",
      "6091/6091 [==============================] - 1s 100us/sample - loss: 0.4728 - val_loss: 0.5175\n",
      "Epoch 58/100\n",
      "6091/6091 [==============================] - 1s 103us/sample - loss: 0.4710 - val_loss: 0.5163\n",
      "Epoch 59/100\n",
      "6091/6091 [==============================] - 1s 119us/sample - loss: 0.4693 - val_loss: 0.5153\n",
      "Epoch 60/100\n",
      "6091/6091 [==============================] - 1s 105us/sample - loss: 0.4676 - val_loss: 0.5142\n",
      "Epoch 61/100\n",
      "6091/6091 [==============================] - 1s 100us/sample - loss: 0.4659 - val_loss: 0.5132\n",
      "Epoch 62/100\n",
      "6091/6091 [==============================] - 1s 133us/sample - loss: 0.4643 - val_loss: 0.5121\n",
      "Epoch 63/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.4627 - val_loss: 0.5111\n",
      "Epoch 64/100\n",
      "6091/6091 [==============================] - 1s 105us/sample - loss: 0.4611 - val_loss: 0.5101\n",
      "Epoch 65/100\n",
      "6091/6091 [==============================] - 1s 96us/sample - loss: 0.4595 - val_loss: 0.5091\n",
      "Epoch 66/100\n",
      "6091/6091 [==============================] - 1s 99us/sample - loss: 0.4579 - val_loss: 0.5082\n",
      "Epoch 67/100\n",
      "6091/6091 [==============================] - 1s 94us/sample - loss: 0.4564 - val_loss: 0.5073\n",
      "Epoch 68/100\n",
      "6091/6091 [==============================] - 1s 92us/sample - loss: 0.4549 - val_loss: 0.5064\n",
      "Epoch 69/100\n",
      "6091/6091 [==============================] - 1s 92us/sample - loss: 0.4534 - val_loss: 0.5055\n",
      "Epoch 70/100\n",
      "6091/6091 [==============================] - 1s 98us/sample - loss: 0.4520 - val_loss: 0.5047\n",
      "Epoch 71/100\n",
      "6091/6091 [==============================] - 1s 93us/sample - loss: 0.4505 - val_loss: 0.5038\n",
      "Epoch 72/100\n",
      "6091/6091 [==============================] - 1s 92us/sample - loss: 0.4491 - val_loss: 0.5030\n",
      "Epoch 73/100\n",
      "6091/6091 [==============================] - 1s 94us/sample - loss: 0.4477 - val_loss: 0.5021\n",
      "Epoch 74/100\n",
      "6091/6091 [==============================] - 1s 94us/sample - loss: 0.4463 - val_loss: 0.5013\n",
      "Epoch 75/100\n",
      "6091/6091 [==============================] - 1s 103us/sample - loss: 0.4449 - val_loss: 0.5006\n",
      "Epoch 76/100\n",
      "6091/6091 [==============================] - 1s 93us/sample - loss: 0.4435 - val_loss: 0.4998\n",
      "Epoch 77/100\n",
      "6091/6091 [==============================] - 1s 93us/sample - loss: 0.4422 - val_loss: 0.4990\n",
      "Epoch 78/100\n",
      "6091/6091 [==============================] - 1s 92us/sample - loss: 0.4409 - val_loss: 0.4983\n",
      "Epoch 79/100\n",
      "6091/6091 [==============================] - 1s 93us/sample - loss: 0.4396 - val_loss: 0.4976\n",
      "Epoch 80/100\n",
      "6091/6091 [==============================] - 1s 93us/sample - loss: 0.4383 - val_loss: 0.4969\n",
      "Epoch 81/100\n",
      "6091/6091 [==============================] - 1s 93us/sample - loss: 0.4370 - val_loss: 0.4962\n",
      "Epoch 82/100\n",
      "6091/6091 [==============================] - 1s 93us/sample - loss: 0.4358 - val_loss: 0.4955\n",
      "Epoch 83/100\n",
      "6091/6091 [==============================] - 1s 93us/sample - loss: 0.4345 - val_loss: 0.4949\n",
      "Epoch 84/100\n",
      "6091/6091 [==============================] - 1s 93us/sample - loss: 0.4333 - val_loss: 0.4942\n",
      "Epoch 85/100\n",
      "6091/6091 [==============================] - 1s 93us/sample - loss: 0.4321 - val_loss: 0.4936\n",
      "Epoch 86/100\n",
      "6091/6091 [==============================] - 1s 93us/sample - loss: 0.4309 - val_loss: 0.4929\n",
      "Epoch 87/100\n",
      "6091/6091 [==============================] - 1s 94us/sample - loss: 0.4297 - val_loss: 0.4923\n",
      "Epoch 88/100\n",
      "6091/6091 [==============================] - 1s 132us/sample - loss: 0.4285 - val_loss: 0.4917\n",
      "Epoch 89/100\n",
      "6091/6091 [==============================] - 1s 96us/sample - loss: 0.4274 - val_loss: 0.4911\n",
      "Epoch 90/100\n",
      "6091/6091 [==============================] - 1s 114us/sample - loss: 0.4262 - val_loss: 0.4906\n",
      "Epoch 91/100\n",
      "6091/6091 [==============================] - 1s 121us/sample - loss: 0.4251 - val_loss: 0.4900\n",
      "Epoch 92/100\n",
      "6091/6091 [==============================] - 1s 117us/sample - loss: 0.4240 - val_loss: 0.4895\n",
      "Epoch 93/100\n",
      "6091/6091 [==============================] - 1s 133us/sample - loss: 0.4229 - val_loss: 0.4889\n",
      "Epoch 94/100\n",
      "6091/6091 [==============================] - 1s 113us/sample - loss: 0.4218 - val_loss: 0.4884\n",
      "Epoch 95/100\n",
      "6091/6091 [==============================] - 1s 98us/sample - loss: 0.4207 - val_loss: 0.4879\n",
      "Epoch 96/100\n",
      "6091/6091 [==============================] - 1s 94us/sample - loss: 0.4196 - val_loss: 0.4874\n",
      "Epoch 97/100\n",
      "6091/6091 [==============================] - 1s 122us/sample - loss: 0.4186 - val_loss: 0.4868\n",
      "Epoch 98/100\n",
      "6091/6091 [==============================] - 1s 124us/sample - loss: 0.4175 - val_loss: 0.4864\n",
      "Epoch 99/100\n",
      "6091/6091 [==============================] - 1s 125us/sample - loss: 0.4165 - val_loss: 0.4859\n",
      "Epoch 100/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4154 - val_loss: 0.4854\n",
      "Predicting...\n",
      "(Took 66.715 sec)\n",
      "Combined confusion matrix:\n",
      "[[3838.  504.]\n",
      " [1125. 2146.]]\n",
      "(Overall, took 328.608 sec)\n",
      "Accuracy: 78.60% +/- 0.78%\n",
      "Precision for positive class: 77.32% +/- 1.40%\n",
      "Precision for negative class: 80.98% +/- 0.80%\n",
      "Recall for positive class: 88.38% +/- 0.84%\n",
      "Recall for negative class: 65.61% +/- 0.73%\n",
      "F for positive class: 82.48% +/- 1.04%\n",
      "F for negative class: 72.49% +/- 0.38%\n",
      "Mean F score: 77.48% +/- 0.54%\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/100\n",
      "6090/6090 [==============================] - 1s 203us/sample - loss: 0.6833 - val_loss: 0.6729\n",
      "Epoch 2/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.6659 - val_loss: 0.6591\n",
      "Epoch 3/100\n",
      "6090/6090 [==============================] - 1s 117us/sample - loss: 0.6519 - val_loss: 0.6475\n",
      "Epoch 4/100\n",
      "6090/6090 [==============================] - 1s 116us/sample - loss: 0.6393 - val_loss: 0.6371\n",
      "Epoch 5/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.6275 - val_loss: 0.6276\n",
      "Epoch 6/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.6164 - val_loss: 0.6186\n",
      "Epoch 7/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.6058 - val_loss: 0.6102\n",
      "Epoch 8/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.5958 - val_loss: 0.6022\n",
      "Epoch 9/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.5862 - val_loss: 0.5947\n",
      "Epoch 10/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.5771 - val_loss: 0.5877\n",
      "Epoch 11/100\n",
      "6090/6090 [==============================] - 1s 131us/sample - loss: 0.5684 - val_loss: 0.5809\n",
      "Epoch 12/100\n",
      "6090/6090 [==============================] - 1s 123us/sample - loss: 0.5601 - val_loss: 0.5745\n",
      "Epoch 13/100\n",
      "6090/6090 [==============================] - 1s 127us/sample - loss: 0.5522 - val_loss: 0.5685\n",
      "Epoch 14/100\n",
      "6090/6090 [==============================] - 1s 126us/sample - loss: 0.5447 - val_loss: 0.5628\n",
      "Epoch 15/100\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.5374 - val_loss: 0.5574\n",
      "Epoch 16/100\n",
      "6090/6090 [==============================] - 1s 117us/sample - loss: 0.5305 - val_loss: 0.5522\n",
      "Epoch 17/100\n",
      "6090/6090 [==============================] - 1s 120us/sample - loss: 0.5239 - val_loss: 0.5474\n",
      "Epoch 18/100\n",
      "6090/6090 [==============================] - 1s 122us/sample - loss: 0.5175 - val_loss: 0.5427\n",
      "Epoch 19/100\n",
      "6090/6090 [==============================] - 1s 136us/sample - loss: 0.5115 - val_loss: 0.5383\n",
      "Epoch 20/100\n",
      "6090/6090 [==============================] - 1s 125us/sample - loss: 0.5056 - val_loss: 0.5341\n",
      "Epoch 21/100\n",
      "6090/6090 [==============================] - 1s 163us/sample - loss: 0.5000 - val_loss: 0.5302\n",
      "Epoch 22/100\n",
      "6090/6090 [==============================] - 1s 140us/sample - loss: 0.4946 - val_loss: 0.5264\n",
      "Epoch 23/100\n",
      "6090/6090 [==============================] - 1s 162us/sample - loss: 0.4894 - val_loss: 0.5227\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/100\n",
      "6090/6090 [==============================] - 1s 170us/sample - loss: 0.4844 - val_loss: 0.5193\n",
      "Epoch 25/100\n",
      "6090/6090 [==============================] - 1s 129us/sample - loss: 0.4796 - val_loss: 0.5161\n",
      "Epoch 26/100\n",
      "6090/6090 [==============================] - 1s 122us/sample - loss: 0.4749 - val_loss: 0.5129\n",
      "Epoch 27/100\n",
      "6090/6090 [==============================] - 1s 126us/sample - loss: 0.4704 - val_loss: 0.5100\n",
      "Epoch 28/100\n",
      "6090/6090 [==============================] - 1s 136us/sample - loss: 0.4661 - val_loss: 0.5072\n",
      "Epoch 29/100\n",
      "6090/6090 [==============================] - 1s 131us/sample - loss: 0.4619 - val_loss: 0.5045\n",
      "Epoch 30/100\n",
      "6090/6090 [==============================] - 1s 133us/sample - loss: 0.4579 - val_loss: 0.5019\n",
      "Epoch 31/100\n",
      "6090/6090 [==============================] - 1s 155us/sample - loss: 0.4540 - val_loss: 0.4994\n",
      "Epoch 32/100\n",
      "6090/6090 [==============================] - 1s 164us/sample - loss: 0.4501 - val_loss: 0.4971\n",
      "Epoch 33/100\n",
      "6090/6090 [==============================] - 1s 128us/sample - loss: 0.4465 - val_loss: 0.4949\n",
      "Epoch 34/100\n",
      "6090/6090 [==============================] - 1s 131us/sample - loss: 0.4429 - val_loss: 0.4927\n",
      "Epoch 35/100\n",
      "6090/6090 [==============================] - 1s 132us/sample - loss: 0.4395 - val_loss: 0.4907\n",
      "Epoch 36/100\n",
      "6090/6090 [==============================] - 1s 131us/sample - loss: 0.4361 - val_loss: 0.4887\n",
      "Epoch 37/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.4329 - val_loss: 0.4869\n",
      "Epoch 38/100\n",
      "6090/6090 [==============================] - 1s 122us/sample - loss: 0.4297 - val_loss: 0.4851\n",
      "Epoch 39/100\n",
      "6090/6090 [==============================] - 1s 119us/sample - loss: 0.4266 - val_loss: 0.4834\n",
      "Epoch 40/100\n",
      "6090/6090 [==============================] - 1s 123us/sample - loss: 0.4236 - val_loss: 0.4818\n",
      "Epoch 41/100\n",
      "6090/6090 [==============================] - 1s 128us/sample - loss: 0.4207 - val_loss: 0.4803\n",
      "Epoch 42/100\n",
      "6090/6090 [==============================] - 1s 134us/sample - loss: 0.4179 - val_loss: 0.4788\n",
      "Epoch 43/100\n",
      "6090/6090 [==============================] - 1s 130us/sample - loss: 0.4151 - val_loss: 0.4774\n",
      "Epoch 44/100\n",
      "6090/6090 [==============================] - 1s 141us/sample - loss: 0.4125 - val_loss: 0.4760\n",
      "Epoch 45/100\n",
      "6090/6090 [==============================] - 1s 172us/sample - loss: 0.4098 - val_loss: 0.4747\n",
      "Epoch 46/100\n",
      "6090/6090 [==============================] - 1s 152us/sample - loss: 0.4073 - val_loss: 0.4735\n",
      "Epoch 47/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.4048 - val_loss: 0.4723\n",
      "Epoch 48/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.4024 - val_loss: 0.4712\n",
      "Epoch 49/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.4000 - val_loss: 0.4701\n",
      "Epoch 50/100\n",
      "6090/6090 [==============================] - 1s 117us/sample - loss: 0.3977 - val_loss: 0.4690\n",
      "Epoch 51/100\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.3954 - val_loss: 0.4681\n",
      "Epoch 52/100\n",
      "6090/6090 [==============================] - 1s 117us/sample - loss: 0.3932 - val_loss: 0.4672\n",
      "Epoch 53/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.3910 - val_loss: 0.4662\n",
      "Epoch 54/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.3889 - val_loss: 0.4654\n",
      "Epoch 55/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.3868 - val_loss: 0.4646\n",
      "Epoch 56/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.3848 - val_loss: 0.4638\n",
      "Epoch 57/100\n",
      "6090/6090 [==============================] - 1s 101us/sample - loss: 0.3828 - val_loss: 0.4631\n",
      "Epoch 58/100\n",
      "6090/6090 [==============================] - 1s 99us/sample - loss: 0.3808 - val_loss: 0.4624\n",
      "Epoch 59/100\n",
      "6090/6090 [==============================] - 1s 101us/sample - loss: 0.3789 - val_loss: 0.4617\n",
      "Epoch 60/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.3771 - val_loss: 0.4610\n",
      "Epoch 61/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.3752 - val_loss: 0.4604\n",
      "Epoch 62/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.3734 - val_loss: 0.4598\n",
      "Epoch 63/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.3716 - val_loss: 0.4592\n",
      "Epoch 64/100\n",
      "6090/6090 [==============================] - 1s 101us/sample - loss: 0.3699 - val_loss: 0.4587\n",
      "Epoch 65/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.3682 - val_loss: 0.4582\n",
      "Epoch 66/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.3665 - val_loss: 0.4577\n",
      "Epoch 67/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.3649 - val_loss: 0.4573\n",
      "Epoch 68/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.3632 - val_loss: 0.4568\n",
      "Epoch 69/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.3616 - val_loss: 0.4564\n",
      "Epoch 70/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.3601 - val_loss: 0.4561\n",
      "Epoch 71/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.3585 - val_loss: 0.4557\n",
      "Epoch 72/100\n",
      "6090/6090 [==============================] - 1s 101us/sample - loss: 0.3570 - val_loss: 0.4553\n",
      "Epoch 73/100\n",
      "6090/6090 [==============================] - 1s 123us/sample - loss: 0.3555 - val_loss: 0.4550\n",
      "Epoch 74/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.3541 - val_loss: 0.4548\n",
      "Epoch 75/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.3526 - val_loss: 0.4545\n",
      "Epoch 76/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.3512 - val_loss: 0.4541\n",
      "Epoch 77/100\n",
      "6090/6090 [==============================] - 1s 101us/sample - loss: 0.3498 - val_loss: 0.4539\n",
      "Epoch 78/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.3484 - val_loss: 0.4536\n",
      "Epoch 79/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.3471 - val_loss: 0.4535\n",
      "Epoch 80/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.3457 - val_loss: 0.4532\n",
      "Epoch 81/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.3444 - val_loss: 0.4531\n",
      "Epoch 82/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.3431 - val_loss: 0.4529\n",
      "Epoch 83/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.3418 - val_loss: 0.4527\n",
      "Epoch 84/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.3406 - val_loss: 0.4526\n",
      "Epoch 85/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.3393 - val_loss: 0.4524\n",
      "Epoch 86/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.3381 - val_loss: 0.4523\n",
      "Epoch 87/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.3369 - val_loss: 0.4523\n",
      "Epoch 88/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.3357 - val_loss: 0.4522\n",
      "Epoch 89/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.3345 - val_loss: 0.4521\n",
      "Epoch 90/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.3334 - val_loss: 0.4522\n",
      "Predicting...\n",
      "(Took 66.020 sec)\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/100\n",
      "6090/6090 [==============================] - 1s 145us/sample - loss: 0.6837 - val_loss: 0.6726\n",
      "Epoch 2/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.6666 - val_loss: 0.6579\n",
      "Epoch 3/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.6526 - val_loss: 0.6461\n",
      "Epoch 4/100\n",
      "6090/6090 [==============================] - 1s 101us/sample - loss: 0.6400 - val_loss: 0.6356\n",
      "Epoch 5/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.6282 - val_loss: 0.6258\n",
      "Epoch 6/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.6172 - val_loss: 0.6169\n",
      "Epoch 7/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.6066 - val_loss: 0.6085\n",
      "Epoch 8/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.5966 - val_loss: 0.6005\n",
      "Epoch 9/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.5871 - val_loss: 0.5931\n",
      "Epoch 10/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.5779 - val_loss: 0.5860\n",
      "Epoch 11/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.5693 - val_loss: 0.5794\n",
      "Epoch 12/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.5610 - val_loss: 0.5730\n",
      "Epoch 13/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.5531 - val_loss: 0.5670\n",
      "Epoch 14/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.5455 - val_loss: 0.5615\n",
      "Epoch 15/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.5383 - val_loss: 0.5562\n",
      "Epoch 16/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.5313 - val_loss: 0.5511\n",
      "Epoch 17/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.5247 - val_loss: 0.5462\n",
      "Epoch 18/100\n",
      "6090/6090 [==============================] - 1s 125us/sample - loss: 0.5183 - val_loss: 0.5417\n",
      "Epoch 19/100\n",
      "6090/6090 [==============================] - 1s 128us/sample - loss: 0.5122 - val_loss: 0.5373\n",
      "Epoch 20/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.5064 - val_loss: 0.5333\n",
      "Epoch 21/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.5007 - val_loss: 0.5294\n",
      "Epoch 22/100\n",
      "6090/6090 [==============================] - 1s 119us/sample - loss: 0.4953 - val_loss: 0.5257\n",
      "Epoch 23/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.4901 - val_loss: 0.5223\n",
      "Epoch 24/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.4851 - val_loss: 0.5189\n",
      "Epoch 25/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.4802 - val_loss: 0.5158\n",
      "Epoch 26/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.4755 - val_loss: 0.5128\n",
      "Epoch 27/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.4710 - val_loss: 0.5099\n",
      "Epoch 28/100\n",
      "6090/6090 [==============================] - 1s 99us/sample - loss: 0.4667 - val_loss: 0.5073\n",
      "Epoch 29/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4625 - val_loss: 0.5047\n",
      "Epoch 30/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.4584 - val_loss: 0.5022\n",
      "Epoch 31/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.4545 - val_loss: 0.4999\n",
      "Epoch 32/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.4507 - val_loss: 0.4977\n",
      "Epoch 33/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.4470 - val_loss: 0.4956\n",
      "Epoch 34/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.4434 - val_loss: 0.4936\n",
      "Epoch 35/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.4400 - val_loss: 0.4916\n",
      "Epoch 36/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.4366 - val_loss: 0.4898\n",
      "Epoch 37/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.4333 - val_loss: 0.4881\n",
      "Epoch 38/100\n",
      "6090/6090 [==============================] - 1s 118us/sample - loss: 0.4301 - val_loss: 0.4865\n",
      "Epoch 39/100\n",
      "6090/6090 [==============================] - 1s 117us/sample - loss: 0.4270 - val_loss: 0.4850\n",
      "Epoch 40/100\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.4240 - val_loss: 0.4835\n",
      "Epoch 41/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.4211 - val_loss: 0.4821\n",
      "Epoch 42/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.4183 - val_loss: 0.4807\n",
      "Epoch 43/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.4155 - val_loss: 0.4794\n",
      "Epoch 44/100\n",
      "6090/6090 [==============================] - 1s 130us/sample - loss: 0.4128 - val_loss: 0.4783\n",
      "Epoch 45/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4102 - val_loss: 0.4771\n",
      "Epoch 46/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4076 - val_loss: 0.4760\n",
      "Epoch 47/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4051 - val_loss: 0.4749\n",
      "Epoch 48/100\n",
      "6090/6090 [==============================] - 1s 123us/sample - loss: 0.4027 - val_loss: 0.4740\n",
      "Epoch 49/100\n",
      "6090/6090 [==============================] - 1s 122us/sample - loss: 0.4003 - val_loss: 0.4730\n",
      "Epoch 50/100\n",
      "6090/6090 [==============================] - 1s 120us/sample - loss: 0.3979 - val_loss: 0.4721\n",
      "Epoch 51/100\n",
      "6090/6090 [==============================] - 1s 117us/sample - loss: 0.3956 - val_loss: 0.4713\n",
      "Epoch 52/100\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.3934 - val_loss: 0.4704\n",
      "Epoch 53/100\n",
      "6090/6090 [==============================] - 1s 126us/sample - loss: 0.3912 - val_loss: 0.4697\n",
      "Epoch 54/100\n",
      "6090/6090 [==============================] - 1s 118us/sample - loss: 0.3891 - val_loss: 0.4690\n",
      "Epoch 55/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.3870 - val_loss: 0.4684\n",
      "Epoch 56/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.3850 - val_loss: 0.4677\n",
      "Epoch 57/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.3830 - val_loss: 0.4671\n",
      "Epoch 58/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.3810 - val_loss: 0.4666\n",
      "Epoch 59/100\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.3791 - val_loss: 0.4660\n",
      "Epoch 60/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.3772 - val_loss: 0.4655\n",
      "Epoch 61/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.3753 - val_loss: 0.4650\n",
      "Epoch 62/100\n",
      "6090/6090 [==============================] - 1s 130us/sample - loss: 0.3735 - val_loss: 0.4645\n",
      "Epoch 63/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.3718 - val_loss: 0.4642\n",
      "Epoch 64/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.3700 - val_loss: 0.4638\n",
      "Epoch 65/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.3683 - val_loss: 0.4635\n",
      "Epoch 66/100\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.3666 - val_loss: 0.4631\n",
      "Epoch 67/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.3650 - val_loss: 0.4628\n",
      "Epoch 68/100\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.3634 - val_loss: 0.4625\n",
      "Epoch 69/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.3618 - val_loss: 0.4623\n",
      "Epoch 70/100\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.3602 - val_loss: 0.4620\n",
      "Epoch 71/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.3587 - val_loss: 0.4619\n",
      "Epoch 72/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.3572 - val_loss: 0.4617\n",
      "Epoch 73/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.3557 - val_loss: 0.4615\n",
      "Epoch 74/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.3542 - val_loss: 0.4614\n",
      "Epoch 75/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.3528 - val_loss: 0.4612\n",
      "Epoch 76/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.3514 - val_loss: 0.4611\n",
      "Epoch 77/100\n",
      "6090/6090 [==============================] - 1s 120us/sample - loss: 0.3500 - val_loss: 0.4610\n",
      "Epoch 78/100\n",
      "6090/6090 [==============================] - 1s 122us/sample - loss: 0.3486 - val_loss: 0.4609\n",
      "Epoch 79/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.3472 - val_loss: 0.4609\n",
      "Epoch 80/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.3459 - val_loss: 0.4608\n",
      "Epoch 81/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.3446 - val_loss: 0.4608\n",
      "Predicting...\n",
      "(Took 55.748 sec)\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/100\n",
      "6090/6090 [==============================] - 1s 149us/sample - loss: 0.6835 - val_loss: 0.6767\n",
      "Epoch 2/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.6655 - val_loss: 0.6644\n",
      "Epoch 3/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.6510 - val_loss: 0.6540\n",
      "Epoch 4/100\n",
      "6090/6090 [==============================] - 1s 99us/sample - loss: 0.6382 - val_loss: 0.6444\n",
      "Epoch 5/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.6264 - val_loss: 0.6354\n",
      "Epoch 6/100\n",
      "6090/6090 [==============================] - 1s 101us/sample - loss: 0.6152 - val_loss: 0.6268\n",
      "Epoch 7/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.6047 - val_loss: 0.6187\n",
      "Epoch 8/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.5947 - val_loss: 0.6110\n",
      "Epoch 9/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.5851 - val_loss: 0.6038\n",
      "Epoch 10/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.5761 - val_loss: 0.5970\n",
      "Epoch 11/100\n",
      "6090/6090 [==============================] - 1s 120us/sample - loss: 0.5674 - val_loss: 0.5904\n",
      "Epoch 12/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.5591 - val_loss: 0.5842\n",
      "Epoch 13/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.5512 - val_loss: 0.5785\n",
      "Epoch 14/100\n",
      "6090/6090 [==============================] - 1s 101us/sample - loss: 0.5436 - val_loss: 0.5730\n",
      "Epoch 15/100\n",
      "6090/6090 [==============================] - 1s 116us/sample - loss: 0.5364 - val_loss: 0.5678\n",
      "Epoch 16/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.5294 - val_loss: 0.5627\n",
      "Epoch 17/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.5228 - val_loss: 0.5581\n",
      "Epoch 18/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.5164 - val_loss: 0.5537\n",
      "Epoch 19/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.5103 - val_loss: 0.5494\n",
      "Epoch 20/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.5045 - val_loss: 0.5456\n",
      "Epoch 21/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.4989 - val_loss: 0.5418\n",
      "Epoch 22/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4934 - val_loss: 0.5381\n",
      "Epoch 23/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.4882 - val_loss: 0.5346\n",
      "Epoch 24/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.4832 - val_loss: 0.5314\n",
      "Epoch 25/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.4784 - val_loss: 0.5285\n",
      "Epoch 26/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4737 - val_loss: 0.5256\n",
      "Epoch 27/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.4691 - val_loss: 0.5227\n",
      "Epoch 28/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4648 - val_loss: 0.5201\n",
      "Epoch 29/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.4605 - val_loss: 0.5176\n",
      "Epoch 30/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.4565 - val_loss: 0.5152\n",
      "Epoch 31/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.4525 - val_loss: 0.5129\n",
      "Epoch 32/100\n",
      "6090/6090 [==============================] - 1s 99us/sample - loss: 0.4487 - val_loss: 0.5108\n",
      "Epoch 33/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.4450 - val_loss: 0.5086\n",
      "Epoch 34/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.4414 - val_loss: 0.5067\n",
      "Epoch 35/100\n",
      "6090/6090 [==============================] - 1s 116us/sample - loss: 0.4379 - val_loss: 0.5047\n",
      "Epoch 36/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.4345 - val_loss: 0.5031\n",
      "Epoch 37/100\n",
      "6090/6090 [==============================] - 1s 117us/sample - loss: 0.4312 - val_loss: 0.5014\n",
      "Epoch 38/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4280 - val_loss: 0.4997\n",
      "Epoch 39/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4249 - val_loss: 0.4982\n",
      "Epoch 40/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.4219 - val_loss: 0.4967\n",
      "Epoch 41/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.4190 - val_loss: 0.4954\n",
      "Epoch 42/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.4161 - val_loss: 0.4940\n",
      "Epoch 43/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.4133 - val_loss: 0.4927\n",
      "Epoch 44/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4106 - val_loss: 0.4916\n",
      "Epoch 45/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4079 - val_loss: 0.4903\n",
      "Epoch 46/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.4054 - val_loss: 0.4893\n",
      "Epoch 47/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.4029 - val_loss: 0.4882\n",
      "Epoch 48/100\n",
      "6090/6090 [==============================] - 1s 116us/sample - loss: 0.4004 - val_loss: 0.4872\n",
      "Epoch 49/100\n",
      "6090/6090 [==============================] - 1s 117us/sample - loss: 0.3980 - val_loss: 0.4863\n",
      "Epoch 50/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.3956 - val_loss: 0.4855\n",
      "Epoch 51/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.3934 - val_loss: 0.4847\n",
      "Epoch 52/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.3911 - val_loss: 0.4838\n",
      "Epoch 53/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.3889 - val_loss: 0.4831\n",
      "Epoch 54/100\n",
      "6090/6090 [==============================] - 1s 99us/sample - loss: 0.3868 - val_loss: 0.4824\n",
      "Epoch 55/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.3847 - val_loss: 0.4817\n",
      "Epoch 56/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.3826 - val_loss: 0.4810\n",
      "Epoch 57/100\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.3806 - val_loss: 0.4804\n",
      "Epoch 58/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.3786 - val_loss: 0.4798\n",
      "Epoch 59/100\n",
      "6090/6090 [==============================] - 1s 123us/sample - loss: 0.3767 - val_loss: 0.4791\n",
      "Epoch 60/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.3748 - val_loss: 0.4787\n",
      "Epoch 61/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.3729 - val_loss: 0.4782\n",
      "Epoch 62/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.3711 - val_loss: 0.4776\n",
      "Epoch 63/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.3693 - val_loss: 0.4773\n",
      "Epoch 64/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.3675 - val_loss: 0.4769\n",
      "Epoch 65/100\n",
      "6090/6090 [==============================] - 1s 122us/sample - loss: 0.3658 - val_loss: 0.4765\n",
      "Epoch 66/100\n",
      "6090/6090 [==============================] - 1s 122us/sample - loss: 0.3641 - val_loss: 0.4762\n",
      "Epoch 67/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.3624 - val_loss: 0.4758\n",
      "Epoch 68/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.3608 - val_loss: 0.4755\n",
      "Epoch 69/100\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.3592 - val_loss: 0.4753\n",
      "Epoch 70/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.3576 - val_loss: 0.4750\n",
      "Epoch 71/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.3560 - val_loss: 0.4748\n",
      "Epoch 72/100\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.3545 - val_loss: 0.4746\n",
      "Epoch 73/100\n",
      "6090/6090 [==============================] - 1s 117us/sample - loss: 0.3530 - val_loss: 0.4744\n",
      "Epoch 74/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.3515 - val_loss: 0.4741\n",
      "Epoch 75/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.3500 - val_loss: 0.4739\n",
      "Epoch 76/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.3486 - val_loss: 0.4739\n",
      "Epoch 77/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.3472 - val_loss: 0.4737\n",
      "Epoch 78/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.3458 - val_loss: 0.4736\n",
      "Epoch 79/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.3444 - val_loss: 0.4735\n",
      "Epoch 80/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.3431 - val_loss: 0.4734\n",
      "Epoch 81/100\n",
      "6090/6090 [==============================] - 1s 116us/sample - loss: 0.3417 - val_loss: 0.4734\n",
      "Epoch 82/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.3404 - val_loss: 0.4732\n",
      "Epoch 83/100\n",
      "6090/6090 [==============================] - 1s 101us/sample - loss: 0.3391 - val_loss: 0.4732\n",
      "Epoch 84/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.3378 - val_loss: 0.4732\n",
      "Epoch 85/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.3366 - val_loss: 0.4732\n",
      "Epoch 86/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.3353 - val_loss: 0.4732\n",
      "Predicting...\n",
      "(Took 57.685 sec)\n",
      "Train on 6091 samples, validate on 1522 samples\n",
      "Epoch 1/100\n",
      "6091/6091 [==============================] - 1s 149us/sample - loss: 0.6839 - val_loss: 0.6752\n",
      "Epoch 2/100\n",
      "6091/6091 [==============================] - 1s 113us/sample - loss: 0.6663 - val_loss: 0.6615\n",
      "Epoch 3/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.6519 - val_loss: 0.6502\n",
      "Epoch 4/100\n",
      "6091/6091 [==============================] - 1s 116us/sample - loss: 0.6392 - val_loss: 0.6400\n",
      "Epoch 5/100\n",
      "6091/6091 [==============================] - 1s 102us/sample - loss: 0.6274 - val_loss: 0.6306\n",
      "Epoch 6/100\n",
      "6091/6091 [==============================] - 1s 105us/sample - loss: 0.6163 - val_loss: 0.6218\n",
      "Epoch 7/100\n",
      "6091/6091 [==============================] - 1s 113us/sample - loss: 0.6058 - val_loss: 0.6134\n",
      "Epoch 8/100\n",
      "6091/6091 [==============================] - 1s 103us/sample - loss: 0.5958 - val_loss: 0.6056\n",
      "Epoch 9/100\n",
      "6091/6091 [==============================] - 1s 104us/sample - loss: 0.5863 - val_loss: 0.5982\n",
      "Epoch 10/100\n",
      "6091/6091 [==============================] - 1s 111us/sample - loss: 0.5773 - val_loss: 0.5912\n",
      "Epoch 11/100\n",
      "6091/6091 [==============================] - 1s 102us/sample - loss: 0.5686 - val_loss: 0.5844\n",
      "Epoch 12/100\n",
      "6091/6091 [==============================] - 1s 99us/sample - loss: 0.5604 - val_loss: 0.5780\n",
      "Epoch 13/100\n",
      "6091/6091 [==============================] - 1s 100us/sample - loss: 0.5526 - val_loss: 0.5720\n",
      "Epoch 14/100\n",
      "6091/6091 [==============================] - 1s 100us/sample - loss: 0.5451 - val_loss: 0.5664\n",
      "Epoch 15/100\n",
      "6091/6091 [==============================] - 1s 99us/sample - loss: 0.5379 - val_loss: 0.5609\n",
      "Epoch 16/100\n",
      "6091/6091 [==============================] - 1s 101us/sample - loss: 0.5310 - val_loss: 0.5558\n",
      "Epoch 17/100\n",
      "6091/6091 [==============================] - 1s 99us/sample - loss: 0.5244 - val_loss: 0.5509\n",
      "Epoch 18/100\n",
      "6091/6091 [==============================] - 1s 110us/sample - loss: 0.5182 - val_loss: 0.5463\n",
      "Epoch 19/100\n",
      "6091/6091 [==============================] - 1s 104us/sample - loss: 0.5121 - val_loss: 0.5418\n",
      "Epoch 20/100\n",
      "6091/6091 [==============================] - 1s 102us/sample - loss: 0.5063 - val_loss: 0.5376\n",
      "Epoch 21/100\n",
      "6091/6091 [==============================] - 1s 100us/sample - loss: 0.5008 - val_loss: 0.5336\n",
      "Epoch 22/100\n",
      "6091/6091 [==============================] - 1s 101us/sample - loss: 0.4954 - val_loss: 0.5297\n",
      "Epoch 23/100\n",
      "6091/6091 [==============================] - 1s 103us/sample - loss: 0.4903 - val_loss: 0.5260\n",
      "Epoch 24/100\n",
      "6091/6091 [==============================] - 1s 100us/sample - loss: 0.4853 - val_loss: 0.5226\n",
      "Epoch 25/100\n",
      "6091/6091 [==============================] - 1s 100us/sample - loss: 0.4805 - val_loss: 0.5192\n",
      "Epoch 26/100\n",
      "6091/6091 [==============================] - 1s 102us/sample - loss: 0.4759 - val_loss: 0.5160\n",
      "Epoch 27/100\n",
      "6091/6091 [==============================] - 1s 100us/sample - loss: 0.4715 - val_loss: 0.5130\n",
      "Epoch 28/100\n",
      "6091/6091 [==============================] - 1s 100us/sample - loss: 0.4671 - val_loss: 0.5101\n",
      "Epoch 29/100\n",
      "6091/6091 [==============================] - 1s 100us/sample - loss: 0.4630 - val_loss: 0.5073\n",
      "Epoch 30/100\n",
      "6091/6091 [==============================] - 1s 102us/sample - loss: 0.4590 - val_loss: 0.5046\n",
      "Epoch 31/100\n",
      "6091/6091 [==============================] - 1s 101us/sample - loss: 0.4551 - val_loss: 0.5021\n",
      "Epoch 32/100\n",
      "6091/6091 [==============================] - 1s 102us/sample - loss: 0.4513 - val_loss: 0.4997\n",
      "Epoch 33/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4477 - val_loss: 0.4974\n",
      "Epoch 34/100\n",
      "6091/6091 [==============================] - 1s 115us/sample - loss: 0.4442 - val_loss: 0.4951\n",
      "Epoch 35/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4407 - val_loss: 0.4930\n",
      "Epoch 36/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.4374 - val_loss: 0.4910\n",
      "Epoch 37/100\n",
      "6091/6091 [==============================] - 1s 106us/sample - loss: 0.4342 - val_loss: 0.4890\n",
      "Epoch 38/100\n",
      "6091/6091 [==============================] - 1s 105us/sample - loss: 0.4310 - val_loss: 0.4871\n",
      "Epoch 39/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.4280 - val_loss: 0.4853\n",
      "Epoch 40/100\n",
      "6091/6091 [==============================] - 1s 101us/sample - loss: 0.4250 - val_loss: 0.4836\n",
      "Epoch 41/100\n",
      "6091/6091 [==============================] - 1s 101us/sample - loss: 0.4221 - val_loss: 0.4820\n",
      "Epoch 42/100\n",
      "6091/6091 [==============================] - 1s 100us/sample - loss: 0.4194 - val_loss: 0.4803\n",
      "Epoch 43/100\n",
      "6091/6091 [==============================] - 1s 103us/sample - loss: 0.4166 - val_loss: 0.4788\n",
      "Epoch 44/100\n",
      "6091/6091 [==============================] - 1s 106us/sample - loss: 0.4140 - val_loss: 0.4774\n",
      "Epoch 45/100\n",
      "6091/6091 [==============================] - 1s 106us/sample - loss: 0.4113 - val_loss: 0.4759\n",
      "Epoch 46/100\n",
      "6091/6091 [==============================] - 1s 118us/sample - loss: 0.4088 - val_loss: 0.4746\n",
      "Epoch 47/100\n",
      "6091/6091 [==============================] - 1s 118us/sample - loss: 0.4063 - val_loss: 0.4733\n",
      "Epoch 48/100\n",
      "6091/6091 [==============================] - 1s 123us/sample - loss: 0.4039 - val_loss: 0.4720\n",
      "Epoch 49/100\n",
      "6091/6091 [==============================] - 1s 116us/sample - loss: 0.4016 - val_loss: 0.4709\n",
      "Epoch 50/100\n",
      "6091/6091 [==============================] - 1s 111us/sample - loss: 0.3993 - val_loss: 0.4697\n",
      "Epoch 51/100\n",
      "6091/6091 [==============================] - 1s 119us/sample - loss: 0.3970 - val_loss: 0.4686\n",
      "Epoch 52/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.3948 - val_loss: 0.4675\n",
      "Epoch 53/100\n",
      "6091/6091 [==============================] - 1s 111us/sample - loss: 0.3927 - val_loss: 0.4665\n",
      "Epoch 54/100\n",
      "6091/6091 [==============================] - 1s 105us/sample - loss: 0.3905 - val_loss: 0.4655\n",
      "Epoch 55/100\n",
      "6091/6091 [==============================] - 1s 105us/sample - loss: 0.3885 - val_loss: 0.4645\n",
      "Epoch 56/100\n",
      "6091/6091 [==============================] - 1s 118us/sample - loss: 0.3864 - val_loss: 0.4637\n",
      "Epoch 57/100\n",
      "6091/6091 [==============================] - 1s 121us/sample - loss: 0.3845 - val_loss: 0.4628\n",
      "Epoch 58/100\n",
      "6091/6091 [==============================] - 1s 113us/sample - loss: 0.3825 - val_loss: 0.4619\n",
      "Epoch 59/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.3806 - val_loss: 0.4612\n",
      "Epoch 60/100\n",
      "6091/6091 [==============================] - 1s 105us/sample - loss: 0.3788 - val_loss: 0.4604\n",
      "Epoch 61/100\n",
      "6091/6091 [==============================] - 1s 106us/sample - loss: 0.3769 - val_loss: 0.4596\n",
      "Epoch 62/100\n",
      "6091/6091 [==============================] - 1s 106us/sample - loss: 0.3751 - val_loss: 0.4589\n",
      "Epoch 63/100\n",
      "6091/6091 [==============================] - 1s 114us/sample - loss: 0.3734 - val_loss: 0.4582\n",
      "Epoch 64/100\n",
      "6091/6091 [==============================] - 1s 103us/sample - loss: 0.3716 - val_loss: 0.4576\n",
      "Epoch 65/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.3699 - val_loss: 0.4569\n",
      "Epoch 66/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.3683 - val_loss: 0.4564\n",
      "Epoch 67/100\n",
      "6091/6091 [==============================] - 1s 106us/sample - loss: 0.3666 - val_loss: 0.4558\n",
      "Epoch 68/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.3650 - val_loss: 0.4552\n",
      "Epoch 69/100\n",
      "6091/6091 [==============================] - 1s 103us/sample - loss: 0.3634 - val_loss: 0.4547\n",
      "Epoch 70/100\n",
      "6091/6091 [==============================] - 1s 116us/sample - loss: 0.3619 - val_loss: 0.4542\n",
      "Epoch 71/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.3603 - val_loss: 0.4537\n",
      "Epoch 72/100\n",
      "6091/6091 [==============================] - 1s 103us/sample - loss: 0.3588 - val_loss: 0.4532\n",
      "Epoch 73/100\n",
      "6091/6091 [==============================] - 1s 100us/sample - loss: 0.3573 - val_loss: 0.4527\n",
      "Epoch 74/100\n",
      "6091/6091 [==============================] - 1s 100us/sample - loss: 0.3559 - val_loss: 0.4523\n",
      "Epoch 75/100\n",
      "6091/6091 [==============================] - 1s 102us/sample - loss: 0.3544 - val_loss: 0.4519\n",
      "Epoch 76/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.3530 - val_loss: 0.4515\n",
      "Epoch 77/100\n",
      "6091/6091 [==============================] - 1s 113us/sample - loss: 0.3516 - val_loss: 0.4512\n",
      "Epoch 78/100\n",
      "6091/6091 [==============================] - 1s 122us/sample - loss: 0.3503 - val_loss: 0.4509\n",
      "Epoch 79/100\n",
      "6091/6091 [==============================] - 1s 117us/sample - loss: 0.3489 - val_loss: 0.4505\n",
      "Epoch 80/100\n",
      "6091/6091 [==============================] - 1s 116us/sample - loss: 0.3476 - val_loss: 0.4502\n",
      "Epoch 81/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.3463 - val_loss: 0.4499\n",
      "Epoch 82/100\n",
      "6091/6091 [==============================] - 1s 115us/sample - loss: 0.3450 - val_loss: 0.4496\n",
      "Epoch 83/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.3437 - val_loss: 0.4493\n",
      "Epoch 84/100\n",
      "6091/6091 [==============================] - 1s 105us/sample - loss: 0.3424 - val_loss: 0.4490\n",
      "Epoch 85/100\n",
      "6091/6091 [==============================] - 1s 106us/sample - loss: 0.3412 - val_loss: 0.4488\n",
      "Epoch 86/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.3400 - val_loss: 0.4485\n",
      "Epoch 87/100\n",
      "6091/6091 [==============================] - 1s 103us/sample - loss: 0.3388 - val_loss: 0.4484\n",
      "Epoch 88/100\n",
      "6091/6091 [==============================] - 1s 126us/sample - loss: 0.3376 - val_loss: 0.4482\n",
      "Epoch 89/100\n",
      "6091/6091 [==============================] - 1s 111us/sample - loss: 0.3364 - val_loss: 0.4480\n",
      "Epoch 90/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.3352 - val_loss: 0.4478\n",
      "Epoch 91/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.3341 - val_loss: 0.4477\n",
      "Epoch 92/100\n",
      "6091/6091 [==============================] - 1s 120us/sample - loss: 0.3330 - val_loss: 0.4475\n",
      "Epoch 93/100\n",
      "6091/6091 [==============================] - 1s 120us/sample - loss: 0.3318 - val_loss: 0.4474\n",
      "Epoch 94/100\n",
      "6091/6091 [==============================] - 1s 114us/sample - loss: 0.3307 - val_loss: 0.4473\n",
      "Epoch 95/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.3297 - val_loss: 0.4472\n",
      "Epoch 96/100\n",
      "6091/6091 [==============================] - 1s 115us/sample - loss: 0.3286 - val_loss: 0.4471\n",
      "Epoch 97/100\n",
      "6091/6091 [==============================] - 1s 111us/sample - loss: 0.3275 - val_loss: 0.4470\n",
      "Epoch 98/100\n",
      "6091/6091 [==============================] - 1s 110us/sample - loss: 0.3265 - val_loss: 0.4469\n",
      "Epoch 99/100\n",
      "6091/6091 [==============================] - 1s 122us/sample - loss: 0.3254 - val_loss: 0.4469\n",
      "Predicting...\n",
      "(Took 66.018 sec)\n",
      "Train on 6091 samples, validate on 1522 samples\n",
      "Epoch 1/100\n",
      "6091/6091 [==============================] - 1s 159us/sample - loss: 0.6834 - val_loss: 0.6764\n",
      "Epoch 2/100\n",
      "6091/6091 [==============================] - 1s 106us/sample - loss: 0.6653 - val_loss: 0.6641\n",
      "Epoch 3/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.6508 - val_loss: 0.6536\n",
      "Epoch 4/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.6380 - val_loss: 0.6438\n",
      "Epoch 5/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.6262 - val_loss: 0.6347\n",
      "Epoch 6/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.6152 - val_loss: 0.6259\n",
      "Epoch 7/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.6046 - val_loss: 0.6177\n",
      "Epoch 8/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.5946 - val_loss: 0.6099\n",
      "Epoch 9/100\n",
      "6091/6091 [==============================] - 1s 106us/sample - loss: 0.5850 - val_loss: 0.6025\n",
      "Epoch 10/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.5760 - val_loss: 0.5954\n",
      "Epoch 11/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.5673 - val_loss: 0.5888\n",
      "Epoch 12/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.5591 - val_loss: 0.5825\n",
      "Epoch 13/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.5512 - val_loss: 0.5767\n",
      "Epoch 14/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.5436 - val_loss: 0.5708\n",
      "Epoch 15/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.5364 - val_loss: 0.5655\n",
      "Epoch 16/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.5295 - val_loss: 0.5605\n",
      "Epoch 17/100\n",
      "6091/6091 [==============================] - 1s 122us/sample - loss: 0.5230 - val_loss: 0.5556\n",
      "Epoch 18/100\n",
      "6091/6091 [==============================] - 1s 118us/sample - loss: 0.5166 - val_loss: 0.5510\n",
      "Epoch 19/100\n",
      "6091/6091 [==============================] - 1s 114us/sample - loss: 0.5105 - val_loss: 0.5466\n",
      "Epoch 20/100\n",
      "6091/6091 [==============================] - 1s 116us/sample - loss: 0.5047 - val_loss: 0.5425\n",
      "Epoch 21/100\n",
      "6091/6091 [==============================] - 1s 111us/sample - loss: 0.4991 - val_loss: 0.5386\n",
      "Epoch 22/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.4937 - val_loss: 0.5350\n",
      "Epoch 23/100\n",
      "6091/6091 [==============================] - 1s 111us/sample - loss: 0.4885 - val_loss: 0.5314\n",
      "Epoch 24/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.4835 - val_loss: 0.5281\n",
      "Epoch 25/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4787 - val_loss: 0.5248\n",
      "Epoch 26/100\n",
      "6091/6091 [==============================] - 1s 121us/sample - loss: 0.4741 - val_loss: 0.5217\n",
      "Epoch 27/100\n",
      "6091/6091 [==============================] - 1s 118us/sample - loss: 0.4696 - val_loss: 0.5189\n",
      "Epoch 28/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.4653 - val_loss: 0.5161\n",
      "Epoch 29/100\n",
      "6091/6091 [==============================] - 1s 110us/sample - loss: 0.4610 - val_loss: 0.5134\n",
      "Epoch 30/100\n",
      "6091/6091 [==============================] - 1s 115us/sample - loss: 0.4570 - val_loss: 0.5110\n",
      "Epoch 31/100\n",
      "6091/6091 [==============================] - 1s 111us/sample - loss: 0.4531 - val_loss: 0.5085\n",
      "Epoch 32/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.4493 - val_loss: 0.5063\n",
      "Epoch 33/100\n",
      "6091/6091 [==============================] - 1s 110us/sample - loss: 0.4456 - val_loss: 0.5041\n",
      "Epoch 34/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.4420 - val_loss: 0.5021\n",
      "Epoch 35/100\n",
      "6091/6091 [==============================] - 1s 110us/sample - loss: 0.4385 - val_loss: 0.5001\n",
      "Epoch 36/100\n",
      "6091/6091 [==============================] - 1s 114us/sample - loss: 0.4352 - val_loss: 0.4982\n",
      "Epoch 37/100\n",
      "6091/6091 [==============================] - 1s 116us/sample - loss: 0.4319 - val_loss: 0.4965\n",
      "Epoch 38/100\n",
      "6091/6091 [==============================] - 1s 118us/sample - loss: 0.4287 - val_loss: 0.4947\n",
      "Epoch 39/100\n",
      "6091/6091 [==============================] - 1s 113us/sample - loss: 0.4256 - val_loss: 0.4930\n",
      "Epoch 40/100\n",
      "6091/6091 [==============================] - 1s 132us/sample - loss: 0.4226 - val_loss: 0.4916\n",
      "Epoch 41/100\n",
      "6091/6091 [==============================] - 1s 113us/sample - loss: 0.4197 - val_loss: 0.4901\n",
      "Epoch 42/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.4169 - val_loss: 0.4887\n",
      "Epoch 43/100\n",
      "6091/6091 [==============================] - 1s 131us/sample - loss: 0.4141 - val_loss: 0.4874\n",
      "Epoch 44/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6091/6091 [==============================] - 1s 152us/sample - loss: 0.4114 - val_loss: 0.4861\n",
      "Epoch 45/100\n",
      "6091/6091 [==============================] - 1s 122us/sample - loss: 0.4087 - val_loss: 0.4848\n",
      "Epoch 46/100\n",
      "6091/6091 [==============================] - 1s 123us/sample - loss: 0.4062 - val_loss: 0.4837\n",
      "Epoch 47/100\n",
      "6091/6091 [==============================] - 1s 115us/sample - loss: 0.4037 - val_loss: 0.4827\n",
      "Epoch 48/100\n",
      "6091/6091 [==============================] - 1s 117us/sample - loss: 0.4012 - val_loss: 0.4816\n",
      "Epoch 49/100\n",
      "6091/6091 [==============================] - 1s 111us/sample - loss: 0.3988 - val_loss: 0.4807\n",
      "Epoch 50/100\n",
      "6091/6091 [==============================] - 1s 105us/sample - loss: 0.3965 - val_loss: 0.4797\n",
      "Epoch 51/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.3942 - val_loss: 0.4788\n",
      "Epoch 52/100\n",
      "6091/6091 [==============================] - 1s 106us/sample - loss: 0.3919 - val_loss: 0.4779\n",
      "Epoch 53/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.3898 - val_loss: 0.4772\n",
      "Epoch 54/100\n",
      "6091/6091 [==============================] - 1s 106us/sample - loss: 0.3876 - val_loss: 0.4764\n",
      "Epoch 55/100\n",
      "6091/6091 [==============================] - 1s 110us/sample - loss: 0.3855 - val_loss: 0.4757\n",
      "Epoch 56/100\n",
      "6091/6091 [==============================] - 1s 106us/sample - loss: 0.3835 - val_loss: 0.4750\n",
      "Epoch 57/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.3815 - val_loss: 0.4744\n",
      "Epoch 58/100\n",
      "6091/6091 [==============================] - 1s 122us/sample - loss: 0.3795 - val_loss: 0.4737\n",
      "Epoch 59/100\n",
      "6091/6091 [==============================] - 1s 106us/sample - loss: 0.3775 - val_loss: 0.4732\n",
      "Epoch 60/100\n",
      "6091/6091 [==============================] - 1s 105us/sample - loss: 0.3757 - val_loss: 0.4727\n",
      "Epoch 61/100\n",
      "6091/6091 [==============================] - 1s 106us/sample - loss: 0.3738 - val_loss: 0.4722\n",
      "Epoch 62/100\n",
      "6091/6091 [==============================] - 1s 104us/sample - loss: 0.3720 - val_loss: 0.4717\n",
      "Epoch 63/100\n",
      "6091/6091 [==============================] - 1s 105us/sample - loss: 0.3702 - val_loss: 0.4712\n",
      "Epoch 64/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.3684 - val_loss: 0.4708\n",
      "Epoch 65/100\n",
      "6091/6091 [==============================] - 1s 104us/sample - loss: 0.3667 - val_loss: 0.4704\n",
      "Epoch 66/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.3650 - val_loss: 0.4701\n",
      "Epoch 67/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.3633 - val_loss: 0.4697\n",
      "Epoch 68/100\n",
      "6091/6091 [==============================] - 1s 102us/sample - loss: 0.3617 - val_loss: 0.4695\n",
      "Epoch 69/100\n",
      "6091/6091 [==============================] - 1s 105us/sample - loss: 0.3601 - val_loss: 0.4692\n",
      "Epoch 70/100\n",
      "6091/6091 [==============================] - 1s 104us/sample - loss: 0.3585 - val_loss: 0.4689\n",
      "Epoch 71/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.3569 - val_loss: 0.4687\n",
      "Epoch 72/100\n",
      "6091/6091 [==============================] - 1s 105us/sample - loss: 0.3554 - val_loss: 0.4684\n",
      "Epoch 73/100\n",
      "6091/6091 [==============================] - 1s 105us/sample - loss: 0.3539 - val_loss: 0.4682\n",
      "Epoch 74/100\n",
      "6091/6091 [==============================] - 1s 116us/sample - loss: 0.3524 - val_loss: 0.4680\n",
      "Epoch 75/100\n",
      "6091/6091 [==============================] - 1s 114us/sample - loss: 0.3509 - val_loss: 0.4679\n",
      "Epoch 76/100\n",
      "6091/6091 [==============================] - 1s 119us/sample - loss: 0.3495 - val_loss: 0.4677\n",
      "Epoch 77/100\n",
      "6091/6091 [==============================] - 1s 115us/sample - loss: 0.3481 - val_loss: 0.4677\n",
      "Epoch 78/100\n",
      "6091/6091 [==============================] - 1s 126us/sample - loss: 0.3467 - val_loss: 0.4675\n",
      "Epoch 79/100\n",
      "6091/6091 [==============================] - 1s 119us/sample - loss: 0.3453 - val_loss: 0.4674\n",
      "Epoch 80/100\n",
      "6091/6091 [==============================] - 1s 126us/sample - loss: 0.3439 - val_loss: 0.4674\n",
      "Epoch 81/100\n",
      "6091/6091 [==============================] - 1s 124us/sample - loss: 0.3426 - val_loss: 0.4673\n",
      "Epoch 82/100\n",
      "6091/6091 [==============================] - 1s 124us/sample - loss: 0.3413 - val_loss: 0.4672\n",
      "Epoch 83/100\n",
      "6091/6091 [==============================] - 1s 128us/sample - loss: 0.3400 - val_loss: 0.4672\n",
      "Predicting...\n",
      "(Took 57.757 sec)\n",
      "Combined confusion matrix:\n",
      "[[3805.  537.]\n",
      " [1058. 2213.]]\n",
      "(Overall, took 303.652 sec)\n",
      "Accuracy: 79.05% +/- 1.01%\n",
      "Precision for positive class: 78.25% +/- 2.02%\n",
      "Precision for negative class: 80.49% +/- 1.21%\n",
      "Recall for positive class: 87.63% +/- 1.18%\n",
      "Recall for negative class: 67.68% +/- 2.44%\n",
      "F for positive class: 82.66% +/- 1.12%\n",
      "F for negative class: 73.51% +/- 1.08%\n",
      "Mean F score: 78.08% +/- 0.94%\n"
     ]
    }
   ],
   "source": [
    "# POS tagging, transformed\n",
    "# Normalized\n",
    "all_metrics(cross_validate_logistic(dftrain_pos_min_norm, 100))\n",
    "# TF-IDF\n",
    "all_metrics(cross_validate_logistic(dftrain_pos_min_tfidf, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/100\n",
      "6090/6090 [==============================] - 1s 154us/sample - loss: 0.6857 - val_loss: 0.6779\n",
      "Epoch 2/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.6737 - val_loss: 0.6679\n",
      "Epoch 3/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.6646 - val_loss: 0.6598\n",
      "Epoch 4/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.6568 - val_loss: 0.6527\n",
      "Epoch 5/100\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.6494 - val_loss: 0.6462\n",
      "Epoch 6/100\n",
      "6090/6090 [==============================] - 1s 116us/sample - loss: 0.6425 - val_loss: 0.6401\n",
      "Epoch 7/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.6358 - val_loss: 0.6342\n",
      "Epoch 8/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.6294 - val_loss: 0.6286\n",
      "Epoch 9/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.6232 - val_loss: 0.6232\n",
      "Epoch 10/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.6172 - val_loss: 0.6180\n",
      "Epoch 11/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.6115 - val_loss: 0.6131\n",
      "Epoch 12/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.6059 - val_loss: 0.6083\n",
      "Epoch 13/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.6005 - val_loss: 0.6037\n",
      "Epoch 14/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.5953 - val_loss: 0.5993\n",
      "Epoch 15/100\n",
      "6090/6090 [==============================] - 1s 118us/sample - loss: 0.5903 - val_loss: 0.5950\n",
      "Epoch 16/100\n",
      "6090/6090 [==============================] - 1s 133us/sample - loss: 0.5854 - val_loss: 0.5909\n",
      "Epoch 17/100\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.5806 - val_loss: 0.5869\n",
      "Epoch 18/100\n",
      "6090/6090 [==============================] - 1s 125us/sample - loss: 0.5761 - val_loss: 0.5830\n",
      "Epoch 19/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.5717 - val_loss: 0.5793\n",
      "Epoch 20/100\n",
      "6090/6090 [==============================] - 1s 133us/sample - loss: 0.5674 - val_loss: 0.5757\n",
      "Epoch 21/100\n",
      "6090/6090 [==============================] - 1s 132us/sample - loss: 0.5632 - val_loss: 0.5723\n",
      "Epoch 22/100\n",
      "6090/6090 [==============================] - 1s 130us/sample - loss: 0.5592 - val_loss: 0.5690\n",
      "Epoch 23/100\n",
      "6090/6090 [==============================] - 1s 136us/sample - loss: 0.5553 - val_loss: 0.5657\n",
      "Epoch 24/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.5515 - val_loss: 0.5626\n",
      "Epoch 25/100\n",
      "6090/6090 [==============================] - 1s 124us/sample - loss: 0.5478 - val_loss: 0.5596\n",
      "Epoch 26/100\n",
      "6090/6090 [==============================] - 1s 133us/sample - loss: 0.5442 - val_loss: 0.5567\n",
      "Epoch 27/100\n",
      "6090/6090 [==============================] - 1s 124us/sample - loss: 0.5407 - val_loss: 0.5539\n",
      "Epoch 28/100\n",
      "6090/6090 [==============================] - 1s 116us/sample - loss: 0.5373 - val_loss: 0.5512\n",
      "Epoch 29/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.5340 - val_loss: 0.5485\n",
      "Epoch 30/100\n",
      "6090/6090 [==============================] - 1s 120us/sample - loss: 0.5308 - val_loss: 0.5460\n",
      "Epoch 31/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.5276 - val_loss: 0.5435\n",
      "Epoch 32/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.5246 - val_loss: 0.5412\n",
      "Epoch 33/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.5216 - val_loss: 0.5389\n",
      "Epoch 34/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.5187 - val_loss: 0.5366\n",
      "Epoch 35/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.5159 - val_loss: 0.5345\n",
      "Epoch 36/100\n",
      "6090/6090 [==============================] - 1s 126us/sample - loss: 0.5131 - val_loss: 0.5324\n",
      "Epoch 37/100\n",
      "6090/6090 [==============================] - 1s 123us/sample - loss: 0.5104 - val_loss: 0.5303\n",
      "Epoch 38/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.5078 - val_loss: 0.5283\n",
      "Epoch 39/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.5052 - val_loss: 0.5263\n",
      "Epoch 40/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.5027 - val_loss: 0.5245\n",
      "Epoch 41/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.5002 - val_loss: 0.5227\n",
      "Epoch 42/100\n",
      "6090/6090 [==============================] - 1s 117us/sample - loss: 0.4978 - val_loss: 0.5209\n",
      "Epoch 43/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.4955 - val_loss: 0.5192\n",
      "Epoch 44/100\n",
      "6090/6090 [==============================] - 1s 117us/sample - loss: 0.4932 - val_loss: 0.5175\n",
      "Epoch 45/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.4909 - val_loss: 0.5159\n",
      "Epoch 46/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.4887 - val_loss: 0.5144\n",
      "Epoch 47/100\n",
      "6090/6090 [==============================] - 1s 116us/sample - loss: 0.4866 - val_loss: 0.5129\n",
      "Epoch 48/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4844 - val_loss: 0.5114\n",
      "Epoch 49/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4824 - val_loss: 0.5099\n",
      "Epoch 50/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4803 - val_loss: 0.5085\n",
      "Epoch 51/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4784 - val_loss: 0.5072\n",
      "Epoch 52/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.4764 - val_loss: 0.5058\n",
      "Epoch 53/100\n",
      "6090/6090 [==============================] - 1s 122us/sample - loss: 0.4745 - val_loss: 0.5045\n",
      "Epoch 54/100\n",
      "6090/6090 [==============================] - 1s 125us/sample - loss: 0.4726 - val_loss: 0.5033\n",
      "Epoch 55/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.4708 - val_loss: 0.5021\n",
      "Epoch 56/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.4689 - val_loss: 0.5008\n",
      "Epoch 57/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.4672 - val_loss: 0.4997\n",
      "Epoch 58/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.4654 - val_loss: 0.4986\n",
      "Epoch 59/100\n",
      "6090/6090 [==============================] - 1s 120us/sample - loss: 0.4637 - val_loss: 0.4975\n",
      "Epoch 60/100\n",
      "6090/6090 [==============================] - 1s 123us/sample - loss: 0.4620 - val_loss: 0.4964\n",
      "Epoch 61/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4603 - val_loss: 0.4953\n",
      "Epoch 62/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.4587 - val_loss: 0.4943\n",
      "Epoch 63/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4571 - val_loss: 0.4933\n",
      "Epoch 64/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4555 - val_loss: 0.4923\n",
      "Epoch 65/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.4539 - val_loss: 0.4914\n",
      "Epoch 66/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.4524 - val_loss: 0.4905\n",
      "Epoch 67/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4509 - val_loss: 0.4896\n",
      "Epoch 68/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.4494 - val_loss: 0.4886\n",
      "Epoch 69/100\n",
      "6090/6090 [==============================] - 1s 116us/sample - loss: 0.4479 - val_loss: 0.4878\n",
      "Epoch 70/100\n",
      "6090/6090 [==============================] - 1s 117us/sample - loss: 0.4465 - val_loss: 0.4870\n",
      "Epoch 71/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.4450 - val_loss: 0.4862\n",
      "Epoch 72/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4436 - val_loss: 0.4853\n",
      "Epoch 73/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.4423 - val_loss: 0.4846\n",
      "Epoch 74/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.4409 - val_loss: 0.4838\n",
      "Epoch 75/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.4396 - val_loss: 0.4831\n",
      "Epoch 76/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.4383 - val_loss: 0.4824\n",
      "Epoch 77/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4369 - val_loss: 0.4816\n",
      "Epoch 78/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4356 - val_loss: 0.4809\n",
      "Epoch 79/100\n",
      "6090/6090 [==============================] - 1s 101us/sample - loss: 0.4344 - val_loss: 0.4802\n",
      "Epoch 80/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.4331 - val_loss: 0.4795\n",
      "Epoch 81/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4319 - val_loss: 0.4789\n",
      "Epoch 82/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.4306 - val_loss: 0.4782\n",
      "Epoch 83/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.4294 - val_loss: 0.4776\n",
      "Epoch 84/100\n",
      "6090/6090 [==============================] - 1s 116us/sample - loss: 0.4282 - val_loss: 0.4770\n",
      "Epoch 85/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.4270 - val_loss: 0.4764\n",
      "Epoch 86/100\n",
      "6090/6090 [==============================] - 1s 121us/sample - loss: 0.4259 - val_loss: 0.4758\n",
      "Epoch 87/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.4247 - val_loss: 0.4753\n",
      "Epoch 88/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.4236 - val_loss: 0.4747\n",
      "Epoch 89/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.4225 - val_loss: 0.4742\n",
      "Epoch 90/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.4214 - val_loss: 0.4736\n",
      "Epoch 91/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.4203 - val_loss: 0.4731\n",
      "Epoch 92/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.4192 - val_loss: 0.4726\n",
      "Epoch 93/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.4181 - val_loss: 0.4721\n",
      "Epoch 94/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.4170 - val_loss: 0.4716\n",
      "Epoch 95/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.4160 - val_loss: 0.4711\n",
      "Epoch 96/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.4150 - val_loss: 0.4707\n",
      "Epoch 97/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.4139 - val_loss: 0.4702\n",
      "Epoch 98/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4129 - val_loss: 0.4697\n",
      "Epoch 99/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.4119 - val_loss: 0.4693\n",
      "Epoch 100/100\n",
      "6090/6090 [==============================] - 1s 99us/sample - loss: 0.4109 - val_loss: 0.4689\n",
      "Predicting...\n",
      "(Took 69.355 sec)\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/100\n",
      "6090/6090 [==============================] - 1s 141us/sample - loss: 0.6865 - val_loss: 0.6775\n",
      "Epoch 2/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.6745 - val_loss: 0.6669\n",
      "Epoch 3/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.6654 - val_loss: 0.6586\n",
      "Epoch 4/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.6575 - val_loss: 0.6515\n",
      "Epoch 5/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.6502 - val_loss: 0.6451\n",
      "Epoch 6/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.6432 - val_loss: 0.6389\n",
      "Epoch 7/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.6365 - val_loss: 0.6332\n",
      "Epoch 8/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.6300 - val_loss: 0.6275\n",
      "Epoch 9/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.6238 - val_loss: 0.6222\n",
      "Epoch 10/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.6178 - val_loss: 0.6172\n",
      "Epoch 11/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.6119 - val_loss: 0.6122\n",
      "Epoch 12/100\n",
      "6090/6090 [==============================] - 1s 118us/sample - loss: 0.6064 - val_loss: 0.6075\n",
      "Epoch 13/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.6009 - val_loss: 0.6029\n",
      "Epoch 14/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.5957 - val_loss: 0.5986\n",
      "Epoch 15/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.5906 - val_loss: 0.5943\n",
      "Epoch 16/100\n",
      "6090/6090 [==============================] - 1s 121us/sample - loss: 0.5857 - val_loss: 0.5903\n",
      "Epoch 17/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.5810 - val_loss: 0.5865\n",
      "Epoch 18/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.5764 - val_loss: 0.5826\n",
      "Epoch 19/100\n",
      "6090/6090 [==============================] - 1s 117us/sample - loss: 0.5719 - val_loss: 0.5790\n",
      "Epoch 20/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.5676 - val_loss: 0.5754\n",
      "Epoch 21/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.5634 - val_loss: 0.5721\n",
      "Epoch 22/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.5594 - val_loss: 0.5687\n",
      "Epoch 23/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.5555 - val_loss: 0.5657\n",
      "Epoch 24/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.5516 - val_loss: 0.5627\n",
      "Epoch 25/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.5479 - val_loss: 0.5597\n",
      "Epoch 26/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.5443 - val_loss: 0.5569\n",
      "Epoch 27/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.5408 - val_loss: 0.5541\n",
      "Epoch 28/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.5374 - val_loss: 0.5514\n",
      "Epoch 29/100\n",
      "6090/6090 [==============================] - 1s 123us/sample - loss: 0.5341 - val_loss: 0.5489\n",
      "Epoch 30/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.5308 - val_loss: 0.5464\n",
      "Epoch 31/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.5277 - val_loss: 0.5440\n",
      "Epoch 32/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.5246 - val_loss: 0.5417\n",
      "Epoch 33/100\n",
      "6090/6090 [==============================] - 1s 117us/sample - loss: 0.5216 - val_loss: 0.5395\n",
      "Epoch 34/100\n",
      "6090/6090 [==============================] - 1s 120us/sample - loss: 0.5187 - val_loss: 0.5373\n",
      "Epoch 35/100\n",
      "6090/6090 [==============================] - 1s 129us/sample - loss: 0.5159 - val_loss: 0.5352\n",
      "Epoch 36/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.5131 - val_loss: 0.5331\n",
      "Epoch 37/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.5104 - val_loss: 0.5311\n",
      "Epoch 38/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.5078 - val_loss: 0.5292\n",
      "Epoch 39/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.5052 - val_loss: 0.5273\n",
      "Epoch 40/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.5027 - val_loss: 0.5255\n",
      "Epoch 41/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.5002 - val_loss: 0.5237\n",
      "Epoch 42/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.4978 - val_loss: 0.5220\n",
      "Epoch 43/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4955 - val_loss: 0.5205\n",
      "Epoch 44/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.4931 - val_loss: 0.5188\n",
      "Epoch 45/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.4909 - val_loss: 0.5173\n",
      "Epoch 46/100\n",
      "6090/6090 [==============================] - 1s 117us/sample - loss: 0.4887 - val_loss: 0.5157\n",
      "Epoch 47/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.4865 - val_loss: 0.5141\n",
      "Epoch 48/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.4844 - val_loss: 0.5127\n",
      "Epoch 49/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.4823 - val_loss: 0.5113\n",
      "Epoch 50/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.4803 - val_loss: 0.5099\n",
      "Epoch 51/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.4783 - val_loss: 0.5086\n",
      "Epoch 52/100\n",
      "6090/6090 [==============================] - 1s 117us/sample - loss: 0.4763 - val_loss: 0.5073\n",
      "Epoch 53/100\n",
      "6090/6090 [==============================] - 1s 117us/sample - loss: 0.4744 - val_loss: 0.5061\n",
      "Epoch 54/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4725 - val_loss: 0.5048\n",
      "Epoch 55/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.4707 - val_loss: 0.5036\n",
      "Epoch 56/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.4689 - val_loss: 0.5025\n",
      "Epoch 57/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.4671 - val_loss: 0.5014\n",
      "Epoch 58/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.4653 - val_loss: 0.5004\n",
      "Epoch 59/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.4636 - val_loss: 0.4992\n",
      "Epoch 60/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.4619 - val_loss: 0.4981\n",
      "Epoch 61/100\n",
      "6090/6090 [==============================] - 1s 99us/sample - loss: 0.4602 - val_loss: 0.4970\n",
      "Epoch 62/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4586 - val_loss: 0.4960\n",
      "Epoch 63/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4570 - val_loss: 0.4951\n",
      "Epoch 64/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.4554 - val_loss: 0.4941\n",
      "Epoch 65/100\n",
      "6090/6090 [==============================] - 1s 124us/sample - loss: 0.4538 - val_loss: 0.4932\n",
      "Epoch 66/100\n",
      "6090/6090 [==============================] - 1s 122us/sample - loss: 0.4523 - val_loss: 0.4923\n",
      "Epoch 67/100\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.4508 - val_loss: 0.4914\n",
      "Epoch 68/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4493 - val_loss: 0.4905\n",
      "Epoch 69/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4478 - val_loss: 0.4897\n",
      "Epoch 70/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4464 - val_loss: 0.4888\n",
      "Epoch 71/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4450 - val_loss: 0.4881\n",
      "Epoch 72/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.4435 - val_loss: 0.4873\n",
      "Epoch 73/100\n",
      "6090/6090 [==============================] - 1s 101us/sample - loss: 0.4422 - val_loss: 0.4865\n",
      "Epoch 74/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.4408 - val_loss: 0.4857\n",
      "Epoch 75/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.4395 - val_loss: 0.4849\n",
      "Epoch 76/100\n",
      "6090/6090 [==============================] - 1s 121us/sample - loss: 0.4381 - val_loss: 0.4842\n",
      "Epoch 77/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.4368 - val_loss: 0.4835\n",
      "Epoch 78/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.4355 - val_loss: 0.4828\n",
      "Epoch 79/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.4343 - val_loss: 0.4821\n",
      "Epoch 80/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.4330 - val_loss: 0.4815\n",
      "Epoch 81/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.4318 - val_loss: 0.4807\n",
      "Epoch 82/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.4305 - val_loss: 0.4801\n",
      "Epoch 83/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.4293 - val_loss: 0.4795\n",
      "Epoch 84/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.4281 - val_loss: 0.4789\n",
      "Epoch 85/100\n",
      "6090/6090 [==============================] - 1s 147us/sample - loss: 0.4269 - val_loss: 0.4783\n",
      "Epoch 86/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4258 - val_loss: 0.4777\n",
      "Epoch 87/100\n",
      "6090/6090 [==============================] - 1s 119us/sample - loss: 0.4247 - val_loss: 0.4771\n",
      "Epoch 88/100\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.4235 - val_loss: 0.4766\n",
      "Epoch 89/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.4224 - val_loss: 0.4761\n",
      "Epoch 90/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.4213 - val_loss: 0.4755\n",
      "Epoch 91/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.4202 - val_loss: 0.4750\n",
      "Epoch 92/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.4191 - val_loss: 0.4745\n",
      "Epoch 93/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.4180 - val_loss: 0.4740\n",
      "Epoch 94/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.4170 - val_loss: 0.4735\n",
      "Epoch 95/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.4159 - val_loss: 0.4730\n",
      "Epoch 96/100\n",
      "6090/6090 [==============================] - 1s 121us/sample - loss: 0.4149 - val_loss: 0.4725\n",
      "Epoch 97/100\n",
      "6090/6090 [==============================] - 1s 119us/sample - loss: 0.4139 - val_loss: 0.4721\n",
      "Epoch 98/100\n",
      "6090/6090 [==============================] - 1s 118us/sample - loss: 0.4129 - val_loss: 0.4716\n",
      "Epoch 99/100\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.4118 - val_loss: 0.4712\n",
      "Epoch 100/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.4109 - val_loss: 0.4708\n",
      "Predicting...\n",
      "(Took 67.814 sec)\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/100\n",
      "6090/6090 [==============================] - 1s 144us/sample - loss: 0.6853 - val_loss: 0.6805\n",
      "Epoch 2/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.6727 - val_loss: 0.6721\n",
      "Epoch 3/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.6633 - val_loss: 0.6653\n",
      "Epoch 4/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.6553 - val_loss: 0.6589\n",
      "Epoch 5/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.6479 - val_loss: 0.6529\n",
      "Epoch 6/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.6410 - val_loss: 0.6471\n",
      "Epoch 7/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.6342 - val_loss: 0.6415\n",
      "Epoch 8/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.6278 - val_loss: 0.6359\n",
      "Epoch 9/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.6217 - val_loss: 0.6308\n",
      "Epoch 10/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.6157 - val_loss: 0.6257\n",
      "Epoch 11/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.6099 - val_loss: 0.6208\n",
      "Epoch 12/100\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.6043 - val_loss: 0.6162\n",
      "Epoch 13/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.5990 - val_loss: 0.6116\n",
      "Epoch 14/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.5937 - val_loss: 0.6074\n",
      "Epoch 15/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.5887 - val_loss: 0.6032\n",
      "Epoch 16/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.5838 - val_loss: 0.5992\n",
      "Epoch 17/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.5791 - val_loss: 0.5954\n",
      "Epoch 18/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.5745 - val_loss: 0.5917\n",
      "Epoch 19/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.5701 - val_loss: 0.5882\n",
      "Epoch 20/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.5658 - val_loss: 0.5848\n",
      "Epoch 21/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.5616 - val_loss: 0.5815\n",
      "Epoch 22/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.5575 - val_loss: 0.5782\n",
      "Epoch 23/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.5536 - val_loss: 0.5751\n",
      "Epoch 24/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.5498 - val_loss: 0.5721\n",
      "Epoch 25/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.5460 - val_loss: 0.5692\n",
      "Epoch 26/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.5424 - val_loss: 0.5664\n",
      "Epoch 27/100\n",
      "6090/6090 [==============================] - 1s 98us/sample - loss: 0.5390 - val_loss: 0.5637\n",
      "Epoch 28/100\n",
      "6090/6090 [==============================] - 1s 99us/sample - loss: 0.5355 - val_loss: 0.5612\n",
      "Epoch 29/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.5322 - val_loss: 0.5586\n",
      "Epoch 30/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.5290 - val_loss: 0.5562\n",
      "Epoch 31/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.5258 - val_loss: 0.5539\n",
      "Epoch 32/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.5227 - val_loss: 0.5516\n",
      "Epoch 33/100\n",
      "6090/6090 [==============================] - 1s 98us/sample - loss: 0.5197 - val_loss: 0.5494\n",
      "Epoch 34/100\n",
      "6090/6090 [==============================] - 1s 99us/sample - loss: 0.5168 - val_loss: 0.5472\n",
      "Epoch 35/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.5140 - val_loss: 0.5452\n",
      "Epoch 36/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.5112 - val_loss: 0.5431\n",
      "Epoch 37/100\n",
      "6090/6090 [==============================] - 1s 98us/sample - loss: 0.5085 - val_loss: 0.5412\n",
      "Epoch 38/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.5059 - val_loss: 0.5393\n",
      "Epoch 39/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.5033 - val_loss: 0.5375\n",
      "Epoch 40/100\n",
      "6090/6090 [==============================] - 1s 98us/sample - loss: 0.5008 - val_loss: 0.5358\n",
      "Epoch 41/100\n",
      "6090/6090 [==============================] - 1s 99us/sample - loss: 0.4983 - val_loss: 0.5341\n",
      "Epoch 42/100\n",
      "6090/6090 [==============================] - 1s 98us/sample - loss: 0.4959 - val_loss: 0.5324\n",
      "Epoch 43/100\n",
      "6090/6090 [==============================] - 1s 98us/sample - loss: 0.4936 - val_loss: 0.5308\n",
      "Epoch 44/100\n",
      "6090/6090 [==============================] - 1s 99us/sample - loss: 0.4913 - val_loss: 0.5292\n",
      "Epoch 45/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.4890 - val_loss: 0.5276\n",
      "Epoch 46/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.4868 - val_loss: 0.5261\n",
      "Epoch 47/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.4847 - val_loss: 0.5247\n",
      "Epoch 48/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.4825 - val_loss: 0.5233\n",
      "Epoch 49/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.4804 - val_loss: 0.5219\n",
      "Epoch 50/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.4784 - val_loss: 0.5207\n",
      "Epoch 51/100\n",
      "6090/6090 [==============================] - 1s 120us/sample - loss: 0.4764 - val_loss: 0.5194\n",
      "Epoch 52/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.4744 - val_loss: 0.5181\n",
      "Epoch 53/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4725 - val_loss: 0.5169\n",
      "Epoch 54/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4706 - val_loss: 0.5158\n",
      "Epoch 55/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.4688 - val_loss: 0.5146\n",
      "Epoch 56/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.4669 - val_loss: 0.5134\n",
      "Epoch 57/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.4651 - val_loss: 0.5123\n",
      "Epoch 58/100\n",
      "6090/6090 [==============================] - 1s 99us/sample - loss: 0.4634 - val_loss: 0.5112\n",
      "Epoch 59/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.4616 - val_loss: 0.5102\n",
      "Epoch 60/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.4599 - val_loss: 0.5092\n",
      "Epoch 61/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.4583 - val_loss: 0.5081\n",
      "Epoch 62/100\n",
      "6090/6090 [==============================] - 1s 99us/sample - loss: 0.4566 - val_loss: 0.5072\n",
      "Epoch 63/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.4550 - val_loss: 0.5062\n",
      "Epoch 64/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4534 - val_loss: 0.5053\n",
      "Epoch 65/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.4519 - val_loss: 0.5044\n",
      "Epoch 66/100\n",
      "6090/6090 [==============================] - 1s 99us/sample - loss: 0.4503 - val_loss: 0.5036\n",
      "Epoch 67/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.4488 - val_loss: 0.5027\n",
      "Epoch 68/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.4473 - val_loss: 0.5018\n",
      "Epoch 69/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.4458 - val_loss: 0.5010\n",
      "Epoch 70/100\n",
      "6090/6090 [==============================] - 1s 98us/sample - loss: 0.4444 - val_loss: 0.5001\n",
      "Epoch 71/100\n",
      "6090/6090 [==============================] - 1s 101us/sample - loss: 0.4430 - val_loss: 0.4993\n",
      "Epoch 72/100\n",
      "6090/6090 [==============================] - 1s 99us/sample - loss: 0.4415 - val_loss: 0.4986\n",
      "Epoch 73/100\n",
      "6090/6090 [==============================] - 1s 99us/sample - loss: 0.4402 - val_loss: 0.4978\n",
      "Epoch 74/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.4388 - val_loss: 0.4970\n",
      "Epoch 75/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4374 - val_loss: 0.4963\n",
      "Epoch 76/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.4361 - val_loss: 0.4956\n",
      "Epoch 77/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.4348 - val_loss: 0.4949\n",
      "Epoch 78/100\n",
      "6090/6090 [==============================] - 1s 101us/sample - loss: 0.4335 - val_loss: 0.4943\n",
      "Epoch 79/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.4322 - val_loss: 0.4937\n",
      "Epoch 80/100\n",
      "6090/6090 [==============================] - 1s 99us/sample - loss: 0.4309 - val_loss: 0.4929\n",
      "Epoch 81/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.4297 - val_loss: 0.4923\n",
      "Epoch 82/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.4285 - val_loss: 0.4917\n",
      "Epoch 83/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4273 - val_loss: 0.4910\n",
      "Epoch 84/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4261 - val_loss: 0.4904\n",
      "Epoch 85/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4249 - val_loss: 0.4898\n",
      "Epoch 86/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.4237 - val_loss: 0.4893\n",
      "Epoch 87/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.4226 - val_loss: 0.4887\n",
      "Epoch 88/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.4214 - val_loss: 0.4882\n",
      "Epoch 89/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.4203 - val_loss: 0.4877\n",
      "Epoch 90/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4192 - val_loss: 0.4871\n",
      "Epoch 91/100\n",
      "6090/6090 [==============================] - 1s 99us/sample - loss: 0.4181 - val_loss: 0.4866\n",
      "Epoch 92/100\n",
      "6090/6090 [==============================] - 1s 101us/sample - loss: 0.4170 - val_loss: 0.4861\n",
      "Epoch 93/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.4160 - val_loss: 0.4856\n",
      "Epoch 94/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.4149 - val_loss: 0.4851\n",
      "Epoch 95/100\n",
      "6090/6090 [==============================] - 1s 101us/sample - loss: 0.4138 - val_loss: 0.4846\n",
      "Epoch 96/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.4128 - val_loss: 0.4841\n",
      "Epoch 97/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.4118 - val_loss: 0.4837\n",
      "Epoch 98/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.4108 - val_loss: 0.4832\n",
      "Epoch 99/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.4098 - val_loss: 0.4828\n",
      "Epoch 100/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4088 - val_loss: 0.4824\n",
      "Predicting...\n",
      "(Took 63.991 sec)\n",
      "Train on 6091 samples, validate on 1522 samples\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6091/6091 [==============================] - 1s 146us/sample - loss: 0.6856 - val_loss: 0.6786\n",
      "Epoch 2/100\n",
      "6091/6091 [==============================] - 1s 99us/sample - loss: 0.6732 - val_loss: 0.6690\n",
      "Epoch 3/100\n",
      "6091/6091 [==============================] - 1s 104us/sample - loss: 0.6640 - val_loss: 0.6614\n",
      "Epoch 4/100\n",
      "6091/6091 [==============================] - 1s 104us/sample - loss: 0.6561 - val_loss: 0.6547\n",
      "Epoch 5/100\n",
      "6091/6091 [==============================] - 1s 101us/sample - loss: 0.6488 - val_loss: 0.6485\n",
      "Epoch 6/100\n",
      "6091/6091 [==============================] - 1s 98us/sample - loss: 0.6419 - val_loss: 0.6425\n",
      "Epoch 7/100\n",
      "6091/6091 [==============================] - 1s 99us/sample - loss: 0.6352 - val_loss: 0.6368\n",
      "Epoch 8/100\n",
      "6091/6091 [==============================] - 1s 99us/sample - loss: 0.6288 - val_loss: 0.6313\n",
      "Epoch 9/100\n",
      "6091/6091 [==============================] - 1s 99us/sample - loss: 0.6226 - val_loss: 0.6261\n",
      "Epoch 10/100\n",
      "6091/6091 [==============================] - 1s 99us/sample - loss: 0.6166 - val_loss: 0.6210\n",
      "Epoch 11/100\n",
      "6091/6091 [==============================] - 1s 98us/sample - loss: 0.6109 - val_loss: 0.6161\n",
      "Epoch 12/100\n",
      "6091/6091 [==============================] - 1s 101us/sample - loss: 0.6053 - val_loss: 0.6114\n",
      "Epoch 13/100\n",
      "6091/6091 [==============================] - 1s 98us/sample - loss: 0.5999 - val_loss: 0.6069\n",
      "Epoch 14/100\n",
      "6091/6091 [==============================] - 1s 99us/sample - loss: 0.5947 - val_loss: 0.6026\n",
      "Epoch 15/100\n",
      "6091/6091 [==============================] - 1s 100us/sample - loss: 0.5897 - val_loss: 0.5984\n",
      "Epoch 16/100\n",
      "6091/6091 [==============================] - 1s 100us/sample - loss: 0.5849 - val_loss: 0.5943\n",
      "Epoch 17/100\n",
      "6091/6091 [==============================] - 1s 103us/sample - loss: 0.5801 - val_loss: 0.5904\n",
      "Epoch 18/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.5756 - val_loss: 0.5866\n",
      "Epoch 19/100\n",
      "6091/6091 [==============================] - 1s 106us/sample - loss: 0.5711 - val_loss: 0.5830\n",
      "Epoch 20/100\n",
      "6091/6091 [==============================] - 1s 99us/sample - loss: 0.5669 - val_loss: 0.5795\n",
      "Epoch 21/100\n",
      "6091/6091 [==============================] - 1s 103us/sample - loss: 0.5628 - val_loss: 0.5761\n",
      "Epoch 22/100\n",
      "6091/6091 [==============================] - 1s 100us/sample - loss: 0.5587 - val_loss: 0.5729\n",
      "Epoch 23/100\n",
      "6091/6091 [==============================] - 1s 104us/sample - loss: 0.5548 - val_loss: 0.5697\n",
      "Epoch 24/100\n",
      "6091/6091 [==============================] - 1s 103us/sample - loss: 0.5510 - val_loss: 0.5667\n",
      "Epoch 25/100\n",
      "6091/6091 [==============================] - 1s 100us/sample - loss: 0.5474 - val_loss: 0.5637\n",
      "Epoch 26/100\n",
      "6091/6091 [==============================] - 1s 100us/sample - loss: 0.5438 - val_loss: 0.5609\n",
      "Epoch 27/100\n",
      "6091/6091 [==============================] - 1s 100us/sample - loss: 0.5404 - val_loss: 0.5581\n",
      "Epoch 28/100\n",
      "6091/6091 [==============================] - 1s 100us/sample - loss: 0.5370 - val_loss: 0.5554\n",
      "Epoch 29/100\n",
      "6091/6091 [==============================] - 1s 102us/sample - loss: 0.5337 - val_loss: 0.5528\n",
      "Epoch 30/100\n",
      "6091/6091 [==============================] - 1s 99us/sample - loss: 0.5305 - val_loss: 0.5503\n",
      "Epoch 31/100\n",
      "6091/6091 [==============================] - 1s 101us/sample - loss: 0.5274 - val_loss: 0.5479\n",
      "Epoch 32/100\n",
      "6091/6091 [==============================] - 1s 99us/sample - loss: 0.5244 - val_loss: 0.5455\n",
      "Epoch 33/100\n",
      "6091/6091 [==============================] - 1s 102us/sample - loss: 0.5214 - val_loss: 0.5433\n",
      "Epoch 34/100\n",
      "6091/6091 [==============================] - 1s 100us/sample - loss: 0.5186 - val_loss: 0.5411\n",
      "Epoch 35/100\n",
      "6091/6091 [==============================] - 1s 100us/sample - loss: 0.5158 - val_loss: 0.5389\n",
      "Epoch 36/100\n",
      "6091/6091 [==============================] - 1s 101us/sample - loss: 0.5130 - val_loss: 0.5368\n",
      "Epoch 37/100\n",
      "6091/6091 [==============================] - 1s 100us/sample - loss: 0.5103 - val_loss: 0.5348\n",
      "Epoch 38/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.5078 - val_loss: 0.5328\n",
      "Epoch 39/100\n",
      "6091/6091 [==============================] - 1s 100us/sample - loss: 0.5052 - val_loss: 0.5309\n",
      "Epoch 40/100\n",
      "6091/6091 [==============================] - 1s 105us/sample - loss: 0.5027 - val_loss: 0.5291\n",
      "Epoch 41/100\n",
      "6091/6091 [==============================] - 1s 100us/sample - loss: 0.5003 - val_loss: 0.5273\n",
      "Epoch 42/100\n",
      "6091/6091 [==============================] - 1s 103us/sample - loss: 0.4979 - val_loss: 0.5255\n",
      "Epoch 43/100\n",
      "6091/6091 [==============================] - 1s 103us/sample - loss: 0.4956 - val_loss: 0.5237\n",
      "Epoch 44/100\n",
      "6091/6091 [==============================] - 1s 104us/sample - loss: 0.4933 - val_loss: 0.5221\n",
      "Epoch 45/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.4911 - val_loss: 0.5204\n",
      "Epoch 46/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4890 - val_loss: 0.5189\n",
      "Epoch 47/100\n",
      "6091/6091 [==============================] - 1s 117us/sample - loss: 0.4868 - val_loss: 0.5174\n",
      "Epoch 48/100\n",
      "6091/6091 [==============================] - 1s 110us/sample - loss: 0.4847 - val_loss: 0.5158\n",
      "Epoch 49/100\n",
      "6091/6091 [==============================] - 1s 117us/sample - loss: 0.4827 - val_loss: 0.5144\n",
      "Epoch 50/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4807 - val_loss: 0.5129\n",
      "Epoch 51/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.4787 - val_loss: 0.5115\n",
      "Epoch 52/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.4768 - val_loss: 0.5102\n",
      "Epoch 53/100\n",
      "6091/6091 [==============================] - 1s 110us/sample - loss: 0.4749 - val_loss: 0.5088\n",
      "Epoch 54/100\n",
      "6091/6091 [==============================] - 1s 125us/sample - loss: 0.4730 - val_loss: 0.5076\n",
      "Epoch 55/100\n",
      "6091/6091 [==============================] - 1s 125us/sample - loss: 0.4712 - val_loss: 0.5063\n",
      "Epoch 56/100\n",
      "6091/6091 [==============================] - 1s 125us/sample - loss: 0.4694 - val_loss: 0.5050\n",
      "Epoch 57/100\n",
      "6091/6091 [==============================] - 1s 118us/sample - loss: 0.4676 - val_loss: 0.5038\n",
      "Epoch 58/100\n",
      "6091/6091 [==============================] - 1s 115us/sample - loss: 0.4659 - val_loss: 0.5026\n",
      "Epoch 59/100\n",
      "6091/6091 [==============================] - 1s 115us/sample - loss: 0.4642 - val_loss: 0.5015\n",
      "Epoch 60/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.4625 - val_loss: 0.5003\n",
      "Epoch 61/100\n",
      "6091/6091 [==============================] - 1s 114us/sample - loss: 0.4609 - val_loss: 0.4992\n",
      "Epoch 62/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.4592 - val_loss: 0.4982\n",
      "Epoch 63/100\n",
      "6091/6091 [==============================] - 1s 106us/sample - loss: 0.4576 - val_loss: 0.4971\n",
      "Epoch 64/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.4561 - val_loss: 0.4961\n",
      "Epoch 65/100\n",
      "6091/6091 [==============================] - 1s 113us/sample - loss: 0.4545 - val_loss: 0.4950\n",
      "Epoch 66/100\n",
      "6091/6091 [==============================] - 1s 148us/sample - loss: 0.4530 - val_loss: 0.4940\n",
      "Epoch 67/100\n",
      "6091/6091 [==============================] - 1s 129us/sample - loss: 0.4515 - val_loss: 0.4931\n",
      "Epoch 68/100\n",
      "6091/6091 [==============================] - 1s 148us/sample - loss: 0.4501 - val_loss: 0.4921\n",
      "Epoch 69/100\n",
      "6091/6091 [==============================] - 1s 134us/sample - loss: 0.4486 - val_loss: 0.4912\n",
      "Epoch 70/100\n",
      "6091/6091 [==============================] - 1s 152us/sample - loss: 0.4472 - val_loss: 0.4903\n",
      "Epoch 71/100\n",
      "6091/6091 [==============================] - 1s 137us/sample - loss: 0.4458 - val_loss: 0.4894\n",
      "Epoch 72/100\n",
      "6091/6091 [==============================] - 1s 130us/sample - loss: 0.4444 - val_loss: 0.4885\n",
      "Epoch 73/100\n",
      "6091/6091 [==============================] - 1s 105us/sample - loss: 0.4430 - val_loss: 0.4877\n",
      "Epoch 74/100\n",
      "6091/6091 [==============================] - 1s 104us/sample - loss: 0.4417 - val_loss: 0.4869\n",
      "Epoch 75/100\n",
      "6091/6091 [==============================] - 1s 111us/sample - loss: 0.4403 - val_loss: 0.4861\n",
      "Epoch 76/100\n",
      "6091/6091 [==============================] - 1s 127us/sample - loss: 0.4390 - val_loss: 0.4852\n",
      "Epoch 77/100\n",
      "6091/6091 [==============================] - 1s 133us/sample - loss: 0.4377 - val_loss: 0.4844\n",
      "Epoch 78/100\n",
      "6091/6091 [==============================] - 1s 134us/sample - loss: 0.4365 - val_loss: 0.4836\n",
      "Epoch 79/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.4352 - val_loss: 0.4829\n",
      "Epoch 80/100\n",
      "6091/6091 [==============================] - 1s 142us/sample - loss: 0.4339 - val_loss: 0.4822\n",
      "Epoch 81/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.4327 - val_loss: 0.4814\n",
      "Epoch 82/100\n",
      "6091/6091 [==============================] - 1s 119us/sample - loss: 0.4315 - val_loss: 0.4807\n",
      "Epoch 83/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.4303 - val_loss: 0.4799\n",
      "Epoch 84/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.4291 - val_loss: 0.4792\n",
      "Epoch 85/100\n",
      "6091/6091 [==============================] - 1s 131us/sample - loss: 0.4280 - val_loss: 0.4786\n",
      "Epoch 86/100\n",
      "6091/6091 [==============================] - 1s 120us/sample - loss: 0.4268 - val_loss: 0.4779\n",
      "Epoch 87/100\n",
      "6091/6091 [==============================] - 1s 105us/sample - loss: 0.4257 - val_loss: 0.4772\n",
      "Epoch 88/100\n",
      "6091/6091 [==============================] - 1s 135us/sample - loss: 0.4246 - val_loss: 0.4766\n",
      "Epoch 89/100\n",
      "6091/6091 [==============================] - 1s 122us/sample - loss: 0.4234 - val_loss: 0.4760\n",
      "Epoch 90/100\n",
      "6091/6091 [==============================] - 1s 114us/sample - loss: 0.4224 - val_loss: 0.4754\n",
      "Epoch 91/100\n",
      "6091/6091 [==============================] - 1s 100us/sample - loss: 0.4213 - val_loss: 0.4748\n",
      "Epoch 92/100\n",
      "6091/6091 [==============================] - 1s 106us/sample - loss: 0.4202 - val_loss: 0.4742\n",
      "Epoch 93/100\n",
      "6091/6091 [==============================] - 1s 103us/sample - loss: 0.4191 - val_loss: 0.4736\n",
      "Epoch 94/100\n",
      "6091/6091 [==============================] - 1s 110us/sample - loss: 0.4181 - val_loss: 0.4730\n",
      "Epoch 95/100\n",
      "6091/6091 [==============================] - 1s 128us/sample - loss: 0.4171 - val_loss: 0.4725\n",
      "Epoch 96/100\n",
      "6091/6091 [==============================] - 1s 113us/sample - loss: 0.4160 - val_loss: 0.4719\n",
      "Epoch 97/100\n",
      "6091/6091 [==============================] - 1s 100us/sample - loss: 0.4150 - val_loss: 0.4714\n",
      "Epoch 98/100\n",
      "6091/6091 [==============================] - 1s 102us/sample - loss: 0.4140 - val_loss: 0.4708\n",
      "Epoch 99/100\n",
      "6091/6091 [==============================] - 1s 125us/sample - loss: 0.4130 - val_loss: 0.4703\n",
      "Epoch 100/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.4120 - val_loss: 0.4697\n",
      "Predicting...\n",
      "(Took 68.169 sec)\n",
      "Train on 6091 samples, validate on 1522 samples\n",
      "Epoch 1/100\n",
      "6091/6091 [==============================] - 1s 150us/sample - loss: 0.6855 - val_loss: 0.6805\n",
      "Epoch 2/100\n",
      "6091/6091 [==============================] - 1s 127us/sample - loss: 0.6727 - val_loss: 0.6722\n",
      "Epoch 3/100\n",
      "6091/6091 [==============================] - 1s 139us/sample - loss: 0.6631 - val_loss: 0.6653\n",
      "Epoch 4/100\n",
      "6091/6091 [==============================] - 1s 105us/sample - loss: 0.6550 - val_loss: 0.6590\n",
      "Epoch 5/100\n",
      "6091/6091 [==============================] - 1s 117us/sample - loss: 0.6476 - val_loss: 0.6530\n",
      "Epoch 6/100\n",
      "6091/6091 [==============================] - 1s 116us/sample - loss: 0.6406 - val_loss: 0.6471\n",
      "Epoch 7/100\n",
      "6091/6091 [==============================] - 1s 105us/sample - loss: 0.6338 - val_loss: 0.6415\n",
      "Epoch 8/100\n",
      "6091/6091 [==============================] - 1s 100us/sample - loss: 0.6273 - val_loss: 0.6360\n",
      "Epoch 9/100\n",
      "6091/6091 [==============================] - 1s 110us/sample - loss: 0.6210 - val_loss: 0.6307\n",
      "Epoch 10/100\n",
      "6091/6091 [==============================] - 1s 120us/sample - loss: 0.6150 - val_loss: 0.6257\n",
      "Epoch 11/100\n",
      "6091/6091 [==============================] - 1s 119us/sample - loss: 0.6092 - val_loss: 0.6209\n",
      "Epoch 12/100\n",
      "6091/6091 [==============================] - 1s 100us/sample - loss: 0.6036 - val_loss: 0.6162\n",
      "Epoch 13/100\n",
      "6091/6091 [==============================] - 1s 104us/sample - loss: 0.5981 - val_loss: 0.6117\n",
      "Epoch 14/100\n",
      "6091/6091 [==============================] - 1s 102us/sample - loss: 0.5929 - val_loss: 0.6074\n",
      "Epoch 15/100\n",
      "6091/6091 [==============================] - 1s 103us/sample - loss: 0.5878 - val_loss: 0.6032\n",
      "Epoch 16/100\n",
      "6091/6091 [==============================] - 1s 101us/sample - loss: 0.5829 - val_loss: 0.5992\n",
      "Epoch 17/100\n",
      "6091/6091 [==============================] - 1s 103us/sample - loss: 0.5782 - val_loss: 0.5954\n",
      "Epoch 18/100\n",
      "6091/6091 [==============================] - 1s 115us/sample - loss: 0.5736 - val_loss: 0.5917\n",
      "Epoch 19/100\n",
      "6091/6091 [==============================] - 1s 131us/sample - loss: 0.5691 - val_loss: 0.5881\n",
      "Epoch 20/100\n",
      "6091/6091 [==============================] - 1s 138us/sample - loss: 0.5649 - val_loss: 0.5846\n",
      "Epoch 21/100\n",
      "6091/6091 [==============================] - 1s 100us/sample - loss: 0.5607 - val_loss: 0.5813\n",
      "Epoch 22/100\n",
      "6091/6091 [==============================] - 1s 138us/sample - loss: 0.5566 - val_loss: 0.5780\n",
      "Epoch 23/100\n",
      "6091/6091 [==============================] - 1s 100us/sample - loss: 0.5527 - val_loss: 0.5749\n",
      "Epoch 24/100\n",
      "6091/6091 [==============================] - 1s 104us/sample - loss: 0.5489 - val_loss: 0.5720\n",
      "Epoch 25/100\n",
      "6091/6091 [==============================] - 1s 102us/sample - loss: 0.5452 - val_loss: 0.5691\n",
      "Epoch 26/100\n",
      "6091/6091 [==============================] - 1s 113us/sample - loss: 0.5416 - val_loss: 0.5663\n",
      "Epoch 27/100\n",
      "6091/6091 [==============================] - 1s 124us/sample - loss: 0.5381 - val_loss: 0.5636\n",
      "Epoch 28/100\n",
      "6091/6091 [==============================] - 1s 118us/sample - loss: 0.5347 - val_loss: 0.5609\n",
      "Epoch 29/100\n",
      "6091/6091 [==============================] - 1s 136us/sample - loss: 0.5314 - val_loss: 0.5585\n",
      "Epoch 30/100\n",
      "6091/6091 [==============================] - 1s 141us/sample - loss: 0.5282 - val_loss: 0.5560\n",
      "Epoch 31/100\n",
      "6091/6091 [==============================] - 1s 123us/sample - loss: 0.5250 - val_loss: 0.5536\n",
      "Epoch 32/100\n",
      "6091/6091 [==============================] - 1s 104us/sample - loss: 0.5219 - val_loss: 0.5513\n",
      "Epoch 33/100\n",
      "6091/6091 [==============================] - 1s 102us/sample - loss: 0.5190 - val_loss: 0.5491\n",
      "Epoch 34/100\n",
      "6091/6091 [==============================] - 1s 104us/sample - loss: 0.5161 - val_loss: 0.5470\n",
      "Epoch 35/100\n",
      "6091/6091 [==============================] - 1s 99us/sample - loss: 0.5133 - val_loss: 0.5449\n",
      "Epoch 36/100\n",
      "6091/6091 [==============================] - 1s 106us/sample - loss: 0.5105 - val_loss: 0.5429\n",
      "Epoch 37/100\n",
      "6091/6091 [==============================] - 1s 99us/sample - loss: 0.5078 - val_loss: 0.5410\n",
      "Epoch 38/100\n",
      "6091/6091 [==============================] - 1s 132us/sample - loss: 0.5052 - val_loss: 0.5390\n",
      "Epoch 39/100\n",
      "6091/6091 [==============================] - 1s 105us/sample - loss: 0.5026 - val_loss: 0.5371\n",
      "Epoch 40/100\n",
      "6091/6091 [==============================] - 1s 101us/sample - loss: 0.5001 - val_loss: 0.5353\n",
      "Epoch 41/100\n",
      "6091/6091 [==============================] - 1s 127us/sample - loss: 0.4977 - val_loss: 0.5336\n",
      "Epoch 42/100\n",
      "6091/6091 [==============================] - 1s 105us/sample - loss: 0.4953 - val_loss: 0.5319\n",
      "Epoch 43/100\n",
      "6091/6091 [==============================] - 1s 99us/sample - loss: 0.4929 - val_loss: 0.5303\n",
      "Epoch 44/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.4906 - val_loss: 0.5287\n",
      "Epoch 45/100\n",
      "6091/6091 [==============================] - 1s 101us/sample - loss: 0.4884 - val_loss: 0.5271\n",
      "Epoch 46/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.4862 - val_loss: 0.5256\n",
      "Epoch 47/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4841 - val_loss: 0.5241\n",
      "Epoch 48/100\n",
      "6091/6091 [==============================] - 1s 142us/sample - loss: 0.4819 - val_loss: 0.5227\n",
      "Epoch 49/100\n",
      "6091/6091 [==============================] - 1s 136us/sample - loss: 0.4799 - val_loss: 0.5213\n",
      "Epoch 50/100\n",
      "6091/6091 [==============================] - 1s 160us/sample - loss: 0.4779 - val_loss: 0.5200\n",
      "Epoch 51/100\n",
      "6091/6091 [==============================] - 1s 155us/sample - loss: 0.4759 - val_loss: 0.5187\n",
      "Epoch 52/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6091/6091 [==============================] - 1s 143us/sample - loss: 0.4740 - val_loss: 0.5174\n",
      "Epoch 53/100\n",
      "6091/6091 [==============================] - 1s 146us/sample - loss: 0.4720 - val_loss: 0.5161\n",
      "Epoch 54/100\n",
      "6091/6091 [==============================] - 1s 141us/sample - loss: 0.4701 - val_loss: 0.5149\n",
      "Epoch 55/100\n",
      "6091/6091 [==============================] - 1s 118us/sample - loss: 0.4683 - val_loss: 0.5137\n",
      "Epoch 56/100\n",
      "6091/6091 [==============================] - 1s 118us/sample - loss: 0.4665 - val_loss: 0.5126\n",
      "Epoch 57/100\n",
      "6091/6091 [==============================] - 1s 116us/sample - loss: 0.4648 - val_loss: 0.5115\n",
      "Epoch 58/100\n",
      "6091/6091 [==============================] - 1s 121us/sample - loss: 0.4630 - val_loss: 0.5104\n",
      "Epoch 59/100\n",
      "6091/6091 [==============================] - 1s 120us/sample - loss: 0.4613 - val_loss: 0.5093\n",
      "Epoch 60/100\n",
      "6091/6091 [==============================] - 1s 122us/sample - loss: 0.4596 - val_loss: 0.5083\n",
      "Epoch 61/100\n",
      "6091/6091 [==============================] - 1s 124us/sample - loss: 0.4580 - val_loss: 0.5072\n",
      "Epoch 62/100\n",
      "6091/6091 [==============================] - 1s 114us/sample - loss: 0.4563 - val_loss: 0.5062\n",
      "Epoch 63/100\n",
      "6091/6091 [==============================] - 1s 115us/sample - loss: 0.4547 - val_loss: 0.5053\n",
      "Epoch 64/100\n",
      "6091/6091 [==============================] - 1s 115us/sample - loss: 0.4531 - val_loss: 0.5043\n",
      "Epoch 65/100\n",
      "6091/6091 [==============================] - 1s 116us/sample - loss: 0.4516 - val_loss: 0.5034\n",
      "Epoch 66/100\n",
      "6091/6091 [==============================] - 1s 118us/sample - loss: 0.4501 - val_loss: 0.5024\n",
      "Epoch 67/100\n",
      "6091/6091 [==============================] - 1s 123us/sample - loss: 0.4485 - val_loss: 0.5015\n",
      "Epoch 68/100\n",
      "6091/6091 [==============================] - 1s 137us/sample - loss: 0.4471 - val_loss: 0.5007\n",
      "Epoch 69/100\n",
      "6091/6091 [==============================] - 1s 132us/sample - loss: 0.4456 - val_loss: 0.4998\n",
      "Epoch 70/100\n",
      "6091/6091 [==============================] - 1s 129us/sample - loss: 0.4442 - val_loss: 0.4990\n",
      "Epoch 71/100\n",
      "6091/6091 [==============================] - 1s 114us/sample - loss: 0.4428 - val_loss: 0.4982\n",
      "Epoch 72/100\n",
      "6091/6091 [==============================] - 1s 121us/sample - loss: 0.4414 - val_loss: 0.4974\n",
      "Epoch 73/100\n",
      "6091/6091 [==============================] - 1s 122us/sample - loss: 0.4400 - val_loss: 0.4966\n",
      "Epoch 74/100\n",
      "6091/6091 [==============================] - 1s 128us/sample - loss: 0.4386 - val_loss: 0.4959\n",
      "Epoch 75/100\n",
      "6091/6091 [==============================] - 1s 128us/sample - loss: 0.4373 - val_loss: 0.4951\n",
      "Epoch 76/100\n",
      "6091/6091 [==============================] - 1s 124us/sample - loss: 0.4360 - val_loss: 0.4944\n",
      "Epoch 77/100\n",
      "6091/6091 [==============================] - 1s 126us/sample - loss: 0.4347 - val_loss: 0.4937\n",
      "Epoch 78/100\n",
      "6091/6091 [==============================] - 1s 121us/sample - loss: 0.4334 - val_loss: 0.4930\n",
      "Epoch 79/100\n",
      "6091/6091 [==============================] - 1s 126us/sample - loss: 0.4321 - val_loss: 0.4923\n",
      "Epoch 80/100\n",
      "6091/6091 [==============================] - 1s 119us/sample - loss: 0.4309 - val_loss: 0.4917\n",
      "Epoch 81/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.4296 - val_loss: 0.4910\n",
      "Epoch 82/100\n",
      "6091/6091 [==============================] - 1s 116us/sample - loss: 0.4284 - val_loss: 0.4904\n",
      "Epoch 83/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.4272 - val_loss: 0.4898\n",
      "Epoch 84/100\n",
      "6091/6091 [==============================] - 1s 104us/sample - loss: 0.4260 - val_loss: 0.4892\n",
      "Epoch 85/100\n",
      "6091/6091 [==============================] - 1s 106us/sample - loss: 0.4248 - val_loss: 0.4886\n",
      "Epoch 86/100\n",
      "6091/6091 [==============================] - 1s 106us/sample - loss: 0.4237 - val_loss: 0.4880\n",
      "Epoch 87/100\n",
      "6091/6091 [==============================] - 1s 117us/sample - loss: 0.4225 - val_loss: 0.4875\n",
      "Epoch 88/100\n",
      "6091/6091 [==============================] - 1s 105us/sample - loss: 0.4214 - val_loss: 0.4869\n",
      "Epoch 89/100\n",
      "6091/6091 [==============================] - 1s 111us/sample - loss: 0.4203 - val_loss: 0.4864\n",
      "Epoch 90/100\n",
      "6091/6091 [==============================] - 1s 114us/sample - loss: 0.4192 - val_loss: 0.4858\n",
      "Epoch 91/100\n",
      "6091/6091 [==============================] - 1s 106us/sample - loss: 0.4181 - val_loss: 0.4853\n",
      "Epoch 92/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4170 - val_loss: 0.4848\n",
      "Epoch 93/100\n",
      "6091/6091 [==============================] - 1s 105us/sample - loss: 0.4159 - val_loss: 0.4843\n",
      "Epoch 94/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.4149 - val_loss: 0.4838\n",
      "Epoch 95/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4138 - val_loss: 0.4833\n",
      "Epoch 96/100\n",
      "6091/6091 [==============================] - 1s 105us/sample - loss: 0.4128 - val_loss: 0.4829\n",
      "Epoch 97/100\n",
      "6091/6091 [==============================] - 1s 105us/sample - loss: 0.4118 - val_loss: 0.4824\n",
      "Epoch 98/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.4108 - val_loss: 0.4820\n",
      "Epoch 99/100\n",
      "6091/6091 [==============================] - 1s 104us/sample - loss: 0.4098 - val_loss: 0.4815\n",
      "Epoch 100/100\n",
      "6091/6091 [==============================] - 1s 105us/sample - loss: 0.4088 - val_loss: 0.4811\n",
      "Predicting...\n",
      "(Took 71.664 sec)\n",
      "Combined confusion matrix:\n",
      "[[3853.  489.]\n",
      " [1123. 2148.]]\n",
      "(Overall, took 341.345 sec)\n",
      "Accuracy: 78.83% +/- 0.95%\n",
      "Precision for positive class: 77.43% +/- 1.73%\n",
      "Precision for negative class: 81.47% +/- 1.82%\n",
      "Recall for positive class: 88.74% +/- 1.25%\n",
      "Recall for negative class: 65.69% +/- 1.12%\n",
      "F for positive class: 82.69% +/- 1.10%\n",
      "F for negative class: 72.72% +/- 0.47%\n",
      "Mean F score: 77.70% +/- 0.77%\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/100\n",
      "6090/6090 [==============================] - 1s 146us/sample - loss: 0.6833 - val_loss: 0.6727\n",
      "Epoch 2/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.6650 - val_loss: 0.6578\n",
      "Epoch 3/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.6502 - val_loss: 0.6456\n",
      "Epoch 4/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.6369 - val_loss: 0.6346\n",
      "Epoch 5/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.6245 - val_loss: 0.6245\n",
      "Epoch 6/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.6129 - val_loss: 0.6150\n",
      "Epoch 7/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.6019 - val_loss: 0.6062\n",
      "Epoch 8/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.5915 - val_loss: 0.5979\n",
      "Epoch 9/100\n",
      "6090/6090 [==============================] - 1s 116us/sample - loss: 0.5816 - val_loss: 0.5899\n",
      "Epoch 10/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.5722 - val_loss: 0.5826\n",
      "Epoch 11/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.5633 - val_loss: 0.5756\n",
      "Epoch 12/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.5548 - val_loss: 0.5690\n",
      "Epoch 13/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.5468 - val_loss: 0.5628\n",
      "Epoch 14/100\n",
      "6090/6090 [==============================] - 1s 132us/sample - loss: 0.5390 - val_loss: 0.5570\n",
      "Epoch 15/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.5317 - val_loss: 0.5514\n",
      "Epoch 16/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.5246 - val_loss: 0.5461\n",
      "Epoch 17/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.5180 - val_loss: 0.5412\n",
      "Epoch 18/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.5115 - val_loss: 0.5365\n",
      "Epoch 19/100\n",
      "6090/6090 [==============================] - 1s 130us/sample - loss: 0.5054 - val_loss: 0.5320\n",
      "Epoch 20/100\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.4995 - val_loss: 0.5278\n",
      "Epoch 21/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.4939 - val_loss: 0.5238\n",
      "Epoch 22/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4885 - val_loss: 0.5200\n",
      "Epoch 23/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.4833 - val_loss: 0.5164\n",
      "Epoch 24/100\n",
      "6090/6090 [==============================] - 1s 101us/sample - loss: 0.4783 - val_loss: 0.5130\n",
      "Epoch 25/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.4735 - val_loss: 0.5098\n",
      "Epoch 26/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.4689 - val_loss: 0.5068\n",
      "Epoch 27/100\n",
      "6090/6090 [==============================] - 1s 99us/sample - loss: 0.4645 - val_loss: 0.5039\n",
      "Epoch 28/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.4602 - val_loss: 0.5012\n",
      "Epoch 29/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4561 - val_loss: 0.4986\n",
      "Epoch 30/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.4521 - val_loss: 0.4961\n",
      "Epoch 31/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.4482 - val_loss: 0.4937\n",
      "Epoch 32/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4445 - val_loss: 0.4915\n",
      "Epoch 33/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.4409 - val_loss: 0.4894\n",
      "Epoch 34/100\n",
      "6090/6090 [==============================] - 1s 101us/sample - loss: 0.4375 - val_loss: 0.4873\n",
      "Epoch 35/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.4341 - val_loss: 0.4855\n",
      "Epoch 36/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.4308 - val_loss: 0.4837\n",
      "Epoch 37/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4277 - val_loss: 0.4819\n",
      "Epoch 38/100\n",
      "6090/6090 [==============================] - 1s 101us/sample - loss: 0.4246 - val_loss: 0.4804\n",
      "Epoch 39/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.4216 - val_loss: 0.4788\n",
      "Epoch 40/100\n",
      "6090/6090 [==============================] - 1s 116us/sample - loss: 0.4187 - val_loss: 0.4773\n",
      "Epoch 41/100\n",
      "6090/6090 [==============================] - 1s 124us/sample - loss: 0.4159 - val_loss: 0.4759\n",
      "Epoch 42/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.4132 - val_loss: 0.4746\n",
      "Epoch 43/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.4105 - val_loss: 0.4733\n",
      "Epoch 44/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4079 - val_loss: 0.4722\n",
      "Epoch 45/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.4054 - val_loss: 0.4711\n",
      "Epoch 46/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4030 - val_loss: 0.4700\n",
      "Epoch 47/100\n",
      "6090/6090 [==============================] - 1s 101us/sample - loss: 0.4006 - val_loss: 0.4690\n",
      "Epoch 48/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.3983 - val_loss: 0.4680\n",
      "Epoch 49/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.3960 - val_loss: 0.4671\n",
      "Epoch 50/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.3938 - val_loss: 0.4662\n",
      "Epoch 51/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.3916 - val_loss: 0.4654\n",
      "Epoch 52/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.3895 - val_loss: 0.4647\n",
      "Epoch 53/100\n",
      "6090/6090 [==============================] - 1s 101us/sample - loss: 0.3874 - val_loss: 0.4639\n",
      "Epoch 54/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.3854 - val_loss: 0.4632\n",
      "Epoch 55/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.3834 - val_loss: 0.4626\n",
      "Epoch 56/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.3815 - val_loss: 0.4620\n",
      "Epoch 57/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.3796 - val_loss: 0.4614\n",
      "Epoch 58/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.3778 - val_loss: 0.4609\n",
      "Epoch 59/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.3760 - val_loss: 0.4604\n",
      "Epoch 60/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.3742 - val_loss: 0.4599\n",
      "Epoch 61/100\n",
      "6090/6090 [==============================] - 1s 119us/sample - loss: 0.3724 - val_loss: 0.4595\n",
      "Epoch 62/100\n",
      "6090/6090 [==============================] - 1s 144us/sample - loss: 0.3707 - val_loss: 0.4590\n",
      "Epoch 63/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.3690 - val_loss: 0.4586\n",
      "Epoch 64/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.3674 - val_loss: 0.4582\n",
      "Epoch 65/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.3658 - val_loss: 0.4579\n",
      "Epoch 66/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.3642 - val_loss: 0.4576\n",
      "Epoch 67/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.3626 - val_loss: 0.4573\n",
      "Epoch 68/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.3611 - val_loss: 0.4570\n",
      "Epoch 69/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.3596 - val_loss: 0.4568\n",
      "Epoch 70/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.3581 - val_loss: 0.4566\n",
      "Epoch 71/100\n",
      "6090/6090 [==============================] - 1s 116us/sample - loss: 0.3567 - val_loss: 0.4564\n",
      "Epoch 72/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.3553 - val_loss: 0.4562\n",
      "Epoch 73/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.3539 - val_loss: 0.4560\n",
      "Epoch 74/100\n",
      "6090/6090 [==============================] - 1s 117us/sample - loss: 0.3525 - val_loss: 0.4559\n",
      "Epoch 75/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.3512 - val_loss: 0.4557\n",
      "Epoch 76/100\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.3498 - val_loss: 0.4556\n",
      "Epoch 77/100\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.3485 - val_loss: 0.4556\n",
      "Epoch 78/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.3472 - val_loss: 0.4555\n",
      "Epoch 79/100\n",
      "6090/6090 [==============================] - 1s 123us/sample - loss: 0.3460 - val_loss: 0.4554\n",
      "Epoch 80/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.3447 - val_loss: 0.4554\n",
      "Epoch 81/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.3435 - val_loss: 0.4554\n",
      "Predicting...\n",
      "(Took 54.274 sec)\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/100\n",
      "6090/6090 [==============================] - 1s 146us/sample - loss: 0.6829 - val_loss: 0.6710\n",
      "Epoch 2/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.6649 - val_loss: 0.6557\n",
      "Epoch 3/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.6503 - val_loss: 0.6433\n",
      "Epoch 4/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.6371 - val_loss: 0.6323\n",
      "Epoch 5/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.6248 - val_loss: 0.6220\n",
      "Epoch 6/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.6132 - val_loss: 0.6126\n",
      "Epoch 7/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.6023 - val_loss: 0.6038\n",
      "Epoch 8/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.5919 - val_loss: 0.5956\n",
      "Epoch 9/100\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.5820 - val_loss: 0.5877\n",
      "Epoch 10/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.5726 - val_loss: 0.5805\n",
      "Epoch 11/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.5638 - val_loss: 0.5735\n",
      "Epoch 12/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.5553 - val_loss: 0.5670\n",
      "Epoch 13/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.5472 - val_loss: 0.5608\n",
      "Epoch 14/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.5395 - val_loss: 0.5551\n",
      "Epoch 15/100\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.5322 - val_loss: 0.5496\n",
      "Epoch 16/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.5252 - val_loss: 0.5443\n",
      "Epoch 17/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6090/6090 [==============================] - 1s 122us/sample - loss: 0.5185 - val_loss: 0.5396\n",
      "Epoch 18/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.5121 - val_loss: 0.5350\n",
      "Epoch 19/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.5059 - val_loss: 0.5306\n",
      "Epoch 20/100\n",
      "6090/6090 [==============================] - 1s 134us/sample - loss: 0.5000 - val_loss: 0.5265\n",
      "Epoch 21/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.4944 - val_loss: 0.5226\n",
      "Epoch 22/100\n",
      "6090/6090 [==============================] - 1s 130us/sample - loss: 0.4890 - val_loss: 0.5188\n",
      "Epoch 23/100\n",
      "6090/6090 [==============================] - 1s 118us/sample - loss: 0.4838 - val_loss: 0.5154\n",
      "Epoch 24/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.4788 - val_loss: 0.5121\n",
      "Epoch 25/100\n",
      "6090/6090 [==============================] - 1s 123us/sample - loss: 0.4740 - val_loss: 0.5089\n",
      "Epoch 26/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.4695 - val_loss: 0.5059\n",
      "Epoch 27/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.4650 - val_loss: 0.5031\n",
      "Epoch 28/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.4607 - val_loss: 0.5004\n",
      "Epoch 29/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.4566 - val_loss: 0.4978\n",
      "Epoch 30/100\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.4526 - val_loss: 0.4954\n",
      "Epoch 31/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.4487 - val_loss: 0.4931\n",
      "Epoch 32/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4450 - val_loss: 0.4909\n",
      "Epoch 33/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.4414 - val_loss: 0.4889\n",
      "Epoch 34/100\n",
      "6090/6090 [==============================] - 1s 119us/sample - loss: 0.4379 - val_loss: 0.4870\n",
      "Epoch 35/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.4345 - val_loss: 0.4851\n",
      "Epoch 36/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.4313 - val_loss: 0.4834\n",
      "Epoch 37/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.4281 - val_loss: 0.4817\n",
      "Epoch 38/100\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.4250 - val_loss: 0.4802\n",
      "Epoch 39/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.4220 - val_loss: 0.4787\n",
      "Epoch 40/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.4191 - val_loss: 0.4772\n",
      "Epoch 41/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4163 - val_loss: 0.4758\n",
      "Epoch 42/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.4136 - val_loss: 0.4745\n",
      "Epoch 43/100\n",
      "6090/6090 [==============================] - 1s 116us/sample - loss: 0.4109 - val_loss: 0.4733\n",
      "Epoch 44/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4083 - val_loss: 0.4721\n",
      "Epoch 45/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.4058 - val_loss: 0.4710\n",
      "Epoch 46/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4033 - val_loss: 0.4700\n",
      "Epoch 47/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4009 - val_loss: 0.4690\n",
      "Epoch 48/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.3986 - val_loss: 0.4680\n",
      "Epoch 49/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.3963 - val_loss: 0.4671\n",
      "Epoch 50/100\n",
      "6090/6090 [==============================] - 1s 101us/sample - loss: 0.3941 - val_loss: 0.4662\n",
      "Epoch 51/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.3919 - val_loss: 0.4655\n",
      "Epoch 52/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.3898 - val_loss: 0.4648\n",
      "Epoch 53/100\n",
      "6090/6090 [==============================] - 1s 119us/sample - loss: 0.3877 - val_loss: 0.4641\n",
      "Epoch 54/100\n",
      "6090/6090 [==============================] - 1s 116us/sample - loss: 0.3857 - val_loss: 0.4634\n",
      "Epoch 55/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.3837 - val_loss: 0.4628\n",
      "Epoch 56/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.3818 - val_loss: 0.4621\n",
      "Epoch 57/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.3799 - val_loss: 0.4616\n",
      "Epoch 58/100\n",
      "6090/6090 [==============================] - 1s 129us/sample - loss: 0.3780 - val_loss: 0.4611\n",
      "Epoch 59/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.3762 - val_loss: 0.4606\n",
      "Epoch 60/100\n",
      "6090/6090 [==============================] - 1s 125us/sample - loss: 0.3744 - val_loss: 0.4602\n",
      "Epoch 61/100\n",
      "6090/6090 [==============================] - 1s 123us/sample - loss: 0.3727 - val_loss: 0.4597\n",
      "Epoch 62/100\n",
      "6090/6090 [==============================] - 1s 134us/sample - loss: 0.3710 - val_loss: 0.4593\n",
      "Epoch 63/100\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.3693 - val_loss: 0.4589\n",
      "Epoch 64/100\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.3676 - val_loss: 0.4585\n",
      "Epoch 65/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.3660 - val_loss: 0.4583\n",
      "Epoch 66/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.3644 - val_loss: 0.4579\n",
      "Epoch 67/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.3629 - val_loss: 0.4577\n",
      "Epoch 68/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.3614 - val_loss: 0.4575\n",
      "Epoch 69/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.3599 - val_loss: 0.4573\n",
      "Epoch 70/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.3584 - val_loss: 0.4571\n",
      "Epoch 71/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.3569 - val_loss: 0.4569\n",
      "Epoch 72/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.3555 - val_loss: 0.4568\n",
      "Epoch 73/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.3541 - val_loss: 0.4565\n",
      "Epoch 74/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.3527 - val_loss: 0.4564\n",
      "Epoch 75/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.3514 - val_loss: 0.4563\n",
      "Epoch 76/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.3501 - val_loss: 0.4562\n",
      "Epoch 77/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.3488 - val_loss: 0.4561\n",
      "Epoch 78/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.3475 - val_loss: 0.4561\n",
      "Epoch 79/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.3462 - val_loss: 0.4560\n",
      "Epoch 80/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.3450 - val_loss: 0.4560\n",
      "Epoch 81/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.3437 - val_loss: 0.4560\n",
      "Predicting...\n",
      "(Took 54.853 sec)\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/100\n",
      "6090/6090 [==============================] - 1s 144us/sample - loss: 0.6831 - val_loss: 0.6751\n",
      "Epoch 2/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.6639 - val_loss: 0.6620\n",
      "Epoch 3/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.6489 - val_loss: 0.6511\n",
      "Epoch 4/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.6357 - val_loss: 0.6408\n",
      "Epoch 5/100\n",
      "6090/6090 [==============================] - 1s 120us/sample - loss: 0.6234 - val_loss: 0.6312\n",
      "Epoch 6/100\n",
      "6090/6090 [==============================] - 1s 140us/sample - loss: 0.6118 - val_loss: 0.6222\n",
      "Epoch 7/100\n",
      "6090/6090 [==============================] - 1s 119us/sample - loss: 0.6009 - val_loss: 0.6135\n",
      "Epoch 8/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.5905 - val_loss: 0.6055\n",
      "Epoch 9/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.5807 - val_loss: 0.5978\n",
      "Epoch 10/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.5713 - val_loss: 0.5905\n",
      "Epoch 11/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.5624 - val_loss: 0.5836\n",
      "Epoch 12/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.5540 - val_loss: 0.5772\n",
      "Epoch 13/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.5459 - val_loss: 0.5711\n",
      "Epoch 14/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.5382 - val_loss: 0.5655\n",
      "Epoch 15/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.5309 - val_loss: 0.5599\n",
      "Epoch 16/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.5239 - val_loss: 0.5547\n",
      "Epoch 17/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.5171 - val_loss: 0.5499\n",
      "Epoch 18/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.5108 - val_loss: 0.5453\n",
      "Epoch 19/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.5046 - val_loss: 0.5410\n",
      "Epoch 20/100\n",
      "6090/6090 [==============================] - 1s 116us/sample - loss: 0.4988 - val_loss: 0.5368\n",
      "Epoch 21/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.4931 - val_loss: 0.5329\n",
      "Epoch 22/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.4877 - val_loss: 0.5293\n",
      "Epoch 23/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4825 - val_loss: 0.5258\n",
      "Epoch 24/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.4775 - val_loss: 0.5224\n",
      "Epoch 25/100\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.4727 - val_loss: 0.5193\n",
      "Epoch 26/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.4681 - val_loss: 0.5163\n",
      "Epoch 27/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4636 - val_loss: 0.5135\n",
      "Epoch 28/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.4593 - val_loss: 0.5107\n",
      "Epoch 29/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.4551 - val_loss: 0.5082\n",
      "Epoch 30/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.4511 - val_loss: 0.5058\n",
      "Epoch 31/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.4473 - val_loss: 0.5035\n",
      "Epoch 32/100\n",
      "6090/6090 [==============================] - 1s 125us/sample - loss: 0.4435 - val_loss: 0.5013\n",
      "Epoch 33/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.4399 - val_loss: 0.4992\n",
      "Epoch 34/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.4364 - val_loss: 0.4972\n",
      "Epoch 35/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.4330 - val_loss: 0.4953\n",
      "Epoch 36/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.4297 - val_loss: 0.4936\n",
      "Epoch 37/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.4266 - val_loss: 0.4919\n",
      "Epoch 38/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.4235 - val_loss: 0.4902\n",
      "Epoch 39/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.4205 - val_loss: 0.4888\n",
      "Epoch 40/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.4176 - val_loss: 0.4873\n",
      "Epoch 41/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.4147 - val_loss: 0.4859\n",
      "Epoch 42/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.4120 - val_loss: 0.4846\n",
      "Epoch 43/100\n",
      "6090/6090 [==============================] - 1s 101us/sample - loss: 0.4093 - val_loss: 0.4834\n",
      "Epoch 44/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.4067 - val_loss: 0.4822\n",
      "Epoch 45/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.4041 - val_loss: 0.4810\n",
      "Epoch 46/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.4016 - val_loss: 0.4800\n",
      "Epoch 47/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.3992 - val_loss: 0.4789\n",
      "Epoch 48/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.3969 - val_loss: 0.4780\n",
      "Epoch 49/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.3946 - val_loss: 0.4771\n",
      "Epoch 50/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.3923 - val_loss: 0.4763\n",
      "Epoch 51/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.3902 - val_loss: 0.4755\n",
      "Epoch 52/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.3880 - val_loss: 0.4747\n",
      "Epoch 53/100\n",
      "6090/6090 [==============================] - 1s 101us/sample - loss: 0.3859 - val_loss: 0.4739\n",
      "Epoch 54/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.3839 - val_loss: 0.4732\n",
      "Epoch 55/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.3819 - val_loss: 0.4725\n",
      "Epoch 56/100\n",
      "6090/6090 [==============================] - 1s 117us/sample - loss: 0.3799 - val_loss: 0.4719\n",
      "Epoch 57/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.3780 - val_loss: 0.4714\n",
      "Epoch 58/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.3762 - val_loss: 0.4708\n",
      "Epoch 59/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.3743 - val_loss: 0.4703\n",
      "Epoch 60/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.3725 - val_loss: 0.4698\n",
      "Epoch 61/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.3708 - val_loss: 0.4693\n",
      "Epoch 62/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.3690 - val_loss: 0.4688\n",
      "Epoch 63/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.3674 - val_loss: 0.4685\n",
      "Epoch 64/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.3657 - val_loss: 0.4682\n",
      "Epoch 65/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.3641 - val_loss: 0.4678\n",
      "Epoch 66/100\n",
      "6090/6090 [==============================] - 1s 101us/sample - loss: 0.3625 - val_loss: 0.4675\n",
      "Epoch 67/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.3609 - val_loss: 0.4672\n",
      "Epoch 68/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.3593 - val_loss: 0.4668\n",
      "Epoch 69/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.3578 - val_loss: 0.4666\n",
      "Epoch 70/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.3563 - val_loss: 0.4663\n",
      "Epoch 71/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.3549 - val_loss: 0.4660\n",
      "Epoch 72/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.3534 - val_loss: 0.4658\n",
      "Epoch 73/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.3520 - val_loss: 0.4658\n",
      "Epoch 74/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.3506 - val_loss: 0.4655\n",
      "Epoch 75/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.3492 - val_loss: 0.4654\n",
      "Epoch 76/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.3479 - val_loss: 0.4652\n",
      "Epoch 77/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.3466 - val_loss: 0.4652\n",
      "Epoch 78/100\n",
      "6090/6090 [==============================] - 1s 101us/sample - loss: 0.3453 - val_loss: 0.4651\n",
      "Epoch 79/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.3440 - val_loss: 0.4651\n",
      "Epoch 80/100\n",
      "6090/6090 [==============================] - 1s 122us/sample - loss: 0.3427 - val_loss: 0.4650\n",
      "Epoch 81/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.3415 - val_loss: 0.4649\n",
      "Epoch 82/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.3402 - val_loss: 0.4649\n",
      "Epoch 83/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.3390 - val_loss: 0.4649\n",
      "Epoch 84/100\n",
      "6090/6090 [==============================] - 1s 116us/sample - loss: 0.3378 - val_loss: 0.4649\n",
      "Epoch 85/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.3366 - val_loss: 0.4649\n",
      "Predicting...\n",
      "(Took 55.818 sec)\n",
      "Train on 6091 samples, validate on 1522 samples\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6091/6091 [==============================] - 1s 166us/sample - loss: 0.6832 - val_loss: 0.6741\n",
      "Epoch 2/100\n",
      "6091/6091 [==============================] - 1s 117us/sample - loss: 0.6648 - val_loss: 0.6600\n",
      "Epoch 3/100\n",
      "6091/6091 [==============================] - 1s 106us/sample - loss: 0.6497 - val_loss: 0.6480\n",
      "Epoch 4/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.6363 - val_loss: 0.6373\n",
      "Epoch 5/100\n",
      "6091/6091 [==============================] - 1s 101us/sample - loss: 0.6239 - val_loss: 0.6273\n",
      "Epoch 6/100\n",
      "6091/6091 [==============================] - 1s 101us/sample - loss: 0.6123 - val_loss: 0.6181\n",
      "Epoch 7/100\n",
      "6091/6091 [==============================] - 1s 104us/sample - loss: 0.6013 - val_loss: 0.6094\n",
      "Epoch 8/100\n",
      "6091/6091 [==============================] - 1s 103us/sample - loss: 0.5910 - val_loss: 0.6012\n",
      "Epoch 9/100\n",
      "6091/6091 [==============================] - 1s 101us/sample - loss: 0.5811 - val_loss: 0.5935\n",
      "Epoch 10/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.5718 - val_loss: 0.5862\n",
      "Epoch 11/100\n",
      "6091/6091 [==============================] - 1s 101us/sample - loss: 0.5629 - val_loss: 0.5793\n",
      "Epoch 12/100\n",
      "6091/6091 [==============================] - 1s 100us/sample - loss: 0.5545 - val_loss: 0.5728\n",
      "Epoch 13/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.5464 - val_loss: 0.5666\n",
      "Epoch 14/100\n",
      "6091/6091 [==============================] - 1s 114us/sample - loss: 0.5388 - val_loss: 0.5608\n",
      "Epoch 15/100\n",
      "6091/6091 [==============================] - 1s 106us/sample - loss: 0.5315 - val_loss: 0.5553\n",
      "Epoch 16/100\n",
      "6091/6091 [==============================] - 1s 101us/sample - loss: 0.5245 - val_loss: 0.5501\n",
      "Epoch 17/100\n",
      "6091/6091 [==============================] - 1s 100us/sample - loss: 0.5179 - val_loss: 0.5451\n",
      "Epoch 18/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.5115 - val_loss: 0.5404\n",
      "Epoch 19/100\n",
      "6091/6091 [==============================] - 1s 103us/sample - loss: 0.5054 - val_loss: 0.5360\n",
      "Epoch 20/100\n",
      "6091/6091 [==============================] - 1s 124us/sample - loss: 0.4997 - val_loss: 0.5318\n",
      "Epoch 21/100\n",
      "6091/6091 [==============================] - 1s 105us/sample - loss: 0.4941 - val_loss: 0.5277\n",
      "Epoch 22/100\n",
      "6091/6091 [==============================] - 1s 114us/sample - loss: 0.4887 - val_loss: 0.5239\n",
      "Epoch 23/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.4836 - val_loss: 0.5203\n",
      "Epoch 24/100\n",
      "6091/6091 [==============================] - 1s 118us/sample - loss: 0.4786 - val_loss: 0.5169\n",
      "Epoch 25/100\n",
      "6091/6091 [==============================] - 1s 110us/sample - loss: 0.4739 - val_loss: 0.5136\n",
      "Epoch 26/100\n",
      "6091/6091 [==============================] - 1s 117us/sample - loss: 0.4693 - val_loss: 0.5104\n",
      "Epoch 27/100\n",
      "6091/6091 [==============================] - 1s 120us/sample - loss: 0.4649 - val_loss: 0.5075\n",
      "Epoch 28/100\n",
      "6091/6091 [==============================] - 1s 134us/sample - loss: 0.4607 - val_loss: 0.5047\n",
      "Epoch 29/100\n",
      "6091/6091 [==============================] - 1s 129us/sample - loss: 0.4566 - val_loss: 0.5019\n",
      "Epoch 30/100\n",
      "6091/6091 [==============================] - 1s 115us/sample - loss: 0.4527 - val_loss: 0.4993\n",
      "Epoch 31/100\n",
      "6091/6091 [==============================] - 1s 115us/sample - loss: 0.4489 - val_loss: 0.4969\n",
      "Epoch 32/100\n",
      "6091/6091 [==============================] - 1s 121us/sample - loss: 0.4452 - val_loss: 0.4946\n",
      "Epoch 33/100\n",
      "6091/6091 [==============================] - 1s 119us/sample - loss: 0.4416 - val_loss: 0.4923\n",
      "Epoch 34/100\n",
      "6091/6091 [==============================] - 1s 102us/sample - loss: 0.4382 - val_loss: 0.4902\n",
      "Epoch 35/100\n",
      "6091/6091 [==============================] - 1s 102us/sample - loss: 0.4349 - val_loss: 0.4882\n",
      "Epoch 36/100\n",
      "6091/6091 [==============================] - 1s 127us/sample - loss: 0.4317 - val_loss: 0.4863\n",
      "Epoch 37/100\n",
      "6091/6091 [==============================] - 1s 124us/sample - loss: 0.4285 - val_loss: 0.4844\n",
      "Epoch 38/100\n",
      "6091/6091 [==============================] - 1s 103us/sample - loss: 0.4255 - val_loss: 0.4826\n",
      "Epoch 39/100\n",
      "6091/6091 [==============================] - 1s 122us/sample - loss: 0.4225 - val_loss: 0.4809\n",
      "Epoch 40/100\n",
      "6091/6091 [==============================] - 1s 123us/sample - loss: 0.4197 - val_loss: 0.4792\n",
      "Epoch 41/100\n",
      "6091/6091 [==============================] - 1s 104us/sample - loss: 0.4169 - val_loss: 0.4777\n",
      "Epoch 42/100\n",
      "6091/6091 [==============================] - 1s 119us/sample - loss: 0.4142 - val_loss: 0.4762\n",
      "Epoch 43/100\n",
      "6091/6091 [==============================] - 1s 106us/sample - loss: 0.4116 - val_loss: 0.4747\n",
      "Epoch 44/100\n",
      "6091/6091 [==============================] - 1s 120us/sample - loss: 0.4090 - val_loss: 0.4734\n",
      "Epoch 45/100\n",
      "6091/6091 [==============================] - 1s 104us/sample - loss: 0.4065 - val_loss: 0.4721\n",
      "Epoch 46/100\n",
      "6091/6091 [==============================] - 1s 114us/sample - loss: 0.4041 - val_loss: 0.4708\n",
      "Epoch 47/100\n",
      "6091/6091 [==============================] - 1s 117us/sample - loss: 0.4018 - val_loss: 0.4696\n",
      "Epoch 48/100\n",
      "6091/6091 [==============================] - 1s 113us/sample - loss: 0.3995 - val_loss: 0.4685\n",
      "Epoch 49/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.3972 - val_loss: 0.4674\n",
      "Epoch 50/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.3950 - val_loss: 0.4664\n",
      "Epoch 51/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.3929 - val_loss: 0.4653\n",
      "Epoch 52/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.3908 - val_loss: 0.4644\n",
      "Epoch 53/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.3887 - val_loss: 0.4635\n",
      "Epoch 54/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.3867 - val_loss: 0.4627\n",
      "Epoch 55/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.3848 - val_loss: 0.4618\n",
      "Epoch 56/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.3828 - val_loss: 0.4610\n",
      "Epoch 57/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.3810 - val_loss: 0.4603\n",
      "Epoch 58/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.3791 - val_loss: 0.4596\n",
      "Epoch 59/100\n",
      "6091/6091 [==============================] - 1s 110us/sample - loss: 0.3773 - val_loss: 0.4588\n",
      "Epoch 60/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.3756 - val_loss: 0.4582\n",
      "Epoch 61/100\n",
      "6091/6091 [==============================] - 1s 119us/sample - loss: 0.3739 - val_loss: 0.4575\n",
      "Epoch 62/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.3722 - val_loss: 0.4569\n",
      "Epoch 63/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.3705 - val_loss: 0.4563\n",
      "Epoch 64/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.3689 - val_loss: 0.4558\n",
      "Epoch 65/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.3673 - val_loss: 0.4552\n",
      "Epoch 66/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.3657 - val_loss: 0.4547\n",
      "Epoch 67/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.3642 - val_loss: 0.4543\n",
      "Epoch 68/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.3627 - val_loss: 0.4538\n",
      "Epoch 69/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.3612 - val_loss: 0.4535\n",
      "Epoch 70/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.3597 - val_loss: 0.4529\n",
      "Epoch 71/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.3583 - val_loss: 0.4526\n",
      "Epoch 72/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.3569 - val_loss: 0.4522\n",
      "Epoch 73/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.3555 - val_loss: 0.4519\n",
      "Epoch 74/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.3542 - val_loss: 0.4515\n",
      "Epoch 75/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.3528 - val_loss: 0.4512\n",
      "Epoch 76/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.3515 - val_loss: 0.4509\n",
      "Epoch 77/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6091/6091 [==============================] - 1s 102us/sample - loss: 0.3502 - val_loss: 0.4506\n",
      "Epoch 78/100\n",
      "6091/6091 [==============================] - 1s 102us/sample - loss: 0.3489 - val_loss: 0.4504\n",
      "Epoch 79/100\n",
      "6091/6091 [==============================] - 1s 99us/sample - loss: 0.3477 - val_loss: 0.4502\n",
      "Epoch 80/100\n",
      "6091/6091 [==============================] - 1s 103us/sample - loss: 0.3464 - val_loss: 0.4500\n",
      "Epoch 81/100\n",
      "6091/6091 [==============================] - 1s 101us/sample - loss: 0.3452 - val_loss: 0.4498\n",
      "Epoch 82/100\n",
      "6091/6091 [==============================] - 1s 106us/sample - loss: 0.3440 - val_loss: 0.4496\n",
      "Epoch 83/100\n",
      "6091/6091 [==============================] - 1s 100us/sample - loss: 0.3428 - val_loss: 0.4495\n",
      "Epoch 84/100\n",
      "6091/6091 [==============================] - 1s 102us/sample - loss: 0.3416 - val_loss: 0.4493\n",
      "Epoch 85/100\n",
      "6091/6091 [==============================] - 1s 101us/sample - loss: 0.3405 - val_loss: 0.4491\n",
      "Epoch 86/100\n",
      "6091/6091 [==============================] - 1s 111us/sample - loss: 0.3394 - val_loss: 0.4490\n",
      "Epoch 87/100\n",
      "6091/6091 [==============================] - 1s 101us/sample - loss: 0.3382 - val_loss: 0.4489\n",
      "Epoch 88/100\n",
      "6091/6091 [==============================] - 1s 101us/sample - loss: 0.3371 - val_loss: 0.4488\n",
      "Epoch 89/100\n",
      "6091/6091 [==============================] - 1s 100us/sample - loss: 0.3361 - val_loss: 0.4488\n",
      "Epoch 90/100\n",
      "6091/6091 [==============================] - 1s 100us/sample - loss: 0.3350 - val_loss: 0.4487\n",
      "Epoch 91/100\n",
      "6091/6091 [==============================] - 1s 103us/sample - loss: 0.3339 - val_loss: 0.4486\n",
      "Epoch 92/100\n",
      "6091/6091 [==============================] - 1s 101us/sample - loss: 0.3329 - val_loss: 0.4485\n",
      "Epoch 93/100\n",
      "6091/6091 [==============================] - 1s 100us/sample - loss: 0.3318 - val_loss: 0.4484\n",
      "Epoch 94/100\n",
      "6091/6091 [==============================] - 1s 110us/sample - loss: 0.3309 - val_loss: 0.4485\n",
      "Predicting...\n",
      "(Took 64.642 sec)\n",
      "Train on 6091 samples, validate on 1522 samples\n",
      "Epoch 1/100\n",
      "6091/6091 [==============================] - 1s 178us/sample - loss: 0.6826 - val_loss: 0.6755\n",
      "Epoch 2/100\n",
      "6091/6091 [==============================] - 1s 114us/sample - loss: 0.6636 - val_loss: 0.6623\n",
      "Epoch 3/100\n",
      "6091/6091 [==============================] - 1s 117us/sample - loss: 0.6484 - val_loss: 0.6510\n",
      "Epoch 4/100\n",
      "6091/6091 [==============================] - 1s 115us/sample - loss: 0.6350 - val_loss: 0.6407\n",
      "Epoch 5/100\n",
      "6091/6091 [==============================] - 1s 110us/sample - loss: 0.6227 - val_loss: 0.6310\n",
      "Epoch 6/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.6110 - val_loss: 0.6217\n",
      "Epoch 7/100\n",
      "6091/6091 [==============================] - 1s 102us/sample - loss: 0.6000 - val_loss: 0.6130\n",
      "Epoch 8/100\n",
      "6091/6091 [==============================] - 1s 102us/sample - loss: 0.5896 - val_loss: 0.6047\n",
      "Epoch 9/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.5797 - val_loss: 0.5970\n",
      "Epoch 10/100\n",
      "6091/6091 [==============================] - 1s 126us/sample - loss: 0.5703 - val_loss: 0.5896\n",
      "Epoch 11/100\n",
      "6091/6091 [==============================] - 1s 104us/sample - loss: 0.5614 - val_loss: 0.5828\n",
      "Epoch 12/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.5530 - val_loss: 0.5762\n",
      "Epoch 13/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.5449 - val_loss: 0.5701\n",
      "Epoch 14/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.5372 - val_loss: 0.5644\n",
      "Epoch 15/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.5298 - val_loss: 0.5589\n",
      "Epoch 16/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.5228 - val_loss: 0.5536\n",
      "Epoch 17/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.5161 - val_loss: 0.5487\n",
      "Epoch 18/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.5097 - val_loss: 0.5441\n",
      "Epoch 19/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.5036 - val_loss: 0.5398\n",
      "Epoch 20/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4978 - val_loss: 0.5356\n",
      "Epoch 21/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4922 - val_loss: 0.5317\n",
      "Epoch 22/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.4868 - val_loss: 0.5280\n",
      "Epoch 23/100\n",
      "6091/6091 [==============================] - ETA: 0s - loss: 0.482 - 1s 116us/sample - loss: 0.4816 - val_loss: 0.5244\n",
      "Epoch 24/100\n",
      "6091/6091 [==============================] - 1s 117us/sample - loss: 0.4766 - val_loss: 0.5211\n",
      "Epoch 25/100\n",
      "6091/6091 [==============================] - 1s 116us/sample - loss: 0.4718 - val_loss: 0.5180\n",
      "Epoch 26/100\n",
      "6091/6091 [==============================] - 1s 114us/sample - loss: 0.4672 - val_loss: 0.5151\n",
      "Epoch 27/100\n",
      "6091/6091 [==============================] - 1s 115us/sample - loss: 0.4628 - val_loss: 0.5122\n",
      "Epoch 28/100\n",
      "6091/6091 [==============================] - 1s 116us/sample - loss: 0.4585 - val_loss: 0.5096\n",
      "Epoch 29/100\n",
      "6091/6091 [==============================] - 1s 113us/sample - loss: 0.4544 - val_loss: 0.5070\n",
      "Epoch 30/100\n",
      "6091/6091 [==============================] - 1s 113us/sample - loss: 0.4504 - val_loss: 0.5046\n",
      "Epoch 31/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.4466 - val_loss: 0.5023\n",
      "Epoch 32/100\n",
      "6091/6091 [==============================] - 1s 114us/sample - loss: 0.4429 - val_loss: 0.5001\n",
      "Epoch 33/100\n",
      "6091/6091 [==============================] - 1s 115us/sample - loss: 0.4393 - val_loss: 0.4980\n",
      "Epoch 34/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.4358 - val_loss: 0.4961\n",
      "Epoch 35/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.4324 - val_loss: 0.4943\n",
      "Epoch 36/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4291 - val_loss: 0.4925\n",
      "Epoch 37/100\n",
      "6091/6091 [==============================] - 1s 111us/sample - loss: 0.4260 - val_loss: 0.4908\n",
      "Epoch 38/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4229 - val_loss: 0.4893\n",
      "Epoch 39/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.4199 - val_loss: 0.4878\n",
      "Epoch 40/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.4170 - val_loss: 0.4864\n",
      "Epoch 41/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.4142 - val_loss: 0.4850\n",
      "Epoch 42/100\n",
      "6091/6091 [==============================] - 1s 110us/sample - loss: 0.4115 - val_loss: 0.4837\n",
      "Epoch 43/100\n",
      "6091/6091 [==============================] - 1s 111us/sample - loss: 0.4088 - val_loss: 0.4825\n",
      "Epoch 44/100\n",
      "6091/6091 [==============================] - 1s 110us/sample - loss: 0.4062 - val_loss: 0.4814\n",
      "Epoch 45/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.4037 - val_loss: 0.4803\n",
      "Epoch 46/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.4012 - val_loss: 0.4793\n",
      "Epoch 47/100\n",
      "6091/6091 [==============================] - 1s 113us/sample - loss: 0.3988 - val_loss: 0.4783\n",
      "Epoch 48/100\n",
      "6091/6091 [==============================] - 1s 111us/sample - loss: 0.3965 - val_loss: 0.4774\n",
      "Epoch 49/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.3942 - val_loss: 0.4765\n",
      "Epoch 50/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.3920 - val_loss: 0.4757\n",
      "Epoch 51/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.3898 - val_loss: 0.4750\n",
      "Epoch 52/100\n",
      "6091/6091 [==============================] - 1s 111us/sample - loss: 0.3877 - val_loss: 0.4743\n",
      "Epoch 53/100\n",
      "6091/6091 [==============================] - 1s 110us/sample - loss: 0.3856 - val_loss: 0.4736\n",
      "Epoch 54/100\n",
      "6091/6091 [==============================] - 1s 127us/sample - loss: 0.3836 - val_loss: 0.4730\n",
      "Epoch 55/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.3816 - val_loss: 0.4724\n",
      "Epoch 56/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.3797 - val_loss: 0.4718\n",
      "Epoch 57/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.3778 - val_loss: 0.4713\n",
      "Epoch 58/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6091/6091 [==============================] - 1s 104us/sample - loss: 0.3759 - val_loss: 0.4708\n",
      "Epoch 59/100\n",
      "6091/6091 [==============================] - 1s 103us/sample - loss: 0.3741 - val_loss: 0.4704\n",
      "Epoch 60/100\n",
      "6091/6091 [==============================] - 1s 101us/sample - loss: 0.3723 - val_loss: 0.4700\n",
      "Epoch 61/100\n",
      "6091/6091 [==============================] - 1s 103us/sample - loss: 0.3706 - val_loss: 0.4696\n",
      "Epoch 62/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.3689 - val_loss: 0.4692\n",
      "Epoch 63/100\n",
      "6091/6091 [==============================] - 1s 100us/sample - loss: 0.3672 - val_loss: 0.4689\n",
      "Epoch 64/100\n",
      "6091/6091 [==============================] - 1s 104us/sample - loss: 0.3655 - val_loss: 0.4686\n",
      "Epoch 65/100\n",
      "6091/6091 [==============================] - 1s 104us/sample - loss: 0.3639 - val_loss: 0.4683\n",
      "Epoch 66/100\n",
      "6091/6091 [==============================] - 1s 101us/sample - loss: 0.3623 - val_loss: 0.4680\n",
      "Epoch 67/100\n",
      "6091/6091 [==============================] - 1s 103us/sample - loss: 0.3608 - val_loss: 0.4678\n",
      "Epoch 68/100\n",
      "6091/6091 [==============================] - 1s 102us/sample - loss: 0.3592 - val_loss: 0.4676\n",
      "Epoch 69/100\n",
      "6091/6091 [==============================] - 1s 100us/sample - loss: 0.3577 - val_loss: 0.4673\n",
      "Epoch 70/100\n",
      "6091/6091 [==============================] - 1s 118us/sample - loss: 0.3563 - val_loss: 0.4672\n",
      "Epoch 71/100\n",
      "6091/6091 [==============================] - 1s 128us/sample - loss: 0.3548 - val_loss: 0.4671\n",
      "Epoch 72/100\n",
      "6091/6091 [==============================] - 1s 124us/sample - loss: 0.3534 - val_loss: 0.4669\n",
      "Epoch 73/100\n",
      "6091/6091 [==============================] - 1s 126us/sample - loss: 0.3520 - val_loss: 0.4668\n",
      "Epoch 74/100\n",
      "6091/6091 [==============================] - 1s 126us/sample - loss: 0.3506 - val_loss: 0.4667\n",
      "Epoch 75/100\n",
      "6091/6091 [==============================] - 1s 122us/sample - loss: 0.3493 - val_loss: 0.4667\n",
      "Epoch 76/100\n",
      "6091/6091 [==============================] - 1s 111us/sample - loss: 0.3479 - val_loss: 0.4666\n",
      "Epoch 77/100\n",
      "6091/6091 [==============================] - 1s 99us/sample - loss: 0.3466 - val_loss: 0.4666\n",
      "Epoch 78/100\n",
      "6091/6091 [==============================] - 1s 99us/sample - loss: 0.3453 - val_loss: 0.4666\n",
      "Epoch 79/100\n",
      "6091/6091 [==============================] - 1s 102us/sample - loss: 0.3441 - val_loss: 0.4665\n",
      "Epoch 80/100\n",
      "6091/6091 [==============================] - 1s 111us/sample - loss: 0.3428 - val_loss: 0.4666\n",
      "Predicting...\n",
      "(Took 54.931 sec)\n",
      "Combined confusion matrix:\n",
      "[[3837.  505.]\n",
      " [1054. 2217.]]\n",
      "(Overall, took 284.850 sec)\n",
      "Accuracy: 79.52% +/- 0.93%\n",
      "Precision for positive class: 78.46% +/- 2.01%\n",
      "Precision for negative class: 81.46% +/- 1.69%\n",
      "Recall for positive class: 88.37% +/- 1.24%\n",
      "Recall for negative class: 67.80% +/- 2.51%\n",
      "F for positive class: 83.11% +/- 0.97%\n",
      "F for negative class: 73.98% +/- 1.14%\n",
      "Mean F score: 78.54% +/- 0.90%\n"
     ]
    }
   ],
   "source": [
    "# Bigrams, transformed\n",
    "# Normalized\n",
    "all_metrics(cross_validate_logistic(dftrain_bi_min_norm, 100))\n",
    "# TF-IDF\n",
    "all_metrics(cross_validate_logistic(dftrain_bi_min_tfidf, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_logistic_predictions(epochs, batch_size, train, test):\n",
    "    print('Training...')\n",
    "    nn = Sequential()\n",
    "    nn.add(Dense(1, activation='sigmoid', input_shape=(train.drop('_target', axis=1).shape[1],)))\n",
    "    nn.compile(optimizer='adam', loss='binary_crossentropy', metrics=[])\n",
    "    nn.fit(np.array(train.drop('_target', axis=1)), np.array(train['_target']), batch_size = batch_size, epochs=epochs)\n",
    "    print('Predicting...')\n",
    "    preds = 1*(nn.predict(np.array(test)) > 0.5)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Train on 7613 samples\n",
      "Epoch 1/23\n",
      "7613/7613 [==============================] - 1s 172us/sample - loss: 0.6369\n",
      "Epoch 2/23\n",
      "7613/7613 [==============================] - 1s 129us/sample - loss: 0.5614\n",
      "Epoch 3/23\n",
      "7613/7613 [==============================] - 1s 121us/sample - loss: 0.5168\n",
      "Epoch 4/23\n",
      "7613/7613 [==============================] - 1s 116us/sample - loss: 0.4853\n",
      "Epoch 5/23\n",
      "7613/7613 [==============================] - 1s 107us/sample - loss: 0.4610\n",
      "Epoch 6/23\n",
      "7613/7613 [==============================] - 1s 104us/sample - loss: 0.4410\n",
      "Epoch 7/23\n",
      "7613/7613 [==============================] - 1s 107us/sample - loss: 0.4244\n",
      "Epoch 8/23\n",
      "7613/7613 [==============================] - 1s 114us/sample - loss: 0.4100\n",
      "Epoch 9/23\n",
      "7613/7613 [==============================] - 1s 108us/sample - loss: 0.3972\n",
      "Epoch 10/23\n",
      "7613/7613 [==============================] - 1s 109us/sample - loss: 0.3858\n",
      "Epoch 11/23\n",
      "7613/7613 [==============================] - 1s 110us/sample - loss: 0.3756\n",
      "Epoch 12/23\n",
      "7613/7613 [==============================] - 1s 110us/sample - loss: 0.3663\n",
      "Epoch 13/23\n",
      "7613/7613 [==============================] - 1s 111us/sample - loss: 0.3576\n",
      "Epoch 14/23\n",
      "7613/7613 [==============================] - 1s 115us/sample - loss: 0.3497\n",
      "Epoch 15/23\n",
      "7613/7613 [==============================] - 1s 111us/sample - loss: 0.3423\n",
      "Epoch 16/23\n",
      "7613/7613 [==============================] - 1s 107us/sample - loss: 0.3354\n",
      "Epoch 17/23\n",
      "7613/7613 [==============================] - 1s 114us/sample - loss: 0.3290\n",
      "Epoch 18/23\n",
      "7613/7613 [==============================] - 1s 113us/sample - loss: 0.3230\n",
      "Epoch 19/23\n",
      "7613/7613 [==============================] - 1s 111us/sample - loss: 0.3171\n",
      "Epoch 20/23\n",
      "7613/7613 [==============================] - 1s 109us/sample - loss: 0.3117\n",
      "Epoch 21/23\n",
      "7613/7613 [==============================] - 1s 110us/sample - loss: 0.3066\n",
      "Epoch 22/23\n",
      "7613/7613 [==============================] - 1s 110us/sample - loss: 0.3017\n",
      "Epoch 23/23\n",
      "7613/7613 [==============================] - 1s 111us/sample - loss: 0.2970\n",
      "Predicting...\n"
     ]
    }
   ],
   "source": [
    "submission['target'] = make_logistic_predictions(23, 32, dftrain_minimal_noprep, dftest_minimal_noprep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission_lg_noprep.csv', index=False)\n",
    "# Score: 80.98%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Train on 7613 samples\n",
      "Epoch 1/46\n",
      "7613/7613 [==============================] - 1s 135us/sample - loss: 0.6800\n",
      "Epoch 2/46\n",
      "7613/7613 [==============================] - 1s 93us/sample - loss: 0.6537\n",
      "Epoch 3/46\n",
      "7613/7613 [==============================] - 1s 92us/sample - loss: 0.6320\n",
      "Epoch 4/46\n",
      "7613/7613 [==============================] - 1s 95us/sample - loss: 0.6128\n",
      "Epoch 5/46\n",
      "7613/7613 [==============================] - 1s 98us/sample - loss: 0.5952\n",
      "Epoch 6/46\n",
      "7613/7613 [==============================] - 1s 85us/sample - loss: 0.5791\n",
      "Epoch 7/46\n",
      "7613/7613 [==============================] - 1s 78us/sample - loss: 0.5643\n",
      "Epoch 8/46\n",
      "7613/7613 [==============================] - 1s 91us/sample - loss: 0.5506\n",
      "Epoch 9/46\n",
      "7613/7613 [==============================] - 1s 109us/sample - loss: 0.5379\n",
      "Epoch 10/46\n",
      "7613/7613 [==============================] - 1s 123us/sample - loss: 0.5261\n",
      "Epoch 11/46\n",
      "7613/7613 [==============================] - 1s 91us/sample - loss: 0.5153\n",
      "Epoch 12/46\n",
      "7613/7613 [==============================] - 1s 80us/sample - loss: 0.5051\n",
      "Epoch 13/46\n",
      "7613/7613 [==============================] - 1s 84us/sample - loss: 0.4957\n",
      "Epoch 14/46\n",
      "7613/7613 [==============================] - 1s 96us/sample - loss: 0.4869\n",
      "Epoch 15/46\n",
      "7613/7613 [==============================] - 1s 76us/sample - loss: 0.4787\n",
      "Epoch 16/46\n",
      "7613/7613 [==============================] - 1s 78us/sample - loss: 0.4710\n",
      "Epoch 17/46\n",
      "7613/7613 [==============================] - ETA: 0s - loss: 0.464 - 1s 78us/sample - loss: 0.4638\n",
      "Epoch 18/46\n",
      "7613/7613 [==============================] - 1s 81us/sample - loss: 0.4570\n",
      "Epoch 19/46\n",
      "7613/7613 [==============================] - 1s 80us/sample - loss: 0.4506\n",
      "Epoch 20/46\n",
      "7613/7613 [==============================] - 1s 83us/sample - loss: 0.4447\n",
      "Epoch 21/46\n",
      "7613/7613 [==============================] - 1s 78us/sample - loss: 0.4390\n",
      "Epoch 22/46\n",
      "7613/7613 [==============================] - 1s 79us/sample - loss: 0.4337\n",
      "Epoch 23/46\n",
      "7613/7613 [==============================] - 1s 84us/sample - loss: 0.4286\n",
      "Epoch 24/46\n",
      "7613/7613 [==============================] - 1s 74us/sample - loss: 0.4238\n",
      "Epoch 25/46\n",
      "7613/7613 [==============================] - 1s 102us/sample - loss: 0.4192\n",
      "Epoch 26/46\n",
      "7613/7613 [==============================] - 1s 81us/sample - loss: 0.4149\n",
      "Epoch 27/46\n",
      "7613/7613 [==============================] - 1s 95us/sample - loss: 0.4108\n",
      "Epoch 28/46\n",
      "7613/7613 [==============================] - 1s 92us/sample - loss: 0.4069\n",
      "Epoch 29/46\n",
      "7613/7613 [==============================] - 1s 83us/sample - loss: 0.4031\n",
      "Epoch 30/46\n",
      "7613/7613 [==============================] - 1s 83us/sample - loss: 0.3996\n",
      "Epoch 31/46\n",
      "7613/7613 [==============================] - 1s 76us/sample - loss: 0.3962\n",
      "Epoch 32/46\n",
      "7613/7613 [==============================] - 1s 78us/sample - loss: 0.3929\n",
      "Epoch 33/46\n",
      "7613/7613 [==============================] - 1s 77us/sample - loss: 0.3898\n",
      "Epoch 34/46\n",
      "7613/7613 [==============================] - 1s 76us/sample - loss: 0.3868\n",
      "Epoch 35/46\n",
      "7613/7613 [==============================] - 1s 70us/sample - loss: 0.3840\n",
      "Epoch 36/46\n",
      "7613/7613 [==============================] - 1s 73us/sample - loss: 0.3812\n",
      "Epoch 37/46\n",
      "7613/7613 [==============================] - 1s 69us/sample - loss: 0.3785\n",
      "Epoch 38/46\n",
      "7613/7613 [==============================] - 1s 79us/sample - loss: 0.3760\n",
      "Epoch 39/46\n",
      "7613/7613 [==============================] - 1s 72us/sample - loss: 0.3735\n",
      "Epoch 40/46\n",
      "7613/7613 [==============================] - 1s 74us/sample - loss: 0.3711\n",
      "Epoch 41/46\n",
      "7613/7613 [==============================] - 1s 76us/sample - loss: 0.3689\n",
      "Epoch 42/46\n",
      "7613/7613 [==============================] - 1s 71us/sample - loss: 0.3667\n",
      "Epoch 43/46\n",
      "7613/7613 [==============================] - 1s 73us/sample - loss: 0.3645\n",
      "Epoch 44/46\n",
      "7613/7613 [==============================] - 1s 70us/sample - loss: 0.3625\n",
      "Epoch 45/46\n",
      "7613/7613 [==============================] - 1s 72us/sample - loss: 0.3605\n",
      "Epoch 46/46\n",
      "7613/7613 [==============================] - 1s 71us/sample - loss: 0.3585\n",
      "Predicting...\n"
     ]
    }
   ],
   "source": [
    "submission['target'] = make_logistic_predictions(46, 32, dftrain_min_tfidf, dftest_min_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission_lg_prep.csv', index=False)\n",
    "# Score: 78.22%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Train on 7613 samples\n",
      "Epoch 1/27\n",
      "7613/7613 [==============================] - 1s 131us/sample - loss: 0.6371\n",
      "Epoch 2/27\n",
      "7613/7613 [==============================] - 1s 89us/sample - loss: 0.5661\n",
      "Epoch 3/27\n",
      "7613/7613 [==============================] - 1s 87us/sample - loss: 0.5247\n",
      "Epoch 4/27\n",
      "7613/7613 [==============================] - 1s 86us/sample - loss: 0.4959\n",
      "Epoch 5/27\n",
      "7613/7613 [==============================] - 1s 92us/sample - loss: 0.4739\n",
      "Epoch 6/27\n",
      "7613/7613 [==============================] - 1s 92us/sample - loss: 0.4563\n",
      "Epoch 7/27\n",
      "7613/7613 [==============================] - 1s 95us/sample - loss: 0.4414\n",
      "Epoch 8/27\n",
      "7613/7613 [==============================] - 1s 94us/sample - loss: 0.4288\n",
      "Epoch 9/27\n",
      "7613/7613 [==============================] - 1s 92us/sample - loss: 0.4177\n",
      "Epoch 10/27\n",
      "7613/7613 [==============================] - 1s 94us/sample - loss: 0.4080\n",
      "Epoch 11/27\n",
      "7613/7613 [==============================] - 1s 98us/sample - loss: 0.3992\n",
      "Epoch 12/27\n",
      "7613/7613 [==============================] - 1s 93us/sample - loss: 0.3912\n",
      "Epoch 13/27\n",
      "7613/7613 [==============================] - 1s 95us/sample - loss: 0.3840\n",
      "Epoch 14/27\n",
      "7613/7613 [==============================] - 1s 91us/sample - loss: 0.3773\n",
      "Epoch 15/27\n",
      "7613/7613 [==============================] - 1s 95us/sample - loss: 0.3712\n",
      "Epoch 16/27\n",
      "7613/7613 [==============================] - 1s 91us/sample - loss: 0.3655\n",
      "Epoch 17/27\n",
      "7613/7613 [==============================] - 1s 92us/sample - loss: 0.3602\n",
      "Epoch 18/27\n",
      "7613/7613 [==============================] - 1s 94us/sample - loss: 0.3552\n",
      "Epoch 19/27\n",
      "7613/7613 [==============================] - 1s 92us/sample - loss: 0.3505\n",
      "Epoch 20/27\n",
      "7613/7613 [==============================] - 1s 96us/sample - loss: 0.3461\n",
      "Epoch 21/27\n",
      "7613/7613 [==============================] - 1s 97us/sample - loss: 0.3420\n",
      "Epoch 22/27\n",
      "7613/7613 [==============================] - 1s 106us/sample - loss: 0.3380\n",
      "Epoch 23/27\n",
      "7613/7613 [==============================] - 1s 99us/sample - loss: 0.3342\n",
      "Epoch 24/27\n",
      "7613/7613 [==============================] - 1s 92us/sample - loss: 0.3307\n",
      "Epoch 25/27\n",
      "7613/7613 [==============================] - 1s 90us/sample - loss: 0.3273\n",
      "Epoch 26/27\n",
      "7613/7613 [==============================] - 1s 87us/sample - loss: 0.3239\n",
      "Epoch 27/27\n",
      "7613/7613 [==============================] - 1s 96us/sample - loss: 0.3208\n",
      "Predicting...\n"
     ]
    }
   ],
   "source": [
    "# Logistic regression, with bigrams, no preprocessing\n",
    "submission['target'] = make_logistic_predictions(27, 32, dftrain_bi_minimal, dftest_bi_minimal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission_lg_bigrams.csv', index=False)\n",
    "# Score: 79.55%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural networks were additional layers did not seem to produce any improvements. Several different numbers of hidden layers, and number of nodes on each hidden layer, were tried. The activation functions were varied between sigmoid and relu without any improvements. Adding dropout helped to avoid overfitting to some extent, but the resulting models were about the same as logistic regression. Better to go with the simpler model, if there isn't a significant improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_neuralNet(dataset, split, epochs, batch_size=32, patience=2):\n",
    "    t0 = time.time()\n",
    "    np.random.seed(1)\n",
    "    train, test = train_test_split(dataset, test_size=split)\n",
    "    train_x = np.array(train.drop('_target', axis=1))\n",
    "    train_y = np.array(train['_target'])\n",
    "    test_x = np.array(test.drop('_target', axis=1))\n",
    "    test_y = np.array(test['_target'])\n",
    "    print('Training...')\n",
    "    nn = Sequential()\n",
    "    nn.add(Dense(32, activation='sigmoid', input_shape=(train_x.shape[1],)))\n",
    "    nn.add(Dropout(0.5))\n",
    "    nn.add(Dense(1, activation='sigmoid'))\n",
    "    nn.compile(optimizer='adam', loss='binary_crossentropy', metrics=[])\n",
    "    es = EarlyStopping(monitor='val_loss', patience=patience)\n",
    "    h = nn.fit(train_x, train_y, batch_size = batch_size, epochs=epochs, validation_data=(test_x, test_y), callbacks=[es])\n",
    "    plt.plot(h.history['loss'])\n",
    "    plt.plot(h.history['val_loss'])\n",
    "    print('Predicting...')\n",
    "    preds = 1*(nn.predict(test_x) > 0.5)\n",
    "    cm = confusion_matrix(test_y, preds)\n",
    "    print(cm)\n",
    "    tp, fn, fp, tn = cm.ravel()\n",
    "    accuracy = (tp+tn)/(tp+fn+fp+tn)\n",
    "    precision = tp/(tp+fp)\n",
    "    recall = tp/(tp+fn)\n",
    "    F = 2*precision*recall/(precision+recall)\n",
    "    print('Accuracy: {:.2f}%\\nPrecision: {:.2f}%\\nRecall: {:.2f}%\\nF: {:.2f}'.format(100*accuracy, 100*precision, 100*recall, 100*F))\n",
    "    t1 = time.time()\n",
    "    print('(Took {:.3f} sec)'.format(t1-t0))\n",
    "    return nn\n",
    "\n",
    "# 5-fold cross-validation, all metrics\n",
    "def cross_validate_neuralNet(dataset, epochs=100, batch_size=32, patience=2):\n",
    "    t_0 = time.time()\n",
    "    # Fixed at 5-fold cross-validation for now\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "    X = np.array(dataset.drop('_target', axis=1))\n",
    "    y = np.array(dataset['_target'])\n",
    "    accs = []\n",
    "    precs_p = []\n",
    "    precs_n = []\n",
    "    recs_p = []\n",
    "    recs_n = []\n",
    "    Fs_p = []\n",
    "    Fs_n = []\n",
    "    mFs = [] # Mean F score, this is the actual competition metric\n",
    "    cm = np.zeros((2,2))\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        t0 = time.time()\n",
    "        train_x, test_x = X[train_index], X[test_index]\n",
    "        train_y, test_y = y[train_index], y[test_index]\n",
    "        #print('Training...')\n",
    "        nn = Sequential()\n",
    "        nn.add(Dense(32, activation='sigmoid', input_shape=(train_x.shape[1],)))\n",
    "        nn.add(Dropout(0.5))\n",
    "        nn.add(Dense(1, activation='sigmoid'))\n",
    "        nn.compile(optimizer='adam', loss='binary_crossentropy', metrics=[])\n",
    "        es = EarlyStopping(monitor='val_loss', patience=patience)\n",
    "        nn.fit(train_x, train_y, batch_size = batch_size, epochs=epochs, validation_data=(test_x, test_y), callbacks=[es])\n",
    "        #print('Predicting...')\n",
    "        preds = 1*(nn.predict(test_x) > 0.5)\n",
    "        cm_batch = confusion_matrix(test_y, preds)\n",
    "        cm += np.array(cm_batch)\n",
    "        tp, fn, fp, tn = cm_batch.ravel()\n",
    "        acc = (tp+tn)/(tp+fn+fp+tn)\n",
    "        accs.append( acc )\n",
    "        prec_p = tp/(tp+fp)\n",
    "        precs_p.append( prec_p )\n",
    "        prec_n = tn/(tn+fn)\n",
    "        precs_n.append( prec_n )\n",
    "        rec_p = tp/(tp+fn)\n",
    "        recs_p.append( rec_p )\n",
    "        rec_n = tn/(tn+fp)\n",
    "        recs_n.append( rec_n )\n",
    "        F_p = 2*prec_p*rec_p/(prec_p+rec_p)\n",
    "        Fs_p.append( F_p )\n",
    "        F_n = 2*prec_n*rec_n/(prec_n+rec_n)\n",
    "        Fs_n.append( F_n )\n",
    "        mF = (F_p + F_n)/2.0\n",
    "        mFs.append( mF )\n",
    "        t1 = time.time()\n",
    "        print('(Took {:.3f} sec)'.format(t1-t0))\n",
    "    t_1 = time.time()\n",
    "    print('Combined confusion matrix:')\n",
    "    print(cm)\n",
    "    print('(Overall, took {:.3f} sec)'.format(t_1-t_0))\n",
    "    return [accs, precs_p, precs_n, recs_p, recs_n, Fs_p, Fs_n, mFs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No preprocessing, raw counts\n",
      "Training...\n",
      "Train on 6851 samples, validate on 762 samples\n",
      "Epoch 1/50\n",
      "6851/6851 [==============================] - 1s 199us/sample - loss: 0.7195 - val_loss: 0.5901\n",
      "Epoch 2/50\n",
      "6851/6851 [==============================] - 1s 139us/sample - loss: 0.6079 - val_loss: 0.5430\n",
      "Epoch 3/50\n",
      "6851/6851 [==============================] - 1s 131us/sample - loss: 0.5467 - val_loss: 0.5091\n",
      "Epoch 4/50\n",
      "6851/6851 [==============================] - 1s 131us/sample - loss: 0.5058 - val_loss: 0.4850\n",
      "Epoch 5/50\n",
      "6851/6851 [==============================] - 1s 137us/sample - loss: 0.4715 - val_loss: 0.4679\n",
      "Epoch 6/50\n",
      "6851/6851 [==============================] - 1s 136us/sample - loss: 0.4459 - val_loss: 0.4575\n",
      "Epoch 7/50\n",
      "6851/6851 [==============================] - 1s 131us/sample - loss: 0.4226 - val_loss: 0.4513\n",
      "Epoch 8/50\n",
      "6851/6851 [==============================] - 1s 129us/sample - loss: 0.4043 - val_loss: 0.4451\n",
      "Epoch 9/50\n",
      "6851/6851 [==============================] - 1s 139us/sample - loss: 0.3862 - val_loss: 0.4417\n",
      "Epoch 10/50\n",
      "6851/6851 [==============================] - 1s 128us/sample - loss: 0.3730 - val_loss: 0.4426\n",
      "Epoch 11/50\n",
      "6851/6851 [==============================] - 1s 145us/sample - loss: 0.3577 - val_loss: 0.4421\n",
      "Predicting...\n",
      "[[395  43]\n",
      " [102 222]]\n",
      "Accuracy: 80.97%\n",
      "Precision: 79.48%\n",
      "Recall: 90.18%\n",
      "F: 84.49\n",
      "(Took 11.936 sec)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.engine.sequential.Sequential at 0x22b885e6278>"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Neural net models\n",
    "# Only with preprocessing, otherwise there are too many weights\n",
    "# Raw counts\n",
    "print('No preprocessing, raw counts')\n",
    "run_neuralNet(dftrain_minimal_noprep, 0.1, 50, patience=2)\n",
    "# Normalized\n",
    "#print('Preprocessed, normalized')\n",
    "#run_neuralNet(dftrain_min_norm, 0.2, 20)\n",
    "# TF-IDF\n",
    "#print('Preprocessed, TF-IDF')\n",
    "#run_neuralNet(dftrain_min_tfidf, 0.2, 20)\n",
    "#print('Done with Neural Net models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/100\n",
      "6090/6090 [==============================] - 1s 207us/sample - loss: 0.6907 - val_loss: 0.6071\n",
      "Epoch 2/100\n",
      "6090/6090 [==============================] - 1s 149us/sample - loss: 0.6120 - val_loss: 0.5549\n",
      "Epoch 3/100\n",
      "6090/6090 [==============================] - 1s 142us/sample - loss: 0.5598 - val_loss: 0.5178\n",
      "Epoch 4/100\n",
      "6090/6090 [==============================] - 1s 144us/sample - loss: 0.5198 - val_loss: 0.4908\n",
      "Epoch 5/100\n",
      "6090/6090 [==============================] - 1s 142us/sample - loss: 0.4775 - val_loss: 0.4713\n",
      "Epoch 6/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.4523 - val_loss: 0.4582\n",
      "Epoch 7/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.4310 - val_loss: 0.4488\n",
      "Epoch 8/100\n",
      "6090/6090 [==============================] - 1s 135us/sample - loss: 0.4037 - val_loss: 0.4432\n",
      "Epoch 9/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.3906 - val_loss: 0.4406\n",
      "Epoch 10/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.3626 - val_loss: 0.4388\n",
      "Epoch 11/100\n",
      "6090/6090 [==============================] - 1s 136us/sample - loss: 0.3558 - val_loss: 0.4405\n",
      "Epoch 12/100\n",
      "6090/6090 [==============================] - 1s 135us/sample - loss: 0.3466 - val_loss: 0.4405\n",
      "(Took 11.393 sec)\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/100\n",
      "6090/6090 [==============================] - 1s 201us/sample - loss: 0.6874 - val_loss: 0.6067\n",
      "Epoch 2/100\n",
      "6090/6090 [==============================] - 1s 140us/sample - loss: 0.6100 - val_loss: 0.5604\n",
      "Epoch 3/100\n",
      "6090/6090 [==============================] - 1s 140us/sample - loss: 0.5541 - val_loss: 0.5274\n",
      "Epoch 4/100\n",
      "6090/6090 [==============================] - 1s 141us/sample - loss: 0.5112 - val_loss: 0.5030\n",
      "Epoch 5/100\n",
      "6090/6090 [==============================] - 1s 140us/sample - loss: 0.4788 - val_loss: 0.4854\n",
      "Epoch 6/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.4498 - val_loss: 0.4750\n",
      "Epoch 7/100\n",
      "6090/6090 [==============================] - 1s 136us/sample - loss: 0.4279 - val_loss: 0.4661\n",
      "Epoch 8/100\n",
      "6090/6090 [==============================] - 1s 136us/sample - loss: 0.4057 - val_loss: 0.4600\n",
      "Epoch 9/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.3843 - val_loss: 0.4562\n",
      "Epoch 10/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.3698 - val_loss: 0.4541\n",
      "Epoch 11/100\n",
      "6090/6090 [==============================] - 1s 144us/sample - loss: 0.3506 - val_loss: 0.4548\n",
      "Epoch 12/100\n",
      "6090/6090 [==============================] - 1s 143us/sample - loss: 0.3401 - val_loss: 0.4563\n",
      "(Took 11.348 sec)\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/100\n",
      "6090/6090 [==============================] - 1s 188us/sample - loss: 0.6826 - val_loss: 0.6095\n",
      "Epoch 2/100\n",
      "6090/6090 [==============================] - 1s 148us/sample - loss: 0.6025 - val_loss: 0.5646\n",
      "Epoch 3/100\n",
      "6090/6090 [==============================] - 1s 146us/sample - loss: 0.5553 - val_loss: 0.5318\n",
      "Epoch 4/100\n",
      "6090/6090 [==============================] - 1s 142us/sample - loss: 0.5155 - val_loss: 0.5059\n",
      "Epoch 5/100\n",
      "6090/6090 [==============================] - 1s 144us/sample - loss: 0.4823 - val_loss: 0.4875\n",
      "Epoch 6/100\n",
      "6090/6090 [==============================] - 1s 140us/sample - loss: 0.4514 - val_loss: 0.4747\n",
      "Epoch 7/100\n",
      "6090/6090 [==============================] - 1s 141us/sample - loss: 0.4283 - val_loss: 0.4656\n",
      "Epoch 8/100\n",
      "6090/6090 [==============================] - 1s 142us/sample - loss: 0.4060 - val_loss: 0.4615\n",
      "Epoch 9/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.3913 - val_loss: 0.4577\n",
      "Epoch 10/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.3667 - val_loss: 0.4573\n",
      "Epoch 11/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.3573 - val_loss: 0.4547\n",
      "Epoch 12/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.3407 - val_loss: 0.4568\n",
      "Epoch 13/100\n",
      "6090/6090 [==============================] - 1s 136us/sample - loss: 0.3288 - val_loss: 0.4602\n",
      "(Took 12.279 sec)\n",
      "Train on 6091 samples, validate on 1522 samples\n",
      "Epoch 1/100\n",
      "6091/6091 [==============================] - 1s 204us/sample - loss: 0.6872 - val_loss: 0.6080\n",
      "Epoch 2/100\n",
      "6091/6091 [==============================] - 1s 143us/sample - loss: 0.6070 - val_loss: 0.5600\n",
      "Epoch 3/100\n",
      "6091/6091 [==============================] - 1s 142us/sample - loss: 0.5524 - val_loss: 0.5227\n",
      "Epoch 4/100\n",
      "6091/6091 [==============================] - 1s 143us/sample - loss: 0.5186 - val_loss: 0.4961\n",
      "Epoch 5/100\n",
      "6091/6091 [==============================] - 1s 141us/sample - loss: 0.4778 - val_loss: 0.4766\n",
      "Epoch 6/100\n",
      "6091/6091 [==============================] - 1s 144us/sample - loss: 0.4463 - val_loss: 0.4625\n",
      "Epoch 7/100\n",
      "6091/6091 [==============================] - 1s 141us/sample - loss: 0.4260 - val_loss: 0.4534\n",
      "Epoch 8/100\n",
      "6091/6091 [==============================] - 1s 143us/sample - loss: 0.4110 - val_loss: 0.4476\n",
      "Epoch 9/100\n",
      "6091/6091 [==============================] - 1s 154us/sample - loss: 0.3844 - val_loss: 0.4434\n",
      "Epoch 10/100\n",
      "6091/6091 [==============================] - 1s 151us/sample - loss: 0.3733 - val_loss: 0.4418\n",
      "Epoch 11/100\n",
      "6091/6091 [==============================] - 1s 151us/sample - loss: 0.3689 - val_loss: 0.4415\n",
      "Epoch 12/100\n",
      "6091/6091 [==============================] - 1s 150us/sample - loss: 0.3479 - val_loss: 0.4428\n",
      "Epoch 13/100\n",
      "6091/6091 [==============================] - 1s 139us/sample - loss: 0.3310 - val_loss: 0.4438\n",
      "(Took 12.643 sec)\n",
      "Train on 6091 samples, validate on 1522 samples\n",
      "Epoch 1/100\n",
      "6091/6091 [==============================] - 1s 195us/sample - loss: 0.6970 - val_loss: 0.6024\n",
      "Epoch 2/100\n",
      "6091/6091 [==============================] - 1s 146us/sample - loss: 0.6059 - val_loss: 0.5566\n",
      "Epoch 3/100\n",
      "6091/6091 [==============================] - 1s 136us/sample - loss: 0.5471 - val_loss: 0.5233\n",
      "Epoch 4/100\n",
      "6091/6091 [==============================] - 1s 138us/sample - loss: 0.5086 - val_loss: 0.4985\n",
      "Epoch 5/100\n",
      "6091/6091 [==============================] - 1s 136us/sample - loss: 0.4859 - val_loss: 0.4818\n",
      "Epoch 6/100\n",
      "6091/6091 [==============================] - 1s 143us/sample - loss: 0.4536 - val_loss: 0.4713\n",
      "Epoch 7/100\n",
      "6091/6091 [==============================] - 1s 142us/sample - loss: 0.4274 - val_loss: 0.4610\n",
      "Epoch 8/100\n",
      "6091/6091 [==============================] - 1s 138us/sample - loss: 0.4053 - val_loss: 0.4536\n",
      "Epoch 9/100\n",
      "6091/6091 [==============================] - 1s 140us/sample - loss: 0.3954 - val_loss: 0.4495\n",
      "Epoch 10/100\n",
      "6091/6091 [==============================] - 1s 141us/sample - loss: 0.3751 - val_loss: 0.4470\n",
      "Epoch 11/100\n",
      "6091/6091 [==============================] - 1s 139us/sample - loss: 0.3609 - val_loss: 0.4473\n",
      "Epoch 12/100\n",
      "6091/6091 [==============================] - 1s 140us/sample - loss: 0.3464 - val_loss: 0.4463\n",
      "Epoch 13/100\n",
      "6091/6091 [==============================] - 1s 142us/sample - loss: 0.3320 - val_loss: 0.4488\n",
      "Epoch 14/100\n",
      "6091/6091 [==============================] - 1s 138us/sample - loss: 0.3236 - val_loss: 0.4515\n",
      "(Took 13.082 sec)\n",
      "Combined confusion matrix:\n",
      "[[3840.  502.]\n",
      " [ 974. 2297.]]\n",
      "(Overall, took 61.096 sec)\n",
      "Accuracy: 80.61% +/- 0.66%\n",
      "Precision for positive class: 79.77% +/- 1.45%\n",
      "Precision for negative class: 82.06% +/- 1.52%\n",
      "Recall for positive class: 88.44% +/- 1.00%\n",
      "Recall for negative class: 70.22% +/- 2.22%\n",
      "F for positive class: 83.87% +/- 0.69%\n",
      "F for negative class: 75.66% +/- 1.30%\n",
      "Mean F score: 79.77% +/- 0.75%\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/100\n",
      "6090/6090 [==============================] - 1s 197us/sample - loss: 0.7422 - val_loss: 0.6733\n",
      "Epoch 2/100\n",
      "6090/6090 [==============================] - 1s 141us/sample - loss: 0.7149 - val_loss: 0.6683\n",
      "Epoch 3/100\n",
      "6090/6090 [==============================] - 1s 144us/sample - loss: 0.6918 - val_loss: 0.6641\n",
      "Epoch 4/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.6859 - val_loss: 0.6614\n",
      "Epoch 5/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.6795 - val_loss: 0.6576\n",
      "Epoch 6/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.6759 - val_loss: 0.6536\n",
      "Epoch 7/100\n",
      "6090/6090 [==============================] - 1s 132us/sample - loss: 0.6674 - val_loss: 0.6486\n",
      "Epoch 8/100\n",
      "6090/6090 [==============================] - 1s 133us/sample - loss: 0.6645 - val_loss: 0.6428\n",
      "Epoch 9/100\n",
      "6090/6090 [==============================] - 1s 129us/sample - loss: 0.6539 - val_loss: 0.6355\n",
      "Epoch 10/100\n",
      "6090/6090 [==============================] - 1s 131us/sample - loss: 0.6491 - val_loss: 0.6279\n",
      "Epoch 11/100\n",
      "6090/6090 [==============================] - 1s 130us/sample - loss: 0.6410 - val_loss: 0.6185\n",
      "Epoch 12/100\n",
      "6090/6090 [==============================] - 1s 130us/sample - loss: 0.6292 - val_loss: 0.6083\n",
      "Epoch 13/100\n",
      "6090/6090 [==============================] - 1s 130us/sample - loss: 0.6262 - val_loss: 0.5984\n",
      "Epoch 14/100\n",
      "6090/6090 [==============================] - 1s 127us/sample - loss: 0.6124 - val_loss: 0.5878\n",
      "Epoch 15/100\n",
      "6090/6090 [==============================] - 1s 130us/sample - loss: 0.6068 - val_loss: 0.5780\n",
      "Epoch 16/100\n",
      "6090/6090 [==============================] - 1s 132us/sample - loss: 0.5969 - val_loss: 0.5680\n",
      "Epoch 17/100\n",
      "6090/6090 [==============================] - 1s 129us/sample - loss: 0.5877 - val_loss: 0.5583\n",
      "Epoch 18/100\n",
      "6090/6090 [==============================] - 1s 136us/sample - loss: 0.5773 - val_loss: 0.5498\n",
      "Epoch 19/100\n",
      "6090/6090 [==============================] - 1s 136us/sample - loss: 0.5710 - val_loss: 0.5414\n",
      "Epoch 20/100\n",
      "6090/6090 [==============================] - 1s 135us/sample - loss: 0.5577 - val_loss: 0.5333\n",
      "Epoch 21/100\n",
      "6090/6090 [==============================] - 1s 136us/sample - loss: 0.5572 - val_loss: 0.5271\n",
      "Epoch 22/100\n",
      "6090/6090 [==============================] - 1s 133us/sample - loss: 0.5498 - val_loss: 0.5205\n",
      "Epoch 23/100\n",
      "6090/6090 [==============================] - 1s 136us/sample - loss: 0.5468 - val_loss: 0.5152\n",
      "Epoch 24/100\n",
      "6090/6090 [==============================] - 1s 130us/sample - loss: 0.5349 - val_loss: 0.5101\n",
      "Epoch 25/100\n",
      "6090/6090 [==============================] - 1s 129us/sample - loss: 0.5299 - val_loss: 0.5053\n",
      "Epoch 26/100\n",
      "6090/6090 [==============================] - 1s 128us/sample - loss: 0.5203 - val_loss: 0.5004\n",
      "Epoch 27/100\n",
      "6090/6090 [==============================] - 1s 129us/sample - loss: 0.5160 - val_loss: 0.4959\n",
      "Epoch 28/100\n",
      "6090/6090 [==============================] - 1s 133us/sample - loss: 0.5197 - val_loss: 0.4935\n",
      "Epoch 29/100\n",
      "6090/6090 [==============================] - 1s 133us/sample - loss: 0.5102 - val_loss: 0.4896\n",
      "Epoch 30/100\n",
      "6090/6090 [==============================] - 1s 129us/sample - loss: 0.5057 - val_loss: 0.4865\n",
      "Epoch 31/100\n",
      "6090/6090 [==============================] - 1s 136us/sample - loss: 0.4974 - val_loss: 0.4835\n",
      "Epoch 32/100\n",
      "6090/6090 [==============================] - 1s 133us/sample - loss: 0.4929 - val_loss: 0.4806\n",
      "Epoch 33/100\n",
      "6090/6090 [==============================] - 1s 134us/sample - loss: 0.4828 - val_loss: 0.4779\n",
      "Epoch 34/100\n",
      "6090/6090 [==============================] - 1s 133us/sample - loss: 0.4823 - val_loss: 0.4752\n",
      "Epoch 35/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.4880 - val_loss: 0.4734\n",
      "Epoch 36/100\n",
      "6090/6090 [==============================] - 1s 135us/sample - loss: 0.4734 - val_loss: 0.4714\n",
      "Epoch 37/100\n",
      "6090/6090 [==============================] - 1s 135us/sample - loss: 0.4739 - val_loss: 0.4694\n",
      "Epoch 38/100\n",
      "6090/6090 [==============================] - 1s 130us/sample - loss: 0.4683 - val_loss: 0.4676\n",
      "Epoch 39/100\n",
      "6090/6090 [==============================] - 1s 132us/sample - loss: 0.4700 - val_loss: 0.4662\n",
      "Epoch 40/100\n",
      "6090/6090 [==============================] - 1s 133us/sample - loss: 0.4643 - val_loss: 0.4648\n",
      "Epoch 41/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.4574 - val_loss: 0.4632\n",
      "Epoch 42/100\n",
      "6090/6090 [==============================] - 1s 134us/sample - loss: 0.4583 - val_loss: 0.4625\n",
      "Epoch 43/100\n",
      "6090/6090 [==============================] - 1s 135us/sample - loss: 0.4527 - val_loss: 0.4609\n",
      "Epoch 44/100\n",
      "6090/6090 [==============================] - 1s 134us/sample - loss: 0.4534 - val_loss: 0.4595\n",
      "Epoch 45/100\n",
      "6090/6090 [==============================] - 1s 132us/sample - loss: 0.4466 - val_loss: 0.4586\n",
      "Epoch 46/100\n",
      "6090/6090 [==============================] - 1s 132us/sample - loss: 0.4440 - val_loss: 0.4575\n",
      "Epoch 47/100\n",
      "6090/6090 [==============================] - 1s 135us/sample - loss: 0.4379 - val_loss: 0.4563\n",
      "Epoch 48/100\n",
      "6090/6090 [==============================] - 1s 129us/sample - loss: 0.4405 - val_loss: 0.4554\n",
      "Epoch 49/100\n",
      "6090/6090 [==============================] - 1s 133us/sample - loss: 0.4468 - val_loss: 0.4550\n",
      "Epoch 50/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.4389 - val_loss: 0.4544\n",
      "Epoch 51/100\n",
      "6090/6090 [==============================] - 1s 133us/sample - loss: 0.4381 - val_loss: 0.4538\n",
      "Epoch 52/100\n",
      "6090/6090 [==============================] - 1s 135us/sample - loss: 0.4319 - val_loss: 0.4532\n",
      "Epoch 53/100\n",
      "6090/6090 [==============================] - 1s 135us/sample - loss: 0.4249 - val_loss: 0.4522\n",
      "Epoch 54/100\n",
      "6090/6090 [==============================] - 1s 132us/sample - loss: 0.4318 - val_loss: 0.4521\n",
      "Epoch 55/100\n",
      "6090/6090 [==============================] - 1s 131us/sample - loss: 0.4299 - val_loss: 0.4516\n",
      "Epoch 56/100\n",
      "6090/6090 [==============================] - 1s 133us/sample - loss: 0.4167 - val_loss: 0.4508\n",
      "Epoch 57/100\n",
      "6090/6090 [==============================] - 1s 134us/sample - loss: 0.4239 - val_loss: 0.4504\n",
      "Epoch 58/100\n",
      "6090/6090 [==============================] - 1s 134us/sample - loss: 0.4085 - val_loss: 0.4499\n",
      "Epoch 59/100\n",
      "6090/6090 [==============================] - 1s 135us/sample - loss: 0.4092 - val_loss: 0.4499\n",
      "Epoch 60/100\n",
      "6090/6090 [==============================] - 1s 147us/sample - loss: 0.4126 - val_loss: 0.4492\n",
      "Epoch 61/100\n",
      "6090/6090 [==============================] - 1s 145us/sample - loss: 0.4083 - val_loss: 0.4489\n",
      "Epoch 62/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.4029 - val_loss: 0.4491\n",
      "Epoch 63/100\n",
      "6090/6090 [==============================] - 1s 150us/sample - loss: 0.4057 - val_loss: 0.4485\n",
      "Epoch 64/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.4040 - val_loss: 0.4482\n",
      "Epoch 65/100\n",
      "6090/6090 [==============================] - 1s 140us/sample - loss: 0.3963 - val_loss: 0.4489\n",
      "Epoch 66/100\n",
      "6090/6090 [==============================] - 1s 136us/sample - loss: 0.3940 - val_loss: 0.4481\n",
      "Epoch 67/100\n",
      "6090/6090 [==============================] - 1s 148us/sample - loss: 0.4057 - val_loss: 0.4480\n",
      "Epoch 68/100\n",
      "6090/6090 [==============================] - 1s 146us/sample - loss: 0.3947 - val_loss: 0.4488\n",
      "Epoch 69/100\n",
      "6090/6090 [==============================] - 1s 141us/sample - loss: 0.3908 - val_loss: 0.4479\n",
      "Epoch 70/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.3934 - val_loss: 0.4480\n",
      "Epoch 71/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.3831 - val_loss: 0.4481\n",
      "(Took 59.528 sec)\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/100\n",
      "6090/6090 [==============================] - 1s 196us/sample - loss: 0.7531 - val_loss: 0.6709\n",
      "Epoch 2/100\n",
      "6090/6090 [==============================] - 1s 150us/sample - loss: 0.7143 - val_loss: 0.6659\n",
      "Epoch 3/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.6990 - val_loss: 0.6601\n",
      "Epoch 4/100\n",
      "6090/6090 [==============================] - 1s 140us/sample - loss: 0.6856 - val_loss: 0.6566\n",
      "Epoch 5/100\n",
      "6090/6090 [==============================] - 1s 134us/sample - loss: 0.6823 - val_loss: 0.6537\n",
      "Epoch 6/100\n",
      "6090/6090 [==============================] - 1s 136us/sample - loss: 0.6739 - val_loss: 0.6500\n",
      "Epoch 7/100\n",
      "6090/6090 [==============================] - 1s 133us/sample - loss: 0.6679 - val_loss: 0.6458\n",
      "Epoch 8/100\n",
      "6090/6090 [==============================] - 1s 133us/sample - loss: 0.6656 - val_loss: 0.6408\n",
      "Epoch 9/100\n",
      "6090/6090 [==============================] - 1s 135us/sample - loss: 0.6588 - val_loss: 0.6362\n",
      "Epoch 10/100\n",
      "6090/6090 [==============================] - 1s 135us/sample - loss: 0.6509 - val_loss: 0.6279\n",
      "Epoch 11/100\n",
      "6090/6090 [==============================] - 1s 133us/sample - loss: 0.6423 - val_loss: 0.6200\n",
      "Epoch 12/100\n",
      "6090/6090 [==============================] - 1s 135us/sample - loss: 0.6346 - val_loss: 0.6108\n",
      "Epoch 13/100\n",
      "6090/6090 [==============================] - 1s 132us/sample - loss: 0.6234 - val_loss: 0.6015\n",
      "Epoch 14/100\n",
      "6090/6090 [==============================] - 1s 136us/sample - loss: 0.6214 - val_loss: 0.5961\n",
      "Epoch 15/100\n",
      "6090/6090 [==============================] - 1s 133us/sample - loss: 0.6094 - val_loss: 0.5823\n",
      "Epoch 16/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.6035 - val_loss: 0.5737\n",
      "Epoch 17/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.5865 - val_loss: 0.5631\n",
      "Epoch 18/100\n",
      "6090/6090 [==============================] - 1s 144us/sample - loss: 0.5787 - val_loss: 0.5538\n",
      "Epoch 19/100\n",
      "6090/6090 [==============================] - 1s 152us/sample - loss: 0.5657 - val_loss: 0.5452\n",
      "Epoch 20/100\n",
      "6090/6090 [==============================] - 1s 158us/sample - loss: 0.5643 - val_loss: 0.5374\n",
      "Epoch 21/100\n",
      "6090/6090 [==============================] - 1s 153us/sample - loss: 0.5600 - val_loss: 0.5309\n",
      "Epoch 22/100\n",
      "6090/6090 [==============================] - 1s 146us/sample - loss: 0.5473 - val_loss: 0.5242\n",
      "Epoch 23/100\n",
      "6090/6090 [==============================] - 1s 145us/sample - loss: 0.5455 - val_loss: 0.5189\n",
      "Epoch 24/100\n",
      "6090/6090 [==============================] - 1s 154us/sample - loss: 0.5393 - val_loss: 0.5140\n",
      "Epoch 25/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.5373 - val_loss: 0.5108\n",
      "Epoch 26/100\n",
      "6090/6090 [==============================] - 1s 147us/sample - loss: 0.5207 - val_loss: 0.5044\n",
      "Epoch 27/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.5180 - val_loss: 0.5005\n",
      "Epoch 28/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.5166 - val_loss: 0.4970\n",
      "Epoch 29/100\n",
      "6090/6090 [==============================] - 1s 160us/sample - loss: 0.5111 - val_loss: 0.4939\n",
      "Epoch 30/100\n",
      "6090/6090 [==============================] - 1s 155us/sample - loss: 0.5104 - val_loss: 0.4915\n",
      "Epoch 31/100\n",
      "6090/6090 [==============================] - 1s 150us/sample - loss: 0.5054 - val_loss: 0.4899\n",
      "Epoch 32/100\n",
      "6090/6090 [==============================] - 1s 148us/sample - loss: 0.4987 - val_loss: 0.4862\n",
      "Epoch 33/100\n",
      "6090/6090 [==============================] - 1s 134us/sample - loss: 0.4961 - val_loss: 0.4851\n",
      "Epoch 34/100\n",
      "6090/6090 [==============================] - 1s 145us/sample - loss: 0.4793 - val_loss: 0.4814\n",
      "Epoch 35/100\n",
      "6090/6090 [==============================] - 1s 158us/sample - loss: 0.4887 - val_loss: 0.4793\n",
      "Epoch 36/100\n",
      "6090/6090 [==============================] - 1s 151us/sample - loss: 0.4792 - val_loss: 0.4771\n",
      "Epoch 37/100\n",
      "6090/6090 [==============================] - 1s 162us/sample - loss: 0.4812 - val_loss: 0.4754\n",
      "Epoch 38/100\n",
      "6090/6090 [==============================] - 1s 168us/sample - loss: 0.4765 - val_loss: 0.4736\n",
      "Epoch 39/100\n",
      "6090/6090 [==============================] - 1s 155us/sample - loss: 0.4746 - val_loss: 0.4721\n",
      "Epoch 40/100\n",
      "6090/6090 [==============================] - 1s 166us/sample - loss: 0.4703 - val_loss: 0.4710\n",
      "Epoch 41/100\n",
      "6090/6090 [==============================] - 1s 166us/sample - loss: 0.4637 - val_loss: 0.4693\n",
      "Epoch 42/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.4537 - val_loss: 0.4675\n",
      "Epoch 43/100\n",
      "6090/6090 [==============================] - 1s 154us/sample - loss: 0.4539 - val_loss: 0.4662\n",
      "Epoch 44/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.4521 - val_loss: 0.4648\n",
      "Epoch 45/100\n",
      "6090/6090 [==============================] - 1s 141us/sample - loss: 0.4498 - val_loss: 0.4637\n",
      "Epoch 46/100\n",
      "6090/6090 [==============================] - 1s 136us/sample - loss: 0.4433 - val_loss: 0.4628\n",
      "Epoch 47/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.4455 - val_loss: 0.4619\n",
      "Epoch 48/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.4463 - val_loss: 0.4611\n",
      "Epoch 49/100\n",
      "6090/6090 [==============================] - 1s 135us/sample - loss: 0.4389 - val_loss: 0.4604\n",
      "Epoch 50/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.4395 - val_loss: 0.4596\n",
      "Epoch 51/100\n",
      "6090/6090 [==============================] - 1s 134us/sample - loss: 0.4338 - val_loss: 0.4587\n",
      "Epoch 52/100\n",
      "6090/6090 [==============================] - 1s 136us/sample - loss: 0.4328 - val_loss: 0.4581\n",
      "Epoch 53/100\n",
      "6090/6090 [==============================] - 1s 145us/sample - loss: 0.4324 - val_loss: 0.4576\n",
      "Epoch 54/100\n",
      "6090/6090 [==============================] - 1s 134us/sample - loss: 0.4282 - val_loss: 0.4569\n",
      "Epoch 55/100\n",
      "6090/6090 [==============================] - 1s 136us/sample - loss: 0.4233 - val_loss: 0.4560\n",
      "Epoch 56/100\n",
      "6090/6090 [==============================] - 1s 145us/sample - loss: 0.4161 - val_loss: 0.4566\n",
      "Epoch 57/100\n",
      "6090/6090 [==============================] - 1s 142us/sample - loss: 0.4157 - val_loss: 0.4560\n",
      "Epoch 58/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.4183 - val_loss: 0.4546\n",
      "Epoch 59/100\n",
      "6090/6090 [==============================] - 1s 135us/sample - loss: 0.4095 - val_loss: 0.4543\n",
      "Epoch 60/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.4128 - val_loss: 0.4539\n",
      "Epoch 61/100\n",
      "6090/6090 [==============================] - 1s 136us/sample - loss: 0.4026 - val_loss: 0.4536\n",
      "Epoch 62/100\n",
      "6090/6090 [==============================] - 1s 133us/sample - loss: 0.4022 - val_loss: 0.4533\n",
      "Epoch 63/100\n",
      "6090/6090 [==============================] - 1s 131us/sample - loss: 0.4054 - val_loss: 0.4529\n",
      "Epoch 64/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.3986 - val_loss: 0.4526\n",
      "Epoch 65/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.3970 - val_loss: 0.4524\n",
      "Epoch 66/100\n",
      "6090/6090 [==============================] - 1s 134us/sample - loss: 0.3968 - val_loss: 0.4523\n",
      "Epoch 67/100\n",
      "6090/6090 [==============================] - 1s 135us/sample - loss: 0.3929 - val_loss: 0.4526\n",
      "Epoch 68/100\n",
      "6090/6090 [==============================] - 1s 135us/sample - loss: 0.3941 - val_loss: 0.4521\n",
      "Epoch 69/100\n",
      "6090/6090 [==============================] - 1s 136us/sample - loss: 0.3931 - val_loss: 0.4522\n",
      "Epoch 70/100\n",
      "6090/6090 [==============================] - 1s 136us/sample - loss: 0.3820 - val_loss: 0.4529\n",
      "(Took 61.777 sec)\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/100\n",
      "6090/6090 [==============================] - 1s 188us/sample - loss: 0.7393 - val_loss: 0.6790\n",
      "Epoch 2/100\n",
      "6090/6090 [==============================] - 1s 130us/sample - loss: 0.7107 - val_loss: 0.6748\n",
      "Epoch 3/100\n",
      "6090/6090 [==============================] - 1s 132us/sample - loss: 0.6909 - val_loss: 0.6716\n",
      "Epoch 4/100\n",
      "6090/6090 [==============================] - 1s 132us/sample - loss: 0.6847 - val_loss: 0.6653\n",
      "Epoch 5/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.6799 - val_loss: 0.6607\n",
      "Epoch 6/100\n",
      "6090/6090 [==============================] - 1s 136us/sample - loss: 0.6727 - val_loss: 0.6578\n",
      "Epoch 7/100\n",
      "6090/6090 [==============================] - 1s 135us/sample - loss: 0.6642 - val_loss: 0.6525\n",
      "Epoch 8/100\n",
      "6090/6090 [==============================] - 1s 131us/sample - loss: 0.6612 - val_loss: 0.6478\n",
      "Epoch 9/100\n",
      "6090/6090 [==============================] - 1s 133us/sample - loss: 0.6545 - val_loss: 0.6426\n",
      "Epoch 10/100\n",
      "6090/6090 [==============================] - 1s 128us/sample - loss: 0.6475 - val_loss: 0.6343\n",
      "Epoch 11/100\n",
      "6090/6090 [==============================] - 1s 132us/sample - loss: 0.6385 - val_loss: 0.6268\n",
      "Epoch 12/100\n",
      "6090/6090 [==============================] - 1s 131us/sample - loss: 0.6280 - val_loss: 0.6169\n",
      "Epoch 13/100\n",
      "6090/6090 [==============================] - 1s 132us/sample - loss: 0.6241 - val_loss: 0.6075\n",
      "Epoch 14/100\n",
      "6090/6090 [==============================] - 1s 131us/sample - loss: 0.6155 - val_loss: 0.5980\n",
      "Epoch 15/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6090/6090 [==============================] - 1s 131us/sample - loss: 0.6036 - val_loss: 0.5889\n",
      "Epoch 16/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.5967 - val_loss: 0.5797\n",
      "Epoch 17/100\n",
      "6090/6090 [==============================] - 1s 134us/sample - loss: 0.5880 - val_loss: 0.5723\n",
      "Epoch 18/100\n",
      "6090/6090 [==============================] - 1s 134us/sample - loss: 0.5744 - val_loss: 0.5635\n",
      "Epoch 19/100\n",
      "6090/6090 [==============================] - 1s 133us/sample - loss: 0.5683 - val_loss: 0.5556\n",
      "Epoch 20/100\n",
      "6090/6090 [==============================] - 1s 131us/sample - loss: 0.5668 - val_loss: 0.5497\n",
      "Epoch 21/100\n",
      "6090/6090 [==============================] - 1s 133us/sample - loss: 0.5593 - val_loss: 0.5437\n",
      "Epoch 22/100\n",
      "6090/6090 [==============================] - 1s 135us/sample - loss: 0.5483 - val_loss: 0.5353\n",
      "Epoch 23/100\n",
      "6090/6090 [==============================] - 1s 132us/sample - loss: 0.5445 - val_loss: 0.5334\n",
      "Epoch 24/100\n",
      "6090/6090 [==============================] - 1s 131us/sample - loss: 0.5306 - val_loss: 0.5259\n",
      "Epoch 25/100\n",
      "6090/6090 [==============================] - 1s 130us/sample - loss: 0.5295 - val_loss: 0.5209\n",
      "Epoch 26/100\n",
      "6090/6090 [==============================] - 1s 128us/sample - loss: 0.5257 - val_loss: 0.5170\n",
      "Epoch 27/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.5190 - val_loss: 0.5135\n",
      "Epoch 28/100\n",
      "6090/6090 [==============================] - 1s 132us/sample - loss: 0.5111 - val_loss: 0.5090\n",
      "Epoch 29/100\n",
      "6090/6090 [==============================] - 1s 134us/sample - loss: 0.5135 - val_loss: 0.5059\n",
      "Epoch 30/100\n",
      "6090/6090 [==============================] - 1s 128us/sample - loss: 0.5033 - val_loss: 0.5039\n",
      "Epoch 31/100\n",
      "6090/6090 [==============================] - 1s 126us/sample - loss: 0.4909 - val_loss: 0.5001\n",
      "Epoch 32/100\n",
      "6090/6090 [==============================] - 1s 127us/sample - loss: 0.4957 - val_loss: 0.4982\n",
      "Epoch 33/100\n",
      "6090/6090 [==============================] - 1s 128us/sample - loss: 0.4897 - val_loss: 0.4959\n",
      "Epoch 34/100\n",
      "6090/6090 [==============================] - 1s 128us/sample - loss: 0.4916 - val_loss: 0.4940\n",
      "Epoch 35/100\n",
      "6090/6090 [==============================] - 1s 131us/sample - loss: 0.4843 - val_loss: 0.4921\n",
      "Epoch 36/100\n",
      "6090/6090 [==============================] - 1s 129us/sample - loss: 0.4799 - val_loss: 0.4898\n",
      "Epoch 37/100\n",
      "6090/6090 [==============================] - 1s 129us/sample - loss: 0.4803 - val_loss: 0.4888\n",
      "Epoch 38/100\n",
      "6090/6090 [==============================] - 1s 128us/sample - loss: 0.4722 - val_loss: 0.4864\n",
      "Epoch 39/100\n",
      "6090/6090 [==============================] - 1s 129us/sample - loss: 0.4635 - val_loss: 0.4847\n",
      "Epoch 40/100\n",
      "6090/6090 [==============================] - 1s 129us/sample - loss: 0.4609 - val_loss: 0.4833\n",
      "Epoch 41/100\n",
      "6090/6090 [==============================] - 1s 128us/sample - loss: 0.4624 - val_loss: 0.4818\n",
      "Epoch 42/100\n",
      "6090/6090 [==============================] - 1s 129us/sample - loss: 0.4561 - val_loss: 0.4813\n",
      "Epoch 43/100\n",
      "6090/6090 [==============================] - 1s 129us/sample - loss: 0.4525 - val_loss: 0.4789\n",
      "Epoch 44/100\n",
      "6090/6090 [==============================] - 1s 127us/sample - loss: 0.4499 - val_loss: 0.4774\n",
      "Epoch 45/100\n",
      "6090/6090 [==============================] - 1s 128us/sample - loss: 0.4408 - val_loss: 0.4758\n",
      "Epoch 46/100\n",
      "6090/6090 [==============================] - 1s 128us/sample - loss: 0.4414 - val_loss: 0.4754\n",
      "Epoch 47/100\n",
      "6090/6090 [==============================] - 1s 128us/sample - loss: 0.4412 - val_loss: 0.4744\n",
      "Epoch 48/100\n",
      "6090/6090 [==============================] - 1s 127us/sample - loss: 0.4417 - val_loss: 0.4735\n",
      "Epoch 49/100\n",
      "6090/6090 [==============================] - 1s 127us/sample - loss: 0.4389 - val_loss: 0.4732\n",
      "Epoch 50/100\n",
      "6090/6090 [==============================] - 1s 129us/sample - loss: 0.4299 - val_loss: 0.4723\n",
      "Epoch 51/100\n",
      "6090/6090 [==============================] - 1s 128us/sample - loss: 0.4278 - val_loss: 0.4716\n",
      "Epoch 52/100\n",
      "6090/6090 [==============================] - 1s 132us/sample - loss: 0.4236 - val_loss: 0.4712\n",
      "Epoch 53/100\n",
      "6090/6090 [==============================] - 1s 135us/sample - loss: 0.4232 - val_loss: 0.4692\n",
      "Epoch 54/100\n",
      "6090/6090 [==============================] - 1s 135us/sample - loss: 0.4249 - val_loss: 0.4702\n",
      "Epoch 55/100\n",
      "6090/6090 [==============================] - 1s 148us/sample - loss: 0.4204 - val_loss: 0.4685\n",
      "Epoch 56/100\n",
      "6090/6090 [==============================] - 1s 164us/sample - loss: 0.4119 - val_loss: 0.4681\n",
      "Epoch 57/100\n",
      "6090/6090 [==============================] - 1s 162us/sample - loss: 0.4177 - val_loss: 0.4684\n",
      "Epoch 58/100\n",
      "6090/6090 [==============================] - 1s 159us/sample - loss: 0.4119 - val_loss: 0.4670\n",
      "Epoch 59/100\n",
      "6090/6090 [==============================] - 1s 136us/sample - loss: 0.4130 - val_loss: 0.4666\n",
      "Epoch 60/100\n",
      "6090/6090 [==============================] - 1s 130us/sample - loss: 0.4045 - val_loss: 0.4661\n",
      "Epoch 61/100\n",
      "6090/6090 [==============================] - 1s 130us/sample - loss: 0.4097 - val_loss: 0.4664\n",
      "Epoch 62/100\n",
      "6090/6090 [==============================] - 1s 135us/sample - loss: 0.4017 - val_loss: 0.4663\n",
      "(Took 51.355 sec)\n",
      "Train on 6091 samples, validate on 1522 samples\n",
      "Epoch 1/100\n",
      "6091/6091 [==============================] - 1s 197us/sample - loss: 0.7246 - val_loss: 0.6772\n",
      "Epoch 2/100\n",
      "6091/6091 [==============================] - 1s 134us/sample - loss: 0.7045 - val_loss: 0.6735\n",
      "Epoch 3/100\n",
      "6091/6091 [==============================] - 1s 134us/sample - loss: 0.6916 - val_loss: 0.6704\n",
      "Epoch 4/100\n",
      "6091/6091 [==============================] - 1s 131us/sample - loss: 0.6817 - val_loss: 0.6676\n",
      "Epoch 5/100\n",
      "6091/6091 [==============================] - 1s 129us/sample - loss: 0.6785 - val_loss: 0.6641\n",
      "Epoch 6/100\n",
      "6091/6091 [==============================] - 1s 127us/sample - loss: 0.6715 - val_loss: 0.6598\n",
      "Epoch 7/100\n",
      "6091/6091 [==============================] - 1s 130us/sample - loss: 0.6709 - val_loss: 0.6549\n",
      "Epoch 8/100\n",
      "6091/6091 [==============================] - 1s 130us/sample - loss: 0.6621 - val_loss: 0.6489\n",
      "Epoch 9/100\n",
      "6091/6091 [==============================] - 1s 142us/sample - loss: 0.6537 - val_loss: 0.6406\n",
      "Epoch 10/100\n",
      "6091/6091 [==============================] - 1s 157us/sample - loss: 0.6461 - val_loss: 0.6308\n",
      "Epoch 11/100\n",
      "6091/6091 [==============================] - 1s 151us/sample - loss: 0.6369 - val_loss: 0.6206\n",
      "Epoch 12/100\n",
      "6091/6091 [==============================] - 1s 162us/sample - loss: 0.6278 - val_loss: 0.6097\n",
      "Epoch 13/100\n",
      "6091/6091 [==============================] - 1s 130us/sample - loss: 0.6193 - val_loss: 0.5989\n",
      "Epoch 14/100\n",
      "6091/6091 [==============================] - 1s 130us/sample - loss: 0.6050 - val_loss: 0.5876\n",
      "Epoch 15/100\n",
      "6091/6091 [==============================] - 1s 135us/sample - loss: 0.6018 - val_loss: 0.5770\n",
      "Epoch 16/100\n",
      "6091/6091 [==============================] - 1s 136us/sample - loss: 0.5865 - val_loss: 0.5668\n",
      "Epoch 17/100\n",
      "6091/6091 [==============================] - 1s 135us/sample - loss: 0.5784 - val_loss: 0.5573\n",
      "Epoch 18/100\n",
      "6091/6091 [==============================] - 1s 130us/sample - loss: 0.5686 - val_loss: 0.5490\n",
      "Epoch 19/100\n",
      "6091/6091 [==============================] - 1s 129us/sample - loss: 0.5611 - val_loss: 0.5409\n",
      "Epoch 20/100\n",
      "6091/6091 [==============================] - 1s 132us/sample - loss: 0.5585 - val_loss: 0.5341\n",
      "Epoch 21/100\n",
      "6091/6091 [==============================] - 1s 129us/sample - loss: 0.5484 - val_loss: 0.5278\n",
      "Epoch 22/100\n",
      "6091/6091 [==============================] - 1s 131us/sample - loss: 0.5396 - val_loss: 0.5225\n",
      "Epoch 23/100\n",
      "6091/6091 [==============================] - 1s 130us/sample - loss: 0.5397 - val_loss: 0.5169\n",
      "Epoch 24/100\n",
      "6091/6091 [==============================] - 1s 137us/sample - loss: 0.5326 - val_loss: 0.5127\n",
      "Epoch 25/100\n",
      "6091/6091 [==============================] - 1s 135us/sample - loss: 0.5203 - val_loss: 0.5081\n",
      "Epoch 26/100\n",
      "6091/6091 [==============================] - 1s 132us/sample - loss: 0.5195 - val_loss: 0.5041\n",
      "Epoch 27/100\n",
      "6091/6091 [==============================] - 1s 142us/sample - loss: 0.5189 - val_loss: 0.5016\n",
      "Epoch 28/100\n",
      "6091/6091 [==============================] - 1s 134us/sample - loss: 0.5043 - val_loss: 0.4974\n",
      "Epoch 29/100\n",
      "6091/6091 [==============================] - 1s 135us/sample - loss: 0.5041 - val_loss: 0.4946\n",
      "Epoch 30/100\n",
      "6091/6091 [==============================] - 1s 135us/sample - loss: 0.5050 - val_loss: 0.4921\n",
      "Epoch 31/100\n",
      "6091/6091 [==============================] - 1s 136us/sample - loss: 0.4855 - val_loss: 0.4886\n",
      "Epoch 32/100\n",
      "6091/6091 [==============================] - 1s 128us/sample - loss: 0.4850 - val_loss: 0.4858\n",
      "Epoch 33/100\n",
      "6091/6091 [==============================] - 1s 130us/sample - loss: 0.4851 - val_loss: 0.4835\n",
      "Epoch 34/100\n",
      "6091/6091 [==============================] - 1s 132us/sample - loss: 0.4831 - val_loss: 0.4818\n",
      "Epoch 35/100\n",
      "6091/6091 [==============================] - 1s 130us/sample - loss: 0.4785 - val_loss: 0.4793\n",
      "Epoch 36/100\n",
      "6091/6091 [==============================] - 1s 130us/sample - loss: 0.4744 - val_loss: 0.4774\n",
      "Epoch 37/100\n",
      "6091/6091 [==============================] - 1s 133us/sample - loss: 0.4715 - val_loss: 0.4758\n",
      "Epoch 38/100\n",
      "6091/6091 [==============================] - 1s 128us/sample - loss: 0.4650 - val_loss: 0.4738\n",
      "Epoch 39/100\n",
      "6091/6091 [==============================] - 1s 140us/sample - loss: 0.4649 - val_loss: 0.4723\n",
      "Epoch 40/100\n",
      "6091/6091 [==============================] - 1s 133us/sample - loss: 0.4575 - val_loss: 0.4703\n",
      "Epoch 41/100\n",
      "6091/6091 [==============================] - 1s 127us/sample - loss: 0.4499 - val_loss: 0.4686\n",
      "Epoch 42/100\n",
      "6091/6091 [==============================] - 1s 127us/sample - loss: 0.4607 - val_loss: 0.4674\n",
      "Epoch 43/100\n",
      "6091/6091 [==============================] - 1s 128us/sample - loss: 0.4455 - val_loss: 0.4659\n",
      "Epoch 44/100\n",
      "6091/6091 [==============================] - 1s 129us/sample - loss: 0.4487 - val_loss: 0.4649\n",
      "Epoch 45/100\n",
      "6091/6091 [==============================] - 1s 132us/sample - loss: 0.4419 - val_loss: 0.4635\n",
      "Epoch 46/100\n",
      "6091/6091 [==============================] - 1s 130us/sample - loss: 0.4525 - val_loss: 0.4628\n",
      "Epoch 47/100\n",
      "6091/6091 [==============================] - 1s 128us/sample - loss: 0.4382 - val_loss: 0.4616\n",
      "Epoch 48/100\n",
      "6091/6091 [==============================] - 1s 127us/sample - loss: 0.4355 - val_loss: 0.4606\n",
      "Epoch 49/100\n",
      "6091/6091 [==============================] - 1s 130us/sample - loss: 0.4337 - val_loss: 0.4594\n",
      "Epoch 50/100\n",
      "6091/6091 [==============================] - 1s 131us/sample - loss: 0.4290 - val_loss: 0.4583\n",
      "Epoch 51/100\n",
      "6091/6091 [==============================] - 1s 132us/sample - loss: 0.4260 - val_loss: 0.4576\n",
      "Epoch 52/100\n",
      "6091/6091 [==============================] - 1s 129us/sample - loss: 0.4282 - val_loss: 0.4570\n",
      "Epoch 53/100\n",
      "6091/6091 [==============================] - 1s 130us/sample - loss: 0.4211 - val_loss: 0.4557\n",
      "Epoch 54/100\n",
      "6091/6091 [==============================] - 1s 131us/sample - loss: 0.4242 - val_loss: 0.4553\n",
      "Epoch 55/100\n",
      "6091/6091 [==============================] - 1s 132us/sample - loss: 0.4160 - val_loss: 0.4548\n",
      "Epoch 56/100\n",
      "6091/6091 [==============================] - 1s 130us/sample - loss: 0.4134 - val_loss: 0.4537\n",
      "Epoch 57/100\n",
      "6091/6091 [==============================] - 1s 132us/sample - loss: 0.4121 - val_loss: 0.4530\n",
      "Epoch 58/100\n",
      "6091/6091 [==============================] - 1s 131us/sample - loss: 0.4049 - val_loss: 0.4527\n",
      "Epoch 59/100\n",
      "6091/6091 [==============================] - 1s 131us/sample - loss: 0.4080 - val_loss: 0.4519\n",
      "Epoch 60/100\n",
      "6091/6091 [==============================] - 1s 131us/sample - loss: 0.4066 - val_loss: 0.4518\n",
      "Epoch 61/100\n",
      "6091/6091 [==============================] - 1s 131us/sample - loss: 0.4021 - val_loss: 0.4513\n",
      "Epoch 62/100\n",
      "6091/6091 [==============================] - 1s 126us/sample - loss: 0.3961 - val_loss: 0.4508\n",
      "Epoch 63/100\n",
      "6091/6091 [==============================] - 1s 145us/sample - loss: 0.4031 - val_loss: 0.4504\n",
      "Epoch 64/100\n",
      "6091/6091 [==============================] - 1s 136us/sample - loss: 0.3966 - val_loss: 0.4509\n",
      "Epoch 65/100\n",
      "6091/6091 [==============================] - 1s 131us/sample - loss: 0.3906 - val_loss: 0.4498\n",
      "Epoch 66/100\n",
      "6091/6091 [==============================] - 1s 128us/sample - loss: 0.3935 - val_loss: 0.4501\n",
      "Epoch 67/100\n",
      "6091/6091 [==============================] - 1s 128us/sample - loss: 0.3922 - val_loss: 0.4491\n",
      "Epoch 68/100\n",
      "6091/6091 [==============================] - 1s 126us/sample - loss: 0.3833 - val_loss: 0.4491\n",
      "Epoch 69/100\n",
      "6091/6091 [==============================] - 1s 131us/sample - loss: 0.3932 - val_loss: 0.4490\n",
      "Epoch 70/100\n",
      "6091/6091 [==============================] - 1s 133us/sample - loss: 0.3812 - val_loss: 0.4486\n",
      "Epoch 71/100\n",
      "6091/6091 [==============================] - 1s 132us/sample - loss: 0.3804 - val_loss: 0.4488\n",
      "Epoch 72/100\n",
      "6091/6091 [==============================] - 1s 135us/sample - loss: 0.3817 - val_loss: 0.4486\n",
      "(Took 59.573 sec)\n",
      "Train on 6091 samples, validate on 1522 samples\n",
      "Epoch 1/100\n",
      "6091/6091 [==============================] - 1s 213us/sample - loss: 0.7296 - val_loss: 0.6798\n",
      "Epoch 2/100\n",
      "6091/6091 [==============================] - 1s 138us/sample - loss: 0.7033 - val_loss: 0.6764\n",
      "Epoch 3/100\n",
      "6091/6091 [==============================] - 1s 145us/sample - loss: 0.6886 - val_loss: 0.6712\n",
      "Epoch 4/100\n",
      "6091/6091 [==============================] - ETA: 0s - loss: 0.684 - 1s 142us/sample - loss: 0.6836 - val_loss: 0.6694\n",
      "Epoch 5/100\n",
      "6091/6091 [==============================] - 1s 144us/sample - loss: 0.6766 - val_loss: 0.6667\n",
      "Epoch 6/100\n",
      "6091/6091 [==============================] - 1s 145us/sample - loss: 0.6691 - val_loss: 0.6600\n",
      "Epoch 7/100\n",
      "6091/6091 [==============================] - 1s 157us/sample - loss: 0.6652 - val_loss: 0.6544\n",
      "Epoch 8/100\n",
      "6091/6091 [==============================] - 1s 141us/sample - loss: 0.6571 - val_loss: 0.6500\n",
      "Epoch 9/100\n",
      "6091/6091 [==============================] - 1s 131us/sample - loss: 0.6486 - val_loss: 0.6415\n",
      "Epoch 10/100\n",
      "6091/6091 [==============================] - 1s 133us/sample - loss: 0.6433 - val_loss: 0.6328\n",
      "Epoch 11/100\n",
      "6091/6091 [==============================] - 1s 134us/sample - loss: 0.6406 - val_loss: 0.6244\n",
      "Epoch 12/100\n",
      "6091/6091 [==============================] - 1s 137us/sample - loss: 0.6261 - val_loss: 0.6138\n",
      "Epoch 13/100\n",
      "6091/6091 [==============================] - 1s 136us/sample - loss: 0.6224 - val_loss: 0.6045\n",
      "Epoch 14/100\n",
      "6091/6091 [==============================] - 1s 136us/sample - loss: 0.6059 - val_loss: 0.5940\n",
      "Epoch 15/100\n",
      "6091/6091 [==============================] - 1s 134us/sample - loss: 0.6042 - val_loss: 0.5851\n",
      "Epoch 16/100\n",
      "6091/6091 [==============================] - 1s 128us/sample - loss: 0.5910 - val_loss: 0.5757\n",
      "Epoch 17/100\n",
      "6091/6091 [==============================] - 1s 131us/sample - loss: 0.5842 - val_loss: 0.5676\n",
      "Epoch 18/100\n",
      "6091/6091 [==============================] - 1s 130us/sample - loss: 0.5766 - val_loss: 0.5591\n",
      "Epoch 19/100\n",
      "6091/6091 [==============================] - 1s 129us/sample - loss: 0.5695 - val_loss: 0.5512\n",
      "Epoch 20/100\n",
      "6091/6091 [==============================] - 1s 128us/sample - loss: 0.5516 - val_loss: 0.5436\n",
      "Epoch 21/100\n",
      "6091/6091 [==============================] - 1s 129us/sample - loss: 0.5543 - val_loss: 0.5379\n",
      "Epoch 22/100\n",
      "6091/6091 [==============================] - 1s 130us/sample - loss: 0.5442 - val_loss: 0.5317\n",
      "Epoch 23/100\n",
      "6091/6091 [==============================] - 1s 129us/sample - loss: 0.5325 - val_loss: 0.5253\n",
      "Epoch 24/100\n",
      "6091/6091 [==============================] - 1s 129us/sample - loss: 0.5343 - val_loss: 0.5208\n",
      "Epoch 25/100\n",
      "6091/6091 [==============================] - 1s 129us/sample - loss: 0.5292 - val_loss: 0.5176\n",
      "Epoch 26/100\n",
      "6091/6091 [==============================] - 1s 130us/sample - loss: 0.5276 - val_loss: 0.5129\n",
      "Epoch 27/100\n",
      "6091/6091 [==============================] - 1s 130us/sample - loss: 0.5171 - val_loss: 0.5103\n",
      "Epoch 28/100\n",
      "6091/6091 [==============================] - 1s 129us/sample - loss: 0.5011 - val_loss: 0.5066\n",
      "Epoch 29/100\n",
      "6091/6091 [==============================] - 1s 128us/sample - loss: 0.5126 - val_loss: 0.5032\n",
      "Epoch 30/100\n",
      "6091/6091 [==============================] - 1s 130us/sample - loss: 0.5019 - val_loss: 0.4996\n",
      "Epoch 31/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6091/6091 [==============================] - 1s 130us/sample - loss: 0.4899 - val_loss: 0.4975\n",
      "Epoch 32/100\n",
      "6091/6091 [==============================] - 1s 129us/sample - loss: 0.4967 - val_loss: 0.4948\n",
      "Epoch 33/100\n",
      "6091/6091 [==============================] - 1s 129us/sample - loss: 0.4826 - val_loss: 0.4921\n",
      "Epoch 34/100\n",
      "6091/6091 [==============================] - 1s 129us/sample - loss: 0.4802 - val_loss: 0.4901\n",
      "Epoch 35/100\n",
      "6091/6091 [==============================] - 1s 138us/sample - loss: 0.4866 - val_loss: 0.4882\n",
      "Epoch 36/100\n",
      "6091/6091 [==============================] - 1s 133us/sample - loss: 0.4775 - val_loss: 0.4866\n",
      "Epoch 37/100\n",
      "6091/6091 [==============================] - 1s 130us/sample - loss: 0.4675 - val_loss: 0.4844\n",
      "Epoch 38/100\n",
      "6091/6091 [==============================] - 1s 133us/sample - loss: 0.4641 - val_loss: 0.4826\n",
      "Epoch 39/100\n",
      "6091/6091 [==============================] - 1s 132us/sample - loss: 0.4645 - val_loss: 0.4806\n",
      "Epoch 40/100\n",
      "6091/6091 [==============================] - 1s 132us/sample - loss: 0.4603 - val_loss: 0.4793\n",
      "Epoch 41/100\n",
      "6091/6091 [==============================] - 1s 128us/sample - loss: 0.4583 - val_loss: 0.4784\n",
      "Epoch 42/100\n",
      "6091/6091 [==============================] - 1s 129us/sample - loss: 0.4549 - val_loss: 0.4766\n",
      "Epoch 43/100\n",
      "6091/6091 [==============================] - 1s 128us/sample - loss: 0.4575 - val_loss: 0.4754\n",
      "Epoch 44/100\n",
      "6091/6091 [==============================] - 1s 128us/sample - loss: 0.4535 - val_loss: 0.4745\n",
      "Epoch 45/100\n",
      "6091/6091 [==============================] - 1s 129us/sample - loss: 0.4531 - val_loss: 0.4734\n",
      "Epoch 46/100\n",
      "6091/6091 [==============================] - 1s 128us/sample - loss: 0.4482 - val_loss: 0.4723\n",
      "Epoch 47/100\n",
      "6091/6091 [==============================] - 1s 130us/sample - loss: 0.4429 - val_loss: 0.4717\n",
      "Epoch 48/100\n",
      "6091/6091 [==============================] - 1s 128us/sample - loss: 0.4296 - val_loss: 0.4708\n",
      "Epoch 49/100\n",
      "6091/6091 [==============================] - 1s 128us/sample - loss: 0.4323 - val_loss: 0.4694\n",
      "Epoch 50/100\n",
      "6091/6091 [==============================] - 1s 129us/sample - loss: 0.4267 - val_loss: 0.4687\n",
      "Epoch 51/100\n",
      "6091/6091 [==============================] - 1s 130us/sample - loss: 0.4261 - val_loss: 0.4682\n",
      "Epoch 52/100\n",
      "6091/6091 [==============================] - 1s 129us/sample - loss: 0.4255 - val_loss: 0.4671\n",
      "Epoch 53/100\n",
      "6091/6091 [==============================] - 1s 130us/sample - loss: 0.4200 - val_loss: 0.4667\n",
      "Epoch 54/100\n",
      "6091/6091 [==============================] - 1s 129us/sample - loss: 0.4220 - val_loss: 0.4658\n",
      "Epoch 55/100\n",
      "6091/6091 [==============================] - 1s 128us/sample - loss: 0.4170 - val_loss: 0.4651\n",
      "Epoch 56/100\n",
      "6091/6091 [==============================] - 1s 130us/sample - loss: 0.4205 - val_loss: 0.4646\n",
      "Epoch 57/100\n",
      "6091/6091 [==============================] - 1s 131us/sample - loss: 0.4166 - val_loss: 0.4644\n",
      "Epoch 58/100\n",
      "6091/6091 [==============================] - 1s 129us/sample - loss: 0.4058 - val_loss: 0.4644\n",
      "Epoch 59/100\n",
      "6091/6091 [==============================] - 1s 127us/sample - loss: 0.4104 - val_loss: 0.4638\n",
      "Epoch 60/100\n",
      "6091/6091 [==============================] - 1s 127us/sample - loss: 0.4023 - val_loss: 0.4636\n",
      "Epoch 61/100\n",
      "6091/6091 [==============================] - 1s 129us/sample - loss: 0.4031 - val_loss: 0.4631\n",
      "Epoch 62/100\n",
      "6091/6091 [==============================] - 1s 128us/sample - loss: 0.4025 - val_loss: 0.4632\n",
      "Epoch 63/100\n",
      "6091/6091 [==============================] - 1s 129us/sample - loss: 0.3988 - val_loss: 0.4627\n",
      "Epoch 64/100\n",
      "6091/6091 [==============================] - 1s 128us/sample - loss: 0.3909 - val_loss: 0.4625\n",
      "Epoch 65/100\n",
      "6091/6091 [==============================] - 1s 128us/sample - loss: 0.3953 - val_loss: 0.4623\n",
      "Epoch 66/100\n",
      "6091/6091 [==============================] - 1s 127us/sample - loss: 0.3928 - val_loss: 0.4626\n",
      "Epoch 67/100\n",
      "6091/6091 [==============================] - 1s 129us/sample - loss: 0.3942 - val_loss: 0.4629\n",
      "(Took 55.053 sec)\n",
      "Combined confusion matrix:\n",
      "[[3811.  531.]\n",
      " [1037. 2234.]]\n",
      "(Overall, took 287.626 sec)\n",
      "Accuracy: 79.40% +/- 0.95%\n",
      "Precision for positive class: 78.62% +/- 2.18%\n",
      "Precision for negative class: 80.82% +/- 1.72%\n",
      "Recall for positive class: 87.78% +/- 1.33%\n",
      "Recall for negative class: 68.34% +/- 2.46%\n",
      "F for positive class: 82.93% +/- 1.03%\n",
      "F for negative class: 74.03% +/- 0.79%\n",
      "Mean F score: 78.48% +/- 0.84%\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/100\n",
      "6090/6090 [==============================] - 1s 184us/sample - loss: 0.7321 - val_loss: 0.6678\n",
      "Epoch 2/100\n",
      "6090/6090 [==============================] - 1s 131us/sample - loss: 0.7022 - val_loss: 0.6574\n",
      "Epoch 3/100\n",
      "6090/6090 [==============================] - 1s 128us/sample - loss: 0.6872 - val_loss: 0.6496\n",
      "Epoch 4/100\n",
      "6090/6090 [==============================] - 1s 128us/sample - loss: 0.6656 - val_loss: 0.6407\n",
      "Epoch 5/100\n",
      "6090/6090 [==============================] - 1s 130us/sample - loss: 0.6570 - val_loss: 0.6313\n",
      "Epoch 6/100\n",
      "6090/6090 [==============================] - 1s 131us/sample - loss: 0.6405 - val_loss: 0.6197\n",
      "Epoch 7/100\n",
      "6090/6090 [==============================] - 1s 129us/sample - loss: 0.6336 - val_loss: 0.6074\n",
      "Epoch 8/100\n",
      "6090/6090 [==============================] - 1s 128us/sample - loss: 0.6192 - val_loss: 0.5920\n",
      "Epoch 9/100\n",
      "6090/6090 [==============================] - 1s 131us/sample - loss: 0.5963 - val_loss: 0.5757\n",
      "Epoch 10/100\n",
      "6090/6090 [==============================] - 1s 130us/sample - loss: 0.5794 - val_loss: 0.5585\n",
      "Epoch 11/100\n",
      "6090/6090 [==============================] - 1s 131us/sample - loss: 0.5661 - val_loss: 0.5425\n",
      "Epoch 12/100\n",
      "6090/6090 [==============================] - 1s 130us/sample - loss: 0.5443 - val_loss: 0.5275\n",
      "Epoch 13/100\n",
      "6090/6090 [==============================] - 1s 130us/sample - loss: 0.5291 - val_loss: 0.5142\n",
      "Epoch 14/100\n",
      "6090/6090 [==============================] - 1s 131us/sample - loss: 0.5204 - val_loss: 0.5027\n",
      "Epoch 15/100\n",
      "6090/6090 [==============================] - 1s 131us/sample - loss: 0.5052 - val_loss: 0.4927\n",
      "Epoch 16/100\n",
      "6090/6090 [==============================] - 1s 130us/sample - loss: 0.4943 - val_loss: 0.4845\n",
      "Epoch 17/100\n",
      "6090/6090 [==============================] - 1s 131us/sample - loss: 0.4829 - val_loss: 0.4773\n",
      "Epoch 18/100\n",
      "6090/6090 [==============================] - 1s 129us/sample - loss: 0.4699 - val_loss: 0.4709\n",
      "Epoch 19/100\n",
      "6090/6090 [==============================] - 1s 129us/sample - loss: 0.4600 - val_loss: 0.4658\n",
      "Epoch 20/100\n",
      "6090/6090 [==============================] - 1s 135us/sample - loss: 0.4456 - val_loss: 0.4614\n",
      "Epoch 21/100\n",
      "6090/6090 [==============================] - 1s 142us/sample - loss: 0.4450 - val_loss: 0.4580\n",
      "Epoch 22/100\n",
      "6090/6090 [==============================] - 1s 145us/sample - loss: 0.4320 - val_loss: 0.4560\n",
      "Epoch 23/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.4222 - val_loss: 0.4528\n",
      "Epoch 24/100\n",
      "6090/6090 [==============================] - 1s 135us/sample - loss: 0.4190 - val_loss: 0.4512\n",
      "Epoch 25/100\n",
      "6090/6090 [==============================] - 1s 133us/sample - loss: 0.4121 - val_loss: 0.4498\n",
      "Epoch 26/100\n",
      "6090/6090 [==============================] - 1s 134us/sample - loss: 0.4071 - val_loss: 0.4490\n",
      "Epoch 27/100\n",
      "6090/6090 [==============================] - 1s 127us/sample - loss: 0.4064 - val_loss: 0.4479\n",
      "Epoch 28/100\n",
      "6090/6090 [==============================] - 1s 128us/sample - loss: 0.3939 - val_loss: 0.4476\n",
      "Epoch 29/100\n",
      "6090/6090 [==============================] - 1s 127us/sample - loss: 0.3820 - val_loss: 0.4466\n",
      "Epoch 30/100\n",
      "6090/6090 [==============================] - 1s 129us/sample - loss: 0.3785 - val_loss: 0.4464\n",
      "Epoch 31/100\n",
      "6090/6090 [==============================] - 1s 131us/sample - loss: 0.3793 - val_loss: 0.4467\n",
      "Epoch 32/100\n",
      "6090/6090 [==============================] - 1s 131us/sample - loss: 0.3717 - val_loss: 0.4469\n",
      "(Took 26.745 sec)\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/100\n",
      "6090/6090 [==============================] - 1s 193us/sample - loss: 0.7294 - val_loss: 0.6649\n",
      "Epoch 2/100\n",
      "6090/6090 [==============================] - 1s 140us/sample - loss: 0.7088 - val_loss: 0.6554\n",
      "Epoch 3/100\n",
      "6090/6090 [==============================] - 1s 151us/sample - loss: 0.6862 - val_loss: 0.6502\n",
      "Epoch 4/100\n",
      "6090/6090 [==============================] - 1s 131us/sample - loss: 0.6693 - val_loss: 0.6394\n",
      "Epoch 5/100\n",
      "6090/6090 [==============================] - 1s 148us/sample - loss: 0.6569 - val_loss: 0.6319\n",
      "Epoch 6/100\n",
      "6090/6090 [==============================] - 1s 158us/sample - loss: 0.6442 - val_loss: 0.6194\n",
      "Epoch 7/100\n",
      "6090/6090 [==============================] - 1s 164us/sample - loss: 0.6327 - val_loss: 0.6043\n",
      "Epoch 8/100\n",
      "6090/6090 [==============================] - 1s 151us/sample - loss: 0.6147 - val_loss: 0.5886\n",
      "Epoch 9/100\n",
      "6090/6090 [==============================] - 1s 141us/sample - loss: 0.5979 - val_loss: 0.5729\n",
      "Epoch 10/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.5767 - val_loss: 0.5552\n",
      "Epoch 11/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.5665 - val_loss: 0.5407\n",
      "Epoch 12/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.5449 - val_loss: 0.5256\n",
      "Epoch 13/100\n",
      "6090/6090 [==============================] - 1s 144us/sample - loss: 0.5254 - val_loss: 0.5120\n",
      "Epoch 14/100\n",
      "6090/6090 [==============================] - 1s 134us/sample - loss: 0.5124 - val_loss: 0.5019\n",
      "Epoch 15/100\n",
      "6090/6090 [==============================] - 1s 136us/sample - loss: 0.4994 - val_loss: 0.4922\n",
      "Epoch 16/100\n",
      "6090/6090 [==============================] - 1s 130us/sample - loss: 0.4903 - val_loss: 0.4835\n",
      "Epoch 17/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.4812 - val_loss: 0.4772\n",
      "Epoch 18/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.4677 - val_loss: 0.4715\n",
      "Epoch 19/100\n",
      "6090/6090 [==============================] - 1s 134us/sample - loss: 0.4587 - val_loss: 0.4673\n",
      "Epoch 20/100\n",
      "6090/6090 [==============================] - 1s 128us/sample - loss: 0.4527 - val_loss: 0.4634\n",
      "Epoch 21/100\n",
      "6090/6090 [==============================] - 1s 127us/sample - loss: 0.4331 - val_loss: 0.4598\n",
      "Epoch 22/100\n",
      "6090/6090 [==============================] - 1s 127us/sample - loss: 0.4356 - val_loss: 0.4572\n",
      "Epoch 23/100\n",
      "6090/6090 [==============================] - 1s 126us/sample - loss: 0.4230 - val_loss: 0.4551\n",
      "Epoch 24/100\n",
      "6090/6090 [==============================] - 1s 129us/sample - loss: 0.4179 - val_loss: 0.4533\n",
      "Epoch 25/100\n",
      "6090/6090 [==============================] - 1s 136us/sample - loss: 0.4191 - val_loss: 0.4534\n",
      "Epoch 26/100\n",
      "6090/6090 [==============================] - 1s 133us/sample - loss: 0.4017 - val_loss: 0.4513\n",
      "Epoch 27/100\n",
      "6090/6090 [==============================] - 1s 170us/sample - loss: 0.4035 - val_loss: 0.4508\n",
      "Epoch 28/100\n",
      "6090/6090 [==============================] - 1s 173us/sample - loss: 0.3916 - val_loss: 0.4501\n",
      "Epoch 29/100\n",
      "6090/6090 [==============================] - 1s 169us/sample - loss: 0.3869 - val_loss: 0.4498\n",
      "Epoch 30/100\n",
      "6090/6090 [==============================] - 1s 159us/sample - loss: 0.3840 - val_loss: 0.4496\n",
      "Epoch 31/100\n",
      "6090/6090 [==============================] - 1s 167us/sample - loss: 0.3804 - val_loss: 0.4501\n",
      "Epoch 32/100\n",
      "6090/6090 [==============================] - 1s 131us/sample - loss: 0.3775 - val_loss: 0.4503\n",
      "(Took 29.009 sec)\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/100\n",
      "6090/6090 [==============================] - 1s 212us/sample - loss: 0.7244 - val_loss: 0.6794\n",
      "Epoch 2/100\n",
      "6090/6090 [==============================] - 1s 142us/sample - loss: 0.6914 - val_loss: 0.6711\n",
      "Epoch 3/100\n",
      "6090/6090 [==============================] - 1s 141us/sample - loss: 0.6779 - val_loss: 0.6631\n",
      "Epoch 4/100\n",
      "6090/6090 [==============================] - 1s 150us/sample - loss: 0.6701 - val_loss: 0.6551\n",
      "Epoch 5/100\n",
      "6090/6090 [==============================] - 1s 146us/sample - loss: 0.6599 - val_loss: 0.6470\n",
      "Epoch 6/100\n",
      "6090/6090 [==============================] - 1s 141us/sample - loss: 0.6414 - val_loss: 0.6327\n",
      "Epoch 7/100\n",
      "6090/6090 [==============================] - 1s 135us/sample - loss: 0.6244 - val_loss: 0.6178\n",
      "Epoch 8/100\n",
      "6090/6090 [==============================] - 1s 134us/sample - loss: 0.6111 - val_loss: 0.5996\n",
      "Epoch 9/100\n",
      "6090/6090 [==============================] - 1s 131us/sample - loss: 0.5976 - val_loss: 0.5818\n",
      "Epoch 10/100\n",
      "6090/6090 [==============================] - 1s 129us/sample - loss: 0.5757 - val_loss: 0.5639\n",
      "Epoch 11/100\n",
      "6090/6090 [==============================] - 1s 130us/sample - loss: 0.5582 - val_loss: 0.5470\n",
      "Epoch 12/100\n",
      "6090/6090 [==============================] - 1s 132us/sample - loss: 0.5347 - val_loss: 0.5326\n",
      "Epoch 13/100\n",
      "6090/6090 [==============================] - 1s 133us/sample - loss: 0.5190 - val_loss: 0.5181\n",
      "Epoch 14/100\n",
      "6090/6090 [==============================] - 1s 133us/sample - loss: 0.5093 - val_loss: 0.5094\n",
      "Epoch 15/100\n",
      "6090/6090 [==============================] - 1s 132us/sample - loss: 0.4945 - val_loss: 0.4993\n",
      "Epoch 16/100\n",
      "6090/6090 [==============================] - 1s 132us/sample - loss: 0.4808 - val_loss: 0.4920\n",
      "Epoch 17/100\n",
      "6090/6090 [==============================] - 1s 133us/sample - loss: 0.4721 - val_loss: 0.4841\n",
      "Epoch 18/100\n",
      "6090/6090 [==============================] - 1s 135us/sample - loss: 0.4505 - val_loss: 0.4796\n",
      "Epoch 19/100\n",
      "6090/6090 [==============================] - 1s 133us/sample - loss: 0.4487 - val_loss: 0.4758\n",
      "Epoch 20/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.4391 - val_loss: 0.4706\n",
      "Epoch 21/100\n",
      "6090/6090 [==============================] - 1s 135us/sample - loss: 0.4316 - val_loss: 0.4696\n",
      "Epoch 22/100\n",
      "6090/6090 [==============================] - 1s 130us/sample - loss: 0.4270 - val_loss: 0.4660\n",
      "Epoch 23/100\n",
      "6090/6090 [==============================] - 1s 132us/sample - loss: 0.4184 - val_loss: 0.4674\n",
      "Epoch 24/100\n",
      "6090/6090 [==============================] - ETA: 0s - loss: 0.406 - 1s 131us/sample - loss: 0.4070 - val_loss: 0.4639\n",
      "Epoch 25/100\n",
      "6090/6090 [==============================] - 1s 132us/sample - loss: 0.4084 - val_loss: 0.4616\n",
      "Epoch 26/100\n",
      "6090/6090 [==============================] - 1s 131us/sample - loss: 0.4023 - val_loss: 0.4612\n",
      "Epoch 27/100\n",
      "6090/6090 [==============================] - 1s 132us/sample - loss: 0.3910 - val_loss: 0.4623\n",
      "Epoch 28/100\n",
      "6090/6090 [==============================] - 1s 131us/sample - loss: 0.3848 - val_loss: 0.4609\n",
      "Epoch 29/100\n",
      "6090/6090 [==============================] - 1s 132us/sample - loss: 0.3773 - val_loss: 0.4603\n",
      "Epoch 30/100\n",
      "6090/6090 [==============================] - 1s 135us/sample - loss: 0.3756 - val_loss: 0.4608\n",
      "Epoch 31/100\n",
      "6090/6090 [==============================] - 1s 130us/sample - loss: 0.3684 - val_loss: 0.4600\n",
      "Epoch 32/100\n",
      "6090/6090 [==============================] - 1s 130us/sample - loss: 0.3668 - val_loss: 0.4606\n",
      "Epoch 33/100\n",
      "6090/6090 [==============================] - 1s 145us/sample - loss: 0.3667 - val_loss: 0.4616\n",
      "(Took 28.435 sec)\n",
      "Train on 6091 samples, validate on 1522 samples\n",
      "Epoch 1/100\n",
      "6091/6091 [==============================] - 1s 190us/sample - loss: 0.7395 - val_loss: 0.6694\n",
      "Epoch 2/100\n",
      "6091/6091 [==============================] - 1s 144us/sample - loss: 0.7024 - val_loss: 0.6579\n",
      "Epoch 3/100\n",
      "6091/6091 [==============================] - 1s 142us/sample - loss: 0.6910 - val_loss: 0.6513\n",
      "Epoch 4/100\n",
      "6091/6091 [==============================] - 1s 139us/sample - loss: 0.6704 - val_loss: 0.6411\n",
      "Epoch 5/100\n",
      "6091/6091 [==============================] - 1s 131us/sample - loss: 0.6514 - val_loss: 0.6312\n",
      "Epoch 6/100\n",
      "6091/6091 [==============================] - 1s 130us/sample - loss: 0.6389 - val_loss: 0.6195\n",
      "Epoch 7/100\n",
      "6091/6091 [==============================] - 1s 132us/sample - loss: 0.6323 - val_loss: 0.6071\n",
      "Epoch 8/100\n",
      "6091/6091 [==============================] - 1s 132us/sample - loss: 0.6119 - val_loss: 0.5921\n",
      "Epoch 9/100\n",
      "6091/6091 [==============================] - 1s 135us/sample - loss: 0.5976 - val_loss: 0.5771\n",
      "Epoch 10/100\n",
      "6091/6091 [==============================] - 1s 135us/sample - loss: 0.5783 - val_loss: 0.5607\n",
      "Epoch 11/100\n",
      "6091/6091 [==============================] - 1s 137us/sample - loss: 0.5613 - val_loss: 0.5451\n",
      "Epoch 12/100\n",
      "6091/6091 [==============================] - 1s 130us/sample - loss: 0.5483 - val_loss: 0.5316\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/100\n",
      "6091/6091 [==============================] - 1s 134us/sample - loss: 0.5339 - val_loss: 0.5188\n",
      "Epoch 14/100\n",
      "6091/6091 [==============================] - 1s 141us/sample - loss: 0.5149 - val_loss: 0.5076\n",
      "Epoch 15/100\n",
      "6091/6091 [==============================] - 1s 134us/sample - loss: 0.5051 - val_loss: 0.4978\n",
      "Epoch 16/100\n",
      "6091/6091 [==============================] - 1s 139us/sample - loss: 0.4864 - val_loss: 0.4892\n",
      "Epoch 17/100\n",
      "6091/6091 [==============================] - 1s 131us/sample - loss: 0.4797 - val_loss: 0.4818\n",
      "Epoch 18/100\n",
      "6091/6091 [==============================] - 1s 130us/sample - loss: 0.4734 - val_loss: 0.4761\n",
      "Epoch 19/100\n",
      "6091/6091 [==============================] - 1s 129us/sample - loss: 0.4661 - val_loss: 0.4711\n",
      "Epoch 20/100\n",
      "6091/6091 [==============================] - 1s 135us/sample - loss: 0.4563 - val_loss: 0.4667\n",
      "Epoch 21/100\n",
      "6091/6091 [==============================] - 1s 133us/sample - loss: 0.4451 - val_loss: 0.4631\n",
      "Epoch 22/100\n",
      "6091/6091 [==============================] - 1s 135us/sample - loss: 0.4370 - val_loss: 0.4601\n",
      "Epoch 23/100\n",
      "6091/6091 [==============================] - 1s 140us/sample - loss: 0.4241 - val_loss: 0.4574\n",
      "Epoch 24/100\n",
      "6091/6091 [==============================] - 1s 142us/sample - loss: 0.4178 - val_loss: 0.4552\n",
      "Epoch 25/100\n",
      "6091/6091 [==============================] - 1s 162us/sample - loss: 0.4101 - val_loss: 0.4539\n",
      "Epoch 26/100\n",
      "6091/6091 [==============================] - 1s 168us/sample - loss: 0.4043 - val_loss: 0.4514\n",
      "Epoch 27/100\n",
      "6091/6091 [==============================] - 1s 146us/sample - loss: 0.3967 - val_loss: 0.4502\n",
      "Epoch 28/100\n",
      "6091/6091 [==============================] - 1s 140us/sample - loss: 0.3955 - val_loss: 0.4497\n",
      "Epoch 29/100\n",
      "6091/6091 [==============================] - 1s 151us/sample - loss: 0.3945 - val_loss: 0.4484\n",
      "Epoch 30/100\n",
      "6091/6091 [==============================] - 1s 171us/sample - loss: 0.3874 - val_loss: 0.4486\n",
      "Epoch 31/100\n",
      "6091/6091 [==============================] - 1s 164us/sample - loss: 0.3806 - val_loss: 0.4481\n",
      "Epoch 32/100\n",
      "6091/6091 [==============================] - 1s 136us/sample - loss: 0.3666 - val_loss: 0.4476\n",
      "Epoch 33/100\n",
      "6091/6091 [==============================] - 1s 130us/sample - loss: 0.3741 - val_loss: 0.4483\n",
      "Epoch 34/100\n",
      "6091/6091 [==============================] - 1s 137us/sample - loss: 0.3658 - val_loss: 0.4475\n",
      "Epoch 35/100\n",
      "6091/6091 [==============================] - 1s 147us/sample - loss: 0.3637 - val_loss: 0.4477\n",
      "Epoch 36/100\n",
      "6091/6091 [==============================] - 1s 181us/sample - loss: 0.3555 - val_loss: 0.4482\n",
      "(Took 32.194 sec)\n",
      "Train on 6091 samples, validate on 1522 samples\n",
      "Epoch 1/100\n",
      "6091/6091 [==============================] - 1s 222us/sample - loss: 0.7278 - val_loss: 0.6758\n",
      "Epoch 2/100\n",
      "6091/6091 [==============================] - 1s 142us/sample - loss: 0.6999 - val_loss: 0.6678\n",
      "Epoch 3/100\n",
      "6091/6091 [==============================] - 1s 161us/sample - loss: 0.6861 - val_loss: 0.6600\n",
      "Epoch 4/100\n",
      "6091/6091 [==============================] - 1s 203us/sample - loss: 0.6687 - val_loss: 0.6518\n",
      "Epoch 5/100\n",
      "6091/6091 [==============================] - 1s 182us/sample - loss: 0.6537 - val_loss: 0.6417\n",
      "Epoch 6/100\n",
      "6091/6091 [==============================] - 1s 137us/sample - loss: 0.6464 - val_loss: 0.6300\n",
      "Epoch 7/100\n",
      "6091/6091 [==============================] - 1s 139us/sample - loss: 0.6261 - val_loss: 0.6147\n",
      "Epoch 8/100\n",
      "6091/6091 [==============================] - 1s 141us/sample - loss: 0.6128 - val_loss: 0.5986\n",
      "Epoch 9/100\n",
      "6091/6091 [==============================] - 1s 142us/sample - loss: 0.5925 - val_loss: 0.5804\n",
      "Epoch 10/100\n",
      "6091/6091 [==============================] - 1s 146us/sample - loss: 0.5783 - val_loss: 0.5632\n",
      "Epoch 11/100\n",
      "6091/6091 [==============================] - 1s 150us/sample - loss: 0.5581 - val_loss: 0.5474\n",
      "Epoch 12/100\n",
      "6091/6091 [==============================] - 1s 139us/sample - loss: 0.5407 - val_loss: 0.5316\n",
      "Epoch 13/100\n",
      "6091/6091 [==============================] - 1s 165us/sample - loss: 0.5239 - val_loss: 0.5175\n",
      "Epoch 14/100\n",
      "6091/6091 [==============================] - 1s 141us/sample - loss: 0.5141 - val_loss: 0.5073\n",
      "Epoch 15/100\n",
      "6091/6091 [==============================] - 1s 159us/sample - loss: 0.5003 - val_loss: 0.4978\n",
      "Epoch 16/100\n",
      "6091/6091 [==============================] - 1s 168us/sample - loss: 0.4823 - val_loss: 0.4894\n",
      "Epoch 17/100\n",
      "6091/6091 [==============================] - 1s 159us/sample - loss: 0.4737 - val_loss: 0.4830\n",
      "Epoch 18/100\n",
      "6091/6091 [==============================] - 1s 140us/sample - loss: 0.4587 - val_loss: 0.4774\n",
      "Epoch 19/100\n",
      "6091/6091 [==============================] - 1s 148us/sample - loss: 0.4560 - val_loss: 0.4737\n",
      "Epoch 20/100\n",
      "6091/6091 [==============================] - 1s 154us/sample - loss: 0.4370 - val_loss: 0.4708\n",
      "Epoch 21/100\n",
      "6091/6091 [==============================] - 1s 152us/sample - loss: 0.4322 - val_loss: 0.4662\n",
      "Epoch 22/100\n",
      "6091/6091 [==============================] - 1s 155us/sample - loss: 0.4354 - val_loss: 0.4651\n",
      "Epoch 23/100\n",
      "6091/6091 [==============================] - 1s 171us/sample - loss: 0.4217 - val_loss: 0.4619\n",
      "Epoch 24/100\n",
      "6091/6091 [==============================] - 1s 170us/sample - loss: 0.4182 - val_loss: 0.4602\n",
      "Epoch 25/100\n",
      "6091/6091 [==============================] - 1s 140us/sample - loss: 0.4067 - val_loss: 0.4593\n",
      "Epoch 26/100\n",
      "6091/6091 [==============================] - 1s 133us/sample - loss: 0.4035 - val_loss: 0.4582\n",
      "Epoch 27/100\n",
      "6091/6091 [==============================] - 1s 134us/sample - loss: 0.4019 - val_loss: 0.4578\n",
      "Epoch 28/100\n",
      "6091/6091 [==============================] - 1s 134us/sample - loss: 0.3952 - val_loss: 0.4569\n",
      "Epoch 29/100\n",
      "6091/6091 [==============================] - 1s 136us/sample - loss: 0.3931 - val_loss: 0.4566\n",
      "Epoch 30/100\n",
      "6091/6091 [==============================] - 1s 133us/sample - loss: 0.3811 - val_loss: 0.4575\n",
      "Epoch 31/100\n",
      "6091/6091 [==============================] - 1s 129us/sample - loss: 0.3778 - val_loss: 0.4571\n",
      "(Took 29.810 sec)\n",
      "Combined confusion matrix:\n",
      "[[3820.  522.]\n",
      " [1018. 2253.]]\n",
      "(Overall, took 146.543 sec)\n",
      "Accuracy: 79.77% +/- 0.95%\n",
      "Precision for positive class: 78.96% +/- 1.93%\n",
      "Precision for negative class: 81.19% +/- 0.81%\n",
      "Recall for positive class: 87.97% +/- 0.77%\n",
      "Recall for negative class: 68.89% +/- 2.56%\n",
      "F for positive class: 83.22% +/- 0.99%\n",
      "F for negative class: 74.52% +/- 1.30%\n",
      "Mean F score: 78.87% +/- 0.95%\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/100\n",
      "6090/6090 [==============================] - 2s 292us/sample - loss: 0.7204 - val_loss: 0.6444\n",
      "Epoch 2/100\n",
      "6090/6090 [==============================] - 1s 134us/sample - loss: 0.6617 - val_loss: 0.6130\n",
      "Epoch 3/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.6200 - val_loss: 0.5804\n",
      "Epoch 4/100\n",
      "6090/6090 [==============================] - 1s 119us/sample - loss: 0.5854 - val_loss: 0.5480\n",
      "Epoch 5/100\n",
      "6090/6090 [==============================] - 1s 133us/sample - loss: 0.5441 - val_loss: 0.5179\n",
      "Epoch 6/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.5154 - val_loss: 0.4938\n",
      "Epoch 7/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.4961 - val_loss: 0.4792\n",
      "Epoch 8/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.4715 - val_loss: 0.4676\n",
      "Epoch 9/100\n",
      "6090/6090 [==============================] - 1s 116us/sample - loss: 0.4547 - val_loss: 0.4601\n",
      "Epoch 10/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.4328 - val_loss: 0.4553\n",
      "Epoch 11/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.4277 - val_loss: 0.4526\n",
      "Epoch 12/100\n",
      "6090/6090 [==============================] - 1s 121us/sample - loss: 0.4169 - val_loss: 0.4516\n",
      "Epoch 13/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.4084 - val_loss: 0.4510\n",
      "Epoch 14/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.3962 - val_loss: 0.4516\n",
      "Epoch 15/100\n",
      "6090/6090 [==============================] - 1s 117us/sample - loss: 0.3920 - val_loss: 0.4519\n",
      "(Took 12.346 sec)\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/100\n",
      "6090/6090 [==============================] - 1s 192us/sample - loss: 0.7383 - val_loss: 0.6411\n",
      "Epoch 2/100\n",
      "6090/6090 [==============================] - 1s 120us/sample - loss: 0.6702 - val_loss: 0.6070\n",
      "Epoch 3/100\n",
      "6090/6090 [==============================] - 1s 123us/sample - loss: 0.6249 - val_loss: 0.5765\n",
      "Epoch 4/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.5887 - val_loss: 0.5471\n",
      "Epoch 5/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.5469 - val_loss: 0.5201\n",
      "Epoch 6/100\n",
      "6090/6090 [==============================] - ETA: 0s - loss: 0.520 - 1s 124us/sample - loss: 0.5190 - val_loss: 0.4968\n",
      "Epoch 7/100\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.4953 - val_loss: 0.4805\n",
      "Epoch 8/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.4784 - val_loss: 0.4692\n",
      "Epoch 9/100\n",
      "6090/6090 [==============================] - 1s 118us/sample - loss: 0.4618 - val_loss: 0.4613\n",
      "Epoch 10/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.4500 - val_loss: 0.4564\n",
      "Epoch 11/100\n",
      "6090/6090 [==============================] - 1s 117us/sample - loss: 0.4369 - val_loss: 0.4523\n",
      "Epoch 12/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.4268 - val_loss: 0.4499\n",
      "Epoch 13/100\n",
      "6090/6090 [==============================] - 1s 117us/sample - loss: 0.4148 - val_loss: 0.4485\n",
      "Epoch 14/100\n",
      "6090/6090 [==============================] - 1s 122us/sample - loss: 0.4033 - val_loss: 0.4477\n",
      "Epoch 15/100\n",
      "6090/6090 [==============================] - 1s 118us/sample - loss: 0.3983 - val_loss: 0.4473\n",
      "Epoch 16/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.3939 - val_loss: 0.4479\n",
      "Epoch 17/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.3834 - val_loss: 0.4499\n",
      "(Took 12.884 sec)\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/100\n",
      "6090/6090 [==============================] - 2s 256us/sample - loss: 0.7210 - val_loss: 0.6412\n",
      "Epoch 2/100\n",
      "6090/6090 [==============================] - 1s 116us/sample - loss: 0.6571 - val_loss: 0.6068\n",
      "Epoch 3/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.6186 - val_loss: 0.5780\n",
      "Epoch 4/100\n",
      "6090/6090 [==============================] - 1s 116us/sample - loss: 0.5777 - val_loss: 0.5498\n",
      "Epoch 5/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.5528 - val_loss: 0.5248\n",
      "Epoch 6/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.5143 - val_loss: 0.5044\n",
      "Epoch 7/100\n",
      "6090/6090 [==============================] - 1s 121us/sample - loss: 0.4966 - val_loss: 0.4903\n",
      "Epoch 8/100\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.4744 - val_loss: 0.4778\n",
      "Epoch 9/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4660 - val_loss: 0.4698\n",
      "Epoch 10/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.4484 - val_loss: 0.4654\n",
      "Epoch 11/100\n",
      "6090/6090 [==============================] - 1s 121us/sample - loss: 0.4406 - val_loss: 0.4614\n",
      "Epoch 12/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.4332 - val_loss: 0.4598\n",
      "Epoch 13/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4137 - val_loss: 0.4599\n",
      "Epoch 14/100\n",
      "6090/6090 [==============================] - 1s 131us/sample - loss: 0.4101 - val_loss: 0.4632\n",
      "(Took 11.243 sec)\n",
      "Train on 6091 samples, validate on 1522 samples\n",
      "Epoch 1/100\n",
      "6091/6091 [==============================] - 2s 282us/sample - loss: 0.7185 - val_loss: 0.6390\n",
      "Epoch 2/100\n",
      "6091/6091 [==============================] - 1s 129us/sample - loss: 0.6571 - val_loss: 0.6039\n",
      "Epoch 3/100\n",
      "6091/6091 [==============================] - 1s 128us/sample - loss: 0.6181 - val_loss: 0.5717\n",
      "Epoch 4/100\n",
      "6091/6091 [==============================] - 1s 128us/sample - loss: 0.5804 - val_loss: 0.5397\n",
      "Epoch 5/100\n",
      "6091/6091 [==============================] - 1s 128us/sample - loss: 0.5503 - val_loss: 0.5119\n",
      "Epoch 6/100\n",
      "6091/6091 [==============================] - 1s 128us/sample - loss: 0.5171 - val_loss: 0.4900\n",
      "Epoch 7/100\n",
      "6091/6091 [==============================] - 1s 117us/sample - loss: 0.5001 - val_loss: 0.4747\n",
      "Epoch 8/100\n",
      "6091/6091 [==============================] - 1s 118us/sample - loss: 0.4716 - val_loss: 0.4634\n",
      "Epoch 9/100\n",
      "6091/6091 [==============================] - 1s 130us/sample - loss: 0.4601 - val_loss: 0.4561\n",
      "Epoch 10/100\n",
      "6091/6091 [==============================] - 1s 116us/sample - loss: 0.4443 - val_loss: 0.4508\n",
      "Epoch 11/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.4317 - val_loss: 0.4474\n",
      "Epoch 12/100\n",
      "6091/6091 [==============================] - 1s 117us/sample - loss: 0.4223 - val_loss: 0.4457\n",
      "Epoch 13/100\n",
      "6091/6091 [==============================] - 1s 114us/sample - loss: 0.4116 - val_loss: 0.4449\n",
      "Epoch 14/100\n",
      "6091/6091 [==============================] - 1s 110us/sample - loss: 0.4076 - val_loss: 0.4450\n",
      "Epoch 15/100\n",
      "6091/6091 [==============================] - 1s 115us/sample - loss: 0.3995 - val_loss: 0.4443\n",
      "Epoch 16/100\n",
      "6091/6091 [==============================] - 1s 119us/sample - loss: 0.3981 - val_loss: 0.4451\n",
      "Epoch 17/100\n",
      "6091/6091 [==============================] - 1s 132us/sample - loss: 0.3935 - val_loss: 0.4468\n",
      "(Took 17.086 sec)\n",
      "Train on 6091 samples, validate on 1522 samples\n",
      "Epoch 1/100\n",
      "6091/6091 [==============================] - 2s 267us/sample - loss: 0.7242 - val_loss: 0.6416\n",
      "Epoch 2/100\n",
      "6091/6091 [==============================] - 1s 160us/sample - loss: 0.6580 - val_loss: 0.6060\n",
      "Epoch 3/100\n",
      "6091/6091 [==============================] - 1s 132us/sample - loss: 0.6132 - val_loss: 0.5780\n",
      "Epoch 4/100\n",
      "6091/6091 [==============================] - 1s 132us/sample - loss: 0.5808 - val_loss: 0.5491\n",
      "Epoch 5/100\n",
      "6091/6091 [==============================] - 1s 121us/sample - loss: 0.5431 - val_loss: 0.5244\n",
      "Epoch 6/100\n",
      "6091/6091 [==============================] - 1s 169us/sample - loss: 0.5214 - val_loss: 0.5041\n",
      "Epoch 7/100\n",
      "6091/6091 [==============================] - 1s 163us/sample - loss: 0.4932 - val_loss: 0.4896\n",
      "Epoch 8/100\n",
      "6091/6091 [==============================] - 1s 126us/sample - loss: 0.4773 - val_loss: 0.4792\n",
      "Epoch 9/100\n",
      "6091/6091 [==============================] - 1s 136us/sample - loss: 0.4607 - val_loss: 0.4713\n",
      "Epoch 10/100\n",
      "6091/6091 [==============================] - 1s 137us/sample - loss: 0.4493 - val_loss: 0.4656\n",
      "Epoch 11/100\n",
      "6091/6091 [==============================] - 1s 119us/sample - loss: 0.4321 - val_loss: 0.4619\n",
      "Epoch 12/100\n",
      "6091/6091 [==============================] - 1s 140us/sample - loss: 0.4218 - val_loss: 0.4598\n",
      "Epoch 13/100\n",
      "6091/6091 [==============================] - 1s 157us/sample - loss: 0.4147 - val_loss: 0.4598\n",
      "Epoch 14/100\n",
      "6091/6091 [==============================] - 1s 120us/sample - loss: 0.4025 - val_loss: 0.4612\n",
      "Epoch 15/100\n",
      "6091/6091 [==============================] - 1s 111us/sample - loss: 0.3913 - val_loss: 0.4585\n",
      "Epoch 16/100\n",
      "6091/6091 [==============================] - 1s 120us/sample - loss: 0.3922 - val_loss: 0.4588\n",
      "Epoch 17/100\n",
      "6091/6091 [==============================] - 1s 113us/sample - loss: 0.3888 - val_loss: 0.4593\n",
      "(Took 18.475 sec)\n",
      "Combined confusion matrix:\n",
      "[[3849.  493.]\n",
      " [1024. 2247.]]\n",
      "(Overall, took 72.264 sec)\n",
      "Accuracy: 80.07% +/- 0.77%\n",
      "Precision for positive class: 79.01% +/- 2.13%\n",
      "Precision for negative class: 82.05% +/- 2.24%\n",
      "Recall for positive class: 88.66% +/- 1.81%\n",
      "Recall for negative class: 68.71% +/- 3.50%\n",
      "F for positive class: 83.53% +/- 0.66%\n",
      "F for negative class: 74.74% +/- 1.53%\n",
      "Mean F score: 79.14% +/- 0.90%\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/100\n",
      "6090/6090 [==============================] - 1s 186us/sample - loss: 0.7263 - val_loss: 0.6744\n",
      "Epoch 2/100\n",
      "6090/6090 [==============================] - 1s 126us/sample - loss: 0.7105 - val_loss: 0.6700\n",
      "Epoch 3/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.6928 - val_loss: 0.6676\n",
      "Epoch 4/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.6879 - val_loss: 0.6643\n",
      "Epoch 5/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.6792 - val_loss: 0.6601\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.6724 - val_loss: 0.6569\n",
      "Epoch 7/100\n",
      "6090/6090 [==============================] - 1s 101us/sample - loss: 0.6667 - val_loss: 0.6515\n",
      "Epoch 8/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.6618 - val_loss: 0.6454\n",
      "Epoch 9/100\n",
      "6090/6090 [==============================] - 1s 123us/sample - loss: 0.6529 - val_loss: 0.6381\n",
      "Epoch 10/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.6461 - val_loss: 0.6295\n",
      "Epoch 11/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.6370 - val_loss: 0.6196\n",
      "Epoch 12/100\n",
      "6090/6090 [==============================] - 1s 101us/sample - loss: 0.6272 - val_loss: 0.6084\n",
      "Epoch 13/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.6198 - val_loss: 0.5976\n",
      "Epoch 14/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.6060 - val_loss: 0.5857\n",
      "Epoch 15/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.5920 - val_loss: 0.5751\n",
      "Epoch 16/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.5830 - val_loss: 0.5630\n",
      "Epoch 17/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.5733 - val_loss: 0.5531\n",
      "Epoch 18/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.5670 - val_loss: 0.5441\n",
      "Epoch 19/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.5501 - val_loss: 0.5354\n",
      "Epoch 20/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.5458 - val_loss: 0.5280\n",
      "Epoch 21/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.5343 - val_loss: 0.5209\n",
      "Epoch 22/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.5280 - val_loss: 0.5146\n",
      "Epoch 23/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.5192 - val_loss: 0.5094\n",
      "Epoch 24/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.5112 - val_loss: 0.5049\n",
      "Epoch 25/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.5090 - val_loss: 0.5011\n",
      "Epoch 26/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.5015 - val_loss: 0.4976\n",
      "Epoch 27/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.4858 - val_loss: 0.4943\n",
      "Epoch 28/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.4849 - val_loss: 0.4917\n",
      "Epoch 29/100\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.4835 - val_loss: 0.4895\n",
      "Epoch 30/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.4726 - val_loss: 0.4875\n",
      "Epoch 31/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4734 - val_loss: 0.4860\n",
      "Epoch 32/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.4570 - val_loss: 0.4848\n",
      "Epoch 33/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.4527 - val_loss: 0.4833\n",
      "Epoch 34/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.4587 - val_loss: 0.4827\n",
      "Epoch 35/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4528 - val_loss: 0.4826\n",
      "Epoch 36/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.4527 - val_loss: 0.4815\n",
      "Epoch 37/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.4486 - val_loss: 0.4817\n",
      "Epoch 38/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.4391 - val_loss: 0.4811\n",
      "Epoch 39/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.4412 - val_loss: 0.4809\n",
      "Epoch 40/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.4364 - val_loss: 0.4810\n",
      "Epoch 41/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.4358 - val_loss: 0.4810\n",
      "(Took 27.747 sec)\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/100\n",
      "6090/6090 [==============================] - 1s 157us/sample - loss: 0.7451 - val_loss: 0.6723\n",
      "Epoch 2/100\n",
      "6090/6090 [==============================] - 1s 121us/sample - loss: 0.7012 - val_loss: 0.6683\n",
      "Epoch 3/100\n",
      "6090/6090 [==============================] - 1s 128us/sample - loss: 0.6975 - val_loss: 0.6644\n",
      "Epoch 4/100\n",
      "6090/6090 [==============================] - 1s 120us/sample - loss: 0.6852 - val_loss: 0.6620\n",
      "Epoch 5/100\n",
      "6090/6090 [==============================] - 1s 121us/sample - loss: 0.6789 - val_loss: 0.6584\n",
      "Epoch 6/100\n",
      "6090/6090 [==============================] - 1s 124us/sample - loss: 0.6696 - val_loss: 0.6555\n",
      "Epoch 7/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.6709 - val_loss: 0.6509\n",
      "Epoch 8/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.6639 - val_loss: 0.6456\n",
      "Epoch 9/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.6555 - val_loss: 0.6387\n",
      "Epoch 10/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.6469 - val_loss: 0.6277\n",
      "Epoch 11/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.6404 - val_loss: 0.6184\n",
      "Epoch 12/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.6280 - val_loss: 0.6066\n",
      "Epoch 13/100\n",
      "6090/6090 [==============================] - 1s 101us/sample - loss: 0.6212 - val_loss: 0.5947\n",
      "Epoch 14/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.6147 - val_loss: 0.5837\n",
      "Epoch 15/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.5983 - val_loss: 0.5715\n",
      "Epoch 16/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.5940 - val_loss: 0.5600\n",
      "Epoch 17/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.5755 - val_loss: 0.5487\n",
      "Epoch 18/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.5664 - val_loss: 0.5390\n",
      "Epoch 19/100\n",
      "6090/6090 [==============================] - 1s 101us/sample - loss: 0.5589 - val_loss: 0.5288\n",
      "Epoch 20/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.5541 - val_loss: 0.5219\n",
      "Epoch 21/100\n",
      "6090/6090 [==============================] - 1s 101us/sample - loss: 0.5340 - val_loss: 0.5134\n",
      "Epoch 22/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.5328 - val_loss: 0.5069\n",
      "Epoch 23/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.5225 - val_loss: 0.5001\n",
      "Epoch 24/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.5146 - val_loss: 0.4948\n",
      "Epoch 25/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.5001 - val_loss: 0.4899\n",
      "Epoch 26/100\n",
      "6090/6090 [==============================] - 1s 101us/sample - loss: 0.5012 - val_loss: 0.4857\n",
      "Epoch 27/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.4894 - val_loss: 0.4821\n",
      "Epoch 28/100\n",
      "6090/6090 [==============================] - 1s 101us/sample - loss: 0.4894 - val_loss: 0.4791\n",
      "Epoch 29/100\n",
      "6090/6090 [==============================] - 1s 101us/sample - loss: 0.4858 - val_loss: 0.4773\n",
      "Epoch 30/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.4759 - val_loss: 0.4744\n",
      "Epoch 31/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4764 - val_loss: 0.4725\n",
      "Epoch 32/100\n",
      "6090/6090 [==============================] - 1s 125us/sample - loss: 0.4642 - val_loss: 0.4706\n",
      "Epoch 33/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.4702 - val_loss: 0.4696\n",
      "Epoch 34/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.4665 - val_loss: 0.4690\n",
      "Epoch 35/100\n",
      "6090/6090 [==============================] - 1s 120us/sample - loss: 0.4567 - val_loss: 0.4676\n",
      "Epoch 36/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.4534 - val_loss: 0.4664\n",
      "Epoch 37/100\n",
      "6090/6090 [==============================] - 1s 101us/sample - loss: 0.4542 - val_loss: 0.4666\n",
      "Epoch 38/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4608 - val_loss: 0.4655\n",
      "Epoch 39/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.4393 - val_loss: 0.4651\n",
      "Epoch 40/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.4432 - val_loss: 0.4647\n",
      "Epoch 41/100\n",
      "6090/6090 [==============================] - 1s 101us/sample - loss: 0.4361 - val_loss: 0.4644\n",
      "Epoch 42/100\n",
      "6090/6090 [==============================] - 1s 101us/sample - loss: 0.4403 - val_loss: 0.4642\n",
      "Epoch 43/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.4330 - val_loss: 0.4642\n",
      "Epoch 44/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.4225 - val_loss: 0.4641\n",
      "Epoch 45/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.4320 - val_loss: 0.4641\n",
      "Epoch 46/100\n",
      "6090/6090 [==============================] - 1s 99us/sample - loss: 0.4212 - val_loss: 0.4642\n",
      "Epoch 47/100\n",
      "6090/6090 [==============================] - 1s 121us/sample - loss: 0.4209 - val_loss: 0.4645\n",
      "(Took 31.464 sec)\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/100\n",
      "6090/6090 [==============================] - 1s 183us/sample - loss: 0.7211 - val_loss: 0.6808\n",
      "Epoch 2/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.6999 - val_loss: 0.6773\n",
      "Epoch 3/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.6872 - val_loss: 0.6727\n",
      "Epoch 4/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.6788 - val_loss: 0.6712\n",
      "Epoch 5/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.6752 - val_loss: 0.6654\n",
      "Epoch 6/100\n",
      "6090/6090 [==============================] - 1s 116us/sample - loss: 0.6690 - val_loss: 0.6625\n",
      "Epoch 7/100\n",
      "6090/6090 [==============================] - 1s 133us/sample - loss: 0.6610 - val_loss: 0.6559\n",
      "Epoch 8/100\n",
      "6090/6090 [==============================] - 1s 134us/sample - loss: 0.6525 - val_loss: 0.6519\n",
      "Epoch 9/100\n",
      "6090/6090 [==============================] - 1s 123us/sample - loss: 0.6465 - val_loss: 0.6416\n",
      "Epoch 10/100\n",
      "6090/6090 [==============================] - 1s 121us/sample - loss: 0.6409 - val_loss: 0.6321\n",
      "Epoch 11/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.6258 - val_loss: 0.6241\n",
      "Epoch 12/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.6185 - val_loss: 0.6131\n",
      "Epoch 13/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.6135 - val_loss: 0.6013\n",
      "Epoch 14/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.5992 - val_loss: 0.5908\n",
      "Epoch 15/100\n",
      "6090/6090 [==============================] - 1s 99us/sample - loss: 0.5900 - val_loss: 0.5817\n",
      "Epoch 16/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.5800 - val_loss: 0.5718\n",
      "Epoch 17/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.5705 - val_loss: 0.5619\n",
      "Epoch 18/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.5580 - val_loss: 0.5519\n",
      "Epoch 19/100\n",
      "6090/6090 [==============================] - 1s 99us/sample - loss: 0.5528 - val_loss: 0.5440\n",
      "Epoch 20/100\n",
      "6090/6090 [==============================] - 1s 99us/sample - loss: 0.5416 - val_loss: 0.5381\n",
      "Epoch 21/100\n",
      "6090/6090 [==============================] - 1s 101us/sample - loss: 0.5276 - val_loss: 0.5301\n",
      "Epoch 22/100\n",
      "6090/6090 [==============================] - 1s 99us/sample - loss: 0.5225 - val_loss: 0.5261\n",
      "Epoch 23/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.5148 - val_loss: 0.5184\n",
      "Epoch 24/100\n",
      "6090/6090 [==============================] - 1s 101us/sample - loss: 0.5097 - val_loss: 0.5144\n",
      "Epoch 25/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.5009 - val_loss: 0.5116\n",
      "Epoch 26/100\n",
      "6090/6090 [==============================] - 1s 99us/sample - loss: 0.4986 - val_loss: 0.5078\n",
      "Epoch 27/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.4889 - val_loss: 0.5037\n",
      "Epoch 28/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.4914 - val_loss: 0.5019\n",
      "Epoch 29/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.4795 - val_loss: 0.5002\n",
      "Epoch 30/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.4734 - val_loss: 0.4978\n",
      "Epoch 31/100\n",
      "6090/6090 [==============================] - 1s 99us/sample - loss: 0.4669 - val_loss: 0.4957\n",
      "Epoch 32/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.4710 - val_loss: 0.4941\n",
      "Epoch 33/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.4694 - val_loss: 0.4938\n",
      "Epoch 34/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.4641 - val_loss: 0.4918\n",
      "Epoch 35/100\n",
      "6090/6090 [==============================] - 1s 101us/sample - loss: 0.4555 - val_loss: 0.4912\n",
      "Epoch 36/100\n",
      "6090/6090 [==============================] - 1s 99us/sample - loss: 0.4499 - val_loss: 0.4911\n",
      "Epoch 37/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.4458 - val_loss: 0.4893\n",
      "Epoch 38/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.4448 - val_loss: 0.4897\n",
      "Epoch 39/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.4346 - val_loss: 0.4882\n",
      "Epoch 40/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.4346 - val_loss: 0.4877\n",
      "Epoch 41/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.4308 - val_loss: 0.4885\n",
      "Epoch 42/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4362 - val_loss: 0.4888\n",
      "(Took 28.180 sec)\n",
      "Train on 6091 samples, validate on 1522 samples\n",
      "Epoch 1/100\n",
      "6091/6091 [==============================] - 1s 157us/sample - loss: 0.7351 - val_loss: 0.6748\n",
      "Epoch 2/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.7141 - val_loss: 0.6694\n",
      "Epoch 3/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.6989 - val_loss: 0.6658\n",
      "Epoch 4/100\n",
      "6091/6091 [==============================] - 1s 100us/sample - loss: 0.6892 - val_loss: 0.6619\n",
      "Epoch 5/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.6802 - val_loss: 0.6590\n",
      "Epoch 6/100\n",
      "6091/6091 [==============================] - 1s 105us/sample - loss: 0.6724 - val_loss: 0.6560\n",
      "Epoch 7/100\n",
      "6091/6091 [==============================] - 1s 104us/sample - loss: 0.6669 - val_loss: 0.6501\n",
      "Epoch 8/100\n",
      "6091/6091 [==============================] - 1s 101us/sample - loss: 0.6604 - val_loss: 0.6447\n",
      "Epoch 9/100\n",
      "6091/6091 [==============================] - 1s 104us/sample - loss: 0.6578 - val_loss: 0.6390\n",
      "Epoch 10/100\n",
      "6091/6091 [==============================] - 1s 98us/sample - loss: 0.6453 - val_loss: 0.6304\n",
      "Epoch 11/100\n",
      "6091/6091 [==============================] - 1s 100us/sample - loss: 0.6325 - val_loss: 0.6212\n",
      "Epoch 12/100\n",
      "6091/6091 [==============================] - 1s 100us/sample - loss: 0.6307 - val_loss: 0.6106\n",
      "Epoch 13/100\n",
      "6091/6091 [==============================] - 1s 104us/sample - loss: 0.6195 - val_loss: 0.6003\n",
      "Epoch 14/100\n",
      "6091/6091 [==============================] - 1s 99us/sample - loss: 0.6024 - val_loss: 0.5884\n",
      "Epoch 15/100\n",
      "6091/6091 [==============================] - 1s 99us/sample - loss: 0.6066 - val_loss: 0.5783\n",
      "Epoch 16/100\n",
      "6091/6091 [==============================] - 1s 101us/sample - loss: 0.5891 - val_loss: 0.5678\n",
      "Epoch 17/100\n",
      "6091/6091 [==============================] - 1s 99us/sample - loss: 0.5802 - val_loss: 0.5570\n",
      "Epoch 18/100\n",
      "6091/6091 [==============================] - 1s 99us/sample - loss: 0.5657 - val_loss: 0.5475\n",
      "Epoch 19/100\n",
      "6091/6091 [==============================] - 1s 99us/sample - loss: 0.5639 - val_loss: 0.5384\n",
      "Epoch 20/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.5471 - val_loss: 0.5296\n",
      "Epoch 21/100\n",
      "6091/6091 [==============================] - 1s 100us/sample - loss: 0.5472 - val_loss: 0.5229\n",
      "Epoch 22/100\n",
      "6091/6091 [==============================] - 1s 99us/sample - loss: 0.5321 - val_loss: 0.5150\n",
      "Epoch 23/100\n",
      "6091/6091 [==============================] - 1s 99us/sample - loss: 0.5241 - val_loss: 0.5083\n",
      "Epoch 24/100\n",
      "6091/6091 [==============================] - 1s 100us/sample - loss: 0.5157 - val_loss: 0.5027\n",
      "Epoch 25/100\n",
      "6091/6091 [==============================] - 1s 99us/sample - loss: 0.5130 - val_loss: 0.4978\n",
      "Epoch 26/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6091/6091 [==============================] - 1s 99us/sample - loss: 0.5093 - val_loss: 0.4938\n",
      "Epoch 27/100\n",
      "6091/6091 [==============================] - 1s 97us/sample - loss: 0.4968 - val_loss: 0.4897\n",
      "Epoch 28/100\n",
      "6091/6091 [==============================] - 1s 98us/sample - loss: 0.4980 - val_loss: 0.4869\n",
      "Epoch 29/100\n",
      "6091/6091 [==============================] - 1s 98us/sample - loss: 0.4895 - val_loss: 0.4836\n",
      "Epoch 30/100\n",
      "6091/6091 [==============================] - 1s 98us/sample - loss: 0.4813 - val_loss: 0.4807\n",
      "Epoch 31/100\n",
      "6091/6091 [==============================] - 1s 98us/sample - loss: 0.4763 - val_loss: 0.4782\n",
      "Epoch 32/100\n",
      "6091/6091 [==============================] - 1s 99us/sample - loss: 0.4682 - val_loss: 0.4759\n",
      "Epoch 33/100\n",
      "6091/6091 [==============================] - 1s 99us/sample - loss: 0.4723 - val_loss: 0.4745\n",
      "Epoch 34/100\n",
      "6091/6091 [==============================] - 1s 98us/sample - loss: 0.4712 - val_loss: 0.4728\n",
      "Epoch 35/100\n",
      "6091/6091 [==============================] - 1s 100us/sample - loss: 0.4651 - val_loss: 0.4716\n",
      "Epoch 36/100\n",
      "6091/6091 [==============================] - 1s 98us/sample - loss: 0.4618 - val_loss: 0.4708\n",
      "Epoch 37/100\n",
      "6091/6091 [==============================] - 1s 98us/sample - loss: 0.4515 - val_loss: 0.4693\n",
      "Epoch 38/100\n",
      "6091/6091 [==============================] - 1s 97us/sample - loss: 0.4572 - val_loss: 0.4687\n",
      "Epoch 39/100\n",
      "6091/6091 [==============================] - 1s 100us/sample - loss: 0.4493 - val_loss: 0.4688\n",
      "Epoch 40/100\n",
      "6091/6091 [==============================] - 1s 105us/sample - loss: 0.4490 - val_loss: 0.4678\n",
      "Epoch 41/100\n",
      "6091/6091 [==============================] - 1s 103us/sample - loss: 0.4386 - val_loss: 0.4665\n",
      "Epoch 42/100\n",
      "6091/6091 [==============================] - 1s 105us/sample - loss: 0.4407 - val_loss: 0.4658\n",
      "Epoch 43/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.4376 - val_loss: 0.4657\n",
      "Epoch 44/100\n",
      "6091/6091 [==============================] - 1s 106us/sample - loss: 0.4331 - val_loss: 0.4653\n",
      "Epoch 45/100\n",
      "6091/6091 [==============================] - 1s 101us/sample - loss: 0.4311 - val_loss: 0.4655\n",
      "Epoch 46/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.4355 - val_loss: 0.4655\n",
      "(Took 29.189 sec)\n",
      "Train on 6091 samples, validate on 1522 samples\n",
      "Epoch 1/100\n",
      "6091/6091 [==============================] - 1s 211us/sample - loss: 0.7231 - val_loss: 0.6815\n",
      "Epoch 2/100\n",
      "6091/6091 [==============================] - 1s 143us/sample - loss: 0.7116 - val_loss: 0.6751\n",
      "Epoch 3/100\n",
      "6091/6091 [==============================] - 1s 149us/sample - loss: 0.6962 - val_loss: 0.6706\n",
      "Epoch 4/100\n",
      "6091/6091 [==============================] - 1s 146us/sample - loss: 0.6849 - val_loss: 0.6681\n",
      "Epoch 5/100\n",
      "6091/6091 [==============================] - 1s 148us/sample - loss: 0.6791 - val_loss: 0.6648\n",
      "Epoch 6/100\n",
      "6091/6091 [==============================] - 1s 146us/sample - loss: 0.6703 - val_loss: 0.6606\n",
      "Epoch 7/100\n",
      "6091/6091 [==============================] - 1s 135us/sample - loss: 0.6679 - val_loss: 0.6566\n",
      "Epoch 8/100\n",
      "6091/6091 [==============================] - 1s 122us/sample - loss: 0.6590 - val_loss: 0.6511\n",
      "Epoch 9/100\n",
      "6091/6091 [==============================] - 1s 102us/sample - loss: 0.6538 - val_loss: 0.6452\n",
      "Epoch 10/100\n",
      "6091/6091 [==============================] - 1s 98us/sample - loss: 0.6470 - val_loss: 0.6371\n",
      "Epoch 11/100\n",
      "6091/6091 [==============================] - 1s 102us/sample - loss: 0.6392 - val_loss: 0.6284\n",
      "Epoch 12/100\n",
      "6091/6091 [==============================] - 1s 100us/sample - loss: 0.6291 - val_loss: 0.6177\n",
      "Epoch 13/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.6181 - val_loss: 0.6088\n",
      "Epoch 14/100\n",
      "6091/6091 [==============================] - 1s 99us/sample - loss: 0.6015 - val_loss: 0.5952\n",
      "Epoch 15/100\n",
      "6091/6091 [==============================] - 1s 98us/sample - loss: 0.5926 - val_loss: 0.5836\n",
      "Epoch 16/100\n",
      "6091/6091 [==============================] - 1s 99us/sample - loss: 0.5862 - val_loss: 0.5718\n",
      "Epoch 17/100\n",
      "6091/6091 [==============================] - 1s 100us/sample - loss: 0.5779 - val_loss: 0.5621\n",
      "Epoch 18/100\n",
      "6091/6091 [==============================] - 1s 106us/sample - loss: 0.5726 - val_loss: 0.5540\n",
      "Epoch 19/100\n",
      "6091/6091 [==============================] - 1s 111us/sample - loss: 0.5551 - val_loss: 0.5451\n",
      "Epoch 20/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.5484 - val_loss: 0.5353\n",
      "Epoch 21/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.5360 - val_loss: 0.5275\n",
      "Epoch 22/100\n",
      "6091/6091 [==============================] - 1s 103us/sample - loss: 0.5281 - val_loss: 0.5208\n",
      "Epoch 23/100\n",
      "6091/6091 [==============================] - 1s 103us/sample - loss: 0.5255 - val_loss: 0.5167\n",
      "Epoch 24/100\n",
      "6091/6091 [==============================] - 1s 106us/sample - loss: 0.5224 - val_loss: 0.5105\n",
      "Epoch 25/100\n",
      "6091/6091 [==============================] - 1s 106us/sample - loss: 0.5039 - val_loss: 0.5063\n",
      "Epoch 26/100\n",
      "6091/6091 [==============================] - 1s 98us/sample - loss: 0.4966 - val_loss: 0.5015\n",
      "Epoch 27/100\n",
      "6091/6091 [==============================] - 1s 101us/sample - loss: 0.5011 - val_loss: 0.4984\n",
      "Epoch 28/100\n",
      "6091/6091 [==============================] - 1s 99us/sample - loss: 0.4896 - val_loss: 0.4960\n",
      "Epoch 29/100\n",
      "6091/6091 [==============================] - 1s 99us/sample - loss: 0.4830 - val_loss: 0.4943\n",
      "Epoch 30/100\n",
      "6091/6091 [==============================] - 1s 98us/sample - loss: 0.4776 - val_loss: 0.4898\n",
      "Epoch 31/100\n",
      "6091/6091 [==============================] - 1s 99us/sample - loss: 0.4725 - val_loss: 0.4887\n",
      "Epoch 32/100\n",
      "6091/6091 [==============================] - 1s 98us/sample - loss: 0.4702 - val_loss: 0.4867\n",
      "Epoch 33/100\n",
      "6091/6091 [==============================] - 1s 100us/sample - loss: 0.4677 - val_loss: 0.4864\n",
      "Epoch 34/100\n",
      "6091/6091 [==============================] - 1s 100us/sample - loss: 0.4598 - val_loss: 0.4839\n",
      "Epoch 35/100\n",
      "6091/6091 [==============================] - 1s 100us/sample - loss: 0.4568 - val_loss: 0.4832\n",
      "Epoch 36/100\n",
      "6091/6091 [==============================] - 1s 99us/sample - loss: 0.4449 - val_loss: 0.4822\n",
      "Epoch 37/100\n",
      "6091/6091 [==============================] - 1s 100us/sample - loss: 0.4516 - val_loss: 0.4813\n",
      "Epoch 38/100\n",
      "6091/6091 [==============================] - 1s 99us/sample - loss: 0.4413 - val_loss: 0.4808\n",
      "Epoch 39/100\n",
      "6091/6091 [==============================] - 1s 99us/sample - loss: 0.4396 - val_loss: 0.4805\n",
      "Epoch 40/100\n",
      "6091/6091 [==============================] - 1s 100us/sample - loss: 0.4513 - val_loss: 0.4803\n",
      "Epoch 41/100\n",
      "6091/6091 [==============================] - 1s 100us/sample - loss: 0.4387 - val_loss: 0.4802\n",
      "Epoch 42/100\n",
      "6091/6091 [==============================] - 1s 99us/sample - loss: 0.4343 - val_loss: 0.4791\n",
      "Epoch 43/100\n",
      "6091/6091 [==============================] - 1s 99us/sample - loss: 0.4387 - val_loss: 0.4791\n",
      "Epoch 44/100\n",
      "6091/6091 [==============================] - 1s 98us/sample - loss: 0.4308 - val_loss: 0.4791\n",
      "Epoch 45/100\n",
      "6091/6091 [==============================] - 1s 104us/sample - loss: 0.4260 - val_loss: 0.4793\n",
      "(Took 30.572 sec)\n",
      "Combined confusion matrix:\n",
      "[[3720.  622.]\n",
      " [ 999. 2272.]]\n",
      "(Overall, took 147.314 sec)\n",
      "Accuracy: 78.71% +/- 0.59%\n",
      "Precision for positive class: 78.83% +/- 1.51%\n",
      "Precision for negative class: 78.53% +/- 1.48%\n",
      "Recall for positive class: 85.67% +/- 1.64%\n",
      "Recall for negative class: 69.46% +/- 1.93%\n",
      "F for positive class: 82.10% +/- 0.97%\n",
      "F for negative class: 73.70% +/- 0.52%\n",
      "Mean F score: 77.90% +/- 0.36%\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/100\n",
      "6090/6090 [==============================] - 1s 148us/sample - loss: 0.7165 - val_loss: 0.6663\n",
      "Epoch 2/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.6861 - val_loss: 0.6554\n",
      "Epoch 3/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.6772 - val_loss: 0.6455\n",
      "Epoch 4/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.6589 - val_loss: 0.6342\n",
      "Epoch 5/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.6415 - val_loss: 0.6200\n",
      "Epoch 6/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.6279 - val_loss: 0.6034\n",
      "Epoch 7/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.6024 - val_loss: 0.5840\n",
      "Epoch 8/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.5880 - val_loss: 0.5651\n",
      "Epoch 9/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.5598 - val_loss: 0.5460\n",
      "Epoch 10/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.5430 - val_loss: 0.5295\n",
      "Epoch 11/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.5201 - val_loss: 0.5158\n",
      "Epoch 12/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.5082 - val_loss: 0.5053\n",
      "Epoch 13/100\n",
      "6090/6090 [==============================] - 1s 98us/sample - loss: 0.4830 - val_loss: 0.4961\n",
      "Epoch 14/100\n",
      "6090/6090 [==============================] - 1s 98us/sample - loss: 0.4866 - val_loss: 0.4909\n",
      "Epoch 15/100\n",
      "6090/6090 [==============================] - 1s 98us/sample - loss: 0.4670 - val_loss: 0.4863\n",
      "Epoch 16/100\n",
      "6090/6090 [==============================] - 1s 98us/sample - loss: 0.4654 - val_loss: 0.4835\n",
      "Epoch 17/100\n",
      "6090/6090 [==============================] - 1s 97us/sample - loss: 0.4521 - val_loss: 0.4817\n",
      "Epoch 18/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.4378 - val_loss: 0.4804\n",
      "Epoch 19/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.4222 - val_loss: 0.4799\n",
      "Epoch 20/100\n",
      "6090/6090 [==============================] - 1s 99us/sample - loss: 0.4246 - val_loss: 0.4799\n",
      "Epoch 21/100\n",
      "6090/6090 [==============================] - 1s 97us/sample - loss: 0.4205 - val_loss: 0.4807\n",
      "(Took 13.732 sec)\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/100\n",
      "6090/6090 [==============================] - 1s 148us/sample - loss: 0.7483 - val_loss: 0.6620\n",
      "Epoch 2/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.6999 - val_loss: 0.6473\n",
      "Epoch 3/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.6782 - val_loss: 0.6348\n",
      "Epoch 4/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.6625 - val_loss: 0.6227\n",
      "Epoch 5/100\n",
      "6090/6090 [==============================] - 1s 98us/sample - loss: 0.6428 - val_loss: 0.6101\n",
      "Epoch 6/100\n",
      "6090/6090 [==============================] - 1s 99us/sample - loss: 0.6275 - val_loss: 0.5950\n",
      "Epoch 7/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.6134 - val_loss: 0.5794\n",
      "Epoch 8/100\n",
      "6090/6090 [==============================] - 1s 98us/sample - loss: 0.5906 - val_loss: 0.5629\n",
      "Epoch 9/100\n",
      "6090/6090 [==============================] - 1s 98us/sample - loss: 0.5757 - val_loss: 0.5448\n",
      "Epoch 10/100\n",
      "6090/6090 [==============================] - 1s 98us/sample - loss: 0.5519 - val_loss: 0.5283\n",
      "Epoch 11/100\n",
      "6090/6090 [==============================] - 1s 98us/sample - loss: 0.5354 - val_loss: 0.5142\n",
      "Epoch 12/100\n",
      "6090/6090 [==============================] - 1s 99us/sample - loss: 0.5206 - val_loss: 0.5022\n",
      "Epoch 13/100\n",
      "6090/6090 [==============================] - 1s 99us/sample - loss: 0.5033 - val_loss: 0.4920\n",
      "Epoch 14/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.4828 - val_loss: 0.4843\n",
      "Epoch 15/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.4747 - val_loss: 0.4777\n",
      "Epoch 16/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.4574 - val_loss: 0.4730\n",
      "Epoch 17/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4582 - val_loss: 0.4698\n",
      "Epoch 18/100\n",
      "6090/6090 [==============================] - 1s 121us/sample - loss: 0.4476 - val_loss: 0.4677\n",
      "Epoch 19/100\n",
      "6090/6090 [==============================] - 1s 99us/sample - loss: 0.4418 - val_loss: 0.4671\n",
      "Epoch 20/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.4208 - val_loss: 0.4662\n",
      "Epoch 21/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.4250 - val_loss: 0.4658\n",
      "Epoch 22/100\n",
      "6090/6090 [==============================] - 1s 133us/sample - loss: 0.4203 - val_loss: 0.4655\n",
      "Epoch 23/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.4146 - val_loss: 0.4658\n",
      "Epoch 24/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.4031 - val_loss: 0.4666\n",
      "(Took 16.050 sec)\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/100\n",
      "6090/6090 [==============================] - 1s 159us/sample - loss: 0.7410 - val_loss: 0.6753\n",
      "Epoch 2/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.6855 - val_loss: 0.6637\n",
      "Epoch 3/100\n",
      "6090/6090 [==============================] - 1s 117us/sample - loss: 0.6761 - val_loss: 0.6539\n",
      "Epoch 4/100\n",
      "6090/6090 [==============================] - 1s 118us/sample - loss: 0.6586 - val_loss: 0.6430\n",
      "Epoch 5/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.6462 - val_loss: 0.6313\n",
      "Epoch 6/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.6268 - val_loss: 0.6134\n",
      "Epoch 7/100\n",
      "6090/6090 [==============================] - 1s 123us/sample - loss: 0.6018 - val_loss: 0.5936\n",
      "Epoch 8/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.5885 - val_loss: 0.5749\n",
      "Epoch 9/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.5605 - val_loss: 0.5561\n",
      "Epoch 10/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.5415 - val_loss: 0.5401\n",
      "Epoch 11/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.5267 - val_loss: 0.5256\n",
      "Epoch 12/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.5046 - val_loss: 0.5136\n",
      "Epoch 13/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.4792 - val_loss: 0.5046\n",
      "Epoch 14/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4762 - val_loss: 0.4969\n",
      "Epoch 15/100\n",
      "6090/6090 [==============================] - 1s 120us/sample - loss: 0.4691 - val_loss: 0.4931\n",
      "Epoch 16/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.4507 - val_loss: 0.4898\n",
      "Epoch 17/100\n",
      "6090/6090 [==============================] - 1s 124us/sample - loss: 0.4394 - val_loss: 0.4881\n",
      "Epoch 18/100\n",
      "6090/6090 [==============================] - 1s 116us/sample - loss: 0.4363 - val_loss: 0.4866\n",
      "Epoch 19/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4202 - val_loss: 0.4851\n",
      "Epoch 20/100\n",
      "6090/6090 [==============================] - 1s 123us/sample - loss: 0.4256 - val_loss: 0.4850\n",
      "Epoch 21/100\n",
      "6090/6090 [==============================] - 1s 147us/sample - loss: 0.4185 - val_loss: 0.4865\n",
      "Epoch 22/100\n",
      "6090/6090 [==============================] - 1s 119us/sample - loss: 0.4052 - val_loss: 0.4859\n",
      "(Took 16.097 sec)\n",
      "Train on 6091 samples, validate on 1522 samples\n",
      "Epoch 1/100\n",
      "6091/6091 [==============================] - 1s 167us/sample - loss: 0.7282 - val_loss: 0.6679\n",
      "Epoch 2/100\n",
      "6091/6091 [==============================] - 1s 117us/sample - loss: 0.6929 - val_loss: 0.6563\n",
      "Epoch 3/100\n",
      "6091/6091 [==============================] - 1s 115us/sample - loss: 0.6746 - val_loss: 0.6458\n",
      "Epoch 4/100\n",
      "6091/6091 [==============================] - 1s 124us/sample - loss: 0.6634 - val_loss: 0.6345\n",
      "Epoch 5/100\n",
      "6091/6091 [==============================] - 1s 119us/sample - loss: 0.6449 - val_loss: 0.6211\n",
      "Epoch 6/100\n",
      "6091/6091 [==============================] - 1s 126us/sample - loss: 0.6228 - val_loss: 0.6038\n",
      "Epoch 7/100\n",
      "6091/6091 [==============================] - 1s 119us/sample - loss: 0.6084 - val_loss: 0.5839\n",
      "Epoch 8/100\n",
      "6091/6091 [==============================] - 1s 117us/sample - loss: 0.5857 - val_loss: 0.5642\n",
      "Epoch 9/100\n",
      "6091/6091 [==============================] - 1s 119us/sample - loss: 0.5629 - val_loss: 0.5450\n",
      "Epoch 10/100\n",
      "6091/6091 [==============================] - 1s 121us/sample - loss: 0.5403 - val_loss: 0.5275\n",
      "Epoch 11/100\n",
      "6091/6091 [==============================] - 1s 115us/sample - loss: 0.5286 - val_loss: 0.5114\n",
      "Epoch 12/100\n",
      "6091/6091 [==============================] - 1s 118us/sample - loss: 0.5050 - val_loss: 0.4988\n",
      "Epoch 13/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6091/6091 [==============================] - 1s 113us/sample - loss: 0.4980 - val_loss: 0.4921\n",
      "Epoch 14/100\n",
      "6091/6091 [==============================] - 1s 116us/sample - loss: 0.4833 - val_loss: 0.4824\n",
      "Epoch 15/100\n",
      "6091/6091 [==============================] - 1s 116us/sample - loss: 0.4700 - val_loss: 0.4771\n",
      "Epoch 16/100\n",
      "6091/6091 [==============================] - 1s 123us/sample - loss: 0.4576 - val_loss: 0.4722\n",
      "Epoch 17/100\n",
      "6091/6091 [==============================] - 1s 123us/sample - loss: 0.4496 - val_loss: 0.4697\n",
      "Epoch 18/100\n",
      "6091/6091 [==============================] - 1s 125us/sample - loss: 0.4377 - val_loss: 0.4689\n",
      "Epoch 19/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.4342 - val_loss: 0.4657\n",
      "Epoch 20/100\n",
      "6091/6091 [==============================] - 1s 103us/sample - loss: 0.4245 - val_loss: 0.4648\n",
      "Epoch 21/100\n",
      "6091/6091 [==============================] - 1s 102us/sample - loss: 0.4232 - val_loss: 0.4642\n",
      "Epoch 22/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.4176 - val_loss: 0.4654\n",
      "Epoch 23/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.4160 - val_loss: 0.4647\n",
      "(Took 17.050 sec)\n",
      "Train on 6091 samples, validate on 1522 samples\n",
      "Epoch 1/100\n",
      "6091/6091 [==============================] - 1s 158us/sample - loss: 0.7266 - val_loss: 0.6716\n",
      "Epoch 2/100\n",
      "6091/6091 [==============================] - 1s 105us/sample - loss: 0.6926 - val_loss: 0.6612\n",
      "Epoch 3/100\n",
      "6091/6091 [==============================] - 1s 99us/sample - loss: 0.6797 - val_loss: 0.6488\n",
      "Epoch 4/100\n",
      "6091/6091 [==============================] - 1s 129us/sample - loss: 0.6542 - val_loss: 0.6366\n",
      "Epoch 5/100\n",
      "6091/6091 [==============================] - 1s 132us/sample - loss: 0.6458 - val_loss: 0.6233\n",
      "Epoch 6/100\n",
      "6091/6091 [==============================] - 1s 124us/sample - loss: 0.6264 - val_loss: 0.6077\n",
      "Epoch 7/100\n",
      "6091/6091 [==============================] - 1s 126us/sample - loss: 0.6072 - val_loss: 0.5900\n",
      "Epoch 8/100\n",
      "6091/6091 [==============================] - 1s 116us/sample - loss: 0.5811 - val_loss: 0.5700\n",
      "Epoch 9/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.5644 - val_loss: 0.5517\n",
      "Epoch 10/100\n",
      "6091/6091 [==============================] - 1s 123us/sample - loss: 0.5395 - val_loss: 0.5348\n",
      "Epoch 11/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.5183 - val_loss: 0.5205\n",
      "Epoch 12/100\n",
      "6091/6091 [==============================] - 1s 110us/sample - loss: 0.5036 - val_loss: 0.5087\n",
      "Epoch 13/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4984 - val_loss: 0.5010\n",
      "Epoch 14/100\n",
      "6091/6091 [==============================] - 1s 113us/sample - loss: 0.4830 - val_loss: 0.4944\n",
      "Epoch 15/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.4708 - val_loss: 0.4886\n",
      "Epoch 16/100\n",
      "6091/6091 [==============================] - 1s 110us/sample - loss: 0.4591 - val_loss: 0.4862\n",
      "Epoch 17/100\n",
      "6091/6091 [==============================] - 1s 111us/sample - loss: 0.4509 - val_loss: 0.4825\n",
      "Epoch 18/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4347 - val_loss: 0.4807\n",
      "Epoch 19/100\n",
      "6091/6091 [==============================] - 1s 103us/sample - loss: 0.4283 - val_loss: 0.4809\n",
      "Epoch 20/100\n",
      "6091/6091 [==============================] - 1s 104us/sample - loss: 0.4273 - val_loss: 0.4804\n",
      "Epoch 21/100\n",
      "6091/6091 [==============================] - 1s 106us/sample - loss: 0.4151 - val_loss: 0.4795\n",
      "Epoch 22/100\n",
      "6091/6091 [==============================] - 1s 103us/sample - loss: 0.4139 - val_loss: 0.4799\n",
      "Epoch 23/100\n",
      "6091/6091 [==============================] - 1s 103us/sample - loss: 0.4037 - val_loss: 0.4806\n",
      "(Took 16.416 sec)\n",
      "Combined confusion matrix:\n",
      "[[3681.  661.]\n",
      " [ 966. 2305.]]\n",
      "(Overall, took 79.481 sec)\n",
      "Accuracy: 78.63% +/- 0.27%\n",
      "Precision for positive class: 79.21% +/- 0.94%\n",
      "Precision for negative class: 77.70% +/- 1.78%\n",
      "Recall for positive class: 84.78% +/- 0.91%\n",
      "Recall for negative class: 70.46% +/- 0.93%\n",
      "F for positive class: 81.89% +/- 0.48%\n",
      "F for negative class: 73.90% +/- 0.98%\n",
      "Mean F score: 77.90% +/- 0.38%\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/100\n",
      "6090/6090 [==============================] - 1s 174us/sample - loss: 0.6969 - val_loss: 0.5995\n",
      "Epoch 2/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.6099 - val_loss: 0.5563\n",
      "Epoch 3/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.5656 - val_loss: 0.5250\n",
      "Epoch 4/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.5325 - val_loss: 0.5025\n",
      "Epoch 5/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.5035 - val_loss: 0.4850\n",
      "Epoch 6/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.4872 - val_loss: 0.4713\n",
      "Epoch 7/100\n",
      "6090/6090 [==============================] - 1s 120us/sample - loss: 0.4610 - val_loss: 0.4614\n",
      "Epoch 8/100\n",
      "6090/6090 [==============================] - 1s 125us/sample - loss: 0.4364 - val_loss: 0.4530\n",
      "Epoch 9/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.4274 - val_loss: 0.4484\n",
      "Epoch 10/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.4078 - val_loss: 0.4442\n",
      "Epoch 11/100\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.4001 - val_loss: 0.4412\n",
      "Epoch 12/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.3868 - val_loss: 0.4401\n",
      "Epoch 13/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.3746 - val_loss: 0.4403\n",
      "Epoch 14/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.3660 - val_loss: 0.4398\n",
      "Epoch 15/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.3594 - val_loss: 0.4407\n",
      "Epoch 16/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.3567 - val_loss: 0.4418\n",
      "(Took 11.997 sec)\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/100\n",
      "6090/6090 [==============================] - 1s 165us/sample - loss: 0.6846 - val_loss: 0.6042\n",
      "Epoch 2/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.6122 - val_loss: 0.5633\n",
      "Epoch 3/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.5634 - val_loss: 0.5333\n",
      "Epoch 4/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.5304 - val_loss: 0.5126\n",
      "Epoch 5/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4953 - val_loss: 0.4966\n",
      "Epoch 6/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.4674 - val_loss: 0.4845\n",
      "Epoch 7/100\n",
      "6090/6090 [==============================] - 1s 118us/sample - loss: 0.4545 - val_loss: 0.4759\n",
      "Epoch 8/100\n",
      "6090/6090 [==============================] - 1s 116us/sample - loss: 0.4311 - val_loss: 0.4697\n",
      "Epoch 9/100\n",
      "6090/6090 [==============================] - 1s 120us/sample - loss: 0.4126 - val_loss: 0.4666\n",
      "Epoch 10/100\n",
      "6090/6090 [==============================] - 1s 136us/sample - loss: 0.4031 - val_loss: 0.4637\n",
      "Epoch 11/100\n",
      "6090/6090 [==============================] - 1s 123us/sample - loss: 0.3914 - val_loss: 0.4624\n",
      "Epoch 12/100\n",
      "6090/6090 [==============================] - 1s 135us/sample - loss: 0.3731 - val_loss: 0.4631\n",
      "Epoch 13/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.3690 - val_loss: 0.4631\n",
      "(Took 10.078 sec)\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/100\n",
      "6090/6090 [==============================] - 1s 157us/sample - loss: 0.6949 - val_loss: 0.6055\n",
      "Epoch 2/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.6106 - val_loss: 0.5664\n",
      "Epoch 3/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.5626 - val_loss: 0.5381\n",
      "Epoch 4/100\n",
      "6090/6090 [==============================] - 1s 116us/sample - loss: 0.5257 - val_loss: 0.5191\n",
      "Epoch 5/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.5022 - val_loss: 0.5027\n",
      "Epoch 6/100\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.4779 - val_loss: 0.4907\n",
      "Epoch 7/100\n",
      "6090/6090 [==============================] - 1s 128us/sample - loss: 0.4610 - val_loss: 0.4835\n",
      "Epoch 8/100\n",
      "6090/6090 [==============================] - 1s 130us/sample - loss: 0.4421 - val_loss: 0.4760\n",
      "Epoch 9/100\n",
      "6090/6090 [==============================] - 1s 116us/sample - loss: 0.4203 - val_loss: 0.4711\n",
      "Epoch 10/100\n",
      "6090/6090 [==============================] - 1s 132us/sample - loss: 0.4043 - val_loss: 0.4687\n",
      "Epoch 11/100\n",
      "6090/6090 [==============================] - 1s 153us/sample - loss: 0.3903 - val_loss: 0.4693\n",
      "Epoch 12/100\n",
      "6090/6090 [==============================] - 1s 129us/sample - loss: 0.3801 - val_loss: 0.4683\n",
      "Epoch 13/100\n",
      "6090/6090 [==============================] - 1s 117us/sample - loss: 0.3755 - val_loss: 0.4691\n",
      "Epoch 14/100\n",
      "6090/6090 [==============================] - 1s 126us/sample - loss: 0.3621 - val_loss: 0.4719\n",
      "(Took 11.274 sec)\n",
      "Train on 6091 samples, validate on 1522 samples\n",
      "Epoch 1/100\n",
      "6091/6091 [==============================] - 1s 170us/sample - loss: 0.7215 - val_loss: 0.6086\n",
      "Epoch 2/100\n",
      "6091/6091 [==============================] - 1s 134us/sample - loss: 0.6219 - val_loss: 0.5642\n",
      "Epoch 3/100\n",
      "6091/6091 [==============================] - 1s 128us/sample - loss: 0.5757 - val_loss: 0.5331\n",
      "Epoch 4/100\n",
      "6091/6091 [==============================] - 1s 122us/sample - loss: 0.5343 - val_loss: 0.5077\n",
      "Epoch 5/100\n",
      "6091/6091 [==============================] - 1s 122us/sample - loss: 0.5036 - val_loss: 0.4886\n",
      "Epoch 6/100\n",
      "6091/6091 [==============================] - 1s 118us/sample - loss: 0.4832 - val_loss: 0.4752\n",
      "Epoch 7/100\n",
      "6091/6091 [==============================] - 1s 124us/sample - loss: 0.4709 - val_loss: 0.4642\n",
      "Epoch 8/100\n",
      "6091/6091 [==============================] - 1s 114us/sample - loss: 0.4427 - val_loss: 0.4567\n",
      "Epoch 9/100\n",
      "6091/6091 [==============================] - 1s 110us/sample - loss: 0.4258 - val_loss: 0.4520\n",
      "Epoch 10/100\n",
      "6091/6091 [==============================] - 1s 120us/sample - loss: 0.4134 - val_loss: 0.4463\n",
      "Epoch 11/100\n",
      "6091/6091 [==============================] - 1s 123us/sample - loss: 0.3943 - val_loss: 0.4431\n",
      "Epoch 12/100\n",
      "6091/6091 [==============================] - 1s 124us/sample - loss: 0.3909 - val_loss: 0.4417\n",
      "Epoch 13/100\n",
      "6091/6091 [==============================] - 1s 111us/sample - loss: 0.3774 - val_loss: 0.4411\n",
      "Epoch 14/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.3779 - val_loss: 0.4402\n",
      "Epoch 15/100\n",
      "6091/6091 [==============================] - 1s 115us/sample - loss: 0.3565 - val_loss: 0.4412\n",
      "Epoch 16/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.3460 - val_loss: 0.4440\n",
      "(Took 12.593 sec)\n",
      "Train on 6091 samples, validate on 1522 samples\n",
      "Epoch 1/100\n",
      "6091/6091 [==============================] - 1s 198us/sample - loss: 0.6867 - val_loss: 0.6079\n",
      "Epoch 2/100\n",
      "6091/6091 [==============================] - 1s 127us/sample - loss: 0.6034 - val_loss: 0.5635\n",
      "Epoch 3/100\n",
      "6091/6091 [==============================] - 1s 111us/sample - loss: 0.5676 - val_loss: 0.5332\n",
      "Epoch 4/100\n",
      "6091/6091 [==============================] - 1s 119us/sample - loss: 0.5210 - val_loss: 0.5092\n",
      "Epoch 5/100\n",
      "6091/6091 [==============================] - 1s 117us/sample - loss: 0.5018 - val_loss: 0.4922\n",
      "Epoch 6/100\n",
      "6091/6091 [==============================] - 1s 113us/sample - loss: 0.4650 - val_loss: 0.4798\n",
      "Epoch 7/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.4519 - val_loss: 0.4698\n",
      "Epoch 8/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4354 - val_loss: 0.4642\n",
      "Epoch 9/100\n",
      "6091/6091 [==============================] - 1s 126us/sample - loss: 0.4140 - val_loss: 0.4594\n",
      "Epoch 10/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.4006 - val_loss: 0.4566\n",
      "Epoch 11/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.3880 - val_loss: 0.4556\n",
      "Epoch 12/100\n",
      "6091/6091 [==============================] - 1s 115us/sample - loss: 0.3754 - val_loss: 0.4570\n",
      "Epoch 13/100\n",
      "6091/6091 [==============================] - 1s 130us/sample - loss: 0.3643 - val_loss: 0.4579\n",
      "(Took 10.226 sec)\n",
      "Combined confusion matrix:\n",
      "[[3810.  532.]\n",
      " [ 987. 2284.]]\n",
      "(Overall, took 56.466 sec)\n",
      "Accuracy: 80.05% +/- 0.85%\n",
      "Precision for positive class: 79.44% +/- 2.00%\n",
      "Precision for negative class: 81.14% +/- 2.44%\n",
      "Recall for positive class: 87.76% +/- 1.84%\n",
      "Recall for negative class: 69.85% +/- 2.59%\n",
      "F for positive class: 83.37% +/- 0.89%\n",
      "F for negative class: 75.04% +/- 1.05%\n",
      "Mean F score: 79.21% +/- 0.82%\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/100\n",
      "6090/6090 [==============================] - 1s 177us/sample - loss: 0.7218 - val_loss: 0.6742\n",
      "Epoch 2/100\n",
      "6090/6090 [==============================] - 1s 120us/sample - loss: 0.7009 - val_loss: 0.6700\n",
      "Epoch 3/100\n",
      "6090/6090 [==============================] - 1s 120us/sample - loss: 0.6930 - val_loss: 0.6666\n",
      "Epoch 4/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.6830 - val_loss: 0.6631\n",
      "Epoch 5/100\n",
      "6090/6090 [==============================] - 1s 119us/sample - loss: 0.6810 - val_loss: 0.6594\n",
      "Epoch 6/100\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.6690 - val_loss: 0.6555\n",
      "Epoch 7/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.6672 - val_loss: 0.6503\n",
      "Epoch 8/100\n",
      "6090/6090 [==============================] - 1s 116us/sample - loss: 0.6583 - val_loss: 0.6428\n",
      "Epoch 9/100\n",
      "6090/6090 [==============================] - 1s 121us/sample - loss: 0.6495 - val_loss: 0.6350\n",
      "Epoch 10/100\n",
      "6090/6090 [==============================] - 1s 126us/sample - loss: 0.6435 - val_loss: 0.6260\n",
      "Epoch 11/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.6374 - val_loss: 0.6169\n",
      "Epoch 12/100\n",
      "6090/6090 [==============================] - 1s 122us/sample - loss: 0.6330 - val_loss: 0.6079\n",
      "Epoch 13/100\n",
      "6090/6090 [==============================] - 1s 126us/sample - loss: 0.6215 - val_loss: 0.5985\n",
      "Epoch 14/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.6166 - val_loss: 0.5905\n",
      "Epoch 15/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.6102 - val_loss: 0.5812\n",
      "Epoch 16/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.5967 - val_loss: 0.5724\n",
      "Epoch 17/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.5964 - val_loss: 0.5649\n",
      "Epoch 18/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.5835 - val_loss: 0.5573\n",
      "Epoch 19/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.5782 - val_loss: 0.5498\n",
      "Epoch 20/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.5704 - val_loss: 0.5435\n",
      "Epoch 21/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.5625 - val_loss: 0.5377\n",
      "Epoch 22/100\n",
      "6090/6090 [==============================] - 1s 117us/sample - loss: 0.5575 - val_loss: 0.5317\n",
      "Epoch 23/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.5451 - val_loss: 0.5256\n",
      "Epoch 24/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.5433 - val_loss: 0.5208\n",
      "Epoch 25/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.5538 - val_loss: 0.5173\n",
      "Epoch 26/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.5419 - val_loss: 0.5136\n",
      "Epoch 27/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.5315 - val_loss: 0.5096\n",
      "Epoch 28/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.5314 - val_loss: 0.5064\n",
      "Epoch 29/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.5271 - val_loss: 0.5031\n",
      "Epoch 30/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.5218 - val_loss: 0.5001\n",
      "Epoch 31/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.5166 - val_loss: 0.4971\n",
      "Epoch 32/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.5167 - val_loss: 0.4946\n",
      "Epoch 33/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.5032 - val_loss: 0.4915\n",
      "Epoch 34/100\n",
      "6090/6090 [==============================] - 1s 116us/sample - loss: 0.5029 - val_loss: 0.4896\n",
      "Epoch 35/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6090/6090 [==============================] - 1s 116us/sample - loss: 0.5020 - val_loss: 0.4865\n",
      "Epoch 36/100\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.4999 - val_loss: 0.4846\n",
      "Epoch 37/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.4938 - val_loss: 0.4825\n",
      "Epoch 38/100\n",
      "6090/6090 [==============================] - 1s 116us/sample - loss: 0.4932 - val_loss: 0.4807\n",
      "Epoch 39/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.4849 - val_loss: 0.4788\n",
      "Epoch 40/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4843 - val_loss: 0.4770\n",
      "Epoch 41/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.4859 - val_loss: 0.4758\n",
      "Epoch 42/100\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.4765 - val_loss: 0.4741\n",
      "Epoch 43/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.4770 - val_loss: 0.4731\n",
      "Epoch 44/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4740 - val_loss: 0.4710\n",
      "Epoch 45/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4633 - val_loss: 0.4693\n",
      "Epoch 46/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4658 - val_loss: 0.4682\n",
      "Epoch 47/100\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.4584 - val_loss: 0.4668\n",
      "Epoch 48/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.4614 - val_loss: 0.4657\n",
      "Epoch 49/100\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.4582 - val_loss: 0.4647\n",
      "Epoch 50/100\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.4556 - val_loss: 0.4636\n",
      "Epoch 51/100\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.4540 - val_loss: 0.4627\n",
      "Epoch 52/100\n",
      "6090/6090 [==============================] - 1s 118us/sample - loss: 0.4512 - val_loss: 0.4618\n",
      "Epoch 53/100\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.4517 - val_loss: 0.4616\n",
      "Epoch 54/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.4471 - val_loss: 0.4605\n",
      "Epoch 55/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.4405 - val_loss: 0.4599\n",
      "Epoch 56/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.4420 - val_loss: 0.4590\n",
      "Epoch 57/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.4441 - val_loss: 0.4584\n",
      "Epoch 58/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4399 - val_loss: 0.4578\n",
      "Epoch 59/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4340 - val_loss: 0.4572\n",
      "Epoch 60/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.4371 - val_loss: 0.4569\n",
      "Epoch 61/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4316 - val_loss: 0.4562\n",
      "Epoch 62/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.4265 - val_loss: 0.4556\n",
      "Epoch 63/100\n",
      "6090/6090 [==============================] - 1s 118us/sample - loss: 0.4293 - val_loss: 0.4551\n",
      "Epoch 64/100\n",
      "6090/6090 [==============================] - 1s 117us/sample - loss: 0.4234 - val_loss: 0.4546\n",
      "Epoch 65/100\n",
      "6090/6090 [==============================] - 1s 117us/sample - loss: 0.4294 - val_loss: 0.4544\n",
      "Epoch 66/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.4221 - val_loss: 0.4540\n",
      "Epoch 67/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.4212 - val_loss: 0.4536\n",
      "Epoch 68/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.4194 - val_loss: 0.4535\n",
      "Epoch 69/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.4169 - val_loss: 0.4531\n",
      "Epoch 70/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.4143 - val_loss: 0.4529\n",
      "Epoch 71/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.4070 - val_loss: 0.4525\n",
      "Epoch 72/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.4102 - val_loss: 0.4523\n",
      "Epoch 73/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.4138 - val_loss: 0.4522\n",
      "Epoch 74/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.4017 - val_loss: 0.4519\n",
      "Epoch 75/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.4093 - val_loss: 0.4517\n",
      "Epoch 76/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.4033 - val_loss: 0.4519\n",
      "Epoch 77/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4024 - val_loss: 0.4516\n",
      "Epoch 78/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.3990 - val_loss: 0.4515\n",
      "Epoch 79/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.4012 - val_loss: 0.4514\n",
      "Epoch 80/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.3953 - val_loss: 0.4515\n",
      "Epoch 81/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.4023 - val_loss: 0.4514\n",
      "Epoch 82/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.3949 - val_loss: 0.4514\n",
      "Epoch 83/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.3936 - val_loss: 0.4513\n",
      "Epoch 84/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.3880 - val_loss: 0.4514\n",
      "Epoch 85/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.3916 - val_loss: 0.4513\n",
      "(Took 59.073 sec)\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/100\n",
      "6090/6090 [==============================] - 1s 165us/sample - loss: 0.7443 - val_loss: 0.6704\n",
      "Epoch 2/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.7162 - val_loss: 0.6661\n",
      "Epoch 3/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.7028 - val_loss: 0.6624\n",
      "Epoch 4/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.6849 - val_loss: 0.6599\n",
      "Epoch 5/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.6877 - val_loss: 0.6560\n",
      "Epoch 6/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.6756 - val_loss: 0.6513\n",
      "Epoch 7/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.6711 - val_loss: 0.6483\n",
      "Epoch 8/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.6583 - val_loss: 0.6417\n",
      "Epoch 9/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.6586 - val_loss: 0.6355\n",
      "Epoch 10/100\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.6436 - val_loss: 0.6279\n",
      "Epoch 11/100\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.6397 - val_loss: 0.6199\n",
      "Epoch 12/100\n",
      "6090/6090 [==============================] - 1s 116us/sample - loss: 0.6439 - val_loss: 0.6124\n",
      "Epoch 13/100\n",
      "6090/6090 [==============================] - 1s 118us/sample - loss: 0.6322 - val_loss: 0.6039\n",
      "Epoch 14/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.6228 - val_loss: 0.5954\n",
      "Epoch 15/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.6139 - val_loss: 0.5869\n",
      "Epoch 16/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.6093 - val_loss: 0.5795\n",
      "Epoch 17/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.5950 - val_loss: 0.5702\n",
      "Epoch 18/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.5983 - val_loss: 0.5630\n",
      "Epoch 19/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.5883 - val_loss: 0.5568\n",
      "Epoch 20/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.5706 - val_loss: 0.5493\n",
      "Epoch 21/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.5677 - val_loss: 0.5424\n",
      "Epoch 22/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.5640 - val_loss: 0.5370\n",
      "Epoch 23/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.5594 - val_loss: 0.5313\n",
      "Epoch 24/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.5534 - val_loss: 0.5267\n",
      "Epoch 25/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.5515 - val_loss: 0.5235\n",
      "Epoch 26/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.5413 - val_loss: 0.5188\n",
      "Epoch 27/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.5357 - val_loss: 0.5151\n",
      "Epoch 28/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.5319 - val_loss: 0.5108\n",
      "Epoch 29/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.5245 - val_loss: 0.5073\n",
      "Epoch 30/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.5276 - val_loss: 0.5056\n",
      "Epoch 31/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.5177 - val_loss: 0.5020\n",
      "Epoch 32/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.5117 - val_loss: 0.4980\n",
      "Epoch 33/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.5106 - val_loss: 0.4959\n",
      "Epoch 34/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.5110 - val_loss: 0.4946\n",
      "Epoch 35/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.4969 - val_loss: 0.4908\n",
      "Epoch 36/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.4988 - val_loss: 0.4888\n",
      "Epoch 37/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.4926 - val_loss: 0.4871\n",
      "Epoch 38/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.4883 - val_loss: 0.4846\n",
      "Epoch 39/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4829 - val_loss: 0.4827\n",
      "Epoch 40/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.4828 - val_loss: 0.4810\n",
      "Epoch 41/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4806 - val_loss: 0.4802\n",
      "Epoch 42/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.4812 - val_loss: 0.4779\n",
      "Epoch 43/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.4758 - val_loss: 0.4767\n",
      "Epoch 44/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.4695 - val_loss: 0.4752\n",
      "Epoch 45/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.4674 - val_loss: 0.4745\n",
      "Epoch 46/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.4682 - val_loss: 0.4734\n",
      "Epoch 47/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.4603 - val_loss: 0.4726\n",
      "Epoch 48/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.4541 - val_loss: 0.4702\n",
      "Epoch 49/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.4615 - val_loss: 0.4695\n",
      "Epoch 50/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.4538 - val_loss: 0.4688\n",
      "Epoch 51/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.4564 - val_loss: 0.4677\n",
      "Epoch 52/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.4466 - val_loss: 0.4674\n",
      "Epoch 53/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.4465 - val_loss: 0.4666\n",
      "Epoch 54/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.4501 - val_loss: 0.4656\n",
      "Epoch 55/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.4346 - val_loss: 0.4651\n",
      "Epoch 56/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.4412 - val_loss: 0.4649\n",
      "Epoch 57/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.4380 - val_loss: 0.4635\n",
      "Epoch 58/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.4371 - val_loss: 0.4628\n",
      "Epoch 59/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.4347 - val_loss: 0.4627\n",
      "Epoch 60/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.4349 - val_loss: 0.4620\n",
      "Epoch 61/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.4336 - val_loss: 0.4618\n",
      "Epoch 62/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.4313 - val_loss: 0.4615\n",
      "Epoch 63/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.4233 - val_loss: 0.4616\n",
      "Epoch 64/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.4211 - val_loss: 0.4605\n",
      "Epoch 65/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.4301 - val_loss: 0.4606\n",
      "Epoch 66/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.4302 - val_loss: 0.4603\n",
      "Epoch 67/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.4202 - val_loss: 0.4604\n",
      "Epoch 68/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.4076 - val_loss: 0.4595\n",
      "Epoch 69/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.4146 - val_loss: 0.4596\n",
      "Epoch 70/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.4147 - val_loss: 0.4595\n",
      "(Took 46.310 sec)\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/100\n",
      "6090/6090 [==============================] - 1s 158us/sample - loss: 0.7318 - val_loss: 0.6824\n",
      "Epoch 2/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.7132 - val_loss: 0.6745\n",
      "Epoch 3/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.6888 - val_loss: 0.6720\n",
      "Epoch 4/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.6860 - val_loss: 0.6679\n",
      "Epoch 5/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.6790 - val_loss: 0.6667\n",
      "Epoch 6/100\n",
      "6090/6090 [==============================] - 1s 119us/sample - loss: 0.6743 - val_loss: 0.6611\n",
      "Epoch 7/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.6658 - val_loss: 0.6579\n",
      "Epoch 8/100\n",
      "6090/6090 [==============================] - 1s 116us/sample - loss: 0.6655 - val_loss: 0.6534\n",
      "Epoch 9/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.6529 - val_loss: 0.6457\n",
      "Epoch 10/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.6501 - val_loss: 0.6389\n",
      "Epoch 11/100\n",
      "6090/6090 [==============================] - 1s 116us/sample - loss: 0.6426 - val_loss: 0.6306\n",
      "Epoch 12/100\n",
      "6090/6090 [==============================] - 1s 117us/sample - loss: 0.6344 - val_loss: 0.6233\n",
      "Epoch 13/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.6245 - val_loss: 0.6143\n",
      "Epoch 14/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.6149 - val_loss: 0.6061\n",
      "Epoch 15/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.6133 - val_loss: 0.5968\n",
      "Epoch 16/100\n",
      "6090/6090 [==============================] - 1s 129us/sample - loss: 0.5946 - val_loss: 0.5861\n",
      "Epoch 17/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.5944 - val_loss: 0.5799\n",
      "Epoch 18/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.5897 - val_loss: 0.5726\n",
      "Epoch 19/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.5792 - val_loss: 0.5652\n",
      "Epoch 20/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.5680 - val_loss: 0.5585\n",
      "Epoch 21/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.5665 - val_loss: 0.5542\n",
      "Epoch 22/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.5610 - val_loss: 0.5480\n",
      "Epoch 23/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.5570 - val_loss: 0.5426\n",
      "Epoch 24/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.5434 - val_loss: 0.5390\n",
      "Epoch 25/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.5453 - val_loss: 0.5339\n",
      "Epoch 26/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.5409 - val_loss: 0.5319\n",
      "Epoch 27/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.5354 - val_loss: 0.5272\n",
      "Epoch 28/100\n",
      "6090/6090 [==============================] - 1s 116us/sample - loss: 0.5300 - val_loss: 0.5251\n",
      "Epoch 29/100\n",
      "6090/6090 [==============================] - 1s 117us/sample - loss: 0.5202 - val_loss: 0.5207\n",
      "Epoch 30/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.5188 - val_loss: 0.5173\n",
      "Epoch 31/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.5089 - val_loss: 0.5154\n",
      "Epoch 32/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.5073 - val_loss: 0.5124\n",
      "Epoch 33/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.5093 - val_loss: 0.5085\n",
      "Epoch 34/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.5002 - val_loss: 0.5075\n",
      "Epoch 35/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.4995 - val_loss: 0.5053\n",
      "Epoch 36/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4969 - val_loss: 0.5029\n",
      "Epoch 37/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.4939 - val_loss: 0.5014\n",
      "Epoch 38/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.4839 - val_loss: 0.4983\n",
      "Epoch 39/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4855 - val_loss: 0.4977\n",
      "Epoch 40/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4874 - val_loss: 0.4982\n",
      "Epoch 41/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4782 - val_loss: 0.4941\n",
      "Epoch 42/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4818 - val_loss: 0.4933\n",
      "Epoch 43/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.4700 - val_loss: 0.4912\n",
      "Epoch 44/100\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.4740 - val_loss: 0.4899\n",
      "Epoch 45/100\n",
      "6090/6090 [==============================] - 1s 117us/sample - loss: 0.4641 - val_loss: 0.4887\n",
      "Epoch 46/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.4634 - val_loss: 0.4878\n",
      "Epoch 47/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.4624 - val_loss: 0.4862\n",
      "Epoch 48/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.4647 - val_loss: 0.4855\n",
      "Epoch 49/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.4536 - val_loss: 0.4845\n",
      "Epoch 50/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.4570 - val_loss: 0.4840\n",
      "Epoch 51/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.4498 - val_loss: 0.4823\n",
      "Epoch 52/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.4488 - val_loss: 0.4814\n",
      "Epoch 53/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.4489 - val_loss: 0.4811\n",
      "Epoch 54/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.4448 - val_loss: 0.4797\n",
      "Epoch 55/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.4400 - val_loss: 0.4789\n",
      "Epoch 56/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.4363 - val_loss: 0.4789\n",
      "Epoch 57/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.4349 - val_loss: 0.4783\n",
      "Epoch 58/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.4373 - val_loss: 0.4779\n",
      "Epoch 59/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.4391 - val_loss: 0.4778\n",
      "Epoch 60/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.4336 - val_loss: 0.4764\n",
      "Epoch 61/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4308 - val_loss: 0.4762\n",
      "Epoch 62/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4270 - val_loss: 0.4751\n",
      "Epoch 63/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4234 - val_loss: 0.4755\n",
      "Epoch 64/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4271 - val_loss: 0.4751\n",
      "(Took 42.944 sec)\n",
      "Train on 6091 samples, validate on 1522 samples\n",
      "Epoch 1/100\n",
      "6091/6091 [==============================] - 1s 174us/sample - loss: 0.7890 - val_loss: 0.6764\n",
      "Epoch 2/100\n",
      "6091/6091 [==============================] - 1s 105us/sample - loss: 0.7046 - val_loss: 0.6709\n",
      "Epoch 3/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.6996 - val_loss: 0.6664\n",
      "Epoch 4/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.6886 - val_loss: 0.6628\n",
      "Epoch 5/100\n",
      "6091/6091 [==============================] - 1s 106us/sample - loss: 0.6818 - val_loss: 0.6595\n",
      "Epoch 6/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.6758 - val_loss: 0.6565\n",
      "Epoch 7/100\n",
      "6091/6091 [==============================] - 1s 105us/sample - loss: 0.6685 - val_loss: 0.6518\n",
      "Epoch 8/100\n",
      "6091/6091 [==============================] - 1s 104us/sample - loss: 0.6666 - val_loss: 0.6475\n",
      "Epoch 9/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.6586 - val_loss: 0.6413\n",
      "Epoch 10/100\n",
      "6091/6091 [==============================] - 1s 115us/sample - loss: 0.6505 - val_loss: 0.6341\n",
      "Epoch 11/100\n",
      "6091/6091 [==============================] - 1s 103us/sample - loss: 0.6447 - val_loss: 0.6263\n",
      "Epoch 12/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.6353 - val_loss: 0.6178\n",
      "Epoch 13/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.6268 - val_loss: 0.6080\n",
      "Epoch 14/100\n",
      "6091/6091 [==============================] - 1s 111us/sample - loss: 0.6222 - val_loss: 0.5993\n",
      "Epoch 15/100\n",
      "6091/6091 [==============================] - 1s 116us/sample - loss: 0.6098 - val_loss: 0.5892\n",
      "Epoch 16/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.6082 - val_loss: 0.5807\n",
      "Epoch 17/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.5954 - val_loss: 0.5721\n",
      "Epoch 18/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.5855 - val_loss: 0.5639\n",
      "Epoch 19/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.5789 - val_loss: 0.5569\n",
      "Epoch 20/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.5750 - val_loss: 0.5498\n",
      "Epoch 21/100\n",
      "6091/6091 [==============================] - 1s 110us/sample - loss: 0.5741 - val_loss: 0.5442\n",
      "Epoch 22/100\n",
      "6091/6091 [==============================] - 1s 106us/sample - loss: 0.5611 - val_loss: 0.5383\n",
      "Epoch 23/100\n",
      "6091/6091 [==============================] - 1s 116us/sample - loss: 0.5598 - val_loss: 0.5336\n",
      "Epoch 24/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.5540 - val_loss: 0.5286\n",
      "Epoch 25/100\n",
      "6091/6091 [==============================] - 1s 117us/sample - loss: 0.5446 - val_loss: 0.5238\n",
      "Epoch 26/100\n",
      "6091/6091 [==============================] - 1s 106us/sample - loss: 0.5479 - val_loss: 0.5202\n",
      "Epoch 27/100\n",
      "6091/6091 [==============================] - 1s 106us/sample - loss: 0.5422 - val_loss: 0.5163\n",
      "Epoch 28/100\n",
      "6091/6091 [==============================] - 1s 111us/sample - loss: 0.5296 - val_loss: 0.5123\n",
      "Epoch 29/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.5189 - val_loss: 0.5079\n",
      "Epoch 30/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.5280 - val_loss: 0.5048\n",
      "Epoch 31/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.5156 - val_loss: 0.5017\n",
      "Epoch 32/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.5078 - val_loss: 0.4984\n",
      "Epoch 33/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.5058 - val_loss: 0.4952\n",
      "Epoch 34/100\n",
      "6091/6091 [==============================] - 1s 118us/sample - loss: 0.5069 - val_loss: 0.4929\n",
      "Epoch 35/100\n",
      "6091/6091 [==============================] - 1s 110us/sample - loss: 0.4989 - val_loss: 0.4901\n",
      "Epoch 36/100\n",
      "6091/6091 [==============================] - 1s 118us/sample - loss: 0.5010 - val_loss: 0.4883\n",
      "Epoch 37/100\n",
      "6091/6091 [==============================] - 1s 106us/sample - loss: 0.4942 - val_loss: 0.4862\n",
      "Epoch 38/100\n",
      "6091/6091 [==============================] - 1s 110us/sample - loss: 0.4985 - val_loss: 0.4846\n",
      "Epoch 39/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.4865 - val_loss: 0.4825\n",
      "Epoch 40/100\n",
      "6091/6091 [==============================] - 1s 121us/sample - loss: 0.4850 - val_loss: 0.4807\n",
      "Epoch 41/100\n",
      "6091/6091 [==============================] - 1s 115us/sample - loss: 0.4850 - val_loss: 0.4791\n",
      "Epoch 42/100\n",
      "6091/6091 [==============================] - 1s 122us/sample - loss: 0.4818 - val_loss: 0.4778\n",
      "Epoch 43/100\n",
      "6091/6091 [==============================] - 1s 115us/sample - loss: 0.4792 - val_loss: 0.4759\n",
      "Epoch 44/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.4726 - val_loss: 0.4741\n",
      "Epoch 45/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.4680 - val_loss: 0.4727\n",
      "Epoch 46/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4668 - val_loss: 0.4710\n",
      "Epoch 47/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.4684 - val_loss: 0.4698\n",
      "Epoch 48/100\n",
      "6091/6091 [==============================] - 1s 117us/sample - loss: 0.4573 - val_loss: 0.4683\n",
      "Epoch 49/100\n",
      "6091/6091 [==============================] - 1s 111us/sample - loss: 0.4614 - val_loss: 0.4675\n",
      "Epoch 50/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.4573 - val_loss: 0.4664\n",
      "Epoch 51/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.4590 - val_loss: 0.4652\n",
      "Epoch 52/100\n",
      "6091/6091 [==============================] - 1s 125us/sample - loss: 0.4561 - val_loss: 0.4640\n",
      "Epoch 53/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4516 - val_loss: 0.4630\n",
      "Epoch 54/100\n",
      "6091/6091 [==============================] - 1s 115us/sample - loss: 0.4454 - val_loss: 0.4621\n",
      "Epoch 55/100\n",
      "6091/6091 [==============================] - 1s 113us/sample - loss: 0.4452 - val_loss: 0.4615\n",
      "Epoch 56/100\n",
      "6091/6091 [==============================] - 1s 113us/sample - loss: 0.4406 - val_loss: 0.4603\n",
      "Epoch 57/100\n",
      "6091/6091 [==============================] - 1s 111us/sample - loss: 0.4362 - val_loss: 0.4595\n",
      "Epoch 58/100\n",
      "6091/6091 [==============================] - 1s 113us/sample - loss: 0.4393 - val_loss: 0.4587\n",
      "Epoch 59/100\n",
      "6091/6091 [==============================] - 1s 114us/sample - loss: 0.4416 - val_loss: 0.4579\n",
      "Epoch 60/100\n",
      "6091/6091 [==============================] - 1s 118us/sample - loss: 0.4401 - val_loss: 0.4577\n",
      "Epoch 61/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4310 - val_loss: 0.4567\n",
      "Epoch 62/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4318 - val_loss: 0.4560\n",
      "Epoch 63/100\n",
      "6091/6091 [==============================] - 1s 105us/sample - loss: 0.4321 - val_loss: 0.4554\n",
      "Epoch 64/100\n",
      "6091/6091 [==============================] - 1s 106us/sample - loss: 0.4324 - val_loss: 0.4550\n",
      "Epoch 65/100\n",
      "6091/6091 [==============================] - 1s 105us/sample - loss: 0.4269 - val_loss: 0.4542\n",
      "Epoch 66/100\n",
      "6091/6091 [==============================] - 1s 106us/sample - loss: 0.4330 - val_loss: 0.4537\n",
      "Epoch 67/100\n",
      "6091/6091 [==============================] - 1s 106us/sample - loss: 0.4174 - val_loss: 0.4541\n",
      "Epoch 68/100\n",
      "6091/6091 [==============================] - 1s 106us/sample - loss: 0.4203 - val_loss: 0.4532\n",
      "Epoch 69/100\n",
      "6091/6091 [==============================] - 1s 104us/sample - loss: 0.4150 - val_loss: 0.4523\n",
      "Epoch 70/100\n",
      "6091/6091 [==============================] - 1s 111us/sample - loss: 0.4111 - val_loss: 0.4524\n",
      "Epoch 71/100\n",
      "6091/6091 [==============================] - 1s 115us/sample - loss: 0.4108 - val_loss: 0.4515\n",
      "Epoch 72/100\n",
      "6091/6091 [==============================] - 1s 118us/sample - loss: 0.4161 - val_loss: 0.4511\n",
      "Epoch 73/100\n",
      "6091/6091 [==============================] - 1s 131us/sample - loss: 0.4032 - val_loss: 0.4509\n",
      "Epoch 74/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.4051 - val_loss: 0.4506\n",
      "Epoch 75/100\n",
      "6091/6091 [==============================] - 1s 113us/sample - loss: 0.4076 - val_loss: 0.4503\n",
      "Epoch 76/100\n",
      "6091/6091 [==============================] - 1s 123us/sample - loss: 0.4100 - val_loss: 0.4504\n",
      "Epoch 77/100\n",
      "6091/6091 [==============================] - 1s 120us/sample - loss: 0.4066 - val_loss: 0.4504\n",
      "(Took 53.126 sec)\n",
      "Train on 6091 samples, validate on 1522 samples\n",
      "Epoch 1/100\n",
      "6091/6091 [==============================] - 1s 159us/sample - loss: 0.7315 - val_loss: 0.6803\n",
      "Epoch 2/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.7076 - val_loss: 0.6751\n",
      "Epoch 3/100\n",
      "6091/6091 [==============================] - 1s 124us/sample - loss: 0.6929 - val_loss: 0.6695\n",
      "Epoch 4/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.6882 - val_loss: 0.6667\n",
      "Epoch 5/100\n",
      "6091/6091 [==============================] - 1s 111us/sample - loss: 0.6752 - val_loss: 0.6635\n",
      "Epoch 6/100\n",
      "6091/6091 [==============================] - 1s 119us/sample - loss: 0.6715 - val_loss: 0.6590\n",
      "Epoch 7/100\n",
      "6091/6091 [==============================] - 1s 111us/sample - loss: 0.6690 - val_loss: 0.6564\n",
      "Epoch 8/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.6641 - val_loss: 0.6525\n",
      "Epoch 9/100\n",
      "6091/6091 [==============================] - 1s 121us/sample - loss: 0.6554 - val_loss: 0.6443\n",
      "Epoch 10/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.6469 - val_loss: 0.6376\n",
      "Epoch 11/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.6397 - val_loss: 0.6288\n",
      "Epoch 12/100\n",
      "6091/6091 [==============================] - 1s 122us/sample - loss: 0.6335 - val_loss: 0.6198\n",
      "Epoch 13/100\n",
      "6091/6091 [==============================] - 1s 116us/sample - loss: 0.6219 - val_loss: 0.6111\n",
      "Epoch 14/100\n",
      "6091/6091 [==============================] - 1s 120us/sample - loss: 0.6233 - val_loss: 0.6027\n",
      "Epoch 15/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.6089 - val_loss: 0.5933\n",
      "Epoch 16/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.6026 - val_loss: 0.5860\n",
      "Epoch 17/100\n",
      "6091/6091 [==============================] - 1s 110us/sample - loss: 0.5916 - val_loss: 0.5769\n",
      "Epoch 18/100\n",
      "6091/6091 [==============================] - 1s 110us/sample - loss: 0.5811 - val_loss: 0.5699\n",
      "Epoch 19/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.5804 - val_loss: 0.5617\n",
      "Epoch 20/100\n",
      "6091/6091 [==============================] - 1s 115us/sample - loss: 0.5703 - val_loss: 0.5549\n",
      "Epoch 21/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.5630 - val_loss: 0.5483\n",
      "Epoch 22/100\n",
      "6091/6091 [==============================] - 1s 123us/sample - loss: 0.5592 - val_loss: 0.5436\n",
      "Epoch 23/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.5549 - val_loss: 0.5391\n",
      "Epoch 24/100\n",
      "6091/6091 [==============================] - 1s 110us/sample - loss: 0.5486 - val_loss: 0.5346\n",
      "Epoch 25/100\n",
      "6091/6091 [==============================] - 1s 116us/sample - loss: 0.5461 - val_loss: 0.5300\n",
      "Epoch 26/100\n",
      "6091/6091 [==============================] - 1s 121us/sample - loss: 0.5394 - val_loss: 0.5259\n",
      "Epoch 27/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.5362 - val_loss: 0.5224\n",
      "Epoch 28/100\n",
      "6091/6091 [==============================] - 1s 114us/sample - loss: 0.5299 - val_loss: 0.5188\n",
      "Epoch 29/100\n",
      "6091/6091 [==============================] - 1s 113us/sample - loss: 0.5249 - val_loss: 0.5159\n",
      "Epoch 30/100\n",
      "6091/6091 [==============================] - 1s 110us/sample - loss: 0.5235 - val_loss: 0.5126\n",
      "Epoch 31/100\n",
      "6091/6091 [==============================] - 1s 137us/sample - loss: 0.5101 - val_loss: 0.5092\n",
      "Epoch 32/100\n",
      "6091/6091 [==============================] - 1s 127us/sample - loss: 0.5107 - val_loss: 0.5065\n",
      "Epoch 33/100\n",
      "6091/6091 [==============================] - 1s 114us/sample - loss: 0.5124 - val_loss: 0.5044\n",
      "Epoch 34/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.4997 - val_loss: 0.5015\n",
      "Epoch 35/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.5016 - val_loss: 0.4994\n",
      "Epoch 36/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.4958 - val_loss: 0.4978\n",
      "Epoch 37/100\n",
      "6091/6091 [==============================] - 1s 117us/sample - loss: 0.4934 - val_loss: 0.4957\n",
      "Epoch 38/100\n",
      "6091/6091 [==============================] - 1s 113us/sample - loss: 0.4893 - val_loss: 0.4932\n",
      "Epoch 39/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6091/6091 [==============================] - 1s 111us/sample - loss: 0.4900 - val_loss: 0.4934\n",
      "Epoch 40/100\n",
      "6091/6091 [==============================] - 1s 119us/sample - loss: 0.4864 - val_loss: 0.4905\n",
      "Epoch 41/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4781 - val_loss: 0.4883\n",
      "Epoch 42/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.4715 - val_loss: 0.4866\n",
      "Epoch 43/100\n",
      "6091/6091 [==============================] - 1s 105us/sample - loss: 0.4758 - val_loss: 0.4854\n",
      "Epoch 44/100\n",
      "6091/6091 [==============================] - 1s 110us/sample - loss: 0.4806 - val_loss: 0.4848\n",
      "Epoch 45/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.4683 - val_loss: 0.4834\n",
      "Epoch 46/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4667 - val_loss: 0.4822\n",
      "Epoch 47/100\n",
      "6091/6091 [==============================] - 1s 106us/sample - loss: 0.4647 - val_loss: 0.4812\n",
      "Epoch 48/100\n",
      "6091/6091 [==============================] - 1s 119us/sample - loss: 0.4591 - val_loss: 0.4808\n",
      "Epoch 49/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.4472 - val_loss: 0.4787\n",
      "Epoch 50/100\n",
      "6091/6091 [==============================] - 1s 117us/sample - loss: 0.4644 - val_loss: 0.4784\n",
      "Epoch 51/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4578 - val_loss: 0.4771\n",
      "Epoch 52/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.4537 - val_loss: 0.4765\n",
      "Epoch 53/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.4509 - val_loss: 0.4759\n",
      "Epoch 54/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.4496 - val_loss: 0.4753\n",
      "Epoch 55/100\n",
      "6091/6091 [==============================] - 1s 110us/sample - loss: 0.4434 - val_loss: 0.4761\n",
      "Epoch 56/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4419 - val_loss: 0.4740\n",
      "Epoch 57/100\n",
      "6091/6091 [==============================] - 1s 121us/sample - loss: 0.4423 - val_loss: 0.4734\n",
      "Epoch 58/100\n",
      "6091/6091 [==============================] - 1s 121us/sample - loss: 0.4334 - val_loss: 0.4729\n",
      "Epoch 59/100\n",
      "6091/6091 [==============================] - 1s 130us/sample - loss: 0.4277 - val_loss: 0.4724\n",
      "Epoch 60/100\n",
      "6091/6091 [==============================] - 1s 117us/sample - loss: 0.4341 - val_loss: 0.4722\n",
      "Epoch 61/100\n",
      "6091/6091 [==============================] - ETA: 0s - loss: 0.431 - 1s 115us/sample - loss: 0.4320 - val_loss: 0.4714\n",
      "Epoch 62/100\n",
      "6091/6091 [==============================] - 1s 114us/sample - loss: 0.4252 - val_loss: 0.4708\n",
      "Epoch 63/100\n",
      "6091/6091 [==============================] - 1s 138us/sample - loss: 0.4262 - val_loss: 0.4707\n",
      "Epoch 64/100\n",
      "6091/6091 [==============================] - 1s 123us/sample - loss: 0.4257 - val_loss: 0.4706\n",
      "Epoch 65/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.4212 - val_loss: 0.4700\n",
      "Epoch 66/100\n",
      "6091/6091 [==============================] - 1s 119us/sample - loss: 0.4160 - val_loss: 0.4705\n",
      "Epoch 67/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.4146 - val_loss: 0.4708\n",
      "(Took 47.313 sec)\n",
      "Combined confusion matrix:\n",
      "[[3783.  559.]\n",
      " [1034. 2237.]]\n",
      "(Overall, took 249.511 sec)\n",
      "Accuracy: 79.08% +/- 0.84%\n",
      "Precision for positive class: 78.54% +/- 1.95%\n",
      "Precision for negative class: 80.02% +/- 1.66%\n",
      "Recall for positive class: 87.13% +/- 1.21%\n",
      "Recall for negative class: 68.42% +/- 2.10%\n",
      "F for positive class: 82.60% +/- 0.95%\n",
      "F for negative class: 73.74% +/- 0.79%\n",
      "Mean F score: 78.17% +/- 0.75%\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/100\n",
      "6090/6090 [==============================] - 1s 172us/sample - loss: 0.7249 - val_loss: 0.6699\n",
      "Epoch 2/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.6986 - val_loss: 0.6624\n",
      "Epoch 3/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.6866 - val_loss: 0.6555\n",
      "Epoch 4/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.6690 - val_loss: 0.6486\n",
      "Epoch 5/100\n",
      "6090/6090 [==============================] - 1s 123us/sample - loss: 0.6606 - val_loss: 0.6398\n",
      "Epoch 6/100\n",
      "6090/6090 [==============================] - 1s 117us/sample - loss: 0.6488 - val_loss: 0.6282\n",
      "Epoch 7/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.6427 - val_loss: 0.6176\n",
      "Epoch 8/100\n",
      "6090/6090 [==============================] - 1s 118us/sample - loss: 0.6236 - val_loss: 0.6014\n",
      "Epoch 9/100\n",
      "6090/6090 [==============================] - 1s 116us/sample - loss: 0.6093 - val_loss: 0.5851\n",
      "Epoch 10/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.5964 - val_loss: 0.5691\n",
      "Epoch 11/100\n",
      "6090/6090 [==============================] - 1s 118us/sample - loss: 0.5783 - val_loss: 0.5531\n",
      "Epoch 12/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.5598 - val_loss: 0.5380\n",
      "Epoch 13/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.5510 - val_loss: 0.5252\n",
      "Epoch 14/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.5387 - val_loss: 0.5141\n",
      "Epoch 15/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.5228 - val_loss: 0.5042\n",
      "Epoch 16/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.5149 - val_loss: 0.4962\n",
      "Epoch 17/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.5118 - val_loss: 0.4891\n",
      "Epoch 18/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.4938 - val_loss: 0.4827\n",
      "Epoch 19/100\n",
      "6090/6090 [==============================] - 1s 116us/sample - loss: 0.4823 - val_loss: 0.4773\n",
      "Epoch 20/100\n",
      "6090/6090 [==============================] - 1s 116us/sample - loss: 0.4804 - val_loss: 0.4727\n",
      "Epoch 21/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.4666 - val_loss: 0.4688\n",
      "Epoch 22/100\n",
      "6090/6090 [==============================] - 1s 123us/sample - loss: 0.4655 - val_loss: 0.4655\n",
      "Epoch 23/100\n",
      "6090/6090 [==============================] - ETA: 0s - loss: 0.452 - 1s 114us/sample - loss: 0.4525 - val_loss: 0.4627\n",
      "Epoch 24/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4498 - val_loss: 0.4600\n",
      "Epoch 25/100\n",
      "6090/6090 [==============================] - 1s 120us/sample - loss: 0.4444 - val_loss: 0.4579\n",
      "Epoch 26/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.4400 - val_loss: 0.4562\n",
      "Epoch 27/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4245 - val_loss: 0.4542\n",
      "Epoch 28/100\n",
      "6090/6090 [==============================] - 1s 116us/sample - loss: 0.4237 - val_loss: 0.4533\n",
      "Epoch 29/100\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.4252 - val_loss: 0.4520\n",
      "Epoch 30/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4196 - val_loss: 0.4511\n",
      "Epoch 31/100\n",
      "6090/6090 [==============================] - 1s 122us/sample - loss: 0.4050 - val_loss: 0.4503\n",
      "Epoch 32/100\n",
      "6090/6090 [==============================] - 1s 117us/sample - loss: 0.4166 - val_loss: 0.4499\n",
      "Epoch 33/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.4114 - val_loss: 0.4495\n",
      "Epoch 34/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4024 - val_loss: 0.4491\n",
      "Epoch 35/100\n",
      "6090/6090 [==============================] - 1s 123us/sample - loss: 0.3958 - val_loss: 0.4490\n",
      "Epoch 36/100\n",
      "6090/6090 [==============================] - 1s 116us/sample - loss: 0.3936 - val_loss: 0.4489\n",
      "Epoch 37/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.3955 - val_loss: 0.4488\n",
      "Epoch 38/100\n",
      "6090/6090 [==============================] - 1s 117us/sample - loss: 0.3819 - val_loss: 0.4489\n",
      "Epoch 39/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.3886 - val_loss: 0.4489\n",
      "(Took 27.897 sec)\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/100\n",
      "6090/6090 [==============================] - 1s 163us/sample - loss: 0.7348 - val_loss: 0.6659\n",
      "Epoch 2/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.7104 - val_loss: 0.6578\n",
      "Epoch 3/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.6847 - val_loss: 0.6499\n",
      "Epoch 4/100\n",
      "6090/6090 [==============================] - 1s 120us/sample - loss: 0.6741 - val_loss: 0.6427\n",
      "Epoch 5/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.6658 - val_loss: 0.6360\n",
      "Epoch 6/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.6552 - val_loss: 0.6253\n",
      "Epoch 7/100\n",
      "6090/6090 [==============================] - 1s 121us/sample - loss: 0.6389 - val_loss: 0.6135\n",
      "Epoch 8/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.6312 - val_loss: 0.5996\n",
      "Epoch 9/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.6126 - val_loss: 0.5857\n",
      "Epoch 10/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.5965 - val_loss: 0.5696\n",
      "Epoch 11/100\n",
      "6090/6090 [==============================] - 1s 123us/sample - loss: 0.5790 - val_loss: 0.5533\n",
      "Epoch 12/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.5691 - val_loss: 0.5399\n",
      "Epoch 13/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.5564 - val_loss: 0.5271\n",
      "Epoch 14/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.5453 - val_loss: 0.5163\n",
      "Epoch 15/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.5263 - val_loss: 0.5066\n",
      "Epoch 16/100\n",
      "6090/6090 [==============================] - 1s 119us/sample - loss: 0.5181 - val_loss: 0.4981\n",
      "Epoch 17/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.5033 - val_loss: 0.4911\n",
      "Epoch 18/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.4986 - val_loss: 0.4850\n",
      "Epoch 19/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.4901 - val_loss: 0.4798\n",
      "Epoch 20/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4765 - val_loss: 0.4752\n",
      "Epoch 21/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4729 - val_loss: 0.4725\n",
      "Epoch 22/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4587 - val_loss: 0.4688\n",
      "Epoch 23/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4573 - val_loss: 0.4662\n",
      "Epoch 24/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4447 - val_loss: 0.4639\n",
      "Epoch 25/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4454 - val_loss: 0.4634\n",
      "Epoch 26/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4393 - val_loss: 0.4609\n",
      "Epoch 27/100\n",
      "6090/6090 [==============================] - 1s 121us/sample - loss: 0.4242 - val_loss: 0.4599\n",
      "Epoch 28/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.4283 - val_loss: 0.4587\n",
      "Epoch 29/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.4237 - val_loss: 0.4585\n",
      "Epoch 30/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4136 - val_loss: 0.4578\n",
      "Epoch 31/100\n",
      "6090/6090 [==============================] - 1s 118us/sample - loss: 0.4176 - val_loss: 0.4576\n",
      "Epoch 32/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.4032 - val_loss: 0.4575\n",
      "Epoch 33/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.4070 - val_loss: 0.4571\n",
      "Epoch 34/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.3990 - val_loss: 0.4574\n",
      "Epoch 35/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4058 - val_loss: 0.4573\n",
      "(Took 24.556 sec)\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/100\n",
      "6090/6090 [==============================] - 1s 160us/sample - loss: 0.7277 - val_loss: 0.6761\n",
      "Epoch 2/100\n",
      "6090/6090 [==============================] - 1s 120us/sample - loss: 0.6997 - val_loss: 0.6682\n",
      "Epoch 3/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.6807 - val_loss: 0.6607\n",
      "Epoch 4/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.6671 - val_loss: 0.6534\n",
      "Epoch 5/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.6658 - val_loss: 0.6462\n",
      "Epoch 6/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.6481 - val_loss: 0.6353\n",
      "Epoch 7/100\n",
      "6090/6090 [==============================] - 1s 120us/sample - loss: 0.6312 - val_loss: 0.6236\n",
      "Epoch 8/100\n",
      "6090/6090 [==============================] - 1s 127us/sample - loss: 0.6279 - val_loss: 0.6110\n",
      "Epoch 9/100\n",
      "6090/6090 [==============================] - 1s 117us/sample - loss: 0.6070 - val_loss: 0.5946\n",
      "Epoch 10/100\n",
      "6090/6090 [==============================] - 1s 119us/sample - loss: 0.5936 - val_loss: 0.5794\n",
      "Epoch 11/100\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.5737 - val_loss: 0.5643\n",
      "Epoch 12/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.5582 - val_loss: 0.5510\n",
      "Epoch 13/100\n",
      "6090/6090 [==============================] - 1s 123us/sample - loss: 0.5481 - val_loss: 0.5382\n",
      "Epoch 14/100\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.5349 - val_loss: 0.5295\n",
      "Epoch 15/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.5284 - val_loss: 0.5188\n",
      "Epoch 16/100\n",
      "6090/6090 [==============================] - 1s 118us/sample - loss: 0.5190 - val_loss: 0.5123\n",
      "Epoch 17/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.4990 - val_loss: 0.5041\n",
      "Epoch 18/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4897 - val_loss: 0.4978\n",
      "Epoch 19/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4787 - val_loss: 0.4947\n",
      "Epoch 20/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.4702 - val_loss: 0.4895\n",
      "Epoch 21/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.4639 - val_loss: 0.4857\n",
      "Epoch 22/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.4555 - val_loss: 0.4842\n",
      "Epoch 23/100\n",
      "6090/6090 [==============================] - 1s 122us/sample - loss: 0.4529 - val_loss: 0.4814\n",
      "Epoch 24/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4547 - val_loss: 0.4788\n",
      "Epoch 25/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4327 - val_loss: 0.4780\n",
      "Epoch 26/100\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.4397 - val_loss: 0.4758\n",
      "Epoch 27/100\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.4360 - val_loss: 0.4766\n",
      "Epoch 28/100\n",
      "6090/6090 [==============================] - 1s 116us/sample - loss: 0.4300 - val_loss: 0.4746\n",
      "Epoch 29/100\n",
      "6090/6090 [==============================] - 1s 123us/sample - loss: 0.4165 - val_loss: 0.4727\n",
      "Epoch 30/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.4183 - val_loss: 0.4745\n",
      "Epoch 31/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4074 - val_loss: 0.4729\n",
      "(Took 22.370 sec)\n",
      "Train on 6091 samples, validate on 1522 samples\n",
      "Epoch 1/100\n",
      "6091/6091 [==============================] - 2s 342us/sample - loss: 0.7287 - val_loss: 0.6710\n",
      "Epoch 2/100\n",
      "6091/6091 [==============================] - 1s 123us/sample - loss: 0.7000 - val_loss: 0.6623\n",
      "Epoch 3/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.6876 - val_loss: 0.6549\n",
      "Epoch 4/100\n",
      "6091/6091 [==============================] - 1s 113us/sample - loss: 0.6727 - val_loss: 0.6475\n",
      "Epoch 5/100\n",
      "6091/6091 [==============================] - 1s 120us/sample - loss: 0.6601 - val_loss: 0.6390\n",
      "Epoch 6/100\n",
      "6091/6091 [==============================] - 1s 115us/sample - loss: 0.6499 - val_loss: 0.6284\n",
      "Epoch 7/100\n",
      "6091/6091 [==============================] - 1s 119us/sample - loss: 0.6388 - val_loss: 0.6154\n",
      "Epoch 8/100\n",
      "6091/6091 [==============================] - 1s 115us/sample - loss: 0.6253 - val_loss: 0.6018\n",
      "Epoch 9/100\n",
      "6091/6091 [==============================] - 1s 116us/sample - loss: 0.6072 - val_loss: 0.5858\n",
      "Epoch 10/100\n",
      "6091/6091 [==============================] - 1s 113us/sample - loss: 0.5955 - val_loss: 0.5703\n",
      "Epoch 11/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.5773 - val_loss: 0.5549\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/100\n",
      "6091/6091 [==============================] - 1s 124us/sample - loss: 0.5650 - val_loss: 0.5408\n",
      "Epoch 13/100\n",
      "6091/6091 [==============================] - ETA: 0s - loss: 0.548 - 1s 115us/sample - loss: 0.5482 - val_loss: 0.5285\n",
      "Epoch 14/100\n",
      "6091/6091 [==============================] - 1s 113us/sample - loss: 0.5382 - val_loss: 0.5178\n",
      "Epoch 15/100\n",
      "6091/6091 [==============================] - 1s 120us/sample - loss: 0.5231 - val_loss: 0.5077\n",
      "Epoch 16/100\n",
      "6091/6091 [==============================] - 1s 106us/sample - loss: 0.5124 - val_loss: 0.4991\n",
      "Epoch 17/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.5036 - val_loss: 0.4914\n",
      "Epoch 18/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.5006 - val_loss: 0.4854\n",
      "Epoch 19/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.4860 - val_loss: 0.4808\n",
      "Epoch 20/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.4825 - val_loss: 0.4754\n",
      "Epoch 21/100\n",
      "6091/6091 [==============================] - 1s 113us/sample - loss: 0.4663 - val_loss: 0.4708\n",
      "Epoch 22/100\n",
      "6091/6091 [==============================] - 1s 118us/sample - loss: 0.4602 - val_loss: 0.4674\n",
      "Epoch 23/100\n",
      "6091/6091 [==============================] - 1s 115us/sample - loss: 0.4552 - val_loss: 0.4641\n",
      "Epoch 24/100\n",
      "6091/6091 [==============================] - 1s 116us/sample - loss: 0.4475 - val_loss: 0.4610\n",
      "Epoch 25/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.4391 - val_loss: 0.4587\n",
      "Epoch 26/100\n",
      "6091/6091 [==============================] - 1s 110us/sample - loss: 0.4350 - val_loss: 0.4568\n",
      "Epoch 27/100\n",
      "6091/6091 [==============================] - 1s 120us/sample - loss: 0.4376 - val_loss: 0.4553\n",
      "Epoch 28/100\n",
      "6091/6091 [==============================] - 1s 117us/sample - loss: 0.4327 - val_loss: 0.4542\n",
      "Epoch 29/100\n",
      "6091/6091 [==============================] - 1s 117us/sample - loss: 0.4328 - val_loss: 0.4526\n",
      "Epoch 30/100\n",
      "6091/6091 [==============================] - 1s 111us/sample - loss: 0.4151 - val_loss: 0.4523\n",
      "Epoch 31/100\n",
      "6091/6091 [==============================] - 1s 111us/sample - loss: 0.4165 - val_loss: 0.4506\n",
      "Epoch 32/100\n",
      "6091/6091 [==============================] - 1s 111us/sample - loss: 0.4098 - val_loss: 0.4516\n",
      "Epoch 33/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.4084 - val_loss: 0.4489\n",
      "Epoch 34/100\n",
      "6091/6091 [==============================] - 1s 110us/sample - loss: 0.4095 - val_loss: 0.4484\n",
      "Epoch 35/100\n",
      "6091/6091 [==============================] - 1s 119us/sample - loss: 0.4004 - val_loss: 0.4482\n",
      "Epoch 36/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.3978 - val_loss: 0.4477\n",
      "Epoch 37/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.3938 - val_loss: 0.4486\n",
      "Epoch 38/100\n",
      "6091/6091 [==============================] - 1s 120us/sample - loss: 0.3955 - val_loss: 0.4478\n",
      "(Took 28.246 sec)\n",
      "Train on 6091 samples, validate on 1522 samples\n",
      "Epoch 1/100\n",
      "6091/6091 [==============================] - 1s 167us/sample - loss: 0.7310 - val_loss: 0.6747\n",
      "Epoch 2/100\n",
      "6091/6091 [==============================] - 1s 115us/sample - loss: 0.7041 - val_loss: 0.6662\n",
      "Epoch 3/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.6829 - val_loss: 0.6584\n",
      "Epoch 4/100\n",
      "6091/6091 [==============================] - 1s 113us/sample - loss: 0.6735 - val_loss: 0.6519\n",
      "Epoch 5/100\n",
      "6091/6091 [==============================] - 1s 115us/sample - loss: 0.6634 - val_loss: 0.6442\n",
      "Epoch 6/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.6455 - val_loss: 0.6366\n",
      "Epoch 7/100\n",
      "6091/6091 [==============================] - 1s 113us/sample - loss: 0.6372 - val_loss: 0.6221\n",
      "Epoch 8/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.6230 - val_loss: 0.6070\n",
      "Epoch 9/100\n",
      "6091/6091 [==============================] - 1s 110us/sample - loss: 0.6073 - val_loss: 0.5918\n",
      "Epoch 10/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.5901 - val_loss: 0.5789\n",
      "Epoch 11/100\n",
      "6091/6091 [==============================] - 1s 122us/sample - loss: 0.5826 - val_loss: 0.5622\n",
      "Epoch 12/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.5638 - val_loss: 0.5489\n",
      "Epoch 13/100\n",
      "6091/6091 [==============================] - 1s 113us/sample - loss: 0.5461 - val_loss: 0.5351\n",
      "Epoch 14/100\n",
      "6091/6091 [==============================] - 1s 115us/sample - loss: 0.5376 - val_loss: 0.5238\n",
      "Epoch 15/100\n",
      "6091/6091 [==============================] - 1s 114us/sample - loss: 0.5298 - val_loss: 0.5155\n",
      "Epoch 16/100\n",
      "6091/6091 [==============================] - 1s 120us/sample - loss: 0.5176 - val_loss: 0.5070\n",
      "Epoch 17/100\n",
      "6091/6091 [==============================] - 1s 110us/sample - loss: 0.5050 - val_loss: 0.5002\n",
      "Epoch 18/100\n",
      "6091/6091 [==============================] - 1s 113us/sample - loss: 0.4874 - val_loss: 0.4924\n",
      "Epoch 19/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4771 - val_loss: 0.4874\n",
      "Epoch 20/100\n",
      "6091/6091 [==============================] - 1s 115us/sample - loss: 0.4815 - val_loss: 0.4830\n",
      "Epoch 21/100\n",
      "6091/6091 [==============================] - 1s 113us/sample - loss: 0.4677 - val_loss: 0.4792\n",
      "Epoch 22/100\n",
      "6091/6091 [==============================] - 1s 113us/sample - loss: 0.4614 - val_loss: 0.4766\n",
      "Epoch 23/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.4522 - val_loss: 0.4739\n",
      "Epoch 24/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4571 - val_loss: 0.4722\n",
      "Epoch 25/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.4408 - val_loss: 0.4713\n",
      "Epoch 26/100\n",
      "6091/6091 [==============================] - 1s 113us/sample - loss: 0.4334 - val_loss: 0.4689\n",
      "Epoch 27/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.4353 - val_loss: 0.4677\n",
      "Epoch 28/100\n",
      "6091/6091 [==============================] - 1s 122us/sample - loss: 0.4311 - val_loss: 0.4666\n",
      "Epoch 29/100\n",
      "6091/6091 [==============================] - 1s 111us/sample - loss: 0.4163 - val_loss: 0.4660\n",
      "Epoch 30/100\n",
      "6091/6091 [==============================] - 1s 124us/sample - loss: 0.4154 - val_loss: 0.4656\n",
      "Epoch 31/100\n",
      "6091/6091 [==============================] - 1s 122us/sample - loss: 0.4193 - val_loss: 0.4649\n",
      "Epoch 32/100\n",
      "6091/6091 [==============================] - 1s 127us/sample - loss: 0.4083 - val_loss: 0.4653\n",
      "Epoch 33/100\n",
      "6091/6091 [==============================] - 1s 117us/sample - loss: 0.4032 - val_loss: 0.4655\n",
      "(Took 23.771 sec)\n",
      "Combined confusion matrix:\n",
      "[[3808.  534.]\n",
      " [1056. 2215.]]\n",
      "(Overall, took 128.079 sec)\n",
      "Accuracy: 79.11% +/- 1.12%\n",
      "Precision for positive class: 78.29% +/- 2.13%\n",
      "Precision for negative class: 80.59% +/- 1.04%\n",
      "Recall for positive class: 87.70% +/- 1.27%\n",
      "Recall for negative class: 67.74% +/- 2.67%\n",
      "F for positive class: 82.72% +/- 1.22%\n",
      "F for negative class: 73.58% +/- 1.18%\n",
      "Mean F score: 78.15% +/- 1.04%\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/100\n",
      "6090/6090 [==============================] - 1s 177us/sample - loss: 0.7061 - val_loss: 0.5931\n",
      "Epoch 2/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.6136 - val_loss: 0.5468\n",
      "Epoch 3/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.5733 - val_loss: 0.5176\n",
      "Epoch 4/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.5269 - val_loss: 0.4914\n",
      "Epoch 5/100\n",
      "6090/6090 [==============================] - 1s 121us/sample - loss: 0.4992 - val_loss: 0.4748\n",
      "Epoch 6/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4701 - val_loss: 0.4621\n",
      "Epoch 7/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.4553 - val_loss: 0.4550\n",
      "Epoch 8/100\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.4399 - val_loss: 0.4485\n",
      "Epoch 9/100\n",
      "6090/6090 [==============================] - 1s 126us/sample - loss: 0.4188 - val_loss: 0.4439\n",
      "Epoch 10/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.4091 - val_loss: 0.4413\n",
      "Epoch 11/100\n",
      "6090/6090 [==============================] - 1s 123us/sample - loss: 0.4028 - val_loss: 0.4402\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.3884 - val_loss: 0.4386\n",
      "Epoch 13/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.3820 - val_loss: 0.4399\n",
      "Epoch 14/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.3671 - val_loss: 0.4405\n",
      "(Took 10.605 sec)\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/100\n",
      "6090/6090 [==============================] - 1s 160us/sample - loss: 0.6960 - val_loss: 0.5967\n",
      "Epoch 2/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.6087 - val_loss: 0.5555\n",
      "Epoch 3/100\n",
      "6090/6090 [==============================] - 1s 117us/sample - loss: 0.5670 - val_loss: 0.5275\n",
      "Epoch 4/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.5216 - val_loss: 0.5064\n",
      "Epoch 5/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4970 - val_loss: 0.4910\n",
      "Epoch 6/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4745 - val_loss: 0.4791\n",
      "Epoch 7/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.4541 - val_loss: 0.4708\n",
      "Epoch 8/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4318 - val_loss: 0.4643\n",
      "Epoch 9/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.4270 - val_loss: 0.4604\n",
      "Epoch 10/100\n",
      "6090/6090 [==============================] - 1s 119us/sample - loss: 0.4052 - val_loss: 0.4573\n",
      "Epoch 11/100\n",
      "6090/6090 [==============================] - 1s 118us/sample - loss: 0.3922 - val_loss: 0.4557\n",
      "Epoch 12/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.3786 - val_loss: 0.4550\n",
      "Epoch 13/100\n",
      "6090/6090 [==============================] - 1s 119us/sample - loss: 0.3765 - val_loss: 0.4558\n",
      "Epoch 14/100\n",
      "6090/6090 [==============================] - 1s 116us/sample - loss: 0.3654 - val_loss: 0.4574\n",
      "(Took 10.429 sec)\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/100\n",
      "6090/6090 [==============================] - 1s 165us/sample - loss: 0.6795 - val_loss: 0.6102\n",
      "Epoch 2/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.6099 - val_loss: 0.5673\n",
      "Epoch 3/100\n",
      "6090/6090 [==============================] - 1s 120us/sample - loss: 0.5648 - val_loss: 0.5320\n",
      "Epoch 4/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.5224 - val_loss: 0.5091\n",
      "Epoch 5/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4889 - val_loss: 0.4931\n",
      "Epoch 6/100\n",
      "6090/6090 [==============================] - 1s 122us/sample - loss: 0.4653 - val_loss: 0.4800\n",
      "Epoch 7/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4505 - val_loss: 0.4717\n",
      "Epoch 8/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.4291 - val_loss: 0.4664\n",
      "Epoch 9/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.4201 - val_loss: 0.4604\n",
      "Epoch 10/100\n",
      "6090/6090 [==============================] - 1s 124us/sample - loss: 0.4032 - val_loss: 0.4588\n",
      "Epoch 11/100\n",
      "6090/6090 [==============================] - 1s 116us/sample - loss: 0.3861 - val_loss: 0.4561\n",
      "Epoch 12/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.3794 - val_loss: 0.4582\n",
      "Epoch 13/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.3776 - val_loss: 0.4564\n",
      "(Took 9.805 sec)\n",
      "Train on 6091 samples, validate on 1522 samples\n",
      "Epoch 1/100\n",
      "6091/6091 [==============================] - 1s 174us/sample - loss: 0.6864 - val_loss: 0.6077\n",
      "Epoch 2/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.6152 - val_loss: 0.5623\n",
      "Epoch 3/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.5614 - val_loss: 0.5268\n",
      "Epoch 4/100\n",
      "6091/6091 [==============================] - 1s 106us/sample - loss: 0.5282 - val_loss: 0.5007\n",
      "Epoch 5/100\n",
      "6091/6091 [==============================] - 1s 105us/sample - loss: 0.4983 - val_loss: 0.4818\n",
      "Epoch 6/100\n",
      "6091/6091 [==============================] - 1s 106us/sample - loss: 0.4736 - val_loss: 0.4684\n",
      "Epoch 7/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.4529 - val_loss: 0.4593\n",
      "Epoch 8/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4315 - val_loss: 0.4521\n",
      "Epoch 9/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.4194 - val_loss: 0.4475\n",
      "Epoch 10/100\n",
      "6091/6091 [==============================] - 1s 113us/sample - loss: 0.4042 - val_loss: 0.4448\n",
      "Epoch 11/100\n",
      "6091/6091 [==============================] - 1s 122us/sample - loss: 0.3844 - val_loss: 0.4425\n",
      "Epoch 12/100\n",
      "6091/6091 [==============================] - 1s 113us/sample - loss: 0.3857 - val_loss: 0.4411\n",
      "Epoch 13/100\n",
      "6091/6091 [==============================] - 1s 120us/sample - loss: 0.3765 - val_loss: 0.4421\n",
      "Epoch 14/100\n",
      "6091/6091 [==============================] - 1s 114us/sample - loss: 0.3664 - val_loss: 0.4432\n",
      "(Took 10.326 sec)\n",
      "Train on 6091 samples, validate on 1522 samples\n",
      "Epoch 1/100\n",
      "6091/6091 [==============================] - 1s 166us/sample - loss: 0.6917 - val_loss: 0.6021\n",
      "Epoch 2/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.6168 - val_loss: 0.5608\n",
      "Epoch 3/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.5588 - val_loss: 0.5298\n",
      "Epoch 4/100\n",
      "6091/6091 [==============================] - 1s 111us/sample - loss: 0.5295 - val_loss: 0.5062\n",
      "Epoch 5/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4959 - val_loss: 0.4885\n",
      "Epoch 6/100\n",
      "6091/6091 [==============================] - 1s 122us/sample - loss: 0.4690 - val_loss: 0.4762\n",
      "Epoch 7/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4502 - val_loss: 0.4678\n",
      "Epoch 8/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.4329 - val_loss: 0.4598\n",
      "Epoch 9/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.4175 - val_loss: 0.4543\n",
      "Epoch 10/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.4038 - val_loss: 0.4520\n",
      "Epoch 11/100\n",
      "6091/6091 [==============================] - 1s 118us/sample - loss: 0.3939 - val_loss: 0.4501\n",
      "Epoch 12/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.3787 - val_loss: 0.4490\n",
      "Epoch 13/100\n",
      "6091/6091 [==============================] - 1s 111us/sample - loss: 0.3785 - val_loss: 0.4512\n",
      "Epoch 14/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.3640 - val_loss: 0.4513\n",
      "(Took 10.334 sec)\n",
      "Combined confusion matrix:\n",
      "[[3821.  521.]\n",
      " [ 954. 2317.]]\n",
      "(Overall, took 52.158 sec)\n",
      "Accuracy: 80.63% +/- 0.67%\n",
      "Precision for positive class: 80.02% +/- 1.49%\n",
      "Precision for negative class: 81.64% +/- 1.10%\n",
      "Recall for positive class: 88.00% +/- 0.91%\n",
      "Recall for negative class: 70.83% +/- 1.91%\n",
      "F for positive class: 83.81% +/- 0.82%\n",
      "F for negative class: 75.84% +/- 0.99%\n",
      "Mean F score: 79.83% +/- 0.64%\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/100\n",
      "6090/6090 [==============================] - 1s 173us/sample - loss: 0.7422 - val_loss: 0.6730\n",
      "Epoch 2/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.7128 - val_loss: 0.6666\n",
      "Epoch 3/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.6985 - val_loss: 0.6618\n",
      "Epoch 4/100\n",
      "6090/6090 [==============================] - 1s 126us/sample - loss: 0.6874 - val_loss: 0.6589\n",
      "Epoch 5/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.6757 - val_loss: 0.6537\n",
      "Epoch 6/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.6705 - val_loss: 0.6500\n",
      "Epoch 7/100\n",
      "6090/6090 [==============================] - 1s 120us/sample - loss: 0.6687 - val_loss: 0.6450\n",
      "Epoch 8/100\n",
      "6090/6090 [==============================] - 1s 126us/sample - loss: 0.6585 - val_loss: 0.6395\n",
      "Epoch 9/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.6532 - val_loss: 0.6321\n",
      "Epoch 10/100\n",
      "6090/6090 [==============================] - 1s 118us/sample - loss: 0.6455 - val_loss: 0.6248\n",
      "Epoch 11/100\n",
      "6090/6090 [==============================] - 1s 116us/sample - loss: 0.6387 - val_loss: 0.6166\n",
      "Epoch 12/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.6350 - val_loss: 0.6078\n",
      "Epoch 13/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.6227 - val_loss: 0.5983\n",
      "Epoch 14/100\n",
      "6090/6090 [==============================] - 1s 124us/sample - loss: 0.6151 - val_loss: 0.5882\n",
      "Epoch 15/100\n",
      "6090/6090 [==============================] - 1s 125us/sample - loss: 0.6079 - val_loss: 0.5789\n",
      "Epoch 16/100\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.6021 - val_loss: 0.5718\n",
      "Epoch 17/100\n",
      "6090/6090 [==============================] - 1s 132us/sample - loss: 0.5890 - val_loss: 0.5603\n",
      "Epoch 18/100\n",
      "6090/6090 [==============================] - 1s 125us/sample - loss: 0.5896 - val_loss: 0.5533\n",
      "Epoch 19/100\n",
      "6090/6090 [==============================] - 1s 125us/sample - loss: 0.5768 - val_loss: 0.5453\n",
      "Epoch 20/100\n",
      "6090/6090 [==============================] - 1s 116us/sample - loss: 0.5653 - val_loss: 0.5374\n",
      "Epoch 21/100\n",
      "6090/6090 [==============================] - 1s 124us/sample - loss: 0.5608 - val_loss: 0.5309\n",
      "Epoch 22/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.5546 - val_loss: 0.5250\n",
      "Epoch 23/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.5476 - val_loss: 0.5192\n",
      "Epoch 24/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.5437 - val_loss: 0.5143\n",
      "Epoch 25/100\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.5392 - val_loss: 0.5097\n",
      "Epoch 26/100\n",
      "6090/6090 [==============================] - 1s 116us/sample - loss: 0.5388 - val_loss: 0.5064\n",
      "Epoch 27/100\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.5318 - val_loss: 0.5031\n",
      "Epoch 28/100\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.5218 - val_loss: 0.4993\n",
      "Epoch 29/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.5213 - val_loss: 0.4957\n",
      "Epoch 30/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.5141 - val_loss: 0.4922\n",
      "Epoch 31/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.5130 - val_loss: 0.4894\n",
      "Epoch 32/100\n",
      "6090/6090 [==============================] - 1s 117us/sample - loss: 0.5098 - val_loss: 0.4871\n",
      "Epoch 33/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.5081 - val_loss: 0.4848\n",
      "Epoch 34/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.4979 - val_loss: 0.4820\n",
      "Epoch 35/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.4971 - val_loss: 0.4805\n",
      "Epoch 36/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.4930 - val_loss: 0.4781\n",
      "Epoch 37/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.4886 - val_loss: 0.4763\n",
      "Epoch 38/100\n",
      "6090/6090 [==============================] - 1s 131us/sample - loss: 0.4815 - val_loss: 0.4742\n",
      "Epoch 39/100\n",
      "6090/6090 [==============================] - 1s 128us/sample - loss: 0.4743 - val_loss: 0.4720\n",
      "Epoch 40/100\n",
      "6090/6090 [==============================] - 1s 128us/sample - loss: 0.4794 - val_loss: 0.4711\n",
      "Epoch 41/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.4760 - val_loss: 0.4692\n",
      "Epoch 42/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.4775 - val_loss: 0.4681\n",
      "Epoch 43/100\n",
      "6090/6090 [==============================] - 1s 124us/sample - loss: 0.4729 - val_loss: 0.4670\n",
      "Epoch 44/100\n",
      "6090/6090 [==============================] - 1s 123us/sample - loss: 0.4685 - val_loss: 0.4659\n",
      "Epoch 45/100\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.4657 - val_loss: 0.4647\n",
      "Epoch 46/100\n",
      "6090/6090 [==============================] - 1s 123us/sample - loss: 0.4674 - val_loss: 0.4642\n",
      "Epoch 47/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.4621 - val_loss: 0.4630\n",
      "Epoch 48/100\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.4547 - val_loss: 0.4619\n",
      "Epoch 49/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.4520 - val_loss: 0.4611\n",
      "Epoch 50/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.4453 - val_loss: 0.4598\n",
      "Epoch 51/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4474 - val_loss: 0.4592\n",
      "Epoch 52/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4547 - val_loss: 0.4585\n",
      "Epoch 53/100\n",
      "6090/6090 [==============================] - 1s 123us/sample - loss: 0.4422 - val_loss: 0.4578\n",
      "Epoch 54/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.4389 - val_loss: 0.4571\n",
      "Epoch 55/100\n",
      "6090/6090 [==============================] - 1s 118us/sample - loss: 0.4425 - val_loss: 0.4565\n",
      "Epoch 56/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4373 - val_loss: 0.4559\n",
      "Epoch 57/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.4340 - val_loss: 0.4554\n",
      "Epoch 58/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.4281 - val_loss: 0.4547\n",
      "Epoch 59/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4277 - val_loss: 0.4542\n",
      "Epoch 60/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4273 - val_loss: 0.4538\n",
      "Epoch 61/100\n",
      "6090/6090 [==============================] - 1s 117us/sample - loss: 0.4271 - val_loss: 0.4536\n",
      "Epoch 62/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.4260 - val_loss: 0.4533\n",
      "Epoch 63/100\n",
      "6090/6090 [==============================] - 1s 119us/sample - loss: 0.4261 - val_loss: 0.4530\n",
      "Epoch 64/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4195 - val_loss: 0.4528\n",
      "Epoch 65/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.4300 - val_loss: 0.4530\n",
      "Epoch 66/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4234 - val_loss: 0.4524\n",
      "Epoch 67/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.4161 - val_loss: 0.4522\n",
      "Epoch 68/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.4181 - val_loss: 0.4520\n",
      "Epoch 69/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4130 - val_loss: 0.4519\n",
      "Epoch 70/100\n",
      "6090/6090 [==============================] - 1s 127us/sample - loss: 0.4124 - val_loss: 0.4518\n",
      "Epoch 71/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.4119 - val_loss: 0.4517\n",
      "Epoch 72/100\n",
      "6090/6090 [==============================] - 1s 122us/sample - loss: 0.4111 - val_loss: 0.4518\n",
      "Epoch 73/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.4043 - val_loss: 0.4517\n",
      "Epoch 74/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4080 - val_loss: 0.4519\n",
      "Epoch 75/100\n",
      "6090/6090 [==============================] - 1s 123us/sample - loss: 0.4007 - val_loss: 0.4518\n",
      "(Took 53.730 sec)\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/100\n",
      "6090/6090 [==============================] - 1s 159us/sample - loss: 0.7897 - val_loss: 0.6720\n",
      "Epoch 2/100\n",
      "6090/6090 [==============================] - 1s 121us/sample - loss: 0.7140 - val_loss: 0.6666\n",
      "Epoch 3/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.7036 - val_loss: 0.6634\n",
      "Epoch 4/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.6937 - val_loss: 0.6597\n",
      "Epoch 5/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.6779 - val_loss: 0.6555\n",
      "Epoch 6/100\n",
      "6090/6090 [==============================] - 1s 122us/sample - loss: 0.6722 - val_loss: 0.6515\n",
      "Epoch 7/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.6645 - val_loss: 0.6468\n",
      "Epoch 8/100\n",
      "6090/6090 [==============================] - 1s 116us/sample - loss: 0.6629 - val_loss: 0.6410\n",
      "Epoch 9/100\n",
      "6090/6090 [==============================] - 1s 118us/sample - loss: 0.6590 - val_loss: 0.6349\n",
      "Epoch 10/100\n",
      "6090/6090 [==============================] - 1s 117us/sample - loss: 0.6483 - val_loss: 0.6272\n",
      "Epoch 11/100\n",
      "6090/6090 [==============================] - 1s 119us/sample - loss: 0.6431 - val_loss: 0.6190\n",
      "Epoch 12/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6090/6090 [==============================] - 1s 116us/sample - loss: 0.6330 - val_loss: 0.6095\n",
      "Epoch 13/100\n",
      "6090/6090 [==============================] - 1s 116us/sample - loss: 0.6233 - val_loss: 0.6002\n",
      "Epoch 14/100\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.6217 - val_loss: 0.5891\n",
      "Epoch 15/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.6070 - val_loss: 0.5794\n",
      "Epoch 16/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.5998 - val_loss: 0.5729\n",
      "Epoch 17/100\n",
      "6090/6090 [==============================] - 1s 130us/sample - loss: 0.5911 - val_loss: 0.5619\n",
      "Epoch 18/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.5781 - val_loss: 0.5529\n",
      "Epoch 19/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.5689 - val_loss: 0.5439\n",
      "Epoch 20/100\n",
      "6090/6090 [==============================] - 1s 123us/sample - loss: 0.5705 - val_loss: 0.5372\n",
      "Epoch 21/100\n",
      "6090/6090 [==============================] - 1s 126us/sample - loss: 0.5575 - val_loss: 0.5307\n",
      "Epoch 22/100\n",
      "6090/6090 [==============================] - 1s 117us/sample - loss: 0.5546 - val_loss: 0.5252\n",
      "Epoch 23/100\n",
      "6090/6090 [==============================] - 1s 118us/sample - loss: 0.5483 - val_loss: 0.5202\n",
      "Epoch 24/100\n",
      "6090/6090 [==============================] - 1s 119us/sample - loss: 0.5431 - val_loss: 0.5157\n",
      "Epoch 25/100\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.5389 - val_loss: 0.5112\n",
      "Epoch 26/100\n",
      "6090/6090 [==============================] - 1s 117us/sample - loss: 0.5253 - val_loss: 0.5068\n",
      "Epoch 27/100\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.5261 - val_loss: 0.5033\n",
      "Epoch 28/100\n",
      "6090/6090 [==============================] - 1s 122us/sample - loss: 0.5232 - val_loss: 0.5004\n",
      "Epoch 29/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.5202 - val_loss: 0.4987\n",
      "Epoch 30/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.5096 - val_loss: 0.4943\n",
      "Epoch 31/100\n",
      "6090/6090 [==============================] - 1s 122us/sample - loss: 0.5185 - val_loss: 0.4923\n",
      "Epoch 32/100\n",
      "6090/6090 [==============================] - 1s 119us/sample - loss: 0.5036 - val_loss: 0.4902\n",
      "Epoch 33/100\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.5002 - val_loss: 0.4884\n",
      "Epoch 34/100\n",
      "6090/6090 [==============================] - 1s 121us/sample - loss: 0.5013 - val_loss: 0.4860\n",
      "Epoch 35/100\n",
      "6090/6090 [==============================] - 1s 130us/sample - loss: 0.4918 - val_loss: 0.4841\n",
      "Epoch 36/100\n",
      "6090/6090 [==============================] - 1s 121us/sample - loss: 0.4948 - val_loss: 0.4815\n",
      "Epoch 37/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.4904 - val_loss: 0.4795\n",
      "Epoch 38/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.4854 - val_loss: 0.4784\n",
      "Epoch 39/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.4812 - val_loss: 0.4764\n",
      "Epoch 40/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.4802 - val_loss: 0.4750\n",
      "Epoch 41/100\n",
      "6090/6090 [==============================] - 1s 122us/sample - loss: 0.4735 - val_loss: 0.4734\n",
      "Epoch 42/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4748 - val_loss: 0.4723\n",
      "Epoch 43/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4628 - val_loss: 0.4708\n",
      "Epoch 44/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4679 - val_loss: 0.4696\n",
      "Epoch 45/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4610 - val_loss: 0.4690\n",
      "Epoch 46/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4564 - val_loss: 0.4677\n",
      "Epoch 47/100\n",
      "6090/6090 [==============================] - 1s 121us/sample - loss: 0.4599 - val_loss: 0.4672\n",
      "Epoch 48/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4552 - val_loss: 0.4665\n",
      "Epoch 49/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4525 - val_loss: 0.4650\n",
      "Epoch 50/100\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.4455 - val_loss: 0.4641\n",
      "Epoch 51/100\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.4409 - val_loss: 0.4635\n",
      "Epoch 52/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4486 - val_loss: 0.4628\n",
      "Epoch 53/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4430 - val_loss: 0.4618\n",
      "Epoch 54/100\n",
      "6090/6090 [==============================] - 1s 123us/sample - loss: 0.4393 - val_loss: 0.4615\n",
      "Epoch 55/100\n",
      "6090/6090 [==============================] - 1s 128us/sample - loss: 0.4368 - val_loss: 0.4609\n",
      "Epoch 56/100\n",
      "6090/6090 [==============================] - 1s 132us/sample - loss: 0.4361 - val_loss: 0.4602\n",
      "Epoch 57/100\n",
      "6090/6090 [==============================] - 1s 144us/sample - loss: 0.4225 - val_loss: 0.4599\n",
      "Epoch 58/100\n",
      "6090/6090 [==============================] - 1s 142us/sample - loss: 0.4381 - val_loss: 0.4592\n",
      "Epoch 59/100\n",
      "6090/6090 [==============================] - 1s 142us/sample - loss: 0.4312 - val_loss: 0.4596\n",
      "Epoch 60/100\n",
      "6090/6090 [==============================] - 1s 134us/sample - loss: 0.4233 - val_loss: 0.4585\n",
      "Epoch 61/100\n",
      "6090/6090 [==============================] - 1s 125us/sample - loss: 0.4256 - val_loss: 0.4582\n",
      "Epoch 62/100\n",
      "6090/6090 [==============================] - 1s 124us/sample - loss: 0.4245 - val_loss: 0.4584\n",
      "Epoch 63/100\n",
      "6090/6090 [==============================] - 1s 116us/sample - loss: 0.4179 - val_loss: 0.4585\n",
      "(Took 46.310 sec)\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/100\n",
      "6090/6090 [==============================] - 1s 167us/sample - loss: 0.7639 - val_loss: 0.6829\n",
      "Epoch 2/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.7141 - val_loss: 0.6758\n",
      "Epoch 3/100\n",
      "6090/6090 [==============================] - 1s 119us/sample - loss: 0.6989 - val_loss: 0.6711\n",
      "Epoch 4/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.6880 - val_loss: 0.6673\n",
      "Epoch 5/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.6779 - val_loss: 0.6635\n",
      "Epoch 6/100\n",
      "6090/6090 [==============================] - 1s 120us/sample - loss: 0.6711 - val_loss: 0.6594\n",
      "Epoch 7/100\n",
      "6090/6090 [==============================] - 1s 127us/sample - loss: 0.6644 - val_loss: 0.6561\n",
      "Epoch 8/100\n",
      "6090/6090 [==============================] - 1s 142us/sample - loss: 0.6615 - val_loss: 0.6496\n",
      "Epoch 9/100\n",
      "6090/6090 [==============================] - 1s 133us/sample - loss: 0.6549 - val_loss: 0.6442\n",
      "Epoch 10/100\n",
      "6090/6090 [==============================] - 1s 129us/sample - loss: 0.6533 - val_loss: 0.6372\n",
      "Epoch 11/100\n",
      "6090/6090 [==============================] - 1s 134us/sample - loss: 0.6421 - val_loss: 0.6307\n",
      "Epoch 12/100\n",
      "6090/6090 [==============================] - 1s 131us/sample - loss: 0.6388 - val_loss: 0.6222\n",
      "Epoch 13/100\n",
      "6090/6090 [==============================] - 1s 127us/sample - loss: 0.6272 - val_loss: 0.6132\n",
      "Epoch 14/100\n",
      "6090/6090 [==============================] - 1s 125us/sample - loss: 0.6130 - val_loss: 0.6039\n",
      "Epoch 15/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.6094 - val_loss: 0.5936\n",
      "Epoch 16/100\n",
      "6090/6090 [==============================] - 1s 118us/sample - loss: 0.5993 - val_loss: 0.5835\n",
      "Epoch 17/100\n",
      "6090/6090 [==============================] - 1s 129us/sample - loss: 0.5873 - val_loss: 0.5739\n",
      "Epoch 18/100\n",
      "6090/6090 [==============================] - 1s 130us/sample - loss: 0.5867 - val_loss: 0.5670\n",
      "Epoch 19/100\n",
      "6090/6090 [==============================] - 1s 126us/sample - loss: 0.5780 - val_loss: 0.5593\n",
      "Epoch 20/100\n",
      "6090/6090 [==============================] - 1s 117us/sample - loss: 0.5679 - val_loss: 0.5524\n",
      "Epoch 21/100\n",
      "6090/6090 [==============================] - 1s 117us/sample - loss: 0.5633 - val_loss: 0.5458\n",
      "Epoch 22/100\n",
      "6090/6090 [==============================] - 1s 126us/sample - loss: 0.5489 - val_loss: 0.5385\n",
      "Epoch 23/100\n",
      "6090/6090 [==============================] - 1s 141us/sample - loss: 0.5456 - val_loss: 0.5339\n",
      "Epoch 24/100\n",
      "6090/6090 [==============================] - 1s 122us/sample - loss: 0.5454 - val_loss: 0.5298\n",
      "Epoch 25/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.5397 - val_loss: 0.5247\n",
      "Epoch 26/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.5314 - val_loss: 0.5214\n",
      "Epoch 27/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.5229 - val_loss: 0.5167\n",
      "Epoch 28/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.5263 - val_loss: 0.5132\n",
      "Epoch 29/100\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.5126 - val_loss: 0.5120\n",
      "Epoch 30/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.5183 - val_loss: 0.5080\n",
      "Epoch 31/100\n",
      "6090/6090 [==============================] - 1s 116us/sample - loss: 0.5087 - val_loss: 0.5050\n",
      "Epoch 32/100\n",
      "6090/6090 [==============================] - 1s 119us/sample - loss: 0.4987 - val_loss: 0.5019\n",
      "Epoch 33/100\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.5000 - val_loss: 0.5005\n",
      "Epoch 34/100\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.4963 - val_loss: 0.4980\n",
      "Epoch 35/100\n",
      "6090/6090 [==============================] - 1s 116us/sample - loss: 0.4969 - val_loss: 0.4956\n",
      "Epoch 36/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.4945 - val_loss: 0.4941\n",
      "Epoch 37/100\n",
      "6090/6090 [==============================] - 1s 118us/sample - loss: 0.4835 - val_loss: 0.4935\n",
      "Epoch 38/100\n",
      "6090/6090 [==============================] - 1s 116us/sample - loss: 0.4833 - val_loss: 0.4907\n",
      "Epoch 39/100\n",
      "6090/6090 [==============================] - 1s 117us/sample - loss: 0.4782 - val_loss: 0.4894\n",
      "Epoch 40/100\n",
      "6090/6090 [==============================] - 1s 119us/sample - loss: 0.4744 - val_loss: 0.4873\n",
      "Epoch 41/100\n",
      "6090/6090 [==============================] - 1s 125us/sample - loss: 0.4694 - val_loss: 0.4866\n",
      "Epoch 42/100\n",
      "6090/6090 [==============================] - 1s 118us/sample - loss: 0.4763 - val_loss: 0.4844\n",
      "Epoch 43/100\n",
      "6090/6090 [==============================] - 1s 120us/sample - loss: 0.4619 - val_loss: 0.4834\n",
      "Epoch 44/100\n",
      "6090/6090 [==============================] - 1s 120us/sample - loss: 0.4685 - val_loss: 0.4823\n",
      "Epoch 45/100\n",
      "6090/6090 [==============================] - 1s 126us/sample - loss: 0.4670 - val_loss: 0.4813\n",
      "Epoch 46/100\n",
      "6090/6090 [==============================] - 1s 118us/sample - loss: 0.4541 - val_loss: 0.4803\n",
      "Epoch 47/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.4556 - val_loss: 0.4803\n",
      "Epoch 48/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.4517 - val_loss: 0.4790\n",
      "Epoch 49/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.4567 - val_loss: 0.4777\n",
      "Epoch 50/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.4543 - val_loss: 0.4776\n",
      "Epoch 51/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.4511 - val_loss: 0.4781\n",
      "Epoch 52/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.4449 - val_loss: 0.4754\n",
      "Epoch 53/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.4392 - val_loss: 0.4760\n",
      "Epoch 54/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.4490 - val_loss: 0.4748\n",
      "Epoch 55/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.4384 - val_loss: 0.4739\n",
      "Epoch 56/100\n",
      "6090/6090 [==============================] - 1s 116us/sample - loss: 0.4342 - val_loss: 0.4737\n",
      "Epoch 57/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.4376 - val_loss: 0.4732\n",
      "Epoch 58/100\n",
      "6090/6090 [==============================] - 1s 125us/sample - loss: 0.4369 - val_loss: 0.4718\n",
      "Epoch 59/100\n",
      "6090/6090 [==============================] - 1s 122us/sample - loss: 0.4356 - val_loss: 0.4714\n",
      "Epoch 60/100\n",
      "6090/6090 [==============================] - 1s 117us/sample - loss: 0.4269 - val_loss: 0.4709\n",
      "Epoch 61/100\n",
      "6090/6090 [==============================] - 1s 117us/sample - loss: 0.4263 - val_loss: 0.4729\n",
      "Epoch 62/100\n",
      "6090/6090 [==============================] - 1s 119us/sample - loss: 0.4264 - val_loss: 0.4703\n",
      "Epoch 63/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.4224 - val_loss: 0.4694\n",
      "Epoch 64/100\n",
      "6090/6090 [==============================] - 1s 129us/sample - loss: 0.4154 - val_loss: 0.4709\n",
      "Epoch 65/100\n",
      "6090/6090 [==============================] - 1s 132us/sample - loss: 0.4197 - val_loss: 0.4692\n",
      "Epoch 66/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.4145 - val_loss: 0.4693\n",
      "Epoch 67/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.4170 - val_loss: 0.4692\n",
      "(Took 49.678 sec)\n",
      "Train on 6091 samples, validate on 1522 samples\n",
      "Epoch 1/100\n",
      "6091/6091 [==============================] - 1s 163us/sample - loss: 0.7172 - val_loss: 0.6766\n",
      "Epoch 2/100\n",
      "6091/6091 [==============================] - 1s 115us/sample - loss: 0.7024 - val_loss: 0.6724\n",
      "Epoch 3/100\n",
      "6091/6091 [==============================] - 1s 114us/sample - loss: 0.6846 - val_loss: 0.6689\n",
      "Epoch 4/100\n",
      "6091/6091 [==============================] - 1s 114us/sample - loss: 0.6870 - val_loss: 0.6662\n",
      "Epoch 5/100\n",
      "6091/6091 [==============================] - 1s 114us/sample - loss: 0.6755 - val_loss: 0.6619\n",
      "Epoch 6/100\n",
      "6091/6091 [==============================] - 1s 115us/sample - loss: 0.6718 - val_loss: 0.6573\n",
      "Epoch 7/100\n",
      "6091/6091 [==============================] - 1s 113us/sample - loss: 0.6600 - val_loss: 0.6507\n",
      "Epoch 8/100\n",
      "6091/6091 [==============================] - 1s 113us/sample - loss: 0.6601 - val_loss: 0.6437\n",
      "Epoch 9/100\n",
      "6091/6091 [==============================] - 1s 114us/sample - loss: 0.6487 - val_loss: 0.6350\n",
      "Epoch 10/100\n",
      "6091/6091 [==============================] - 1s 113us/sample - loss: 0.6433 - val_loss: 0.6255\n",
      "Epoch 11/100\n",
      "6091/6091 [==============================] - 1s 113us/sample - loss: 0.6352 - val_loss: 0.6153\n",
      "Epoch 12/100\n",
      "6091/6091 [==============================] - 1s 113us/sample - loss: 0.6297 - val_loss: 0.6054\n",
      "Epoch 13/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.6225 - val_loss: 0.5959\n",
      "Epoch 14/100\n",
      "6091/6091 [==============================] - 1s 113us/sample - loss: 0.6041 - val_loss: 0.5848\n",
      "Epoch 15/100\n",
      "6091/6091 [==============================] - 1s 113us/sample - loss: 0.6002 - val_loss: 0.5745\n",
      "Epoch 16/100\n",
      "6091/6091 [==============================] - 1s 113us/sample - loss: 0.5970 - val_loss: 0.5664\n",
      "Epoch 17/100\n",
      "6091/6091 [==============================] - 1s 118us/sample - loss: 0.5919 - val_loss: 0.5594\n",
      "Epoch 18/100\n",
      "6091/6091 [==============================] - 1s 123us/sample - loss: 0.5763 - val_loss: 0.5502\n",
      "Epoch 19/100\n",
      "6091/6091 [==============================] - 1s 123us/sample - loss: 0.5653 - val_loss: 0.5429\n",
      "Epoch 20/100\n",
      "6091/6091 [==============================] - 1s 123us/sample - loss: 0.5619 - val_loss: 0.5360\n",
      "Epoch 21/100\n",
      "6091/6091 [==============================] - 1s 134us/sample - loss: 0.5511 - val_loss: 0.5298\n",
      "Epoch 22/100\n",
      "6091/6091 [==============================] - 1s 121us/sample - loss: 0.5481 - val_loss: 0.5240\n",
      "Epoch 23/100\n",
      "6091/6091 [==============================] - 1s 118us/sample - loss: 0.5392 - val_loss: 0.5191\n",
      "Epoch 24/100\n",
      "6091/6091 [==============================] - 1s 117us/sample - loss: 0.5390 - val_loss: 0.5142\n",
      "Epoch 25/100\n",
      "6091/6091 [==============================] - 1s 115us/sample - loss: 0.5244 - val_loss: 0.5096\n",
      "Epoch 26/100\n",
      "6091/6091 [==============================] - 1s 114us/sample - loss: 0.5196 - val_loss: 0.5059\n",
      "Epoch 27/100\n",
      "6091/6091 [==============================] - 1s 121us/sample - loss: 0.5255 - val_loss: 0.5029\n",
      "Epoch 28/100\n",
      "6091/6091 [==============================] - 1s 122us/sample - loss: 0.5192 - val_loss: 0.5001\n",
      "Epoch 29/100\n",
      "6091/6091 [==============================] - 1s 125us/sample - loss: 0.5103 - val_loss: 0.4967\n",
      "Epoch 30/100\n",
      "6091/6091 [==============================] - 1s 143us/sample - loss: 0.5105 - val_loss: 0.4944\n",
      "Epoch 31/100\n",
      "6091/6091 [==============================] - 1s 128us/sample - loss: 0.5099 - val_loss: 0.4915\n",
      "Epoch 32/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6091/6091 [==============================] - 1s 127us/sample - loss: 0.5013 - val_loss: 0.4891\n",
      "Epoch 33/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.5018 - val_loss: 0.4870\n",
      "Epoch 34/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.4946 - val_loss: 0.4847\n",
      "Epoch 35/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4896 - val_loss: 0.4825\n",
      "Epoch 36/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.4869 - val_loss: 0.4805\n",
      "Epoch 37/100\n",
      "6091/6091 [==============================] - 1s 110us/sample - loss: 0.4839 - val_loss: 0.4785\n",
      "Epoch 38/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.4767 - val_loss: 0.4765\n",
      "Epoch 39/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4725 - val_loss: 0.4749\n",
      "Epoch 40/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4700 - val_loss: 0.4730\n",
      "Epoch 41/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.4775 - val_loss: 0.4718\n",
      "Epoch 42/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4647 - val_loss: 0.4704\n",
      "Epoch 43/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4711 - val_loss: 0.4690\n",
      "Epoch 44/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4699 - val_loss: 0.4680\n",
      "Epoch 45/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.4630 - val_loss: 0.4666\n",
      "Epoch 46/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4569 - val_loss: 0.4653\n",
      "Epoch 47/100\n",
      "6091/6091 [==============================] - 1s 110us/sample - loss: 0.4548 - val_loss: 0.4642\n",
      "Epoch 48/100\n",
      "6091/6091 [==============================] - 1s 110us/sample - loss: 0.4491 - val_loss: 0.4631\n",
      "Epoch 49/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.4523 - val_loss: 0.4621\n",
      "Epoch 50/100\n",
      "6091/6091 [==============================] - 1s 114us/sample - loss: 0.4489 - val_loss: 0.4616\n",
      "Epoch 51/100\n",
      "6091/6091 [==============================] - 1s 114us/sample - loss: 0.4528 - val_loss: 0.4605\n",
      "Epoch 52/100\n",
      "6091/6091 [==============================] - 1s 116us/sample - loss: 0.4443 - val_loss: 0.4595\n",
      "Epoch 53/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.4369 - val_loss: 0.4583\n",
      "Epoch 54/100\n",
      "6091/6091 [==============================] - 1s 114us/sample - loss: 0.4382 - val_loss: 0.4575\n",
      "Epoch 55/100\n",
      "6091/6091 [==============================] - 1s 110us/sample - loss: 0.4414 - val_loss: 0.4568\n",
      "Epoch 56/100\n",
      "6091/6091 [==============================] - 1s 110us/sample - loss: 0.4355 - val_loss: 0.4564\n",
      "Epoch 57/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4337 - val_loss: 0.4555\n",
      "Epoch 58/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.4326 - val_loss: 0.4550\n",
      "Epoch 59/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.4298 - val_loss: 0.4543\n",
      "Epoch 60/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.4301 - val_loss: 0.4543\n",
      "Epoch 61/100\n",
      "6091/6091 [==============================] - 1s 110us/sample - loss: 0.4281 - val_loss: 0.4535\n",
      "Epoch 62/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.4265 - val_loss: 0.4530\n",
      "Epoch 63/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.4254 - val_loss: 0.4528\n",
      "Epoch 64/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4232 - val_loss: 0.4521\n",
      "Epoch 65/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4182 - val_loss: 0.4515\n",
      "Epoch 66/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.4145 - val_loss: 0.4512\n",
      "Epoch 67/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.4230 - val_loss: 0.4515\n",
      "Epoch 68/100\n",
      "6091/6091 [==============================] - 1s 110us/sample - loss: 0.4175 - val_loss: 0.4506\n",
      "Epoch 69/100\n",
      "6091/6091 [==============================] - 1s 110us/sample - loss: 0.4158 - val_loss: 0.4504\n",
      "Epoch 70/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.4081 - val_loss: 0.4502\n",
      "Epoch 71/100\n",
      "6091/6091 [==============================] - 1s 111us/sample - loss: 0.4103 - val_loss: 0.4495\n",
      "Epoch 72/100\n",
      "6091/6091 [==============================] - 1s 110us/sample - loss: 0.4012 - val_loss: 0.4495\n",
      "Epoch 73/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.4066 - val_loss: 0.4490\n",
      "Epoch 74/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.4109 - val_loss: 0.4488\n",
      "Epoch 75/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.4049 - val_loss: 0.4495\n",
      "Epoch 76/100\n",
      "6091/6091 [==============================] - 1s 110us/sample - loss: 0.4009 - val_loss: 0.4485\n",
      "Epoch 77/100\n",
      "6091/6091 [==============================] - 1s 111us/sample - loss: 0.3933 - val_loss: 0.4484\n",
      "Epoch 78/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.3972 - val_loss: 0.4483\n",
      "Epoch 79/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.3953 - val_loss: 0.4485\n",
      "Epoch 80/100\n",
      "6091/6091 [==============================] - 1s 110us/sample - loss: 0.3979 - val_loss: 0.4485\n",
      "(Took 56.069 sec)\n",
      "Train on 6091 samples, validate on 1522 samples\n",
      "Epoch 1/100\n",
      "6091/6091 [==============================] - 1s 161us/sample - loss: 0.8132 - val_loss: 0.6786\n",
      "Epoch 2/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.7200 - val_loss: 0.6725\n",
      "Epoch 3/100\n",
      "6091/6091 [==============================] - 1s 117us/sample - loss: 0.7086 - val_loss: 0.6681\n",
      "Epoch 4/100\n",
      "6091/6091 [==============================] - 1s 120us/sample - loss: 0.6915 - val_loss: 0.6643\n",
      "Epoch 5/100\n",
      "6091/6091 [==============================] - 1s 119us/sample - loss: 0.6742 - val_loss: 0.6606\n",
      "Epoch 6/100\n",
      "6091/6091 [==============================] - 1s 116us/sample - loss: 0.6730 - val_loss: 0.6559\n",
      "Epoch 7/100\n",
      "6091/6091 [==============================] - 1s 116us/sample - loss: 0.6688 - val_loss: 0.6517\n",
      "Epoch 8/100\n",
      "6091/6091 [==============================] - 1s 115us/sample - loss: 0.6599 - val_loss: 0.6465\n",
      "Epoch 9/100\n",
      "6091/6091 [==============================] - 1s 121us/sample - loss: 0.6548 - val_loss: 0.6421\n",
      "Epoch 10/100\n",
      "6091/6091 [==============================] - 1s 116us/sample - loss: 0.6478 - val_loss: 0.6338\n",
      "Epoch 11/100\n",
      "6091/6091 [==============================] - 1s 116us/sample - loss: 0.6407 - val_loss: 0.6273\n",
      "Epoch 12/100\n",
      "6091/6091 [==============================] - 1s 114us/sample - loss: 0.6399 - val_loss: 0.6188\n",
      "Epoch 13/100\n",
      "6091/6091 [==============================] - 1s 118us/sample - loss: 0.6324 - val_loss: 0.6108\n",
      "Epoch 14/100\n",
      "6091/6091 [==============================] - 1s 115us/sample - loss: 0.6160 - val_loss: 0.6017\n",
      "Epoch 15/100\n",
      "6091/6091 [==============================] - 1s 111us/sample - loss: 0.6108 - val_loss: 0.5929\n",
      "Epoch 16/100\n",
      "6091/6091 [==============================] - 1s 113us/sample - loss: 0.5979 - val_loss: 0.5820\n",
      "Epoch 17/100\n",
      "6091/6091 [==============================] - 1s 110us/sample - loss: 0.5916 - val_loss: 0.5733\n",
      "Epoch 18/100\n",
      "6091/6091 [==============================] - 1s 111us/sample - loss: 0.5862 - val_loss: 0.5650\n",
      "Epoch 19/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.5740 - val_loss: 0.5584\n",
      "Epoch 20/100\n",
      "6091/6091 [==============================] - 1s 110us/sample - loss: 0.5740 - val_loss: 0.5507\n",
      "Epoch 21/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.5621 - val_loss: 0.5439\n",
      "Epoch 22/100\n",
      "6091/6091 [==============================] - 1s 110us/sample - loss: 0.5580 - val_loss: 0.5389\n",
      "Epoch 23/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.5518 - val_loss: 0.5330\n",
      "Epoch 24/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.5444 - val_loss: 0.5278\n",
      "Epoch 25/100\n",
      "6091/6091 [==============================] - 1s 110us/sample - loss: 0.5358 - val_loss: 0.5232\n",
      "Epoch 26/100\n",
      "6091/6091 [==============================] - 1s 111us/sample - loss: 0.5336 - val_loss: 0.5194\n",
      "Epoch 27/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.5316 - val_loss: 0.5160\n",
      "Epoch 28/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.5156 - val_loss: 0.5112\n",
      "Epoch 29/100\n",
      "6091/6091 [==============================] - 1s 106us/sample - loss: 0.5208 - val_loss: 0.5080\n",
      "Epoch 30/100\n",
      "6091/6091 [==============================] - 1s 104us/sample - loss: 0.5050 - val_loss: 0.5051\n",
      "Epoch 31/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.5093 - val_loss: 0.5022\n",
      "Epoch 32/100\n",
      "6091/6091 [==============================] - 1s 114us/sample - loss: 0.5099 - val_loss: 0.5003\n",
      "Epoch 33/100\n",
      "6091/6091 [==============================] - 1s 118us/sample - loss: 0.4972 - val_loss: 0.4977\n",
      "Epoch 34/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.4926 - val_loss: 0.4959\n",
      "Epoch 35/100\n",
      "6091/6091 [==============================] - 1s 110us/sample - loss: 0.4916 - val_loss: 0.4933\n",
      "Epoch 36/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.4911 - val_loss: 0.4918\n",
      "Epoch 37/100\n",
      "6091/6091 [==============================] - 1s 117us/sample - loss: 0.4847 - val_loss: 0.4904\n",
      "Epoch 38/100\n",
      "6091/6091 [==============================] - 1s 111us/sample - loss: 0.4868 - val_loss: 0.4883\n",
      "Epoch 39/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4792 - val_loss: 0.4876\n",
      "Epoch 40/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.4752 - val_loss: 0.4863\n",
      "Epoch 41/100\n",
      "6091/6091 [==============================] - 1s 105us/sample - loss: 0.4740 - val_loss: 0.4839\n",
      "Epoch 42/100\n",
      "6091/6091 [==============================] - 1s 104us/sample - loss: 0.4692 - val_loss: 0.4825\n",
      "Epoch 43/100\n",
      "6091/6091 [==============================] - 1s 104us/sample - loss: 0.4660 - val_loss: 0.4819\n",
      "Epoch 44/100\n",
      "6091/6091 [==============================] - 1s 104us/sample - loss: 0.4643 - val_loss: 0.4815\n",
      "Epoch 45/100\n",
      "6091/6091 [==============================] - 1s 103us/sample - loss: 0.4576 - val_loss: 0.4788\n",
      "Epoch 46/100\n",
      "6091/6091 [==============================] - 1s 104us/sample - loss: 0.4599 - val_loss: 0.4779\n",
      "Epoch 47/100\n",
      "6091/6091 [==============================] - 1s 105us/sample - loss: 0.4569 - val_loss: 0.4769\n",
      "Epoch 48/100\n",
      "6091/6091 [==============================] - 1s 103us/sample - loss: 0.4547 - val_loss: 0.4763\n",
      "Epoch 49/100\n",
      "6091/6091 [==============================] - 1s 105us/sample - loss: 0.4526 - val_loss: 0.4757\n",
      "Epoch 50/100\n",
      "6091/6091 [==============================] - 1s 105us/sample - loss: 0.4480 - val_loss: 0.4746\n",
      "Epoch 51/100\n",
      "6091/6091 [==============================] - 1s 105us/sample - loss: 0.4483 - val_loss: 0.4739\n",
      "Epoch 52/100\n",
      "6091/6091 [==============================] - 1s 106us/sample - loss: 0.4452 - val_loss: 0.4731\n",
      "Epoch 53/100\n",
      "6091/6091 [==============================] - 1s 105us/sample - loss: 0.4429 - val_loss: 0.4728\n",
      "Epoch 54/100\n",
      "6091/6091 [==============================] - 1s 105us/sample - loss: 0.4415 - val_loss: 0.4725\n",
      "Epoch 55/100\n",
      "6091/6091 [==============================] - 1s 106us/sample - loss: 0.4396 - val_loss: 0.4717\n",
      "Epoch 56/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.4395 - val_loss: 0.4710\n",
      "Epoch 57/100\n",
      "6091/6091 [==============================] - 1s 106us/sample - loss: 0.4363 - val_loss: 0.4706\n",
      "Epoch 58/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.4306 - val_loss: 0.4704\n",
      "Epoch 59/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.4290 - val_loss: 0.4697\n",
      "Epoch 60/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.4236 - val_loss: 0.4695\n",
      "Epoch 61/100\n",
      "6091/6091 [==============================] - 1s 105us/sample - loss: 0.4293 - val_loss: 0.4699\n",
      "Epoch 62/100\n",
      "6091/6091 [==============================] - 1s 104us/sample - loss: 0.4303 - val_loss: 0.4688\n",
      "Epoch 63/100\n",
      "6091/6091 [==============================] - 1s 106us/sample - loss: 0.4273 - val_loss: 0.4688\n",
      "Epoch 64/100\n",
      "6091/6091 [==============================] - 1s 104us/sample - loss: 0.4160 - val_loss: 0.4686\n",
      "Epoch 65/100\n",
      "6091/6091 [==============================] - 1s 105us/sample - loss: 0.4180 - val_loss: 0.4687\n",
      "Epoch 66/100\n",
      "6091/6091 [==============================] - 1s 106us/sample - loss: 0.4163 - val_loss: 0.4695\n",
      "(Took 44.836 sec)\n",
      "Combined confusion matrix:\n",
      "[[3828.  514.]\n",
      " [1044. 2227.]]\n",
      "(Overall, took 251.156 sec)\n",
      "Accuracy: 79.53% +/- 1.14%\n",
      "Precision for positive class: 78.59% +/- 2.40%\n",
      "Precision for negative class: 81.27% +/- 1.83%\n",
      "Recall for positive class: 88.17% +/- 1.24%\n",
      "Recall for negative class: 68.13% +/- 2.97%\n",
      "F for positive class: 83.09% +/- 1.07%\n",
      "F for negative class: 74.08% +/- 1.33%\n",
      "Mean F score: 78.58% +/- 1.13%\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/100\n",
      "6090/6090 [==============================] - 1s 161us/sample - loss: 0.7264 - val_loss: 0.6691\n",
      "Epoch 2/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.6993 - val_loss: 0.6602\n",
      "Epoch 3/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.6898 - val_loss: 0.6529\n",
      "Epoch 4/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.6677 - val_loss: 0.6450\n",
      "Epoch 5/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.6608 - val_loss: 0.6352\n",
      "Epoch 6/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.6528 - val_loss: 0.6242\n",
      "Epoch 7/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.6355 - val_loss: 0.6105\n",
      "Epoch 8/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.6161 - val_loss: 0.5938\n",
      "Epoch 9/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.6024 - val_loss: 0.5774\n",
      "Epoch 10/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.5865 - val_loss: 0.5606\n",
      "Epoch 11/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.5734 - val_loss: 0.5440\n",
      "Epoch 12/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.5510 - val_loss: 0.5292\n",
      "Epoch 13/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.5424 - val_loss: 0.5162\n",
      "Epoch 14/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.5313 - val_loss: 0.5053\n",
      "Epoch 15/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.5152 - val_loss: 0.4955\n",
      "Epoch 16/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.5126 - val_loss: 0.4888\n",
      "Epoch 17/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.4866 - val_loss: 0.4805\n",
      "Epoch 18/100\n",
      "6090/6090 [==============================] - 1s 118us/sample - loss: 0.4776 - val_loss: 0.4740\n",
      "Epoch 19/100\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.4746 - val_loss: 0.4693\n",
      "Epoch 20/100\n",
      "6090/6090 [==============================] - 1s 121us/sample - loss: 0.4701 - val_loss: 0.4660\n",
      "Epoch 21/100\n",
      "6090/6090 [==============================] - 1s 127us/sample - loss: 0.4607 - val_loss: 0.4629\n",
      "Epoch 22/100\n",
      "6090/6090 [==============================] - 1s 127us/sample - loss: 0.4510 - val_loss: 0.4600\n",
      "Epoch 23/100\n",
      "6090/6090 [==============================] - 1s 131us/sample - loss: 0.4491 - val_loss: 0.4581\n",
      "Epoch 24/100\n",
      "6090/6090 [==============================] - 1s 127us/sample - loss: 0.4449 - val_loss: 0.4562\n",
      "Epoch 25/100\n",
      "6090/6090 [==============================] - 1s 128us/sample - loss: 0.4343 - val_loss: 0.4548\n",
      "Epoch 26/100\n",
      "6090/6090 [==============================] - 1s 131us/sample - loss: 0.4335 - val_loss: 0.4529\n",
      "Epoch 27/100\n",
      "6090/6090 [==============================] - 1s 128us/sample - loss: 0.4275 - val_loss: 0.4522\n",
      "Epoch 28/100\n",
      "6090/6090 [==============================] - 1s 132us/sample - loss: 0.4265 - val_loss: 0.4518\n",
      "Epoch 29/100\n",
      "6090/6090 [==============================] - 1s 127us/sample - loss: 0.4190 - val_loss: 0.4512\n",
      "Epoch 30/100\n",
      "6090/6090 [==============================] - ETA: 0s - loss: 0.417 - 1s 125us/sample - loss: 0.4155 - val_loss: 0.4506\n",
      "Epoch 31/100\n",
      "6090/6090 [==============================] - 1s 122us/sample - loss: 0.4141 - val_loss: 0.4501\n",
      "Epoch 32/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6090/6090 [==============================] - 1s 124us/sample - loss: 0.4033 - val_loss: 0.4499\n",
      "Epoch 33/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.4041 - val_loss: 0.4497\n",
      "Epoch 34/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.3950 - val_loss: 0.4496\n",
      "Epoch 35/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.3936 - val_loss: 0.4497\n",
      "Epoch 36/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.3973 - val_loss: 0.4500\n",
      "(Took 26.149 sec)\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/100\n",
      "6090/6090 [==============================] - 1s 167us/sample - loss: 0.7329 - val_loss: 0.6669\n",
      "Epoch 2/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.7057 - val_loss: 0.6579\n",
      "Epoch 3/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.6911 - val_loss: 0.6492\n",
      "Epoch 4/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.6795 - val_loss: 0.6427\n",
      "Epoch 5/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.6598 - val_loss: 0.6326\n",
      "Epoch 6/100\n",
      "6090/6090 [==============================] - 1s 121us/sample - loss: 0.6520 - val_loss: 0.6210\n",
      "Epoch 7/100\n",
      "6090/6090 [==============================] - 1s 116us/sample - loss: 0.6376 - val_loss: 0.6078\n",
      "Epoch 8/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.6172 - val_loss: 0.5900\n",
      "Epoch 9/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.6037 - val_loss: 0.5737\n",
      "Epoch 10/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.5876 - val_loss: 0.5565\n",
      "Epoch 11/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.5691 - val_loss: 0.5394\n",
      "Epoch 12/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.5485 - val_loss: 0.5250\n",
      "Epoch 13/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.5404 - val_loss: 0.5131\n",
      "Epoch 14/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.5247 - val_loss: 0.5021\n",
      "Epoch 15/100\n",
      "6090/6090 [==============================] - 1s 121us/sample - loss: 0.5086 - val_loss: 0.4930\n",
      "Epoch 16/100\n",
      "6090/6090 [==============================] - 1s 123us/sample - loss: 0.5011 - val_loss: 0.4860\n",
      "Epoch 17/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.4893 - val_loss: 0.4795\n",
      "Epoch 18/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4878 - val_loss: 0.4744\n",
      "Epoch 19/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4809 - val_loss: 0.4709\n",
      "Epoch 20/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4673 - val_loss: 0.4664\n",
      "Epoch 21/100\n",
      "6090/6090 [==============================] - 1s 120us/sample - loss: 0.4659 - val_loss: 0.4642\n",
      "Epoch 22/100\n",
      "6090/6090 [==============================] - 1s 120us/sample - loss: 0.4577 - val_loss: 0.4613\n",
      "Epoch 23/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.4466 - val_loss: 0.4590\n",
      "Epoch 24/100\n",
      "6090/6090 [==============================] - 1s 116us/sample - loss: 0.4484 - val_loss: 0.4574\n",
      "Epoch 25/100\n",
      "6090/6090 [==============================] - 1s 120us/sample - loss: 0.4371 - val_loss: 0.4559\n",
      "Epoch 26/100\n",
      "6090/6090 [==============================] - 1s 117us/sample - loss: 0.4359 - val_loss: 0.4546\n",
      "Epoch 27/100\n",
      "6090/6090 [==============================] - 1s 116us/sample - loss: 0.4330 - val_loss: 0.4539\n",
      "Epoch 28/100\n",
      "6090/6090 [==============================] - 1s 119us/sample - loss: 0.4319 - val_loss: 0.4539\n",
      "Epoch 29/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.4188 - val_loss: 0.4529\n",
      "Epoch 30/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.4200 - val_loss: 0.4524\n",
      "Epoch 31/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.4133 - val_loss: 0.4524\n",
      "Epoch 32/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4063 - val_loss: 0.4527\n",
      "Epoch 33/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.4032 - val_loss: 0.4528\n",
      "(Took 23.340 sec)\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/100\n",
      "6090/6090 [==============================] - 1s 161us/sample - loss: 0.7640 - val_loss: 0.6754\n",
      "Epoch 2/100\n",
      "6090/6090 [==============================] - 1s 118us/sample - loss: 0.7053 - val_loss: 0.6653\n",
      "Epoch 3/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.6864 - val_loss: 0.6598\n",
      "Epoch 4/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.6711 - val_loss: 0.6492\n",
      "Epoch 5/100\n",
      "6090/6090 [==============================] - 1s 128us/sample - loss: 0.6585 - val_loss: 0.6399\n",
      "Epoch 6/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.6426 - val_loss: 0.6282\n",
      "Epoch 7/100\n",
      "6090/6090 [==============================] - 1s 120us/sample - loss: 0.6327 - val_loss: 0.6160\n",
      "Epoch 8/100\n",
      "6090/6090 [==============================] - 1s 122us/sample - loss: 0.6162 - val_loss: 0.6010\n",
      "Epoch 9/100\n",
      "6090/6090 [==============================] - 1s 120us/sample - loss: 0.6055 - val_loss: 0.5865\n",
      "Epoch 10/100\n",
      "6090/6090 [==============================] - 1s 124us/sample - loss: 0.5925 - val_loss: 0.5718\n",
      "Epoch 11/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.5720 - val_loss: 0.5545\n",
      "Epoch 12/100\n",
      "6090/6090 [==============================] - 1s 121us/sample - loss: 0.5639 - val_loss: 0.5424\n",
      "Epoch 13/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.5480 - val_loss: 0.5314\n",
      "Epoch 14/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.5280 - val_loss: 0.5191\n",
      "Epoch 15/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.5189 - val_loss: 0.5081\n",
      "Epoch 16/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.5026 - val_loss: 0.4995\n",
      "Epoch 17/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4953 - val_loss: 0.4933\n",
      "Epoch 18/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4866 - val_loss: 0.4865\n",
      "Epoch 19/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.4791 - val_loss: 0.4826\n",
      "Epoch 20/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.4775 - val_loss: 0.4783\n",
      "Epoch 21/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.4708 - val_loss: 0.4787\n",
      "Epoch 22/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4650 - val_loss: 0.4733\n",
      "Epoch 23/100\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.4428 - val_loss: 0.4716\n",
      "Epoch 24/100\n",
      "6090/6090 [==============================] - 1s 120us/sample - loss: 0.4482 - val_loss: 0.4691\n",
      "Epoch 25/100\n",
      "6090/6090 [==============================] - 1s 116us/sample - loss: 0.4434 - val_loss: 0.4671\n",
      "Epoch 26/100\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.4298 - val_loss: 0.4668\n",
      "Epoch 27/100\n",
      "6090/6090 [==============================] - 1s 127us/sample - loss: 0.4294 - val_loss: 0.4662\n",
      "Epoch 28/100\n",
      "6090/6090 [==============================] - 1s 124us/sample - loss: 0.4303 - val_loss: 0.4645\n",
      "Epoch 29/100\n",
      "6090/6090 [==============================] - 1s 129us/sample - loss: 0.4214 - val_loss: 0.4644\n",
      "Epoch 30/100\n",
      "6090/6090 [==============================] - 1s 120us/sample - loss: 0.4200 - val_loss: 0.4627\n",
      "Epoch 31/100\n",
      "6090/6090 [==============================] - 1s 123us/sample - loss: 0.4130 - val_loss: 0.4635\n",
      "Epoch 32/100\n",
      "6090/6090 [==============================] - 1s 127us/sample - loss: 0.4048 - val_loss: 0.4632\n",
      "(Took 23.566 sec)\n",
      "Train on 6091 samples, validate on 1522 samples\n",
      "Epoch 1/100\n",
      "6091/6091 [==============================] - 1s 175us/sample - loss: 0.7194 - val_loss: 0.6719\n",
      "Epoch 2/100\n",
      "6091/6091 [==============================] - 1s 116us/sample - loss: 0.6936 - val_loss: 0.6635\n",
      "Epoch 3/100\n",
      "6091/6091 [==============================] - 1s 113us/sample - loss: 0.6827 - val_loss: 0.6563\n",
      "Epoch 4/100\n",
      "6091/6091 [==============================] - 1s 115us/sample - loss: 0.6644 - val_loss: 0.6484\n",
      "Epoch 5/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.6620 - val_loss: 0.6378\n",
      "Epoch 6/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.6461 - val_loss: 0.6256\n",
      "Epoch 7/100\n",
      "6091/6091 [==============================] - 1s 110us/sample - loss: 0.6347 - val_loss: 0.6098\n",
      "Epoch 8/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.6121 - val_loss: 0.5919\n",
      "Epoch 9/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.5944 - val_loss: 0.5736\n",
      "Epoch 10/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.5821 - val_loss: 0.5572\n",
      "Epoch 11/100\n",
      "6091/6091 [==============================] - 1s 113us/sample - loss: 0.5647 - val_loss: 0.5417\n",
      "Epoch 12/100\n",
      "6091/6091 [==============================] - 1s 114us/sample - loss: 0.5488 - val_loss: 0.5277\n",
      "Epoch 13/100\n",
      "6091/6091 [==============================] - 1s 113us/sample - loss: 0.5357 - val_loss: 0.5153\n",
      "Epoch 14/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.5228 - val_loss: 0.5048\n",
      "Epoch 15/100\n",
      "6091/6091 [==============================] - 1s 113us/sample - loss: 0.5083 - val_loss: 0.4970\n",
      "Epoch 16/100\n",
      "6091/6091 [==============================] - 1s 113us/sample - loss: 0.5061 - val_loss: 0.4888\n",
      "Epoch 17/100\n",
      "6091/6091 [==============================] - ETA: 0s - loss: 0.489 - 1s 115us/sample - loss: 0.4877 - val_loss: 0.4817\n",
      "Epoch 18/100\n",
      "6091/6091 [==============================] - 1s 116us/sample - loss: 0.4894 - val_loss: 0.4767\n",
      "Epoch 19/100\n",
      "6091/6091 [==============================] - 1s 114us/sample - loss: 0.4747 - val_loss: 0.4721\n",
      "Epoch 20/100\n",
      "6091/6091 [==============================] - 1s 115us/sample - loss: 0.4747 - val_loss: 0.4684\n",
      "Epoch 21/100\n",
      "6091/6091 [==============================] - 1s 115us/sample - loss: 0.4640 - val_loss: 0.4650\n",
      "Epoch 22/100\n",
      "6091/6091 [==============================] - 1s 125us/sample - loss: 0.4522 - val_loss: 0.4619\n",
      "Epoch 23/100\n",
      "6091/6091 [==============================] - 1s 123us/sample - loss: 0.4421 - val_loss: 0.4589\n",
      "Epoch 24/100\n",
      "6091/6091 [==============================] - 1s 121us/sample - loss: 0.4400 - val_loss: 0.4567\n",
      "Epoch 25/100\n",
      "6091/6091 [==============================] - 1s 122us/sample - loss: 0.4410 - val_loss: 0.4550\n",
      "Epoch 26/100\n",
      "6091/6091 [==============================] - 1s 121us/sample - loss: 0.4318 - val_loss: 0.4535\n",
      "Epoch 27/100\n",
      "6091/6091 [==============================] - 1s 126us/sample - loss: 0.4289 - val_loss: 0.4521\n",
      "Epoch 28/100\n",
      "6091/6091 [==============================] - 1s 125us/sample - loss: 0.4241 - val_loss: 0.4511\n",
      "Epoch 29/100\n",
      "6091/6091 [==============================] - 1s 113us/sample - loss: 0.4272 - val_loss: 0.4501\n",
      "Epoch 30/100\n",
      "6091/6091 [==============================] - 1s 114us/sample - loss: 0.4223 - val_loss: 0.4495\n",
      "Epoch 31/100\n",
      "6091/6091 [==============================] - 1s 115us/sample - loss: 0.4120 - val_loss: 0.4488\n",
      "Epoch 32/100\n",
      "6091/6091 [==============================] - 1s 114us/sample - loss: 0.4043 - val_loss: 0.4483\n",
      "Epoch 33/100\n",
      "6091/6091 [==============================] - 1s 118us/sample - loss: 0.3988 - val_loss: 0.4479\n",
      "Epoch 34/100\n",
      "6091/6091 [==============================] - 1s 128us/sample - loss: 0.4022 - val_loss: 0.4474\n",
      "Epoch 35/100\n",
      "6091/6091 [==============================] - 1s 120us/sample - loss: 0.3915 - val_loss: 0.4473\n",
      "Epoch 36/100\n",
      "6091/6091 [==============================] - 1s 124us/sample - loss: 0.3956 - val_loss: 0.4476\n",
      "Epoch 37/100\n",
      "6091/6091 [==============================] - 1s 123us/sample - loss: 0.3925 - val_loss: 0.4472\n",
      "Epoch 38/100\n",
      "6091/6091 [==============================] - 1s 115us/sample - loss: 0.3871 - val_loss: 0.4480\n",
      "Epoch 39/100\n",
      "6091/6091 [==============================] - 1s 130us/sample - loss: 0.3884 - val_loss: 0.4484\n",
      "(Took 28.665 sec)\n",
      "Train on 6091 samples, validate on 1522 samples\n",
      "Epoch 1/100\n",
      "6091/6091 [==============================] - 1s 203us/sample - loss: 0.7302 - val_loss: 0.6753\n",
      "Epoch 2/100\n",
      "6091/6091 [==============================] - 1s 127us/sample - loss: 0.7014 - val_loss: 0.6654\n",
      "Epoch 3/100\n",
      "6091/6091 [==============================] - 1s 133us/sample - loss: 0.6846 - val_loss: 0.6565\n",
      "Epoch 4/100\n",
      "6091/6091 [==============================] - 1s 125us/sample - loss: 0.6708 - val_loss: 0.6495\n",
      "Epoch 5/100\n",
      "6091/6091 [==============================] - 1s 130us/sample - loss: 0.6517 - val_loss: 0.6399\n",
      "Epoch 6/100\n",
      "6091/6091 [==============================] - 1s 113us/sample - loss: 0.6477 - val_loss: 0.6292\n",
      "Epoch 7/100\n",
      "6091/6091 [==============================] - 1s 116us/sample - loss: 0.6297 - val_loss: 0.6152\n",
      "Epoch 8/100\n",
      "6091/6091 [==============================] - 1s 116us/sample - loss: 0.6149 - val_loss: 0.5994\n",
      "Epoch 9/100\n",
      "6091/6091 [==============================] - 1s 120us/sample - loss: 0.6013 - val_loss: 0.5851\n",
      "Epoch 10/100\n",
      "6091/6091 [==============================] - 1s 123us/sample - loss: 0.5844 - val_loss: 0.5676\n",
      "Epoch 11/100\n",
      "6091/6091 [==============================] - 1s 135us/sample - loss: 0.5678 - val_loss: 0.5516\n",
      "Epoch 12/100\n",
      "6091/6091 [==============================] - 1s 118us/sample - loss: 0.5522 - val_loss: 0.5378\n",
      "Epoch 13/100\n",
      "6091/6091 [==============================] - 1s 119us/sample - loss: 0.5383 - val_loss: 0.5258\n",
      "Epoch 14/100\n",
      "6091/6091 [==============================] - 1s 120us/sample - loss: 0.5300 - val_loss: 0.5158\n",
      "Epoch 15/100\n",
      "6091/6091 [==============================] - 1s 119us/sample - loss: 0.5163 - val_loss: 0.5073\n",
      "Epoch 16/100\n",
      "6091/6091 [==============================] - 1s 117us/sample - loss: 0.5073 - val_loss: 0.4981\n",
      "Epoch 17/100\n",
      "6091/6091 [==============================] - 1s 116us/sample - loss: 0.4846 - val_loss: 0.4910\n",
      "Epoch 18/100\n",
      "6091/6091 [==============================] - 1s 120us/sample - loss: 0.4840 - val_loss: 0.4858\n",
      "Epoch 19/100\n",
      "6091/6091 [==============================] - 1s 119us/sample - loss: 0.4779 - val_loss: 0.4819\n",
      "Epoch 20/100\n",
      "6091/6091 [==============================] - 1s 131us/sample - loss: 0.4725 - val_loss: 0.4780\n",
      "Epoch 21/100\n",
      "6091/6091 [==============================] - 1s 128us/sample - loss: 0.4624 - val_loss: 0.4756\n",
      "Epoch 22/100\n",
      "6091/6091 [==============================] - 1s 121us/sample - loss: 0.4609 - val_loss: 0.4728\n",
      "Epoch 23/100\n",
      "6091/6091 [==============================] - 1s 133us/sample - loss: 0.4479 - val_loss: 0.4702\n",
      "Epoch 24/100\n",
      "6091/6091 [==============================] - 1s 115us/sample - loss: 0.4526 - val_loss: 0.4691\n",
      "Epoch 25/100\n",
      "6091/6091 [==============================] - 1s 115us/sample - loss: 0.4407 - val_loss: 0.4674\n",
      "Epoch 26/100\n",
      "6091/6091 [==============================] - 1s 115us/sample - loss: 0.4383 - val_loss: 0.4663\n",
      "Epoch 27/100\n",
      "6091/6091 [==============================] - 1s 119us/sample - loss: 0.4308 - val_loss: 0.4656\n",
      "Epoch 28/100\n",
      "6091/6091 [==============================] - 1s 125us/sample - loss: 0.4249 - val_loss: 0.4652\n",
      "Epoch 29/100\n",
      "6091/6091 [==============================] - 1s 116us/sample - loss: 0.4218 - val_loss: 0.4647\n",
      "Epoch 30/100\n",
      "6091/6091 [==============================] - 1s 117us/sample - loss: 0.4138 - val_loss: 0.4640\n",
      "Epoch 31/100\n",
      "6091/6091 [==============================] - 1s 115us/sample - loss: 0.4085 - val_loss: 0.4641\n",
      "Epoch 32/100\n",
      "6091/6091 [==============================] - 1s 118us/sample - loss: 0.4065 - val_loss: 0.4643\n",
      "(Took 24.643 sec)\n",
      "Combined confusion matrix:\n",
      "[[3802.  540.]\n",
      " [1025. 2246.]]\n",
      "(Overall, took 127.145 sec)\n",
      "Accuracy: 79.44% +/- 0.97%\n",
      "Precision for positive class: 78.79% +/- 2.29%\n",
      "Precision for negative class: 80.66% +/- 2.22%\n",
      "Recall for positive class: 87.57% +/- 1.78%\n",
      "Recall for negative class: 68.70% +/- 3.20%\n",
      "F for positive class: 82.93% +/- 0.96%\n",
      "F for negative class: 74.15% +/- 1.31%\n",
      "Mean F score: 78.54% +/- 0.98%\n"
     ]
    }
   ],
   "source": [
    "# Cross-validating neural networks (1 hidden layer with 32 neurons, 50% dropout)\n",
    "all_metrics(cross_validate_neuralNet(dftrain_minimal_noprep))\n",
    "all_metrics(cross_validate_neuralNet(dftrain_min_norm_noprep))\n",
    "all_metrics(cross_validate_neuralNet(dftrain_min_tfidf_noprep))\n",
    "all_metrics(cross_validate_neuralNet(dftrain_minimal))\n",
    "all_metrics(cross_validate_neuralNet(dftrain_min_norm))\n",
    "all_metrics(cross_validate_neuralNet(dftrain_min_tfidf))\n",
    "all_metrics(cross_validate_neuralNet(dftrain_pos_minimal))\n",
    "all_metrics(cross_validate_neuralNet(dftrain_pos_min_norm))\n",
    "all_metrics(cross_validate_neuralNet(dftrain_pos_min_tfidf))\n",
    "all_metrics(cross_validate_neuralNet(dftrain_bi_minimal))\n",
    "all_metrics(cross_validate_neuralNet(dftrain_bi_min_norm))\n",
    "all_metrics(cross_validate_neuralNet(dftrain_bi_min_tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Patience = 2\n",
    "def make_neuralNet_predictions(epochs, batch_size, train, test):\n",
    "    print('Training...')\n",
    "    nn = Sequential()\n",
    "    nn.add(Dense(32, activation='sigmoid', input_shape=(train.drop('_target', axis=1).shape[1],)))\n",
    "    nn.add(Dropout(0.5))\n",
    "    nn.add(Dense(1, activation='sigmoid'))\n",
    "    nn.compile(optimizer='adam', loss='binary_crossentropy', metrics=[])\n",
    "    nn.fit(np.array(train.drop('_target', axis=1)), np.array(train['_target']), batch_size = batch_size, epochs=epochs)\n",
    "    print('Predicting...')\n",
    "    preds = 1*(nn.predict(np.array(test)) > 0.5)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Train on 7613 samples\n",
      "Epoch 1/11\n",
      "7613/7613 [==============================] - 1s 170us/sample - loss: 0.6796\n",
      "Epoch 2/11\n",
      "7613/7613 [==============================] - 1s 132us/sample - loss: 0.5896\n",
      "Epoch 3/11\n",
      "7613/7613 [==============================] - 1s 132us/sample - loss: 0.5411\n",
      "Epoch 4/11\n",
      "7613/7613 [==============================] - 1s 122us/sample - loss: 0.4952\n",
      "Epoch 5/11\n",
      "7613/7613 [==============================] - 1s 120us/sample - loss: 0.4644\n",
      "Epoch 6/11\n",
      "7613/7613 [==============================] - 1s 121us/sample - loss: 0.4347\n",
      "Epoch 7/11\n",
      "7613/7613 [==============================] - 1s 116us/sample - loss: 0.4187\n",
      "Epoch 8/11\n",
      "7613/7613 [==============================] - 1s 116us/sample - loss: 0.3994\n",
      "Epoch 9/11\n",
      "7613/7613 [==============================] - 1s 118us/sample - loss: 0.3761\n",
      "Epoch 10/11\n",
      "7613/7613 [==============================] - 1s 125us/sample - loss: 0.3645\n",
      "Epoch 11/11\n",
      "7613/7613 [==============================] - 1s 116us/sample - loss: 0.3515\n",
      "Predicting...\n"
     ]
    }
   ],
   "source": [
    "# Epochs found by validation as usual, since early stopping is not possible here\n",
    "submission['target'] = make_neuralNet_predictions(11, 32, dftrain_minimal_noprep, dftest_minimal_noprep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission_nn.csv', index=False)\n",
    "# Score: 80.265%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic boosted ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A final consideration was made to ensemble together a set of logistic regression models, with some boosting of incorrectly predicted samples. First, I did this without the document frequency threshold by accident, and the result was actually higher than when I used a document frequency threshold of 1 like I usually did. However, both final results were lower in score than the plain logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_x, train_y, batch_size, epochs):\n",
    "    nn = Sequential()\n",
    "    nn.add(Dense(1, activation='sigmoid', input_shape=(train_x.shape[1],)))\n",
    "    nn.compile(optimizer='adam', loss='binary_crossentropy', metrics=[])\n",
    "    nn.fit(train_x, train_y, batch_size = batch_size, epochs=epochs)\n",
    "    return nn\n",
    "\n",
    "def train_ensemble(n, batch_size, epochs, data_x, data_y):\n",
    "    #with tf.device('/GPU:0'):\n",
    "    models = []\n",
    "    total = data_x.shape[0]\n",
    "    all_inds = np.arange(data_x.shape[0])\n",
    "    dist = np.repeat(1, data_x.shape[0])\n",
    "    for i in range(n):\n",
    "        train_x = []\n",
    "        train_y = []\n",
    "        if i == 0:\n",
    "            # First run uses all samples\n",
    "            train_x = data_x\n",
    "            train_y = data_y\n",
    "        else:\n",
    "            # Next runs weight samples more if they came up as errors\n",
    "            inds = np.random.choice(all_inds, data_x.shape[0], replace=False, p=dist/total)\n",
    "            train_x = data_x[inds]\n",
    "            train_y = data_y[inds]\n",
    "        print('Training model', i)\n",
    "        # Todo: simple model for debug...\n",
    "        model = train_model(train_x, train_y, batch_size, epochs)\n",
    "        print('Making predictions')\n",
    "        raw_pred = model.predict(train_x) # Batch size has to match for this to work, apparently.\n",
    "        #print(raw_pred[0:10])\n",
    "        pred = np.argmax(raw_pred, axis=1)\n",
    "        #print(pred[0:10])\n",
    "        errors = np.not_equal(pred, train_y)\n",
    "        #print(train_y[0:10])\n",
    "        #print('Model {0} accuracy is {1:.3f}'.format(i, 100*(1-np.sum(errors)/train_y.shape[0])))\n",
    "        dist += errors\n",
    "        total += sum(errors)\n",
    "        models.append(model)\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model 0\n",
      "Train on 7613 samples\n",
      "Epoch 1/23\n",
      "7613/7613 [==============================] - 1s 126us/sample - loss: 0.6635\n",
      "Epoch 2/23\n",
      "7613/7613 [==============================] - 1s 88us/sample - loss: 0.6094\n",
      "Epoch 3/23\n",
      "7613/7613 [==============================] - 1s 90us/sample - loss: 0.5689\n",
      "Epoch 4/23\n",
      "7613/7613 [==============================] - 1s 87us/sample - loss: 0.5376\n",
      "Epoch 5/23\n",
      "7613/7613 [==============================] - 1s 81us/sample - loss: 0.5127\n",
      "Epoch 6/23\n",
      "7613/7613 [==============================] - 1s 87us/sample - loss: 0.4924\n",
      "Epoch 7/23\n",
      "7613/7613 [==============================] - 1s 87us/sample - loss: 0.4757\n",
      "Epoch 8/23\n",
      "7613/7613 [==============================] - 1s 85us/sample - loss: 0.4613\n",
      "Epoch 9/23\n",
      "7613/7613 [==============================] - 1s 86us/sample - loss: 0.4489\n",
      "Epoch 10/23\n",
      "7613/7613 [==============================] - 1s 87us/sample - loss: 0.4382\n",
      "Epoch 11/23\n",
      "7613/7613 [==============================] - 1s 95us/sample - loss: 0.4287\n",
      "Epoch 12/23\n",
      "7613/7613 [==============================] - 1s 89us/sample - loss: 0.4203\n",
      "Epoch 13/23\n",
      "7613/7613 [==============================] - 1s 81us/sample - loss: 0.4126\n",
      "Epoch 14/23\n",
      "7613/7613 [==============================] - 1s 83us/sample - loss: 0.4057\n",
      "Epoch 15/23\n",
      "7613/7613 [==============================] - 1s 85us/sample - loss: 0.3994\n",
      "Epoch 16/23\n",
      "7613/7613 [==============================] - 1s 86us/sample - loss: 0.3936\n",
      "Epoch 17/23\n",
      "7613/7613 [==============================] - 1s 86us/sample - loss: 0.3882\n",
      "Epoch 18/23\n",
      "7613/7613 [==============================] - 1s 87us/sample - loss: 0.3833\n",
      "Epoch 19/23\n",
      "7613/7613 [==============================] - 1s 88us/sample - loss: 0.3786\n",
      "Epoch 20/23\n",
      "7613/7613 [==============================] - 1s 87us/sample - loss: 0.3743\n",
      "Epoch 21/23\n",
      "7613/7613 [==============================] - 1s 88us/sample - loss: 0.3702\n",
      "Epoch 22/23\n",
      "7613/7613 [==============================] - 1s 89us/sample - loss: 0.3664\n",
      "Epoch 23/23\n",
      "7613/7613 [==============================] - 1s 98us/sample - loss: 0.3627\n",
      "Making predictions\n",
      "Training model 1\n",
      "Train on 7613 samples\n",
      "Epoch 1/23\n",
      "7613/7613 [==============================] - 1s 128us/sample - loss: 0.6647\n",
      "Epoch 2/23\n",
      "7613/7613 [==============================] - 1s 89us/sample - loss: 0.6099\n",
      "Epoch 3/23\n",
      "7613/7613 [==============================] - 1s 97us/sample - loss: 0.5694\n",
      "Epoch 4/23\n",
      "7613/7613 [==============================] - 1s 85us/sample - loss: 0.5381\n",
      "Epoch 5/23\n",
      "7613/7613 [==============================] - 1s 88us/sample - loss: 0.5133\n",
      "Epoch 6/23\n",
      "7613/7613 [==============================] - 1s 92us/sample - loss: 0.4930\n",
      "Epoch 7/23\n",
      "7613/7613 [==============================] - 1s 93us/sample - loss: 0.4761\n",
      "Epoch 8/23\n",
      "7613/7613 [==============================] - 1s 94us/sample - loss: 0.4618\n",
      "Epoch 9/23\n",
      "7613/7613 [==============================] - 1s 94us/sample - loss: 0.4495\n",
      "Epoch 10/23\n",
      "7613/7613 [==============================] - 1s 103us/sample - loss: 0.4387\n",
      "Epoch 11/23\n",
      "7613/7613 [==============================] - 1s 91us/sample - loss: 0.4292\n",
      "Epoch 12/23\n",
      "7613/7613 [==============================] - 1s 98us/sample - loss: 0.4207\n",
      "Epoch 13/23\n",
      "7613/7613 [==============================] - 1s 94us/sample - loss: 0.4130\n",
      "Epoch 14/23\n",
      "7613/7613 [==============================] - 1s 92us/sample - loss: 0.4061\n",
      "Epoch 15/23\n",
      "7613/7613 [==============================] - 1s 105us/sample - loss: 0.3997\n",
      "Epoch 16/23\n",
      "7613/7613 [==============================] - 1s 103us/sample - loss: 0.3939\n",
      "Epoch 17/23\n",
      "7613/7613 [==============================] - 1s 91us/sample - loss: 0.3886\n",
      "Epoch 18/23\n",
      "7613/7613 [==============================] - 1s 88us/sample - loss: 0.3836\n",
      "Epoch 19/23\n",
      "7613/7613 [==============================] - 1s 96us/sample - loss: 0.3789\n",
      "Epoch 20/23\n",
      "7613/7613 [==============================] - 1s 100us/sample - loss: 0.3746\n",
      "Epoch 21/23\n",
      "7613/7613 [==============================] - 1s 85us/sample - loss: 0.3705\n",
      "Epoch 22/23\n",
      "7613/7613 [==============================] - 1s 83us/sample - loss: 0.3666\n",
      "Epoch 23/23\n",
      "7613/7613 [==============================] - 1s 82us/sample - loss: 0.3630\n",
      "Making predictions\n",
      "Training model 2\n",
      "Train on 7613 samples\n",
      "Epoch 1/23\n",
      "7613/7613 [==============================] - 1s 108us/sample - loss: 0.6665\n",
      "Epoch 2/23\n",
      "7613/7613 [==============================] - 1s 87us/sample - loss: 0.6118\n",
      "Epoch 3/23\n",
      "7613/7613 [==============================] - 1s 91us/sample - loss: 0.5711\n",
      "Epoch 4/23\n",
      "7613/7613 [==============================] - 1s 86us/sample - loss: 0.5396\n",
      "Epoch 5/23\n",
      "7613/7613 [==============================] - 1s 95us/sample - loss: 0.5144\n",
      "Epoch 6/23\n",
      "7613/7613 [==============================] - 1s 84us/sample - loss: 0.4940\n",
      "Epoch 7/23\n",
      "7613/7613 [==============================] - 1s 85us/sample - loss: 0.4770\n",
      "Epoch 8/23\n",
      "7613/7613 [==============================] - 1s 98us/sample - loss: 0.4626\n",
      "Epoch 9/23\n",
      "7613/7613 [==============================] - 1s 105us/sample - loss: 0.4501\n",
      "Epoch 10/23\n",
      "7613/7613 [==============================] - 1s 96us/sample - loss: 0.4393\n",
      "Epoch 11/23\n",
      "7613/7613 [==============================] - 1s 103us/sample - loss: 0.4297\n",
      "Epoch 12/23\n",
      "7613/7613 [==============================] - 1s 79us/sample - loss: 0.4212\n",
      "Epoch 13/23\n",
      "7613/7613 [==============================] - 1s 79us/sample - loss: 0.4135\n",
      "Epoch 14/23\n",
      "7613/7613 [==============================] - 1s 74us/sample - loss: 0.4065\n",
      "Epoch 15/23\n",
      "7613/7613 [==============================] - 1s 86us/sample - loss: 0.4001\n",
      "Epoch 16/23\n",
      "7613/7613 [==============================] - 1s 81us/sample - loss: 0.3943\n",
      "Epoch 17/23\n",
      "7613/7613 [==============================] - 1s 77us/sample - loss: 0.3889\n",
      "Epoch 18/23\n",
      "7613/7613 [==============================] - 1s 76us/sample - loss: 0.3839\n",
      "Epoch 19/23\n",
      "7613/7613 [==============================] - 1s 90us/sample - loss: 0.3793\n",
      "Epoch 20/23\n",
      "7613/7613 [==============================] - 1s 80us/sample - loss: 0.3749\n",
      "Epoch 21/23\n",
      "7613/7613 [==============================] - 1s 84us/sample - loss: 0.3707\n",
      "Epoch 22/23\n",
      "7613/7613 [==============================] - 1s 122us/sample - loss: 0.3668\n",
      "Epoch 23/23\n",
      "7613/7613 [==============================] - 1s 120us/sample - loss: 0.3632\n",
      "Making predictions\n",
      "Training model 3\n",
      "Train on 7613 samples\n",
      "Epoch 1/23\n",
      "7613/7613 [==============================] - 1s 123us/sample - loss: 0.6642\n",
      "Epoch 2/23\n",
      "7613/7613 [==============================] - 1s 86us/sample - loss: 0.6098\n",
      "Epoch 3/23\n",
      "7613/7613 [==============================] - 1s 89us/sample - loss: 0.5693\n",
      "Epoch 4/23\n",
      "7613/7613 [==============================] - 1s 84us/sample - loss: 0.5381\n",
      "Epoch 5/23\n",
      "7613/7613 [==============================] - 1s 82us/sample - loss: 0.5133\n",
      "Epoch 6/23\n",
      "7613/7613 [==============================] - 1s 77us/sample - loss: 0.4930\n",
      "Epoch 7/23\n",
      "7613/7613 [==============================] - 1s 81us/sample - loss: 0.4761\n",
      "Epoch 8/23\n",
      "7613/7613 [==============================] - 1s 76us/sample - loss: 0.4618\n",
      "Epoch 9/23\n",
      "7613/7613 [==============================] - 1s 88us/sample - loss: 0.4495\n",
      "Epoch 10/23\n",
      "7613/7613 [==============================] - 1s 80us/sample - loss: 0.4388\n",
      "Epoch 11/23\n",
      "7613/7613 [==============================] - 1s 80us/sample - loss: 0.4292\n",
      "Epoch 12/23\n",
      "7613/7613 [==============================] - 1s 92us/sample - loss: 0.4207\n",
      "Epoch 13/23\n",
      "7613/7613 [==============================] - 1s 89us/sample - loss: 0.4131\n",
      "Epoch 14/23\n",
      "7613/7613 [==============================] - 1s 87us/sample - loss: 0.4062\n",
      "Epoch 15/23\n",
      "7613/7613 [==============================] - 1s 79us/sample - loss: 0.3998\n",
      "Epoch 16/23\n",
      "7613/7613 [==============================] - 1s 77us/sample - loss: 0.3940\n",
      "Epoch 17/23\n",
      "7613/7613 [==============================] - 1s 78us/sample - loss: 0.3886\n",
      "Epoch 18/23\n",
      "7613/7613 [==============================] - 1s 75us/sample - loss: 0.3836\n",
      "Epoch 19/23\n",
      "7613/7613 [==============================] - 1s 75us/sample - loss: 0.3790\n",
      "Epoch 20/23\n",
      "7613/7613 [==============================] - 1s 91us/sample - loss: 0.3746\n",
      "Epoch 21/23\n",
      "7613/7613 [==============================] - 1s 100us/sample - loss: 0.3705\n",
      "Epoch 22/23\n",
      "7613/7613 [==============================] - 1s 92us/sample - loss: 0.3667\n",
      "Epoch 23/23\n",
      "7613/7613 [==============================] - 1s 94us/sample - loss: 0.3631\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making predictions\n",
      "Training model 4\n",
      "Train on 7613 samples\n",
      "Epoch 1/23\n",
      "7613/7613 [==============================] - 1s 108us/sample - loss: 0.6661\n",
      "Epoch 2/23\n",
      "7613/7613 [==============================] - 1s 75us/sample - loss: 0.6115\n",
      "Epoch 3/23\n",
      "7613/7613 [==============================] - 1s 77us/sample - loss: 0.5706\n",
      "Epoch 4/23\n",
      "7613/7613 [==============================] - 1s 77us/sample - loss: 0.5391\n",
      "Epoch 5/23\n",
      "7613/7613 [==============================] - 1s 75us/sample - loss: 0.5141\n",
      "Epoch 6/23\n",
      "7613/7613 [==============================] - 1s 74us/sample - loss: 0.4937\n",
      "Epoch 7/23\n",
      "7613/7613 [==============================] - 1s 74us/sample - loss: 0.4767\n",
      "Epoch 8/23\n",
      "7613/7613 [==============================] - 1s 87us/sample - loss: 0.4623\n",
      "Epoch 9/23\n",
      "7613/7613 [==============================] - 1s 82us/sample - loss: 0.4499\n",
      "Epoch 10/23\n",
      "7613/7613 [==============================] - 1s 73us/sample - loss: 0.4391\n",
      "Epoch 11/23\n",
      "7613/7613 [==============================] - 1s 73us/sample - loss: 0.4295\n",
      "Epoch 12/23\n",
      "7613/7613 [==============================] - 1s 74us/sample - loss: 0.4209\n",
      "Epoch 13/23\n",
      "7613/7613 [==============================] - 1s 74us/sample - loss: 0.4133\n",
      "Epoch 14/23\n",
      "7613/7613 [==============================] - 1s 73us/sample - loss: 0.4063\n",
      "Epoch 15/23\n",
      "7613/7613 [==============================] - 1s 74us/sample - loss: 0.3999\n",
      "Epoch 16/23\n",
      "7613/7613 [==============================] - 1s 72us/sample - loss: 0.3941\n",
      "Epoch 17/23\n",
      "7613/7613 [==============================] - 1s 71us/sample - loss: 0.3887\n",
      "Epoch 18/23\n",
      "7613/7613 [==============================] - 1s 72us/sample - loss: 0.3837\n",
      "Epoch 19/23\n",
      "7613/7613 [==============================] - 1s 71us/sample - loss: 0.3790\n",
      "Epoch 20/23\n",
      "7613/7613 [==============================] - 1s 76us/sample - loss: 0.3747\n",
      "Epoch 21/23\n",
      "7613/7613 [==============================] - 1s 71us/sample - loss: 0.3706\n",
      "Epoch 22/23\n",
      "7613/7613 [==============================] - 1s 73us/sample - loss: 0.3667\n",
      "Epoch 23/23\n",
      "7613/7613 [==============================] - 1s 72us/sample - loss: 0.3630\n",
      "Making predictions\n",
      "Training model 5\n",
      "Train on 7613 samples\n",
      "Epoch 1/23\n",
      "7613/7613 [==============================] - 1s 100us/sample - loss: 0.6646\n",
      "Epoch 2/23\n",
      "7613/7613 [==============================] - 1s 72us/sample - loss: 0.6103\n",
      "Epoch 3/23\n",
      "7613/7613 [==============================] - 1s 73us/sample - loss: 0.5696\n",
      "Epoch 4/23\n",
      "7613/7613 [==============================] - 1s 73us/sample - loss: 0.5383\n",
      "Epoch 5/23\n",
      "7613/7613 [==============================] - 1s 73us/sample - loss: 0.5134\n",
      "Epoch 6/23\n",
      "7613/7613 [==============================] - 1s 76us/sample - loss: 0.4931\n",
      "Epoch 7/23\n",
      "7613/7613 [==============================] - 1s 76us/sample - loss: 0.4762\n",
      "Epoch 8/23\n",
      "7613/7613 [==============================] - 1s 73us/sample - loss: 0.4618\n",
      "Epoch 9/23\n",
      "7613/7613 [==============================] - 1s 81us/sample - loss: 0.4495\n",
      "Epoch 10/23\n",
      "7613/7613 [==============================] - 1s 81us/sample - loss: 0.4387\n",
      "Epoch 11/23\n",
      "7613/7613 [==============================] - 1s 80us/sample - loss: 0.4292\n",
      "Epoch 12/23\n",
      "7613/7613 [==============================] - 1s 81us/sample - loss: 0.4207\n",
      "Epoch 13/23\n",
      "7613/7613 [==============================] - 1s 85us/sample - loss: 0.4131\n",
      "Epoch 14/23\n",
      "7613/7613 [==============================] - 1s 80us/sample - loss: 0.4061\n",
      "Epoch 15/23\n",
      "7613/7613 [==============================] - 1s 86us/sample - loss: 0.3998\n",
      "Epoch 16/23\n",
      "7613/7613 [==============================] - 1s 82us/sample - loss: 0.3940\n",
      "Epoch 17/23\n",
      "7613/7613 [==============================] - 1s 80us/sample - loss: 0.3886\n",
      "Epoch 18/23\n",
      "7613/7613 [==============================] - 1s 74us/sample - loss: 0.3836\n",
      "Epoch 19/23\n",
      "7613/7613 [==============================] - 1s 86us/sample - loss: 0.3789\n",
      "Epoch 20/23\n",
      "7613/7613 [==============================] - 1s 81us/sample - loss: 0.3746\n",
      "Epoch 21/23\n",
      "7613/7613 [==============================] - 1s 77us/sample - loss: 0.3705\n",
      "Epoch 22/23\n",
      "7613/7613 [==============================] - 1s 78us/sample - loss: 0.3667\n",
      "Epoch 23/23\n",
      "7613/7613 [==============================] - 1s 83us/sample - loss: 0.3630\n",
      "Making predictions\n",
      "Training model 6\n",
      "Train on 7613 samples\n",
      "Epoch 1/23\n",
      "7613/7613 [==============================] - 1s 103us/sample - loss: 0.6646\n",
      "Epoch 2/23\n",
      "7613/7613 [==============================] - 1s 78us/sample - loss: 0.6102\n",
      "Epoch 3/23\n",
      "7613/7613 [==============================] - 1s 77us/sample - loss: 0.5696\n",
      "Epoch 4/23\n",
      "7613/7613 [==============================] - 1s 76us/sample - loss: 0.5383\n",
      "Epoch 5/23\n",
      "7613/7613 [==============================] - 1s 77us/sample - loss: 0.5135\n",
      "Epoch 6/23\n",
      "7613/7613 [==============================] - 1s 74us/sample - loss: 0.4932\n",
      "Epoch 7/23\n",
      "7613/7613 [==============================] - 1s 74us/sample - loss: 0.4764\n",
      "Epoch 8/23\n",
      "7613/7613 [==============================] - 1s 75us/sample - loss: 0.4620\n",
      "Epoch 9/23\n",
      "7613/7613 [==============================] - 1s 83us/sample - loss: 0.4497\n",
      "Epoch 10/23\n",
      "7613/7613 [==============================] - 1s 95us/sample - loss: 0.4389\n",
      "Epoch 11/23\n",
      "7613/7613 [==============================] - 1s 78us/sample - loss: 0.4294\n",
      "Epoch 12/23\n",
      "7613/7613 [==============================] - 1s 79us/sample - loss: 0.4209\n",
      "Epoch 13/23\n",
      "7613/7613 [==============================] - 1s 77us/sample - loss: 0.4133\n",
      "Epoch 14/23\n",
      "7613/7613 [==============================] - 1s 76us/sample - loss: 0.4063\n",
      "Epoch 15/23\n",
      "7613/7613 [==============================] - 1s 79us/sample - loss: 0.3999\n",
      "Epoch 16/23\n",
      "7613/7613 [==============================] - 1s 88us/sample - loss: 0.3941\n",
      "Epoch 17/23\n",
      "7613/7613 [==============================] - 1s 85us/sample - loss: 0.3887\n",
      "Epoch 18/23\n",
      "7613/7613 [==============================] - 1s 82us/sample - loss: 0.3838\n",
      "Epoch 19/23\n",
      "7613/7613 [==============================] - 1s 80us/sample - loss: 0.3791\n",
      "Epoch 20/23\n",
      "7613/7613 [==============================] - 1s 83us/sample - loss: 0.3747\n",
      "Epoch 21/23\n",
      "7613/7613 [==============================] - 1s 79us/sample - loss: 0.3706\n",
      "Epoch 22/23\n",
      "7613/7613 [==============================] - 1s 76us/sample - loss: 0.3668\n",
      "Epoch 23/23\n",
      "7613/7613 [==============================] - 1s 78us/sample - loss: 0.3632\n",
      "Making predictions\n",
      "Training model 7\n",
      "Train on 7613 samples\n",
      "Epoch 1/23\n",
      "7613/7613 [==============================] - 1s 107us/sample - loss: 0.6645\n",
      "Epoch 2/23\n",
      "7613/7613 [==============================] - 1s 76us/sample - loss: 0.6100\n",
      "Epoch 3/23\n",
      "7613/7613 [==============================] - 1s 75us/sample - loss: 0.5693\n",
      "Epoch 4/23\n",
      "7613/7613 [==============================] - 1s 76us/sample - loss: 0.5379\n",
      "Epoch 5/23\n",
      "7613/7613 [==============================] - 1s 75us/sample - loss: 0.5131\n",
      "Epoch 6/23\n",
      "7613/7613 [==============================] - 1s 76us/sample - loss: 0.4928\n",
      "Epoch 7/23\n",
      "7613/7613 [==============================] - 1s 76us/sample - loss: 0.4759\n",
      "Epoch 8/23\n",
      "7613/7613 [==============================] - 1s 74us/sample - loss: 0.4616\n",
      "Epoch 9/23\n",
      "7613/7613 [==============================] - 1s 75us/sample - loss: 0.4493\n",
      "Epoch 10/23\n",
      "7613/7613 [==============================] - 1s 75us/sample - loss: 0.4385\n",
      "Epoch 11/23\n",
      "7613/7613 [==============================] - 1s 75us/sample - loss: 0.4290\n",
      "Epoch 12/23\n",
      "7613/7613 [==============================] - 1s 79us/sample - loss: 0.4206\n",
      "Epoch 13/23\n",
      "7613/7613 [==============================] - 1s 76us/sample - loss: 0.4129\n",
      "Epoch 14/23\n",
      "7613/7613 [==============================] - 1s 76us/sample - loss: 0.4060\n",
      "Epoch 15/23\n",
      "7613/7613 [==============================] - 1s 75us/sample - loss: 0.3997\n",
      "Epoch 16/23\n",
      "7613/7613 [==============================] - 1s 75us/sample - loss: 0.3939\n",
      "Epoch 17/23\n",
      "7613/7613 [==============================] - 1s 76us/sample - loss: 0.3885\n",
      "Epoch 18/23\n",
      "7613/7613 [==============================] - 1s 77us/sample - loss: 0.3835\n",
      "Epoch 19/23\n",
      "7613/7613 [==============================] - 1s 78us/sample - loss: 0.3789\n",
      "Epoch 20/23\n",
      "7613/7613 [==============================] - 1s 79us/sample - loss: 0.3745\n",
      "Epoch 21/23\n",
      "7613/7613 [==============================] - 1s 79us/sample - loss: 0.3704\n",
      "Epoch 22/23\n",
      "7613/7613 [==============================] - 1s 77us/sample - loss: 0.3666\n",
      "Epoch 23/23\n",
      "7613/7613 [==============================] - 1s 78us/sample - loss: 0.3630\n",
      "Making predictions\n",
      "Training model 8\n",
      "Train on 7613 samples\n",
      "Epoch 1/23\n",
      "7613/7613 [==============================] - 1s 106us/sample - loss: 0.6653\n",
      "Epoch 2/23\n",
      "7613/7613 [==============================] - 1s 78us/sample - loss: 0.6105\n",
      "Epoch 3/23\n",
      "7613/7613 [==============================] - 1s 77us/sample - loss: 0.5696\n",
      "Epoch 4/23\n",
      "7613/7613 [==============================] - 1s 78us/sample - loss: 0.5381\n",
      "Epoch 5/23\n",
      "7613/7613 [==============================] - 1s 85us/sample - loss: 0.5132\n",
      "Epoch 6/23\n",
      "7613/7613 [==============================] - 1s 77us/sample - loss: 0.4929\n",
      "Epoch 7/23\n",
      "7613/7613 [==============================] - 1s 75us/sample - loss: 0.4760\n",
      "Epoch 8/23\n",
      "7613/7613 [==============================] - 1s 77us/sample - loss: 0.4617\n",
      "Epoch 9/23\n",
      "7613/7613 [==============================] - 1s 76us/sample - loss: 0.4493\n",
      "Epoch 10/23\n",
      "7613/7613 [==============================] - 1s 87us/sample - loss: 0.4385\n",
      "Epoch 11/23\n",
      "7613/7613 [==============================] - 1s 97us/sample - loss: 0.4290\n",
      "Epoch 12/23\n",
      "7613/7613 [==============================] - 1s 77us/sample - loss: 0.4205\n",
      "Epoch 13/23\n",
      "7613/7613 [==============================] - 1s 77us/sample - loss: 0.4128\n",
      "Epoch 14/23\n",
      "7613/7613 [==============================] - 1s 77us/sample - loss: 0.4059\n",
      "Epoch 15/23\n",
      "7613/7613 [==============================] - 1s 76us/sample - loss: 0.3996\n",
      "Epoch 16/23\n",
      "7613/7613 [==============================] - 1s 86us/sample - loss: 0.3938\n",
      "Epoch 17/23\n",
      "7613/7613 [==============================] - 1s 75us/sample - loss: 0.3884\n",
      "Epoch 18/23\n",
      "7613/7613 [==============================] - 1s 81us/sample - loss: 0.3834\n",
      "Epoch 19/23\n",
      "7613/7613 [==============================] - 1s 81us/sample - loss: 0.3787\n",
      "Epoch 20/23\n",
      "7613/7613 [==============================] - 1s 88us/sample - loss: 0.3744\n",
      "Epoch 21/23\n",
      "7613/7613 [==============================] - 1s 75us/sample - loss: 0.3703\n",
      "Epoch 22/23\n",
      "7613/7613 [==============================] - 1s 77us/sample - loss: 0.3665\n",
      "Epoch 23/23\n",
      "7613/7613 [==============================] - 1s 78us/sample - loss: 0.3629\n",
      "Making predictions\n",
      "Training model 9\n",
      "Train on 7613 samples\n",
      "Epoch 1/23\n",
      "7613/7613 [==============================] - 1s 107us/sample - loss: 0.6657\n",
      "Epoch 2/23\n",
      "7613/7613 [==============================] - 1s 79us/sample - loss: 0.6110\n",
      "Epoch 3/23\n",
      "7613/7613 [==============================] - 1s 79us/sample - loss: 0.5703\n",
      "Epoch 4/23\n",
      "7613/7613 [==============================] - 1s 77us/sample - loss: 0.5389\n",
      "Epoch 5/23\n",
      "7613/7613 [==============================] - 1s 82us/sample - loss: 0.5139\n",
      "Epoch 6/23\n",
      "7613/7613 [==============================] - 1s 79us/sample - loss: 0.4935\n",
      "Epoch 7/23\n",
      "7613/7613 [==============================] - 1s 79us/sample - loss: 0.4766\n",
      "Epoch 8/23\n",
      "7613/7613 [==============================] - 1s 80us/sample - loss: 0.4622\n",
      "Epoch 9/23\n",
      "7613/7613 [==============================] - 1s 77us/sample - loss: 0.4497\n",
      "Epoch 10/23\n",
      "7613/7613 [==============================] - 1s 79us/sample - loss: 0.4390\n",
      "Epoch 11/23\n",
      "7613/7613 [==============================] - 1s 84us/sample - loss: 0.4294\n",
      "Epoch 12/23\n",
      "7613/7613 [==============================] - 1s 81us/sample - loss: 0.4209\n",
      "Epoch 13/23\n",
      "7613/7613 [==============================] - 1s 79us/sample - loss: 0.4132\n",
      "Epoch 14/23\n",
      "7613/7613 [==============================] - 1s 77us/sample - loss: 0.4063\n",
      "Epoch 15/23\n",
      "7613/7613 [==============================] - 1s 77us/sample - loss: 0.3999\n",
      "Epoch 16/23\n",
      "7613/7613 [==============================] - 1s 75us/sample - loss: 0.3941\n",
      "Epoch 17/23\n",
      "7613/7613 [==============================] - 1s 76us/sample - loss: 0.3887\n",
      "Epoch 18/23\n",
      "7613/7613 [==============================] - 1s 80us/sample - loss: 0.3837\n",
      "Epoch 19/23\n",
      "7613/7613 [==============================] - 1s 77us/sample - loss: 0.3790\n",
      "Epoch 20/23\n",
      "7613/7613 [==============================] - 1s 76us/sample - loss: 0.3747\n",
      "Epoch 21/23\n",
      "7613/7613 [==============================] - 1s 76us/sample - loss: 0.3706\n",
      "Epoch 22/23\n",
      "7613/7613 [==============================] - 1s 80us/sample - loss: 0.3667\n",
      "Epoch 23/23\n",
      "7613/7613 [==============================] - 1s 76us/sample - loss: 0.3630\n",
      "Making predictions\n",
      "Training model 10\n",
      "Train on 7613 samples\n",
      "Epoch 1/23\n",
      "7613/7613 [==============================] - 1s 119us/sample - loss: 0.6662\n",
      "Epoch 2/23\n",
      "7613/7613 [==============================] - 1s 87us/sample - loss: 0.6117\n",
      "Epoch 3/23\n",
      "7613/7613 [==============================] - 1s 81us/sample - loss: 0.5707\n",
      "Epoch 4/23\n",
      "7613/7613 [==============================] - 1s 88us/sample - loss: 0.5391\n",
      "Epoch 5/23\n",
      "7613/7613 [==============================] - 1s 103us/sample - loss: 0.5140\n",
      "Epoch 6/23\n",
      "7613/7613 [==============================] - 1s 94us/sample - loss: 0.4935\n",
      "Epoch 7/23\n",
      "7613/7613 [==============================] - 1s 84us/sample - loss: 0.4765\n",
      "Epoch 8/23\n",
      "7613/7613 [==============================] - 1s 76us/sample - loss: 0.4621\n",
      "Epoch 9/23\n",
      "7613/7613 [==============================] - 1s 75us/sample - loss: 0.4496\n",
      "Epoch 10/23\n",
      "7613/7613 [==============================] - 1s 77us/sample - loss: 0.4388\n",
      "Epoch 11/23\n",
      "7613/7613 [==============================] - 1s 90us/sample - loss: 0.4292\n",
      "Epoch 12/23\n",
      "7613/7613 [==============================] - 1s 83us/sample - loss: 0.4206\n",
      "Epoch 13/23\n",
      "7613/7613 [==============================] - 1s 87us/sample - loss: 0.4130\n",
      "Epoch 14/23\n",
      "7613/7613 [==============================] - 1s 79us/sample - loss: 0.4060\n",
      "Epoch 15/23\n",
      "7613/7613 [==============================] - 1s 80us/sample - loss: 0.3997\n",
      "Epoch 16/23\n",
      "7613/7613 [==============================] - 1s 81us/sample - loss: 0.3939\n",
      "Epoch 17/23\n",
      "7613/7613 [==============================] - 1s 78us/sample - loss: 0.3884\n",
      "Epoch 18/23\n",
      "7613/7613 [==============================] - 1s 79us/sample - loss: 0.3835\n",
      "Epoch 19/23\n",
      "7613/7613 [==============================] - 1s 80us/sample - loss: 0.3788\n",
      "Epoch 20/23\n",
      "7613/7613 [==============================] - 1s 79us/sample - loss: 0.3745\n",
      "Epoch 21/23\n",
      "7613/7613 [==============================] - 1s 77us/sample - loss: 0.3703\n",
      "Epoch 22/23\n",
      "7613/7613 [==============================] - 1s 76us/sample - loss: 0.3665\n",
      "Epoch 23/23\n",
      "7613/7613 [==============================] - 1s 76us/sample - loss: 0.3629\n",
      "Making predictions\n",
      "Training model 11\n",
      "Train on 7613 samples\n",
      "Epoch 1/23\n",
      "7613/7613 [==============================] - 1s 108us/sample - loss: 0.6657\n",
      "Epoch 2/23\n",
      "7613/7613 [==============================] - 1s 85us/sample - loss: 0.6106\n",
      "Epoch 3/23\n",
      "7613/7613 [==============================] - 1s 80us/sample - loss: 0.56970s - lo\n",
      "Epoch 4/23\n",
      "7613/7613 [==============================] - 1s 90us/sample - loss: 0.5383\n",
      "Epoch 5/23\n",
      "7613/7613 [==============================] - 1s 83us/sample - loss: 0.5133\n",
      "Epoch 6/23\n",
      "7613/7613 [==============================] - 1s 83us/sample - loss: 0.49300s - l\n",
      "Epoch 7/23\n",
      "7613/7613 [==============================] - 1s 84us/sample - loss: 0.4762\n",
      "Epoch 8/23\n",
      "7613/7613 [==============================] - 1s 79us/sample - loss: 0.4618\n",
      "Epoch 9/23\n",
      "7613/7613 [==============================] - 1s 81us/sample - loss: 0.4494\n",
      "Epoch 10/23\n",
      "7613/7613 [==============================] - 1s 79us/sample - loss: 0.4387\n",
      "Epoch 11/23\n",
      "7613/7613 [==============================] - 1s 78us/sample - loss: 0.4291\n",
      "Epoch 12/23\n",
      "7613/7613 [==============================] - 1s 78us/sample - loss: 0.4206\n",
      "Epoch 13/23\n",
      "7613/7613 [==============================] - 1s 79us/sample - loss: 0.4130\n",
      "Epoch 14/23\n",
      "7613/7613 [==============================] - 1s 79us/sample - loss: 0.4061\n",
      "Epoch 15/23\n",
      "7613/7613 [==============================] - 1s 80us/sample - loss: 0.3997\n",
      "Epoch 16/23\n",
      "7613/7613 [==============================] - 1s 80us/sample - loss: 0.3939\n",
      "Epoch 17/23\n",
      "7613/7613 [==============================] - 1s 81us/sample - loss: 0.3885\n",
      "Epoch 18/23\n",
      "7613/7613 [==============================] - 1s 79us/sample - loss: 0.3835\n",
      "Epoch 19/23\n",
      "7613/7613 [==============================] - 1s 78us/sample - loss: 0.3789\n",
      "Epoch 20/23\n",
      "7613/7613 [==============================] - 1s 79us/sample - loss: 0.3745\n",
      "Epoch 21/23\n",
      "7613/7613 [==============================] - 1s 79us/sample - loss: 0.3705\n",
      "Epoch 22/23\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7613/7613 [==============================] - 1s 76us/sample - loss: 0.3666\n",
      "Epoch 23/23\n",
      "7613/7613 [==============================] - 1s 79us/sample - loss: 0.3629\n",
      "Making predictions\n",
      "Training model 12\n",
      "Train on 7613 samples\n",
      "Epoch 1/23\n",
      "7613/7613 [==============================] - 1s 114us/sample - loss: 0.6657\n",
      "Epoch 2/23\n",
      "7613/7613 [==============================] - 1s 82us/sample - loss: 0.6111\n",
      "Epoch 3/23\n",
      "7613/7613 [==============================] - 1s 78us/sample - loss: 0.5704\n",
      "Epoch 4/23\n",
      "7613/7613 [==============================] - 1s 78us/sample - loss: 0.5389\n",
      "Epoch 5/23\n",
      "7613/7613 [==============================] - 1s 82us/sample - loss: 0.5139\n",
      "Epoch 6/23\n",
      "7613/7613 [==============================] - 1s 77us/sample - loss: 0.4935\n",
      "Epoch 7/23\n",
      "7613/7613 [==============================] - 1s 82us/sample - loss: 0.4765\n",
      "Epoch 8/23\n",
      "7613/7613 [==============================] - 1s 76us/sample - loss: 0.4621\n",
      "Epoch 9/23\n",
      "7613/7613 [==============================] - 1s 79us/sample - loss: 0.4497\n",
      "Epoch 10/23\n",
      "7613/7613 [==============================] - 1s 79us/sample - loss: 0.4389\n",
      "Epoch 11/23\n",
      "7613/7613 [==============================] - 1s 91us/sample - loss: 0.4294\n",
      "Epoch 12/23\n",
      "7613/7613 [==============================] - 1s 84us/sample - loss: 0.4208\n",
      "Epoch 13/23\n",
      "7613/7613 [==============================] - 1s 86us/sample - loss: 0.4131\n",
      "Epoch 14/23\n",
      "7613/7613 [==============================] - 1s 79us/sample - loss: 0.4062\n",
      "Epoch 15/23\n",
      "7613/7613 [==============================] - 1s 79us/sample - loss: 0.3999\n",
      "Epoch 16/23\n",
      "7613/7613 [==============================] - 1s 79us/sample - loss: 0.3940\n",
      "Epoch 17/23\n",
      "7613/7613 [==============================] - 1s 77us/sample - loss: 0.38870s - los\n",
      "Epoch 18/23\n",
      "7613/7613 [==============================] - 1s 87us/sample - loss: 0.3836\n",
      "Epoch 19/23\n",
      "7613/7613 [==============================] - 1s 76us/sample - loss: 0.3790\n",
      "Epoch 20/23\n",
      "7613/7613 [==============================] - 1s 80us/sample - loss: 0.3746\n",
      "Epoch 21/23\n",
      "7613/7613 [==============================] - 1s 80us/sample - loss: 0.3705\n",
      "Epoch 22/23\n",
      "7613/7613 [==============================] - 1s 78us/sample - loss: 0.3667\n",
      "Epoch 23/23\n",
      "7613/7613 [==============================] - 1s 77us/sample - loss: 0.3630\n",
      "Making predictions\n",
      "Training model 13\n",
      "Train on 7613 samples\n",
      "Epoch 1/23\n",
      "7613/7613 [==============================] - 1s 108us/sample - loss: 0.6647\n",
      "Epoch 2/23\n",
      "7613/7613 [==============================] - 1s 85us/sample - loss: 0.6103\n",
      "Epoch 3/23\n",
      "7613/7613 [==============================] - 1s 79us/sample - loss: 0.5696\n",
      "Epoch 4/23\n",
      "7613/7613 [==============================] - 1s 79us/sample - loss: 0.5383\n",
      "Epoch 5/23\n",
      "7613/7613 [==============================] - 1s 78us/sample - loss: 0.5134\n",
      "Epoch 6/23\n",
      "7613/7613 [==============================] - 1s 83us/sample - loss: 0.4931\n",
      "Epoch 7/23\n",
      "7613/7613 [==============================] - 1s 80us/sample - loss: 0.4762\n",
      "Epoch 8/23\n",
      "7613/7613 [==============================] - 1s 76us/sample - loss: 0.4618\n",
      "Epoch 9/23\n",
      "7613/7613 [==============================] - 1s 77us/sample - loss: 0.4494\n",
      "Epoch 10/23\n",
      "7613/7613 [==============================] - 1s 76us/sample - loss: 0.4386\n",
      "Epoch 11/23\n",
      "7613/7613 [==============================] - 1s 87us/sample - loss: 0.4291\n",
      "Epoch 12/23\n",
      "7613/7613 [==============================] - 1s 80us/sample - loss: 0.4206\n",
      "Epoch 13/23\n",
      "7613/7613 [==============================] - 1s 83us/sample - loss: 0.4130\n",
      "Epoch 14/23\n",
      "7613/7613 [==============================] - 1s 78us/sample - loss: 0.4060\n",
      "Epoch 15/23\n",
      "7613/7613 [==============================] - 1s 78us/sample - loss: 0.3997\n",
      "Epoch 16/23\n",
      "7613/7613 [==============================] - 1s 77us/sample - loss: 0.3939\n",
      "Epoch 17/23\n",
      "7613/7613 [==============================] - 1s 77us/sample - loss: 0.3885\n",
      "Epoch 18/23\n",
      "7613/7613 [==============================] - 1s 77us/sample - loss: 0.3835\n",
      "Epoch 19/23\n",
      "7613/7613 [==============================] - 1s 80us/sample - loss: 0.3789\n",
      "Epoch 20/23\n",
      "7613/7613 [==============================] - 1s 81us/sample - loss: 0.3745\n",
      "Epoch 21/23\n",
      "7613/7613 [==============================] - 1s 80us/sample - loss: 0.3704\n",
      "Epoch 22/23\n",
      "7613/7613 [==============================] - 1s 78us/sample - loss: 0.3666\n",
      "Epoch 23/23\n",
      "7613/7613 [==============================] - 1s 76us/sample - loss: 0.3629\n",
      "Making predictions\n",
      "Training model 14\n",
      "Train on 7613 samples\n",
      "Epoch 1/23\n",
      "7613/7613 [==============================] - 1s 109us/sample - loss: 0.6655\n",
      "Epoch 2/23\n",
      "7613/7613 [==============================] - 1s 76us/sample - loss: 0.6107\n",
      "Epoch 3/23\n",
      "7613/7613 [==============================] - 1s 77us/sample - loss: 0.5699\n",
      "Epoch 4/23\n",
      "7613/7613 [==============================] - 1s 77us/sample - loss: 0.5385\n",
      "Epoch 5/23\n",
      "7613/7613 [==============================] - 1s 76us/sample - loss: 0.5135\n",
      "Epoch 6/23\n",
      "7613/7613 [==============================] - 1s 82us/sample - loss: 0.4932\n",
      "Epoch 7/23\n",
      "7613/7613 [==============================] - 1s 77us/sample - loss: 0.4762\n",
      "Epoch 8/23\n",
      "7613/7613 [==============================] - 1s 77us/sample - loss: 0.4618\n",
      "Epoch 9/23\n",
      "7613/7613 [==============================] - 1s 92us/sample - loss: 0.4495\n",
      "Epoch 10/23\n",
      "7613/7613 [==============================] - 1s 87us/sample - loss: 0.4387\n",
      "Epoch 11/23\n",
      "7613/7613 [==============================] - 1s 103us/sample - loss: 0.4292\n",
      "Epoch 12/23\n",
      "7613/7613 [==============================] - 1s 92us/sample - loss: 0.4207\n",
      "Epoch 13/23\n",
      "7613/7613 [==============================] - 1s 88us/sample - loss: 0.4130\n",
      "Epoch 14/23\n",
      "7613/7613 [==============================] - 1s 80us/sample - loss: 0.4061\n",
      "Epoch 15/23\n",
      "7613/7613 [==============================] - 1s 81us/sample - loss: 0.3997\n",
      "Epoch 16/23\n",
      "7613/7613 [==============================] - 1s 83us/sample - loss: 0.3939\n",
      "Epoch 17/23\n",
      "7613/7613 [==============================] - 1s 80us/sample - loss: 0.3886\n",
      "Epoch 18/23\n",
      "7613/7613 [==============================] - 1s 81us/sample - loss: 0.3836\n",
      "Epoch 19/23\n",
      "7613/7613 [==============================] - 1s 81us/sample - loss: 0.3789\n",
      "Epoch 20/23\n",
      "7613/7613 [==============================] - 1s 81us/sample - loss: 0.3745\n",
      "Epoch 21/23\n",
      "7613/7613 [==============================] - 1s 79us/sample - loss: 0.3705\n",
      "Epoch 22/23\n",
      "7613/7613 [==============================] - 1s 80us/sample - loss: 0.3666\n",
      "Epoch 23/23\n",
      "7613/7613 [==============================] - 1s 77us/sample - loss: 0.3630\n",
      "Making predictions\n",
      "Training model 15\n",
      "Train on 7613 samples\n",
      "Epoch 1/23\n",
      "7613/7613 [==============================] - 1s 104us/sample - loss: 0.6655\n",
      "Epoch 2/23\n",
      "7613/7613 [==============================] - 1s 79us/sample - loss: 0.6106\n",
      "Epoch 3/23\n",
      "7613/7613 [==============================] - 1s 76us/sample - loss: 0.5698\n",
      "Epoch 4/23\n",
      "7613/7613 [==============================] - 1s 77us/sample - loss: 0.5383\n",
      "Epoch 5/23\n",
      "7613/7613 [==============================] - 1s 76us/sample - loss: 0.5134\n",
      "Epoch 6/23\n",
      "7613/7613 [==============================] - 1s 76us/sample - loss: 0.4930\n",
      "Epoch 7/23\n",
      "7613/7613 [==============================] - 1s 79us/sample - loss: 0.4761\n",
      "Epoch 8/23\n",
      "7613/7613 [==============================] - 1s 79us/sample - loss: 0.46170s - l\n",
      "Epoch 9/23\n",
      "7613/7613 [==============================] - 1s 77us/sample - loss: 0.4493\n",
      "Epoch 10/23\n",
      "7613/7613 [==============================] - 1s 76us/sample - loss: 0.4385\n",
      "Epoch 11/23\n",
      "7613/7613 [==============================] - 1s 83us/sample - loss: 0.4290\n",
      "Epoch 12/23\n",
      "7613/7613 [==============================] - 1s 84us/sample - loss: 0.4205\n",
      "Epoch 13/23\n",
      "7613/7613 [==============================] - 1s 84us/sample - loss: 0.4128\n",
      "Epoch 14/23\n",
      "7613/7613 [==============================] - 1s 87us/sample - loss: 0.4059\n",
      "Epoch 15/23\n",
      "7613/7613 [==============================] - 1s 80us/sample - loss: 0.3996\n",
      "Epoch 16/23\n",
      "7613/7613 [==============================] - 1s 82us/sample - loss: 0.3937\n",
      "Epoch 17/23\n",
      "7613/7613 [==============================] - 1s 83us/sample - loss: 0.3884\n",
      "Epoch 18/23\n",
      "7613/7613 [==============================] - 1s 77us/sample - loss: 0.3834\n",
      "Epoch 19/23\n",
      "7613/7613 [==============================] - 1s 77us/sample - loss: 0.3787\n",
      "Epoch 20/23\n",
      "7613/7613 [==============================] - 1s 79us/sample - loss: 0.3744\n",
      "Epoch 21/23\n",
      "7613/7613 [==============================] - 1s 79us/sample - loss: 0.3703\n",
      "Epoch 22/23\n",
      "7613/7613 [==============================] - 1s 78us/sample - loss: 0.3664\n",
      "Epoch 23/23\n",
      "7613/7613 [==============================] - 1s 76us/sample - loss: 0.3628\n",
      "Making predictions\n",
      "Training model 16\n",
      "Train on 7613 samples\n",
      "Epoch 1/23\n",
      "7613/7613 [==============================] - 1s 107us/sample - loss: 0.6645\n",
      "Epoch 2/23\n",
      "7613/7613 [==============================] - 1s 83us/sample - loss: 0.6098\n",
      "Epoch 3/23\n",
      "7613/7613 [==============================] - 1s 81us/sample - loss: 0.5690\n",
      "Epoch 4/23\n",
      "7613/7613 [==============================] - 1s 81us/sample - loss: 0.5376\n",
      "Epoch 5/23\n",
      "7613/7613 [==============================] - 1s 79us/sample - loss: 0.5128\n",
      "Epoch 6/23\n",
      "7613/7613 [==============================] - 1s 79us/sample - loss: 0.4925\n",
      "Epoch 7/23\n",
      "7613/7613 [==============================] - 1s 81us/sample - loss: 0.4756\n",
      "Epoch 8/23\n",
      "7613/7613 [==============================] - 1s 85us/sample - loss: 0.4612\n",
      "Epoch 9/23\n",
      "7613/7613 [==============================] - 1s 78us/sample - loss: 0.4489\n",
      "Epoch 10/23\n",
      "7613/7613 [==============================] - 1s 91us/sample - loss: 0.4382\n",
      "Epoch 11/23\n",
      "7613/7613 [==============================] - 1s 93us/sample - loss: 0.4287\n",
      "Epoch 12/23\n",
      "7613/7613 [==============================] - 1s 88us/sample - loss: 0.4202\n",
      "Epoch 13/23\n",
      "7613/7613 [==============================] - 1s 86us/sample - loss: 0.4126\n",
      "Epoch 14/23\n",
      "7613/7613 [==============================] - 1s 84us/sample - loss: 0.4057\n",
      "Epoch 15/23\n",
      "7613/7613 [==============================] - 1s 80us/sample - loss: 0.3994\n",
      "Epoch 16/23\n",
      "7613/7613 [==============================] - 1s 80us/sample - loss: 0.3936\n",
      "Epoch 17/23\n",
      "7613/7613 [==============================] - 1s 79us/sample - loss: 0.3882\n",
      "Epoch 18/23\n",
      "7613/7613 [==============================] - 1s 84us/sample - loss: 0.3833\n",
      "Epoch 19/23\n",
      "7613/7613 [==============================] - 1s 81us/sample - loss: 0.3786\n",
      "Epoch 20/23\n",
      "7613/7613 [==============================] - 1s 87us/sample - loss: 0.3743\n",
      "Epoch 21/23\n",
      "7613/7613 [==============================] - 1s 83us/sample - loss: 0.3702\n",
      "Epoch 22/23\n",
      "7613/7613 [==============================] - 1s 80us/sample - loss: 0.3664\n",
      "Epoch 23/23\n",
      "7613/7613 [==============================] - 1s 88us/sample - loss: 0.3628\n",
      "Making predictions\n",
      "Training model 17\n",
      "Train on 7613 samples\n",
      "Epoch 1/23\n",
      "7613/7613 [==============================] - 1s 111us/sample - loss: 0.6646\n",
      "Epoch 2/23\n",
      "7613/7613 [==============================] - 1s 90us/sample - loss: 0.6103\n",
      "Epoch 3/23\n",
      "7613/7613 [==============================] - 1s 81us/sample - loss: 0.5698\n",
      "Epoch 4/23\n",
      "7613/7613 [==============================] - 1s 80us/sample - loss: 0.5384\n",
      "Epoch 5/23\n",
      "7613/7613 [==============================] - 1s 89us/sample - loss: 0.5135\n",
      "Epoch 6/23\n",
      "7613/7613 [==============================] - 1s 83us/sample - loss: 0.4932\n",
      "Epoch 7/23\n",
      "7613/7613 [==============================] - 1s 81us/sample - loss: 0.4762\n",
      "Epoch 8/23\n",
      "7613/7613 [==============================] - 1s 82us/sample - loss: 0.4619\n",
      "Epoch 9/23\n",
      "7613/7613 [==============================] - 1s 80us/sample - loss: 0.4495\n",
      "Epoch 10/23\n",
      "7613/7613 [==============================] - 1s 90us/sample - loss: 0.4387\n",
      "Epoch 11/23\n",
      "7613/7613 [==============================] - 1s 86us/sample - loss: 0.4292\n",
      "Epoch 12/23\n",
      "7613/7613 [==============================] - 1s 81us/sample - loss: 0.4208\n",
      "Epoch 13/23\n",
      "7613/7613 [==============================] - 1s 96us/sample - loss: 0.4130\n",
      "Epoch 14/23\n",
      "7613/7613 [==============================] - 1s 86us/sample - loss: 0.4061\n",
      "Epoch 15/23\n",
      "7613/7613 [==============================] - 1s 78us/sample - loss: 0.3998\n",
      "Epoch 16/23\n",
      "7613/7613 [==============================] - 1s 84us/sample - loss: 0.3940\n",
      "Epoch 17/23\n",
      "7613/7613 [==============================] - 1s 81us/sample - loss: 0.3886\n",
      "Epoch 18/23\n",
      "7613/7613 [==============================] - 1s 87us/sample - loss: 0.3836\n",
      "Epoch 19/23\n",
      "7613/7613 [==============================] - 1s 79us/sample - loss: 0.3789\n",
      "Epoch 20/23\n",
      "7613/7613 [==============================] - 1s 77us/sample - loss: 0.3746\n",
      "Epoch 21/23\n",
      "7613/7613 [==============================] - 1s 84us/sample - loss: 0.3705\n",
      "Epoch 22/23\n",
      "7613/7613 [==============================] - 1s 79us/sample - loss: 0.3666\n",
      "Epoch 23/23\n",
      "7613/7613 [==============================] - 1s 78us/sample - loss: 0.3630\n",
      "Making predictions\n",
      "Training model 18\n",
      "Train on 7613 samples\n",
      "Epoch 1/23\n",
      "7613/7613 [==============================] - 1s 106us/sample - loss: 0.6655\n",
      "Epoch 2/23\n",
      "7613/7613 [==============================] - 1s 79us/sample - loss: 0.6107\n",
      "Epoch 3/23\n",
      "7613/7613 [==============================] - 1s 83us/sample - loss: 0.5697\n",
      "Epoch 4/23\n",
      "7613/7613 [==============================] - 1s 85us/sample - loss: 0.5383\n",
      "Epoch 5/23\n",
      "7613/7613 [==============================] - 1s 86us/sample - loss: 0.5132\n",
      "Epoch 6/23\n",
      "7613/7613 [==============================] - 1s 81us/sample - loss: 0.4929\n",
      "Epoch 7/23\n",
      "7613/7613 [==============================] - 1s 79us/sample - loss: 0.4760\n",
      "Epoch 8/23\n",
      "7613/7613 [==============================] - 1s 80us/sample - loss: 0.4616\n",
      "Epoch 9/23\n",
      "7613/7613 [==============================] - 1s 78us/sample - loss: 0.4492\n",
      "Epoch 10/23\n",
      "7613/7613 [==============================] - 1s 104us/sample - loss: 0.4384\n",
      "Epoch 11/23\n",
      "7613/7613 [==============================] - 1s 79us/sample - loss: 0.4289\n",
      "Epoch 12/23\n",
      "7613/7613 [==============================] - 1s 79us/sample - loss: 0.4204\n",
      "Epoch 13/23\n",
      "7613/7613 [==============================] - 1s 78us/sample - loss: 0.4128\n",
      "Epoch 14/23\n",
      "7613/7613 [==============================] - 1s 80us/sample - loss: 0.4058\n",
      "Epoch 15/23\n",
      "7613/7613 [==============================] - 1s 81us/sample - loss: 0.3995\n",
      "Epoch 16/23\n",
      "7613/7613 [==============================] - 1s 78us/sample - loss: 0.3937\n",
      "Epoch 17/23\n",
      "7613/7613 [==============================] - 1s 94us/sample - loss: 0.3883\n",
      "Epoch 18/23\n",
      "7613/7613 [==============================] - 1s 91us/sample - loss: 0.3834\n",
      "Epoch 19/23\n",
      "7613/7613 [==============================] - 1s 87us/sample - loss: 0.3787\n",
      "Epoch 20/23\n",
      "7613/7613 [==============================] - 1s 86us/sample - loss: 0.3744\n",
      "Epoch 21/23\n",
      "7613/7613 [==============================] - 1s 89us/sample - loss: 0.3703\n",
      "Epoch 22/23\n",
      "7613/7613 [==============================] - 1s 83us/sample - loss: 0.3665\n",
      "Epoch 23/23\n",
      "7613/7613 [==============================] - 1s 82us/sample - loss: 0.3628\n",
      "Making predictions\n",
      "Training model 19\n",
      "Train on 7613 samples\n",
      "Epoch 1/23\n",
      "7613/7613 [==============================] - 1s 111us/sample - loss: 0.6661\n",
      "Epoch 2/23\n",
      "7613/7613 [==============================] - 1s 81us/sample - loss: 0.6118\n",
      "Epoch 3/23\n",
      "7613/7613 [==============================] - 1s 79us/sample - loss: 0.5709\n",
      "Epoch 4/23\n",
      "7613/7613 [==============================] - 1s 78us/sample - loss: 0.5394\n",
      "Epoch 5/23\n",
      "7613/7613 [==============================] - 1s 83us/sample - loss: 0.5143\n",
      "Epoch 6/23\n",
      "7613/7613 [==============================] - 1s 78us/sample - loss: 0.4938\n",
      "Epoch 7/23\n",
      "7613/7613 [==============================] - 1s 80us/sample - loss: 0.4768\n",
      "Epoch 8/23\n",
      "7613/7613 [==============================] - 1s 81us/sample - loss: 0.4624\n",
      "Epoch 9/23\n",
      "7613/7613 [==============================] - 1s 78us/sample - loss: 0.4499\n",
      "Epoch 10/23\n",
      "7613/7613 [==============================] - 1s 88us/sample - loss: 0.4390\n",
      "Epoch 11/23\n",
      "7613/7613 [==============================] - 1s 79us/sample - loss: 0.4295\n",
      "Epoch 12/23\n",
      "7613/7613 [==============================] - 1s 79us/sample - loss: 0.4209\n",
      "Epoch 13/23\n",
      "7613/7613 [==============================] - 1s 81us/sample - loss: 0.4133\n",
      "Epoch 14/23\n",
      "7613/7613 [==============================] - 1s 79us/sample - loss: 0.4063\n",
      "Epoch 15/23\n",
      "7613/7613 [==============================] - 1s 81us/sample - loss: 0.3999\n",
      "Epoch 16/23\n",
      "7613/7613 [==============================] - 1s 82us/sample - loss: 0.3941\n",
      "Epoch 17/23\n",
      "7613/7613 [==============================] - 1s 81us/sample - loss: 0.3887\n",
      "Epoch 18/23\n",
      "7613/7613 [==============================] - 1s 90us/sample - loss: 0.3837\n",
      "Epoch 19/23\n",
      "7613/7613 [==============================] - 1s 90us/sample - loss: 0.3791\n",
      "Epoch 20/23\n",
      "7613/7613 [==============================] - 1s 84us/sample - loss: 0.3747\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/23\n",
      "7613/7613 [==============================] - 1s 83us/sample - loss: 0.3706\n",
      "Epoch 22/23\n",
      "7613/7613 [==============================] - 1s 83us/sample - loss: 0.3667\n",
      "Epoch 23/23\n",
      "7613/7613 [==============================] - 1s 85us/sample - loss: 0.3631\n",
      "Making predictions\n",
      "Training model 20\n",
      "Train on 7613 samples\n",
      "Epoch 1/23\n",
      "7613/7613 [==============================] - 1s 111us/sample - loss: 0.6631\n",
      "Epoch 2/23\n",
      "7613/7613 [==============================] - 1s 81us/sample - loss: 0.6089\n",
      "Epoch 3/23\n",
      "7613/7613 [==============================] - 1s 81us/sample - loss: 0.5684\n",
      "Epoch 4/23\n",
      "7613/7613 [==============================] - 1s 91us/sample - loss: 0.5372\n",
      "Epoch 5/23\n",
      "7613/7613 [==============================] - 1s 77us/sample - loss: 0.5124\n",
      "Epoch 6/23\n",
      "7613/7613 [==============================] - 1s 81us/sample - loss: 0.4923\n",
      "Epoch 7/23\n",
      "7613/7613 [==============================] - 1s 80us/sample - loss: 0.4754\n",
      "Epoch 8/23\n",
      "7613/7613 [==============================] - 1s 85us/sample - loss: 0.4612\n",
      "Epoch 9/23\n",
      "7613/7613 [==============================] - 1s 92us/sample - loss: 0.4488\n",
      "Epoch 10/23\n",
      "7613/7613 [==============================] - 1s 81us/sample - loss: 0.4381\n",
      "Epoch 11/23\n",
      "7613/7613 [==============================] - 1s 80us/sample - loss: 0.4286\n",
      "Epoch 12/23\n",
      "7613/7613 [==============================] - 1s 78us/sample - loss: 0.4201\n",
      "Epoch 13/23\n",
      "7613/7613 [==============================] - 1s 78us/sample - loss: 0.4126\n",
      "Epoch 14/23\n",
      "7613/7613 [==============================] - 1s 79us/sample - loss: 0.4056\n",
      "Epoch 15/23\n",
      "7613/7613 [==============================] - 1s 78us/sample - loss: 0.3994\n",
      "Epoch 16/23\n",
      "7613/7613 [==============================] - 1s 78us/sample - loss: 0.3935\n",
      "Epoch 17/23\n",
      "7613/7613 [==============================] - 1s 81us/sample - loss: 0.3882\n",
      "Epoch 18/23\n",
      "7613/7613 [==============================] - 1s 78us/sample - loss: 0.3832\n",
      "Epoch 19/23\n",
      "7613/7613 [==============================] - 1s 83us/sample - loss: 0.3786\n",
      "Epoch 20/23\n",
      "7613/7613 [==============================] - 1s 90us/sample - loss: 0.3742\n",
      "Epoch 21/23\n",
      "7613/7613 [==============================] - 1s 103us/sample - loss: 0.3702\n",
      "Epoch 22/23\n",
      "7613/7613 [==============================] - 1s 81us/sample - loss: 0.3663\n",
      "Epoch 23/23\n",
      "7613/7613 [==============================] - 1s 78us/sample - loss: 0.3627\n",
      "Making predictions\n",
      "Training model 21\n",
      "Train on 7613 samples\n",
      "Epoch 1/23\n",
      "7613/7613 [==============================] - 1s 106us/sample - loss: 0.6664\n",
      "Epoch 2/23\n",
      "7613/7613 [==============================] - 1s 78us/sample - loss: 0.6114\n",
      "Epoch 3/23\n",
      "7613/7613 [==============================] - 1s 77us/sample - loss: 0.5705\n",
      "Epoch 4/23\n",
      "7613/7613 [==============================] - 1s 86us/sample - loss: 0.5390\n",
      "Epoch 5/23\n",
      "7613/7613 [==============================] - 1s 87us/sample - loss: 0.5140\n",
      "Epoch 6/23\n",
      "7613/7613 [==============================] - 1s 79us/sample - loss: 0.4936\n",
      "Epoch 7/23\n",
      "7613/7613 [==============================] - 1s 78us/sample - loss: 0.4766\n",
      "Epoch 8/23\n",
      "7613/7613 [==============================] - 1s 94us/sample - loss: 0.4622\n",
      "Epoch 9/23\n",
      "7613/7613 [==============================] - 1s 83us/sample - loss: 0.4498\n",
      "Epoch 10/23\n",
      "7613/7613 [==============================] - 1s 85us/sample - loss: 0.4389\n",
      "Epoch 11/23\n",
      "7613/7613 [==============================] - 1s 79us/sample - loss: 0.4294\n",
      "Epoch 12/23\n",
      "7613/7613 [==============================] - 1s 79us/sample - loss: 0.4209\n",
      "Epoch 13/23\n",
      "7613/7613 [==============================] - 1s 77us/sample - loss: 0.4132\n",
      "Epoch 14/23\n",
      "7613/7613 [==============================] - 1s 86us/sample - loss: 0.4063\n",
      "Epoch 15/23\n",
      "7613/7613 [==============================] - 1s 80us/sample - loss: 0.3999\n",
      "Epoch 16/23\n",
      "7613/7613 [==============================] - 1s 80us/sample - loss: 0.3941\n",
      "Epoch 17/23\n",
      "7613/7613 [==============================] - 1s 79us/sample - loss: 0.3887\n",
      "Epoch 18/23\n",
      "7613/7613 [==============================] - 1s 79us/sample - loss: 0.3837\n",
      "Epoch 19/23\n",
      "7613/7613 [==============================] - 1s 84us/sample - loss: 0.3790\n",
      "Epoch 20/23\n",
      "7613/7613 [==============================] - 1s 81us/sample - loss: 0.3747\n",
      "Epoch 21/23\n",
      "7613/7613 [==============================] - 1s 80us/sample - loss: 0.3706\n",
      "Epoch 22/23\n",
      "7613/7613 [==============================] - 1s 79us/sample - loss: 0.3667\n",
      "Epoch 23/23\n",
      "7613/7613 [==============================] - 1s 86us/sample - loss: 0.3631\n",
      "Making predictions\n",
      "Training model 22\n",
      "Train on 7613 samples\n",
      "Epoch 1/23\n",
      "7613/7613 [==============================] - 1s 107us/sample - loss: 0.6682\n",
      "Epoch 2/23\n",
      "7613/7613 [==============================] - 1s 79us/sample - loss: 0.6128\n",
      "Epoch 3/23\n",
      "7613/7613 [==============================] - 1s 80us/sample - loss: 0.5715\n",
      "Epoch 4/23\n",
      "7613/7613 [==============================] - 1s 80us/sample - loss: 0.5396\n",
      "Epoch 5/23\n",
      "7613/7613 [==============================] - 1s 81us/sample - loss: 0.5143\n",
      "Epoch 6/23\n",
      "7613/7613 [==============================] - 1s 80us/sample - loss: 0.4938\n",
      "Epoch 7/23\n",
      "7613/7613 [==============================] - 1s 82us/sample - loss: 0.4767\n",
      "Epoch 8/23\n",
      "7613/7613 [==============================] - 1s 100us/sample - loss: 0.4623\n",
      "Epoch 9/23\n",
      "7613/7613 [==============================] - 1s 81us/sample - loss: 0.4498\n",
      "Epoch 10/23\n",
      "7613/7613 [==============================] - 1s 79us/sample - loss: 0.4389\n",
      "Epoch 11/23\n",
      "7613/7613 [==============================] - 1s 78us/sample - loss: 0.4294\n",
      "Epoch 12/23\n",
      "7613/7613 [==============================] - 1s 78us/sample - loss: 0.4208\n",
      "Epoch 13/23\n",
      "7613/7613 [==============================] - 1s 80us/sample - loss: 0.4131\n",
      "Epoch 14/23\n",
      "7613/7613 [==============================] - 1s 85us/sample - loss: 0.4062\n",
      "Epoch 15/23\n",
      "7613/7613 [==============================] - 1s 89us/sample - loss: 0.3998\n",
      "Epoch 16/23\n",
      "7613/7613 [==============================] - 1s 84us/sample - loss: 0.3940\n",
      "Epoch 17/23\n",
      "7613/7613 [==============================] - ETA: 0s - loss: 0.388 - 1s 78us/sample - loss: 0.3886\n",
      "Epoch 18/23\n",
      "7613/7613 [==============================] - 1s 80us/sample - loss: 0.3836\n",
      "Epoch 19/23\n",
      "7613/7613 [==============================] - 1s 82us/sample - loss: 0.3790\n",
      "Epoch 20/23\n",
      "7613/7613 [==============================] - 1s 86us/sample - loss: 0.3746\n",
      "Epoch 21/23\n",
      "7613/7613 [==============================] - 1s 87us/sample - loss: 0.3704\n",
      "Epoch 22/23\n",
      "7613/7613 [==============================] - 1s 80us/sample - loss: 0.3666\n",
      "Epoch 23/23\n",
      "7613/7613 [==============================] - 1s 82us/sample - loss: 0.3630\n",
      "Making predictions\n",
      "Training model 23\n",
      "Train on 7613 samples\n",
      "Epoch 1/23\n",
      "7613/7613 [==============================] - 1s 123us/sample - loss: 0.6636\n",
      "Epoch 2/23\n",
      "7613/7613 [==============================] - 1s 84us/sample - loss: 0.6091\n",
      "Epoch 3/23\n",
      "7613/7613 [==============================] - 1s 98us/sample - loss: 0.5685\n",
      "Epoch 4/23\n",
      "7613/7613 [==============================] - 1s 85us/sample - loss: 0.5372\n",
      "Epoch 5/23\n",
      "7613/7613 [==============================] - 1s 87us/sample - loss: 0.5125\n",
      "Epoch 6/23\n",
      "7613/7613 [==============================] - 1s 83us/sample - loss: 0.4922\n",
      "Epoch 7/23\n",
      "7613/7613 [==============================] - 1s 86us/sample - loss: 0.4754\n",
      "Epoch 8/23\n",
      "7613/7613 [==============================] - 1s 79us/sample - loss: 0.4611\n",
      "Epoch 9/23\n",
      "7613/7613 [==============================] - 1s 81us/sample - loss: 0.4488\n",
      "Epoch 10/23\n",
      "7613/7613 [==============================] - 1s 79us/sample - loss: 0.4380\n",
      "Epoch 11/23\n",
      "7613/7613 [==============================] - 1s 78us/sample - loss: 0.4286\n",
      "Epoch 12/23\n",
      "7613/7613 [==============================] - 1s 79us/sample - loss: 0.4202\n",
      "Epoch 13/23\n",
      "7613/7613 [==============================] - 1s 80us/sample - loss: 0.4125\n",
      "Epoch 14/23\n",
      "7613/7613 [==============================] - 1s 80us/sample - loss: 0.4056\n",
      "Epoch 15/23\n",
      "7613/7613 [==============================] - 1s 80us/sample - loss: 0.3994\n",
      "Epoch 16/23\n",
      "7613/7613 [==============================] - 1s 88us/sample - loss: 0.3936\n",
      "Epoch 17/23\n",
      "7613/7613 [==============================] - 1s 78us/sample - loss: 0.3882\n",
      "Epoch 18/23\n",
      "7613/7613 [==============================] - 1s 77us/sample - loss: 0.3833\n",
      "Epoch 19/23\n",
      "7613/7613 [==============================] - 1s 84us/sample - loss: 0.3786\n",
      "Epoch 20/23\n",
      "7613/7613 [==============================] - 1s 78us/sample - loss: 0.3743\n",
      "Epoch 21/23\n",
      "7613/7613 [==============================] - 1s 77us/sample - loss: 0.3702\n",
      "Epoch 22/23\n",
      "7613/7613 [==============================] - 1s 78us/sample - loss: 0.3664\n",
      "Epoch 23/23\n",
      "7613/7613 [==============================] - 1s 77us/sample - loss: 0.3628\n",
      "Making predictions\n",
      "Training model 24\n",
      "Train on 7613 samples\n",
      "Epoch 1/23\n",
      "7613/7613 [==============================] - 1s 107us/sample - loss: 0.6643\n",
      "Epoch 2/23\n",
      "7613/7613 [==============================] - 1s 91us/sample - loss: 0.6097\n",
      "Epoch 3/23\n",
      "7613/7613 [==============================] - 1s 85us/sample - loss: 0.5691\n",
      "Epoch 4/23\n",
      "7613/7613 [==============================] - 1s 84us/sample - loss: 0.5378\n",
      "Epoch 5/23\n",
      "7613/7613 [==============================] - 1s 87us/sample - loss: 0.5129\n",
      "Epoch 6/23\n",
      "7613/7613 [==============================] - 1s 91us/sample - loss: 0.4927\n",
      "Epoch 7/23\n",
      "7613/7613 [==============================] - 1s 96us/sample - loss: 0.4758\n",
      "Epoch 8/23\n",
      "7613/7613 [==============================] - 1s 83us/sample - loss: 0.4615\n",
      "Epoch 9/23\n",
      "7613/7613 [==============================] - 1s 80us/sample - loss: 0.4492\n",
      "Epoch 10/23\n",
      "7613/7613 [==============================] - 1s 80us/sample - loss: 0.4384\n",
      "Epoch 11/23\n",
      "7613/7613 [==============================] - 1s 80us/sample - loss: 0.4289\n",
      "Epoch 12/23\n",
      "7613/7613 [==============================] - 1s 82us/sample - loss: 0.4204\n",
      "Epoch 13/23\n",
      "7613/7613 [==============================] - 1s 82us/sample - loss: 0.4129\n",
      "Epoch 14/23\n",
      "7613/7613 [==============================] - 1s 81us/sample - loss: 0.4059\n",
      "Epoch 15/23\n",
      "7613/7613 [==============================] - 1s 81us/sample - loss: 0.3996\n",
      "Epoch 16/23\n",
      "7613/7613 [==============================] - 1s 85us/sample - loss: 0.3938\n",
      "Epoch 17/23\n",
      "7613/7613 [==============================] - 1s 80us/sample - loss: 0.3884\n",
      "Epoch 18/23\n",
      "7613/7613 [==============================] - 1s 95us/sample - loss: 0.3834\n",
      "Epoch 19/23\n",
      "7613/7613 [==============================] - 1s 81us/sample - loss: 0.3787\n",
      "Epoch 20/23\n",
      "7613/7613 [==============================] - 1s 83us/sample - loss: 0.3744\n",
      "Epoch 21/23\n",
      "7613/7613 [==============================] - 1s 83us/sample - loss: 0.3703\n",
      "Epoch 22/23\n",
      "7613/7613 [==============================] - 1s 83us/sample - loss: 0.3665\n",
      "Epoch 23/23\n",
      "7613/7613 [==============================] - 1s 81us/sample - loss: 0.3628\n",
      "Making predictions\n"
     ]
    }
   ],
   "source": [
    "models = train_ensemble(25, 32, 23, np.array(dftrain_minimal.drop('_target', axis=1)), np.array(dftrain_minimal['_target']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making prediction 0\n",
      "Making prediction 1\n",
      "Making prediction 2\n",
      "Making prediction 3\n",
      "Making prediction 4\n",
      "Making prediction 5\n",
      "Making prediction 6\n",
      "Making prediction 7\n",
      "Making prediction 8\n",
      "Making prediction 9\n",
      "Making prediction 10\n",
      "Making prediction 11\n",
      "Making prediction 12\n",
      "Making prediction 13\n",
      "Making prediction 14\n",
      "Making prediction 15\n",
      "Making prediction 16\n",
      "Making prediction 17\n",
      "Making prediction 18\n",
      "Making prediction 19\n",
      "Making prediction 20\n",
      "Making prediction 21\n",
      "Making prediction 22\n",
      "Making prediction 23\n",
      "Making prediction 24\n",
      "Prediction complete\n"
     ]
    }
   ],
   "source": [
    "# Make test predictions\n",
    "raw_preds = np.zeros((dftest_minimal.shape[0],1))\n",
    "for i in range(len(models)):\n",
    "    print('Making prediction', i)\n",
    "    raw_preds += models[i].predict(dftest_minimal)\n",
    "raw_preds /= 25\n",
    "preds = 1*(raw_preds >= 0.5)\n",
    "print('Prediction complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['target'] = preds\n",
    "submission.to_csv('submission_lg_ensemble.csv', index=False)\n",
    "# Score: 80.572%, then 79.243%, ugh."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support vector machines\n",
    "def run_SVC(dataset, split, C, class_weight=None):\n",
    "    t0 = time.time()\n",
    "    np.random.seed(1)\n",
    "    train, test = train_test_split(dataset, test_size=split)\n",
    "    train_x = train.drop('_target', axis=1)\n",
    "    train_y = train['_target']\n",
    "    test_x = test.drop('_target', axis=1)\n",
    "    test_y = test['_target']\n",
    "    print('Training...')\n",
    "    svc = svm.LinearSVC(C=C, class_weight=class_weight)\n",
    "    svc.fit(train_x, train_y)\n",
    "    print('Predicting...')\n",
    "    preds = svc.predict(test_x)\n",
    "    cm = confusion_matrix(test_y, preds)\n",
    "    print(cm)\n",
    "    tp, fn, fp, tn = cm.ravel()\n",
    "    accuracy = (tp+tn)/(tp+fn+fp+tn)\n",
    "    precision = tp/(tp+fp)\n",
    "    recall = tp/(tp+fn)\n",
    "    F = 2*precision*recall/(precision+recall)\n",
    "    print('Accuracy: {:.2f}%\\nPrecision: {:.2f}%\\nRecall: {:.2f}%\\nF: {:.2f}'.format(100*accuracy, 100*precision, 100*recall, 100*F))\n",
    "    t1 = time.time()\n",
    "    print('(Took {:.3f} sec)'.format(t1-t0))\n",
    "    return svc\n",
    "\n",
    "# 5-fold cross-validation, all metrics\n",
    "def cross_validate_SVC(dataset, C, class_weight=None):\n",
    "    t_0 = time.time()\n",
    "    # Fixed at 5-fold cross-validation for now\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "    X = np.array(dataset.drop('_target', axis=1))\n",
    "    y = np.array(dataset['_target'])\n",
    "    accs = []\n",
    "    precs_p = []\n",
    "    precs_n = []\n",
    "    recs_p = []\n",
    "    recs_n = []\n",
    "    Fs_p = []\n",
    "    Fs_n = []\n",
    "    mFs = [] # Mean F score, this is the actual competition metric\n",
    "    cm = np.zeros((2,2))\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        t0 = time.time()\n",
    "        train_x, test_x = X[train_index], X[test_index]\n",
    "        train_y, test_y = y[train_index], y[test_index]\n",
    "        #print('Training...')\n",
    "        svc = svm.LinearSVC(C=C, class_weight=class_weight)\n",
    "        svc.fit(train_x, train_y)\n",
    "        #print('Predicting...')\n",
    "        preds = svc.predict(test_x)\n",
    "        cm_batch = confusion_matrix(test_y, preds)\n",
    "        cm += np.array(cm_batch)\n",
    "        tp, fn, fp, tn = cm_batch.ravel()\n",
    "        acc = (tp+tn)/(tp+fn+fp+tn)\n",
    "        accs.append( acc )\n",
    "        prec_p = tp/(tp+fp)\n",
    "        precs_p.append( prec_p )\n",
    "        prec_n = tn/(tn+fn)\n",
    "        precs_n.append( prec_n )\n",
    "        rec_p = tp/(tp+fn)\n",
    "        recs_p.append( rec_p )\n",
    "        rec_n = tn/(tn+fp)\n",
    "        recs_n.append( rec_n )\n",
    "        F_p = 2*prec_p*rec_p/(prec_p+rec_p)\n",
    "        Fs_p.append( F_p )\n",
    "        F_n = 2*prec_n*rec_n/(prec_n+rec_n)\n",
    "        Fs_n.append( F_n )\n",
    "        mF = (F_p + F_n)/2.0\n",
    "        mFs.append( mF )\n",
    "        t1 = time.time()\n",
    "        print('(Took {:.3f} sec)'.format(t1-t0))\n",
    "    t_1 = time.time()\n",
    "    print('Combined confusion matrix:')\n",
    "    print(cm)\n",
    "    print('(Overall, took {:.3f} sec)'.format(t_1-t_0))\n",
    "    return [accs, precs_p, precs_n, recs_p, recs_n, Fs_p, Fs_n, mFs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Took 0.857 sec)\n",
      "(Took 0.920 sec)\n",
      "(Took 0.920 sec)\n",
      "(Took 0.905 sec)\n",
      "(Took 0.888 sec)\n",
      "Combined confusion matrix:\n",
      "[[3850.  492.]\n",
      " [1003. 2268.]]\n",
      "(Overall, took 4.813 sec)\n",
      "Accuracy: 80.36% +/- 0.64%\n",
      "Precision for positive class: 79.33% +/- 1.43%\n",
      "Precision for negative class: 82.19% +/- 1.97%\n",
      "Recall for positive class: 88.67% +/- 1.36%\n",
      "Recall for negative class: 69.34% +/- 1.39%\n",
      "F for positive class: 83.73% +/- 0.83%\n",
      "F for negative class: 75.21% +/- 0.67%\n",
      "Mean F score: 79.47% +/- 0.54%\n",
      "(Took 0.618 sec)\n",
      "(Took 0.655 sec)\n",
      "(Took 0.649 sec)\n",
      "(Took 0.660 sec)\n",
      "(Took 0.672 sec)\n",
      "Combined confusion matrix:\n",
      "[[4070.  272.]\n",
      " [2117. 1154.]]\n",
      "(Overall, took 3.591 sec)\n",
      "Accuracy: 68.62% +/- 1.74%\n",
      "Precision for positive class: 65.79% +/- 2.08%\n",
      "Precision for negative class: 80.98% +/- 3.55%\n",
      "Recall for positive class: 93.74% +/- 1.29%\n",
      "Recall for negative class: 35.31% +/- 1.95%\n",
      "F for positive class: 77.30% +/- 1.48%\n",
      "F for negative class: 49.15% +/- 1.97%\n",
      "Mean F score: 63.23% +/- 1.66%\n",
      "(Took 0.613 sec)\n",
      "(Took 0.674 sec)\n",
      "(Took 0.684 sec)\n",
      "(Took 0.692 sec)\n",
      "(Took 0.707 sec)\n",
      "Combined confusion matrix:\n",
      "[[4150.  192.]\n",
      " [1660. 1611.]]\n",
      "(Overall, took 3.703 sec)\n",
      "Accuracy: 75.67% +/- 0.95%\n",
      "Precision for positive class: 71.43% +/- 1.63%\n",
      "Precision for negative class: 89.36% +/- 1.60%\n",
      "Recall for positive class: 95.58% +/- 0.64%\n",
      "Recall for negative class: 49.26% +/- 1.10%\n",
      "F for positive class: 81.75% +/- 0.99%\n",
      "F for negative class: 63.50% +/- 0.71%\n",
      "Mean F score: 72.63% +/- 0.72%\n",
      "(Took 0.388 sec)\n",
      "(Took 0.462 sec)\n",
      "(Took 0.441 sec)\n",
      "(Took 0.464 sec)\n",
      "(Took 0.401 sec)\n",
      "Combined confusion matrix:\n",
      "[[3912.  430.]\n",
      " [1102. 2169.]]\n",
      "(Overall, took 2.325 sec)\n",
      "Accuracy: 79.88% +/- 0.61%\n",
      "Precision for positive class: 78.02% +/- 1.47%\n",
      "Precision for negative class: 83.46% +/- 1.40%\n",
      "Recall for positive class: 90.10% +/- 1.03%\n",
      "Recall for negative class: 66.31% +/- 1.67%\n",
      "F for positive class: 83.62% +/- 0.81%\n",
      "F for negative class: 73.89% +/- 0.75%\n",
      "Mean F score: 78.75% +/- 0.49%\n",
      "(Took 0.283 sec)\n",
      "(Took 0.316 sec)\n",
      "(Took 0.292 sec)\n",
      "(Took 0.303 sec)\n",
      "(Took 0.319 sec)\n",
      "Combined confusion matrix:\n",
      "[[4203.  139.]\n",
      " [2365.  906.]]\n",
      "(Overall, took 1.667 sec)\n",
      "Accuracy: 67.11% +/- 3.16%\n",
      "Precision for positive class: 64.03% +/- 3.08%\n",
      "Precision for negative class: 86.72% +/- 2.67%\n",
      "Recall for positive class: 96.80% +/- 0.87%\n",
      "Recall for negative class: 27.79% +/- 5.13%\n",
      "F for positive class: 77.05% +/- 2.16%\n",
      "F for negative class: 41.96% +/- 6.02%\n",
      "Mean F score: 59.50% +/- 4.06%\n",
      "(Took 0.290 sec)\n",
      "(Took 0.310 sec)\n",
      "(Took 0.328 sec)\n",
      "(Took 0.304 sec)\n",
      "(Took 0.303 sec)\n",
      "Combined confusion matrix:\n",
      "[[3888.  454.]\n",
      " [1206. 2065.]]\n",
      "(Overall, took 1.675 sec)\n",
      "Accuracy: 78.20% +/- 1.14%\n",
      "Precision for positive class: 76.32% +/- 1.80%\n",
      "Precision for negative class: 81.97% +/- 1.15%\n",
      "Recall for positive class: 89.54% +/- 0.73%\n",
      "Recall for negative class: 63.14% +/- 1.70%\n",
      "F for positive class: 82.40% +/- 1.19%\n",
      "F for negative class: 71.33% +/- 1.17%\n",
      "Mean F score: 76.86% +/- 1.05%\n",
      "(Took 0.512 sec)\n",
      "(Took 0.528 sec)\n",
      "(Took 0.625 sec)\n",
      "(Took 0.612 sec)\n",
      "(Took 0.583 sec)\n",
      "Combined confusion matrix:\n",
      "[[3828.  514.]\n",
      " [1023. 2248.]]\n",
      "(Overall, took 3.065 sec)\n",
      "Accuracy: 79.81% +/- 1.02%\n",
      "Precision for positive class: 78.91% +/- 1.80%\n",
      "Precision for negative class: 81.39% +/- 2.25%\n",
      "Recall for positive class: 88.17% +/- 1.21%\n",
      "Recall for negative class: 68.74% +/- 2.01%\n",
      "F for positive class: 83.28% +/- 0.91%\n",
      "F for negative class: 74.52% +/- 1.38%\n",
      "Mean F score: 78.90% +/- 1.06%\n",
      "(Took 0.366 sec)\n",
      "(Took 0.414 sec)\n",
      "(Took 0.385 sec)\n",
      "(Took 0.383 sec)\n",
      "(Took 0.382 sec)\n",
      "Combined confusion matrix:\n",
      "[[3886.  456.]\n",
      " [1988. 1283.]]\n",
      "(Overall, took 2.153 sec)\n",
      "Accuracy: 67.90% +/- 1.68%\n",
      "Precision for positive class: 66.17% +/- 2.24%\n",
      "Precision for negative class: 73.82% +/- 2.67%\n",
      "Recall for positive class: 89.51% +/- 1.19%\n",
      "Recall for negative class: 39.27% +/- 2.23%\n",
      "F for positive class: 76.07% +/- 1.43%\n",
      "F for negative class: 51.23% +/- 1.88%\n",
      "Mean F score: 63.65% +/- 1.64%\n",
      "(Took 0.360 sec)\n",
      "(Took 0.399 sec)\n",
      "(Took 0.416 sec)\n",
      "(Took 0.389 sec)\n",
      "(Took 0.476 sec)\n",
      "Combined confusion matrix:\n",
      "[[4107.  235.]\n",
      " [1624. 1647.]]\n",
      "(Overall, took 2.223 sec)\n",
      "Accuracy: 75.58% +/- 1.24%\n",
      "Precision for positive class: 71.66% +/- 1.89%\n",
      "Precision for negative class: 87.53% +/- 2.29%\n",
      "Recall for positive class: 94.60% +/- 0.96%\n",
      "Recall for negative class: 50.38% +/- 1.67%\n",
      "F for positive class: 81.54% +/- 1.14%\n",
      "F for negative class: 63.93% +/- 1.27%\n",
      "Mean F score: 72.73% +/- 1.14%\n",
      "(Took 0.529 sec)\n",
      "(Took 0.503 sec)\n",
      "(Took 0.496 sec)\n",
      "(Took 0.482 sec)\n",
      "(Took 0.485 sec)\n",
      "Combined confusion matrix:\n",
      "[[3840.  502.]\n",
      " [1010. 2261.]]\n",
      "(Overall, took 2.669 sec)\n",
      "Accuracy: 80.14% +/- 0.60%\n",
      "Precision for positive class: 79.17% +/- 1.37%\n",
      "Precision for negative class: 81.83% +/- 1.42%\n",
      "Recall for positive class: 88.44% +/- 0.86%\n",
      "Recall for negative class: 69.13% +/- 0.99%\n",
      "F for positive class: 83.54% +/- 0.81%\n",
      "F for negative class: 74.94% +/- 0.57%\n",
      "Mean F score: 79.24% +/- 0.47%\n",
      "(Took 0.332 sec)\n",
      "(Took 0.366 sec)\n",
      "(Took 0.363 sec)\n",
      "(Took 0.358 sec)\n",
      "(Took 0.369 sec)\n",
      "Combined confusion matrix:\n",
      "[[4034.  308.]\n",
      " [2048. 1223.]]\n",
      "(Overall, took 1.986 sec)\n",
      "Accuracy: 69.05% +/- 2.15%\n",
      "Precision for positive class: 66.33% +/- 2.29%\n",
      "Precision for negative class: 79.92% +/- 4.51%\n",
      "Recall for positive class: 92.91% +/- 1.67%\n",
      "Recall for negative class: 37.42% +/- 2.80%\n",
      "F for positive class: 77.39% +/- 1.71%\n",
      "F for negative class: 50.94% +/- 3.00%\n",
      "Mean F score: 64.17% +/- 2.29%\n",
      "(Took 0.339 sec)\n",
      "(Took 0.360 sec)\n",
      "(Took 0.380 sec)\n",
      "(Took 0.365 sec)\n",
      "(Took 0.367 sec)\n",
      "Combined confusion matrix:\n",
      "[[4106.  236.]\n",
      " [1595. 1676.]]\n",
      "(Overall, took 1.985 sec)\n",
      "Accuracy: 75.95% +/- 0.62%\n",
      "Precision for positive class: 72.02% +/- 1.40%\n",
      "Precision for negative class: 87.65% +/- 1.82%\n",
      "Recall for positive class: 94.57% +/- 0.66%\n",
      "Recall for negative class: 51.24% +/- 0.61%\n",
      "F for positive class: 81.76% +/- 0.77%\n",
      "F for negative class: 64.67% +/- 0.48%\n",
      "Mean F score: 73.21% +/- 0.35%\n"
     ]
    }
   ],
   "source": [
    "# SVC cross-validation (C tuned to 0.031 in test run)\n",
    "all_metrics(cross_validate_SVC(dftrain_minimal_noprep, C=0.031))\n",
    "all_metrics(cross_validate_SVC(dftrain_min_norm_noprep, C=0.031))\n",
    "all_metrics(cross_validate_SVC(dftrain_min_tfidf_noprep, C=0.031))\n",
    "all_metrics(cross_validate_SVC(dftrain_minimal, C=0.031))\n",
    "all_metrics(cross_validate_SVC(dftrain_min_norm, C=0.031))\n",
    "all_metrics(cross_validate_SVC(dftrain_min_tfidf, C=0.031))\n",
    "all_metrics(cross_validate_SVC(dftrain_pos_minimal, C=0.031))\n",
    "all_metrics(cross_validate_SVC(dftrain_pos_min_norm, C=0.031))\n",
    "all_metrics(cross_validate_SVC(dftrain_pos_min_tfidf, C=0.031))\n",
    "all_metrics(cross_validate_SVC(dftrain_bi_minimal, C=0.031))\n",
    "all_metrics(cross_validate_SVC(dftrain_bi_min_norm, C=0.031))\n",
    "all_metrics(cross_validate_SVC(dftrain_bi_min_tfidf, C=0.031))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_SVC_predictions(C, train, test):\n",
    "    print('Training...')\n",
    "    svc = svm.LinearSVC(C=C)\n",
    "    svc.fit(train.drop('_target', axis=1), train['_target'])\n",
    "    print('Predicting...')\n",
    "    preds = svc.predict(test)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Predicting...\n"
     ]
    }
   ],
   "source": [
    "submission['target'] = make_SVC_predictions(0.031, dftrain_minimal_noprep, dftest_minimal_noprep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission_svm_noprep.csv', index=False)\n",
    "# Score: 79.345%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes (multinomial)\n",
    "def run_NB(dataset, split, smoothing):\n",
    "    t0 = time.time()\n",
    "    np.random.seed(1)\n",
    "    train, test = train_test_split(dataset, test_size=split)\n",
    "    train_x = train.drop('_target', axis=1)\n",
    "    train_y = train['_target']\n",
    "    test_x = test.drop('_target', axis=1)\n",
    "    test_y = test['_target']\n",
    "    print('Training...')\n",
    "    nb = naive_bayes.MultinomialNB(alpha=smoothing)\n",
    "    nb.fit(train_x, train_y)\n",
    "    print('Predicting...')\n",
    "    preds = nb.predict(test_x)\n",
    "    cm = confusion_matrix(test_y, preds)\n",
    "    print(cm)\n",
    "    tp, fn, fp, tn = cm.ravel()\n",
    "    accuracy = (tp+tn)/(tp+fn+fp+tn)\n",
    "    precision = tp/(tp+fp)\n",
    "    recall = tp/(tp+fn)\n",
    "    F = 2*precision*recall/(precision+recall)\n",
    "    print('Accuracy: {:.2f}%\\nPrecision: {:.2f}%\\nRecall: {:.2f}%\\nF: {:.2f}'.format(100*accuracy, 100*precision, 100*recall, 100*F))\n",
    "    t1 = time.time()\n",
    "    print('(Took {:.3f} sec)'.format(t1-t0))\n",
    "    return nb\n",
    "\n",
    "# 5-fold cross-validation, all metrics\n",
    "def cross_validate_NB(dataset, smoothing):\n",
    "    t_0 = time.time()\n",
    "    # Fixed at 5-fold cross-validation for now\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "    X = np.array(dataset.drop('_target', axis=1))\n",
    "    y = np.array(dataset['_target'])\n",
    "    accs = []\n",
    "    precs_p = []\n",
    "    precs_n = []\n",
    "    recs_p = []\n",
    "    recs_n = []\n",
    "    Fs_p = []\n",
    "    Fs_n = []\n",
    "    mFs = [] # Mean F score, this is the actual competition metric\n",
    "    cm = np.zeros((2,2))\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        t0 = time.time()\n",
    "        train_x, test_x = X[train_index], X[test_index]\n",
    "        train_y, test_y = y[train_index], y[test_index]\n",
    "        #print('Training...')\n",
    "        nb = naive_bayes.MultinomialNB(alpha=smoothing)\n",
    "        nb.fit(train_x, train_y)\n",
    "        #print('Predicting...')\n",
    "        preds = nb.predict(test_x)\n",
    "        cm_batch = confusion_matrix(test_y, preds)\n",
    "        cm += np.array(cm_batch)\n",
    "        tp, fn, fp, tn = cm_batch.ravel()\n",
    "        acc = (tp+tn)/(tp+fn+fp+tn)\n",
    "        accs.append( acc )\n",
    "        prec_p = tp/(tp+fp)\n",
    "        precs_p.append( prec_p )\n",
    "        prec_n = tn/(tn+fn)\n",
    "        precs_n.append( prec_n )\n",
    "        rec_p = tp/(tp+fn)\n",
    "        recs_p.append( rec_p )\n",
    "        rec_n = tn/(tn+fp)\n",
    "        recs_n.append( rec_n )\n",
    "        F_p = 2*prec_p*rec_p/(prec_p+rec_p)\n",
    "        Fs_p.append( F_p )\n",
    "        F_n = 2*prec_n*rec_n/(prec_n+rec_n)\n",
    "        Fs_n.append( F_n )\n",
    "        mF = (F_p + F_n)/2.0\n",
    "        mFs.append( mF )\n",
    "        t1 = time.time()\n",
    "        print('(Took {:.3f} sec)'.format(t1-t0))\n",
    "    t_1 = time.time()\n",
    "    print('Combined confusion matrix:')\n",
    "    print(cm)\n",
    "    print('(Overall, took {:.3f} sec)'.format(t_1-t_0))\n",
    "    return [accs, precs_p, precs_n, recs_p, recs_n, Fs_p, Fs_n, mFs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Took 0.866 sec)\n",
      "(Took 0.882 sec)\n",
      "(Took 0.966 sec)\n",
      "(Took 0.890 sec)\n",
      "(Took 0.990 sec)\n",
      "Combined confusion matrix:\n",
      "[[3769.  573.]\n",
      " [ 983. 2288.]]\n",
      "(Overall, took 4.976 sec)\n",
      "Accuracy: 79.56% +/- 0.70%\n",
      "Precision for positive class: 79.31% +/- 1.54%\n",
      "Precision for negative class: 79.98% +/- 1.22%\n",
      "Recall for positive class: 86.80% +/- 1.13%\n",
      "Recall for negative class: 69.96% +/- 1.36%\n",
      "F for positive class: 82.88% +/- 0.99%\n",
      "F for negative class: 74.62% +/- 0.38%\n",
      "Mean F score: 78.75% +/- 0.52%\n",
      "(Took 0.679 sec)\n",
      "(Took 0.756 sec)\n",
      "(Took 0.741 sec)\n",
      "(Took 0.732 sec)\n",
      "(Took 0.749 sec)\n",
      "Combined confusion matrix:\n",
      "[[4292.   50.]\n",
      " [2234. 1037.]]\n",
      "(Overall, took 4.072 sec)\n",
      "Accuracy: 70.00% +/- 1.64%\n",
      "Precision for positive class: 65.77% +/- 1.96%\n",
      "Precision for negative class: 95.44% +/- 1.83%\n",
      "Recall for positive class: 98.85% +/- 0.50%\n",
      "Recall for negative class: 31.73% +/- 1.41%\n",
      "F for positive class: 78.98% +/- 1.39%\n",
      "F for negative class: 47.61% +/- 1.48%\n",
      "Mean F score: 63.29% +/- 1.39%\n",
      "(Took 0.708 sec)\n",
      "(Took 0.769 sec)\n",
      "(Took 0.742 sec)\n",
      "(Took 0.777 sec)\n",
      "(Took 0.734 sec)\n",
      "Combined confusion matrix:\n",
      "[[4078.  264.]\n",
      " [1385. 1886.]]\n",
      "(Overall, took 4.152 sec)\n",
      "Accuracy: 78.34% +/- 0.78%\n",
      "Precision for positive class: 74.64% +/- 1.50%\n",
      "Precision for negative class: 87.74% +/- 2.42%\n",
      "Recall for positive class: 93.93% +/- 1.23%\n",
      "Recall for negative class: 57.67% +/- 0.75%\n",
      "F for positive class: 83.17% +/- 0.90%\n",
      "F for negative class: 69.58% +/- 0.31%\n",
      "Mean F score: 76.38% +/- 0.54%\n",
      "(Took 0.362 sec)\n",
      "(Took 0.376 sec)\n",
      "(Took 0.378 sec)\n",
      "(Took 0.378 sec)\n",
      "(Took 0.375 sec)\n",
      "Combined confusion matrix:\n",
      "[[3730.  612.]\n",
      " [ 966. 2305.]]\n",
      "(Overall, took 2.019 sec)\n",
      "Accuracy: 79.27% +/- 1.22%\n",
      "Precision for positive class: 79.41% +/- 1.73%\n",
      "Precision for negative class: 79.02% +/- 0.47%\n",
      "Recall for positive class: 85.89% +/- 1.12%\n",
      "Recall for negative class: 70.47% +/- 1.53%\n",
      "F for positive class: 82.52% +/- 1.42%\n",
      "F for negative class: 74.50% +/- 1.05%\n",
      "Mean F score: 78.51% +/- 1.09%\n",
      "(Took 0.322 sec)\n",
      "(Took 0.313 sec)\n",
      "(Took 0.307 sec)\n",
      "(Took 0.306 sec)\n",
      "(Took 0.309 sec)\n",
      "Combined confusion matrix:\n",
      "[[4014.  328.]\n",
      " [1409. 1862.]]\n",
      "(Overall, took 1.727 sec)\n",
      "Accuracy: 77.18% +/- 1.67%\n",
      "Precision for positive class: 74.02% +/- 2.31%\n",
      "Precision for negative class: 85.02% +/- 0.92%\n",
      "Recall for positive class: 92.44% +/- 0.49%\n",
      "Recall for negative class: 56.96% +/- 2.77%\n",
      "F for positive class: 82.20% +/- 1.47%\n",
      "F for negative class: 68.20% +/- 2.05%\n",
      "Mean F score: 75.20% +/- 1.68%\n",
      "(Took 0.289 sec)\n",
      "(Took 0.341 sec)\n",
      "(Took 0.324 sec)\n",
      "(Took 0.354 sec)\n",
      "(Took 0.312 sec)\n",
      "Combined confusion matrix:\n",
      "[[3687.  655.]\n",
      " [ 967. 2304.]]\n",
      "(Overall, took 1.782 sec)\n",
      "Accuracy: 78.69% +/- 0.77%\n",
      "Precision for positive class: 79.21% +/- 1.36%\n",
      "Precision for negative class: 77.87% +/- 1.32%\n",
      "Recall for positive class: 84.91% +/- 1.35%\n",
      "Recall for negative class: 70.44% +/- 1.24%\n",
      "F for positive class: 81.96% +/- 1.08%\n",
      "F for negative class: 73.96% +/- 0.70%\n",
      "Mean F score: 77.96% +/- 0.63%\n",
      "(Took 0.517 sec)\n",
      "(Took 0.534 sec)\n",
      "(Took 0.523 sec)\n",
      "(Took 0.516 sec)\n",
      "(Took 0.538 sec)\n",
      "Combined confusion matrix:\n",
      "[[3718.  624.]\n",
      " [ 996. 2275.]]\n",
      "(Overall, took 2.908 sec)\n",
      "Accuracy: 78.72% +/- 0.94%\n",
      "Precision for positive class: 78.87% +/- 2.01%\n",
      "Precision for negative class: 78.48% +/- 1.11%\n",
      "Recall for positive class: 85.63% +/- 0.89%\n",
      "Recall for negative class: 69.58% +/- 2.11%\n",
      "F for positive class: 82.10% +/- 1.06%\n",
      "F for negative class: 73.74% +/- 0.89%\n",
      "Mean F score: 77.92% +/- 0.85%\n",
      "(Took 0.412 sec)\n",
      "(Took 0.459 sec)\n",
      "(Took 0.429 sec)\n",
      "(Took 0.432 sec)\n",
      "(Took 0.434 sec)\n",
      "Combined confusion matrix:\n",
      "[[4273.   69.]\n",
      " [2137. 1134.]]\n",
      "(Overall, took 2.443 sec)\n",
      "Accuracy: 71.02% +/- 1.76%\n",
      "Precision for positive class: 66.66% +/- 2.05%\n",
      "Precision for negative class: 94.26% +/- 2.07%\n",
      "Recall for positive class: 98.41% +/- 0.56%\n",
      "Recall for negative class: 34.70% +/- 1.92%\n",
      "F for positive class: 79.48% +/- 1.44%\n",
      "F for negative class: 50.71% +/- 2.09%\n",
      "Mean F score: 65.09% +/- 1.71%\n",
      "(Took 0.386 sec)\n",
      "(Took 0.438 sec)\n",
      "(Took 0.414 sec)\n",
      "(Took 0.428 sec)\n",
      "(Took 0.415 sec)\n",
      "Combined confusion matrix:\n",
      "[[4057.  285.]\n",
      " [1360. 1911.]]\n",
      "(Overall, took 2.325 sec)\n",
      "Accuracy: 78.39% +/- 1.05%\n",
      "Precision for positive class: 74.89% +/- 1.78%\n",
      "Precision for negative class: 87.02% +/- 1.37%\n",
      "Recall for positive class: 93.44% +/- 0.59%\n",
      "Recall for negative class: 58.44% +/- 1.56%\n",
      "F for positive class: 83.14% +/- 1.05%\n",
      "F for negative class: 69.91% +/- 1.04%\n",
      "Mean F score: 76.52% +/- 0.95%\n",
      "(Took 0.451 sec)\n",
      "(Took 0.529 sec)\n",
      "(Took 0.482 sec)\n",
      "(Took 0.466 sec)\n",
      "(Took 0.483 sec)\n",
      "Combined confusion matrix:\n",
      "[[3754.  588.]\n",
      " [ 980. 2291.]]\n",
      "(Overall, took 2.625 sec)\n",
      "Accuracy: 79.40% +/- 0.90%\n",
      "Precision for positive class: 79.29% +/- 1.87%\n",
      "Precision for negative class: 79.58% +/- 0.88%\n",
      "Recall for positive class: 86.46% +/- 0.63%\n",
      "Recall for negative class: 70.06% +/- 1.51%\n",
      "F for positive class: 82.71% +/- 1.08%\n",
      "F for negative class: 74.51% +/- 0.56%\n",
      "Mean F score: 78.61% +/- 0.76%\n",
      "(Took 0.356 sec)\n",
      "(Took 0.406 sec)\n",
      "(Took 0.396 sec)\n",
      "(Took 0.373 sec)\n",
      "(Took 0.372 sec)\n",
      "Combined confusion matrix:\n",
      "[[4258.   84.]\n",
      " [2064. 1207.]]\n",
      "(Overall, took 2.135 sec)\n",
      "Accuracy: 71.78% +/- 1.67%\n",
      "Precision for positive class: 67.36% +/- 2.06%\n",
      "Precision for negative class: 93.55% +/- 2.52%\n",
      "Recall for positive class: 98.07% +/- 0.82%\n",
      "Recall for negative class: 36.93% +/- 1.89%\n",
      "F for positive class: 79.85% +/- 1.42%\n",
      "F for negative class: 52.93% +/- 1.77%\n",
      "Mean F score: 66.39% +/- 1.54%\n",
      "(Took 0.350 sec)\n",
      "(Took 0.384 sec)\n",
      "(Took 0.397 sec)\n",
      "(Took 0.399 sec)\n",
      "(Took 0.421 sec)\n",
      "Combined confusion matrix:\n",
      "[[4058.  284.]\n",
      " [1346. 1925.]]\n",
      "(Overall, took 2.160 sec)\n",
      "Accuracy: 78.59% +/- 1.14%\n",
      "Precision for positive class: 75.09% +/- 1.90%\n",
      "Precision for negative class: 87.14% +/- 1.07%\n",
      "Recall for positive class: 93.46% +/- 0.50%\n",
      "Recall for negative class: 58.88% +/- 1.45%\n",
      "F for positive class: 83.27% +/- 1.15%\n",
      "F for negative class: 70.26% +/- 0.85%\n",
      "Mean F score: 76.77% +/- 0.97%\n"
     ]
    }
   ],
   "source": [
    "# Tested various smoothing values, found best was 0.85\n",
    "all_metrics(cross_validate_NB(dftrain_minimal_noprep, smoothing=0.85))\n",
    "all_metrics(cross_validate_NB(dftrain_min_norm_noprep, smoothing=0.85))\n",
    "all_metrics(cross_validate_NB(dftrain_min_tfidf_noprep, smoothing=0.85))\n",
    "all_metrics(cross_validate_NB(dftrain_minimal, smoothing=0.85))\n",
    "all_metrics(cross_validate_NB(dftrain_min_norm, smoothing=0.85))\n",
    "all_metrics(cross_validate_NB(dftrain_min_tfidf, smoothing=0.85))\n",
    "all_metrics(cross_validate_NB(dftrain_pos_minimal, smoothing=0.85))\n",
    "all_metrics(cross_validate_NB(dftrain_pos_min_norm, smoothing=0.85))\n",
    "all_metrics(cross_validate_NB(dftrain_pos_min_tfidf, smoothing=0.85))\n",
    "all_metrics(cross_validate_NB(dftrain_bi_minimal, smoothing=0.85))\n",
    "all_metrics(cross_validate_NB(dftrain_bi_min_norm, smoothing=0.85))\n",
    "all_metrics(cross_validate_NB(dftrain_bi_min_tfidf, smoothing=0.85))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probably not worth submitting the NB model, since it did worse than some of the others.\n",
    "def make_NB_predictions(smoothing, train, test):\n",
    "    print('Training...')\n",
    "    nb = naive_bayes.MultinomialNB(alpha=smoothing)\n",
    "    nb.fit(train.drop('_target', axis=1), train['_target'])\n",
    "    print('Predicting...')\n",
    "    preds = nb.predict(test)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Predicting...\n"
     ]
    }
   ],
   "source": [
    "submission['target'] = make_NB_predictions(0.85, dftrain_minimal_noprep, dftest_minimal_noprep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission_nb.csv', index=False)\n",
    "# Score: 79.652%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding sentiment scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Add sentiment scores (found by a teammate, and included in these files)\n",
    "sent_train = pd.read_csv('sparse.csv')\n",
    "sent_test = pd.read_csv('Test Sentiment.csv')\n",
    "dftrain_minimal_noprep['_sentiment'] = sent_train['Sentiment_Score']\n",
    "dftest_minimal_noprep['_sentiment'] = sent_test['Sentiment_Score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/100\n",
      "6090/6090 [==============================] - 1s 178us/sample - loss: 0.6464 - val_loss: 0.6041\n",
      "Epoch 2/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.5755 - val_loss: 0.5599\n",
      "Epoch 3/100\n",
      "6090/6090 [==============================] - 1s 133us/sample - loss: 0.5312 - val_loss: 0.5316\n",
      "Epoch 4/100\n",
      "6090/6090 [==============================] - 1s 141us/sample - loss: 0.4992 - val_loss: 0.5119\n",
      "Epoch 5/100\n",
      "6090/6090 [==============================] - 1s 136us/sample - loss: 0.4739 - val_loss: 0.4980\n",
      "Epoch 6/100\n",
      "6090/6090 [==============================] - 1s 129us/sample - loss: 0.4535 - val_loss: 0.4868\n",
      "Epoch 7/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.4359 - val_loss: 0.4781\n",
      "Epoch 8/100\n",
      "6090/6090 [==============================] - 1s 128us/sample - loss: 0.4207 - val_loss: 0.4711\n",
      "Epoch 9/100\n",
      "6090/6090 [==============================] - 1s 128us/sample - loss: 0.4074 - val_loss: 0.4655\n",
      "Epoch 10/100\n",
      "6090/6090 [==============================] - 1s 128us/sample - loss: 0.3953 - val_loss: 0.4610\n",
      "Epoch 11/100\n",
      "6090/6090 [==============================] - 1s 128us/sample - loss: 0.3844 - val_loss: 0.4573\n",
      "Epoch 12/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.3746 - val_loss: 0.4540\n",
      "Epoch 13/100\n",
      "6090/6090 [==============================] - 1s 129us/sample - loss: 0.3653 - val_loss: 0.4517\n",
      "Epoch 14/100\n",
      "6090/6090 [==============================] - 1s 130us/sample - loss: 0.3569 - val_loss: 0.4493\n",
      "Epoch 15/100\n",
      "6090/6090 [==============================] - 1s 129us/sample - loss: 0.3491 - val_loss: 0.4476\n",
      "Epoch 16/100\n",
      "6090/6090 [==============================] - 1s 143us/sample - loss: 0.3417 - val_loss: 0.4462\n",
      "Epoch 17/100\n",
      "6090/6090 [==============================] - 1s 129us/sample - loss: 0.3348 - val_loss: 0.4450\n",
      "Epoch 18/100\n",
      "6090/6090 [==============================] - 1s 127us/sample - loss: 0.3284 - val_loss: 0.4441\n",
      "Epoch 19/100\n",
      "6090/6090 [==============================] - 1s 129us/sample - loss: 0.3222 - val_loss: 0.4434\n",
      "Epoch 20/100\n",
      "6090/6090 [==============================] - 1s 135us/sample - loss: 0.3163 - val_loss: 0.4430\n",
      "Epoch 21/100\n",
      "6090/6090 [==============================] - 1s 132us/sample - loss: 0.3108 - val_loss: 0.4427\n",
      "Epoch 22/100\n",
      "6090/6090 [==============================] - 1s 144us/sample - loss: 0.3055 - val_loss: 0.4424\n",
      "Epoch 23/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.3004 - val_loss: 0.4427\n",
      "Predicting...\n",
      "(Took 19.742 sec)\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/100\n",
      "6090/6090 [==============================] - 1s 172us/sample - loss: 0.6496 - val_loss: 0.6098\n",
      "Epoch 2/100\n",
      "6090/6090 [==============================] - 1s 136us/sample - loss: 0.5754 - val_loss: 0.5677\n",
      "Epoch 3/100\n",
      "6090/6090 [==============================] - 1s 136us/sample - loss: 0.5294 - val_loss: 0.5416\n",
      "Epoch 4/100\n",
      "6090/6090 [==============================] - 1s 136us/sample - loss: 0.4967 - val_loss: 0.5242\n",
      "Epoch 5/100\n",
      "6090/6090 [==============================] - 1s 146us/sample - loss: 0.4714 - val_loss: 0.5109\n",
      "Epoch 6/100\n",
      "6090/6090 [==============================] - 1s 135us/sample - loss: 0.4508 - val_loss: 0.5010\n",
      "Epoch 7/100\n",
      "6090/6090 [==============================] - 1s 129us/sample - loss: 0.4333 - val_loss: 0.4929\n",
      "Epoch 8/100\n",
      "6090/6090 [==============================] - 1s 130us/sample - loss: 0.4183 - val_loss: 0.4865\n",
      "Epoch 9/100\n",
      "6090/6090 [==============================] - 1s 129us/sample - loss: 0.4050 - val_loss: 0.4809\n",
      "Epoch 10/100\n",
      "6090/6090 [==============================] - 1s 128us/sample - loss: 0.3930 - val_loss: 0.4767\n",
      "Epoch 11/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.3825 - val_loss: 0.4729\n",
      "Epoch 12/100\n",
      "6090/6090 [==============================] - 1s 131us/sample - loss: 0.3726 - val_loss: 0.4700\n",
      "Epoch 13/100\n",
      "6090/6090 [==============================] - 1s 129us/sample - loss: 0.3635 - val_loss: 0.4674\n",
      "Epoch 14/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.3552 - val_loss: 0.4654\n",
      "Epoch 15/100\n",
      "6090/6090 [==============================] - 1s 136us/sample - loss: 0.3475 - val_loss: 0.4637\n",
      "Epoch 16/100\n",
      "6090/6090 [==============================] - 1s 136us/sample - loss: 0.3402 - val_loss: 0.4624\n",
      "Epoch 17/100\n",
      "6090/6090 [==============================] - 1s 146us/sample - loss: 0.3334 - val_loss: 0.4619\n",
      "Epoch 18/100\n",
      "6090/6090 [==============================] - 1s 147us/sample - loss: 0.3271 - val_loss: 0.4607\n",
      "Epoch 19/100\n",
      "6090/6090 [==============================] - 1s 144us/sample - loss: 0.3209 - val_loss: 0.4601\n",
      "Epoch 20/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.3151 - val_loss: 0.4597\n",
      "Epoch 21/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.3097 - val_loss: 0.4596\n",
      "Epoch 22/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.3044 - val_loss: 0.4598\n",
      "Predicting...\n",
      "(Took 19.225 sec)\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/100\n",
      "6090/6090 [==============================] - 1s 171us/sample - loss: 0.6473 - val_loss: 0.6114\n",
      "Epoch 2/100\n",
      "6090/6090 [==============================] - 1s 140us/sample - loss: 0.5747 - val_loss: 0.5683\n",
      "Epoch 3/100\n",
      "6090/6090 [==============================] - 1s 144us/sample - loss: 0.5296 - val_loss: 0.5409\n",
      "Epoch 4/100\n",
      "6090/6090 [==============================] - 1s 131us/sample - loss: 0.4972 - val_loss: 0.5221\n",
      "Epoch 5/100\n",
      "6090/6090 [==============================] - 1s 132us/sample - loss: 0.4722 - val_loss: 0.5089\n",
      "Epoch 6/100\n",
      "6090/6090 [==============================] - 1s 131us/sample - loss: 0.4518 - val_loss: 0.4983\n",
      "Epoch 7/100\n",
      "6090/6090 [==============================] - 1s 131us/sample - loss: 0.4345 - val_loss: 0.4899\n",
      "Epoch 8/100\n",
      "6090/6090 [==============================] - 1s 129us/sample - loss: 0.4194 - val_loss: 0.4830\n",
      "Epoch 9/100\n",
      "6090/6090 [==============================] - 1s 135us/sample - loss: 0.4062 - val_loss: 0.4774\n",
      "Epoch 10/100\n",
      "6090/6090 [==============================] - 1s 129us/sample - loss: 0.3942 - val_loss: 0.4733\n",
      "Epoch 11/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.3835 - val_loss: 0.4695\n",
      "Epoch 12/100\n",
      "6090/6090 [==============================] - 1s 131us/sample - loss: 0.3736 - val_loss: 0.4657\n",
      "Epoch 13/100\n",
      "6090/6090 [==============================] - 1s 133us/sample - loss: 0.3645 - val_loss: 0.4629\n",
      "Epoch 14/100\n",
      "6090/6090 [==============================] - 1s 129us/sample - loss: 0.3561 - val_loss: 0.4611\n",
      "Epoch 15/100\n",
      "6090/6090 [==============================] - 1s 128us/sample - loss: 0.3482 - val_loss: 0.4598\n",
      "Epoch 16/100\n",
      "6090/6090 [==============================] - 1s 128us/sample - loss: 0.3409 - val_loss: 0.4582\n",
      "Epoch 17/100\n",
      "6090/6090 [==============================] - 1s 129us/sample - loss: 0.3338 - val_loss: 0.4574\n",
      "Epoch 18/100\n",
      "6090/6090 [==============================] - 1s 129us/sample - loss: 0.3274 - val_loss: 0.4568\n",
      "Epoch 19/100\n",
      "6090/6090 [==============================] - 1s 129us/sample - loss: 0.3212 - val_loss: 0.4563\n",
      "Epoch 20/100\n",
      "6090/6090 [==============================] - 1s 135us/sample - loss: 0.3153 - val_loss: 0.4559\n",
      "Epoch 21/100\n",
      "6090/6090 [==============================] - 1s 132us/sample - loss: 0.3097 - val_loss: 0.4550\n",
      "Epoch 22/100\n",
      "6090/6090 [==============================] - 1s 132us/sample - loss: 0.3043 - val_loss: 0.4554\n",
      "Predicting...\n",
      "(Took 18.667 sec)\n",
      "Train on 6091 samples, validate on 1522 samples\n",
      "Epoch 1/100\n",
      "6091/6091 [==============================] - 1s 184us/sample - loss: 0.6472 - val_loss: 0.6089\n",
      "Epoch 2/100\n",
      "6091/6091 [==============================] - 1s 132us/sample - loss: 0.5754 - val_loss: 0.5648\n",
      "Epoch 3/100\n",
      "6091/6091 [==============================] - 1s 134us/sample - loss: 0.5308 - val_loss: 0.5371\n",
      "Epoch 4/100\n",
      "6091/6091 [==============================] - 1s 139us/sample - loss: 0.4989 - val_loss: 0.5176\n",
      "Epoch 5/100\n",
      "6091/6091 [==============================] - 1s 141us/sample - loss: 0.4739 - val_loss: 0.5026\n",
      "Epoch 6/100\n",
      "6091/6091 [==============================] - 1s 137us/sample - loss: 0.4534 - val_loss: 0.4916\n",
      "Epoch 7/100\n",
      "6091/6091 [==============================] - 1s 129us/sample - loss: 0.4359 - val_loss: 0.4821\n",
      "Epoch 8/100\n",
      "6091/6091 [==============================] - 1s 132us/sample - loss: 0.4207 - val_loss: 0.4749\n",
      "Epoch 9/100\n",
      "6091/6091 [==============================] - 1s 126us/sample - loss: 0.4073 - val_loss: 0.4686\n",
      "Epoch 10/100\n",
      "6091/6091 [==============================] - 1s 124us/sample - loss: 0.3952 - val_loss: 0.4639\n",
      "Epoch 11/100\n",
      "6091/6091 [==============================] - 1s 119us/sample - loss: 0.3843 - val_loss: 0.4597\n",
      "Epoch 12/100\n",
      "6091/6091 [==============================] - 1s 122us/sample - loss: 0.3742 - val_loss: 0.4563\n",
      "Epoch 13/100\n",
      "6091/6091 [==============================] - 1s 121us/sample - loss: 0.3650 - val_loss: 0.4531\n",
      "Epoch 14/100\n",
      "6091/6091 [==============================] - 1s 125us/sample - loss: 0.3566 - val_loss: 0.4509\n",
      "Epoch 15/100\n",
      "6091/6091 [==============================] - 1s 124us/sample - loss: 0.3486 - val_loss: 0.4489\n",
      "Epoch 16/100\n",
      "6091/6091 [==============================] - 1s 124us/sample - loss: 0.3412 - val_loss: 0.4473\n",
      "Epoch 17/100\n",
      "6091/6091 [==============================] - 1s 118us/sample - loss: 0.3342 - val_loss: 0.4461\n",
      "Epoch 18/100\n",
      "6091/6091 [==============================] - 1s 122us/sample - loss: 0.3276 - val_loss: 0.4450\n",
      "Epoch 19/100\n",
      "6091/6091 [==============================] - 1s 123us/sample - loss: 0.3213 - val_loss: 0.4443\n",
      "Epoch 20/100\n",
      "6091/6091 [==============================] - 1s 123us/sample - loss: 0.3154 - val_loss: 0.4440\n",
      "Epoch 21/100\n",
      "6091/6091 [==============================] - 1s 134us/sample - loss: 0.3099 - val_loss: 0.4433\n",
      "Epoch 22/100\n",
      "6091/6091 [==============================] - 1s 146us/sample - loss: 0.3044 - val_loss: 0.4433\n",
      "Epoch 23/100\n",
      "6091/6091 [==============================] - 1s 138us/sample - loss: 0.2993 - val_loss: 0.4433\n",
      "Predicting...\n",
      "(Took 19.136 sec)\n",
      "Train on 6091 samples, validate on 1522 samples\n",
      "Epoch 1/100\n",
      "6091/6091 [==============================] - 1s 177us/sample - loss: 0.6445 - val_loss: 0.6111\n",
      "Epoch 2/100\n",
      "6091/6091 [==============================] - 1s 137us/sample - loss: 0.5731 - val_loss: 0.5687\n",
      "Epoch 3/100\n",
      "6091/6091 [==============================] - 1s 130us/sample - loss: 0.5285 - val_loss: 0.5418\n",
      "Epoch 4/100\n",
      "6091/6091 [==============================] - 1s 128us/sample - loss: 0.4966 - val_loss: 0.5227\n",
      "Epoch 5/100\n",
      "6091/6091 [==============================] - 1s 129us/sample - loss: 0.4715 - val_loss: 0.5084\n",
      "Epoch 6/100\n",
      "6091/6091 [==============================] - 1s 135us/sample - loss: 0.4512 - val_loss: 0.4975\n",
      "Epoch 7/100\n",
      "6091/6091 [==============================] - 1s 137us/sample - loss: 0.4337 - val_loss: 0.4889\n",
      "Epoch 8/100\n",
      "6091/6091 [==============================] - 1s 137us/sample - loss: 0.4186 - val_loss: 0.4819\n",
      "Epoch 9/100\n",
      "6091/6091 [==============================] - 1s 137us/sample - loss: 0.4052 - val_loss: 0.4758\n",
      "Epoch 10/100\n",
      "6091/6091 [==============================] - 1s 140us/sample - loss: 0.3933 - val_loss: 0.4711\n",
      "Epoch 11/100\n",
      "6091/6091 [==============================] - 1s 145us/sample - loss: 0.3825 - val_loss: 0.4670\n",
      "Epoch 12/100\n",
      "6091/6091 [==============================] - 1s 139us/sample - loss: 0.3725 - val_loss: 0.4637\n",
      "Epoch 13/100\n",
      "6091/6091 [==============================] - 1s 139us/sample - loss: 0.3634 - val_loss: 0.4609\n",
      "Epoch 14/100\n",
      "6091/6091 [==============================] - 1s 137us/sample - loss: 0.3550 - val_loss: 0.4587\n",
      "Epoch 15/100\n",
      "6091/6091 [==============================] - 1s 140us/sample - loss: 0.3472 - val_loss: 0.4568\n",
      "Epoch 16/100\n",
      "6091/6091 [==============================] - 1s 140us/sample - loss: 0.3398 - val_loss: 0.4549\n",
      "Epoch 17/100\n",
      "6091/6091 [==============================] - 1s 144us/sample - loss: 0.3328 - val_loss: 0.4541\n",
      "Epoch 18/100\n",
      "6091/6091 [==============================] - 1s 135us/sample - loss: 0.3264 - val_loss: 0.4531\n",
      "Epoch 19/100\n",
      "6091/6091 [==============================] - 1s 132us/sample - loss: 0.3203 - val_loss: 0.4521\n",
      "Epoch 20/100\n",
      "6091/6091 [==============================] - 1s 133us/sample - loss: 0.3144 - val_loss: 0.4516\n",
      "Epoch 21/100\n",
      "6091/6091 [==============================] - 1s 131us/sample - loss: 0.3088 - val_loss: 0.4510\n",
      "Epoch 22/100\n",
      "6091/6091 [==============================] - 1s 132us/sample - loss: 0.3035 - val_loss: 0.4507\n",
      "Epoch 23/100\n",
      "6091/6091 [==============================] - 1s 132us/sample - loss: 0.2985 - val_loss: 0.4508\n",
      "Predicting...\n",
      "(Took 20.057 sec)\n",
      "Combined confusion matrix:\n",
      "[[3820.  522.]\n",
      " [ 958. 2313.]]\n",
      "(Overall, took 97.438 sec)\n",
      "Accuracy: 80.56% +/- 0.50%\n",
      "Precision for positive class: 79.95% +/- 1.22%\n",
      "Precision for negative class: 81.57% +/- 1.94%\n",
      "Recall for positive class: 87.99% +/- 0.84%\n",
      "Recall for negative class: 70.71% +/- 1.69%\n",
      "F for positive class: 83.77% +/- 0.34%\n",
      "F for negative class: 75.74% +/- 1.28%\n",
      "Mean F score: 79.76% +/- 0.68%\n"
     ]
    }
   ],
   "source": [
    "# Run just the best logistic model again\n",
    "all_metrics(cross_validate_logistic(dftrain_minimal_noprep, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Train on 7613 samples\n",
      "Epoch 1/23\n",
      "7613/7613 [==============================] - 1s 143us/sample - loss: 0.6346\n",
      "Epoch 2/23\n",
      "7613/7613 [==============================] - 1s 113us/sample - loss: 0.5579\n",
      "Epoch 3/23\n",
      "7613/7613 [==============================] - 1s 112us/sample - loss: 0.5135\n",
      "Epoch 4/23\n",
      "7613/7613 [==============================] - 1s 115us/sample - loss: 0.4823\n",
      "Epoch 5/23\n",
      "7613/7613 [==============================] - 1s 121us/sample - loss: 0.4582\n",
      "Epoch 6/23\n",
      "7613/7613 [==============================] - 1s 121us/sample - loss: 0.4386\n",
      "Epoch 7/23\n",
      "7613/7613 [==============================] - 1s 110us/sample - loss: 0.4221\n",
      "Epoch 8/23\n",
      "7613/7613 [==============================] - 1s 108us/sample - loss: 0.4078\n",
      "Epoch 9/23\n",
      "7613/7613 [==============================] - 1s 107us/sample - loss: 0.3951\n",
      "Epoch 10/23\n",
      "7613/7613 [==============================] - 1s 107us/sample - loss: 0.3839\n",
      "Epoch 11/23\n",
      "7613/7613 [==============================] - 1s 109us/sample - loss: 0.3738\n",
      "Epoch 12/23\n",
      "7613/7613 [==============================] - 1s 109us/sample - loss: 0.3646\n",
      "Epoch 13/23\n",
      "7613/7613 [==============================] - 1s 108us/sample - loss: 0.3561\n",
      "Epoch 14/23\n",
      "7613/7613 [==============================] - 1s 120us/sample - loss: 0.3483\n",
      "Epoch 15/23\n",
      "7613/7613 [==============================] - 1s 121us/sample - loss: 0.3410\n",
      "Epoch 16/23\n",
      "7613/7613 [==============================] - 1s 111us/sample - loss: 0.3342\n",
      "Epoch 17/23\n",
      "7613/7613 [==============================] - 1s 106us/sample - loss: 0.3278\n",
      "Epoch 18/23\n",
      "7613/7613 [==============================] - 1s 110us/sample - loss: 0.3218\n",
      "Epoch 19/23\n",
      "7613/7613 [==============================] - 1s 108us/sample - loss: 0.3160\n",
      "Epoch 20/23\n",
      "7613/7613 [==============================] - 1s 108us/sample - loss: 0.3107\n",
      "Epoch 21/23\n",
      "7613/7613 [==============================] - 1s 110us/sample - loss: 0.3055\n",
      "Epoch 22/23\n",
      "7613/7613 [==============================] - 1s 109us/sample - loss: 0.3007\n",
      "Epoch 23/23\n",
      "7613/7613 [==============================] - 1s 114us/sample - loss: 0.2961\n",
      "Predicting...\n"
     ]
    }
   ],
   "source": [
    "submission['target'] = make_logistic_predictions(23, 32, dftrain_minimal_noprep, dftest_minimal_noprep)\n",
    "submission.to_csv('submission_lg_sent.csv', index=False)\n",
    "# Score: 80.061%\n",
    "# Seems that adding sentiment did not help. I don't think I'm going to add it to the rest of the models, either.\n",
    "# Kaggle is starting to slow down my submission evaluations because I have submitted too many times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the best models were trained on the full train data, and used to make predictions which were submitted to Kaggle. The best model submission ended up being the simple logistic regression model with no preprocessing (other than removing words that only occur in one document and that don't occur in both the train and test data). The final F score was 0.8098. This is significantly lower than the cross-validated result, and that indicates that the testing data is somewhat different in distribution than the training data. This can be expected, given the amount of variation there is in natural language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7613, 2595)"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dftrain_minimal.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3263, 2594)"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dftest_minimal.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "Birmingham\n",
      "Est. September 2012 - Bristol\n",
      "AFRICA\n",
      "Philadelphia, PA\n",
      "London, UK\n",
      "Pretoria\n",
      "World Wide!!\n",
      "nan\n",
      "Paranaque City\n",
      "Live On Webcam\n",
      "nan\n",
      "milky way\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "GREENSBORO,NORTH CAROLINA\n",
      "nan\n",
      "Live On Webcam\n",
      "England.\n",
      "Sheffield Township, Ohio\n",
      "India\n",
      "Barbados\n",
      "Anaheim\n",
      "Abuja\n",
      "USA\n",
      "South Africa\n",
      "Sao Paulo, Brazil\n",
      "hollywoodland \n",
      "Edmonton, Alberta - Treaty 6\n",
      "nan\n",
      "Inang Pamantasan\n",
      "Twitter Lockout in progress\n",
      "Concord, CA\n",
      "Calgary, AB\n",
      "Birmingham\n",
      "San Francisco\n",
      "CLVLND\n",
      "Nashville, TN\n",
      "Santa Clara, CA\n",
      "UK\n",
      "St. Louis, MO\n",
      "Walker County, Alabama\n",
      "Australia\n",
      "North Carolina\n",
      "nan\n",
      "Norf Carolina\n",
      "San Mateo County, CA\n",
      "North Carolina\n",
      "Njoro, Kenya\n",
      "nan\n",
      "Your Sister's Bedroom\n",
      "nan\n",
      "Arlington, TX\n",
      "South Bloomfield, OH\n",
      "nan\n",
      "New Hanover County, NC\n",
      "Maldives\n",
      "Manchester, NH\n",
      "Wilmington, NC\n",
      "nan\n",
      "New Hanover County, NC\n",
      "nan\n",
      "global\n",
      "Alberta | Sask. | Montana\n",
      "Charlotte\n",
      "Baton Rouge, LA\n",
      "Hagerstown, MD\n",
      "Gloucestershire , UK\n",
      "nan\n",
      "UK\n",
      "Nairobi, Kenya\n",
      "Instagram - @heyimginog \n",
      "304\n",
      "Switzerland\n",
      "304\n",
      "US\n",
      "304\n",
      "Instagram - @heyimginog \n",
      "304\n",
      "nan\n",
      "Somewhere Only We Know ?\n",
      "nan\n",
      "Belgium\n",
      "Switzerland\n",
      "US\n",
      "nan\n",
      "dope show\n",
      "Switzerland\n",
      "Switzerland\n",
      "Oshawa, Canada\n",
      "Baker City Oregon\n",
      "nan\n",
      "nan\n",
      "United States\n",
      "304\n",
      "304\n",
      "marysville ca \n",
      "304\n",
      "Hermosa Beach, CA\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "304\n",
      "304\n",
      "nan\n",
      "19.600858, -99.047821\n",
      "Pennsylvania\n",
      "Salt Lake City, Utah\n",
      "Palo Alto, CA\n",
      "nan\n",
      "Spain but Opa-Locka, FL\n",
      "Jaipur, India\n",
      "Hyderabad Telangana INDIA\n",
      "Eagle Pass, Texas\n",
      "bangalore\n",
      "Financial News and Views\n",
      "nan\n",
      "Indonesia\n",
      "y(our) boyfriends legs \n",
      "New Mexico, USA\n",
      "Somewhere Out There\n",
      "nan\n",
      "Mumbai india\n",
      "sri lanka\n",
      "Not a U.S resident\n",
      "nan\n",
      "Lehigh Valley, PA\n",
      "Canada\n",
      "nan\n",
      "Thrissur\n",
      "Havenford\n",
      "India\n",
      "92\n",
      "nan\n",
      "Israel\n",
      "Fashion Heaven. IG: TMId_\n",
      "San Francisco, CA\n",
      "italy\n",
      "nyc\n",
      "Toronto\n",
      "nan\n",
      "Jackson\n",
      "New York / Worldwide\n",
      "nan\n",
      "New Orleans, LA\n",
      "West Wales\n",
      "nan\n",
      "Happily Married with 2 kids \n",
      "Cambridge, MA\n",
      "Arizona \n",
      "Mumbai\n",
      "nan\n",
      "Amsterdam\n",
      "Swindon,England \n",
      "nan\n",
      "Happily Married with 2 kids \n",
      "nan\n",
      "Williamstown, VT\n",
      "North Carolina, USA\n",
      "nan\n",
      "Karachi\n",
      "Happily Married with 2 kids \n",
      "Happily Married with 2 kids \n",
      "Loveland Colorado\n",
      "|| c h i c a g o ||\n",
      "nan\n",
      "L. A.\n",
      "nan\n",
      "Canada\n",
      "VISIT MY YOUTUBE CHANNEL.\n",
      "Lexington\n",
      "nan\n",
      "USA\n",
      "Hannover, Germany\n",
      "nan\n",
      "nan\n",
      "Playa\n",
      "Davidson, NC\n",
      "Higher Places\n",
      "Horsemind, MI\n",
      "New York, NY\n",
      "Boksburg\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "V-RP @OZRP_ ?MV, AU, R18+?\n",
      "Greater Manchester, UK\n",
      "Boston\n",
      "nan\n",
      "nan\n",
      "The Canopy Kingdom\n",
      "nan\n",
      "USA\n",
      "nan\n",
      "nan\n",
      "the own zone layer \n",
      "London\n",
      "Trancy Manor\n",
      "nan\n",
      "South 37\n",
      "nan\n",
      "West Lancashire, UK.\n",
      "PA\n",
      "nan\n",
      "ยรยข Views From The Six ยรยข\n",
      "nan\n",
      "University of Toronto\n",
      "nan\n",
      "Swaning Around\n",
      "nan\n",
      "London\n",
      "Albany/NY\n",
      "California, USA\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "Wild Wild Web\n",
      "Subconscious LA\n",
      "Spain\n",
      "nan\n",
      "CA physically- Boston Strong?\n",
      "Yeezy Taught Me , NV\n",
      "nan\n",
      "CA physically- Boston Strong?\n",
      "United States\n",
      "nan\n",
      "nan\n",
      "Rock Hill, SC\n",
      "Coolidge, AZ\n",
      "Republic of Texas\n",
      "nan\n",
      "nan\n",
      "Phoenix, AZ\n",
      "Ljubljana, Slovenia\n",
      "Connecticut\n",
      "Tacoma,Washington\n",
      "nan\n",
      "nan\n",
      "Subconscious LA\n",
      "BIG D  HOUSTON/BOSTON/DENVER\n",
      "Chandler, AZ\n",
      "ColoRADo\n",
      "sindria\n",
      "nan\n",
      "nan\n",
      "Texas\n",
      "Elk Grove, CA, USA\n",
      "Texas\n",
      "nan\n",
      "The Shire\n",
      "Austin, TX\n",
      "Oakland\n",
      "nan\n",
      "nan\n",
      "Albuquerque\n",
      "nan\n",
      "Buenos Aires, Argentina\n",
      "nan\n",
      "nan\n",
      "San Antonio-ish, TX\n",
      "San Francisco\n",
      "Oregon, USA\n",
      "Harlingen, TX\n",
      "Buffalo NY\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "Las Vegas\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "Tokyo\n",
      "nan\n",
      "California, United States\n",
      "California, United States\n",
      "#FLIGHTCITY UK  \n",
      "nan\n",
      "nan\n",
      "Alphen aan den Rijn, Holland\n",
      "nan\n",
      "Wrigley Field\n",
      "nan\n",
      "probably the strip club\n",
      "Canada\n",
      "England\n",
      "USA\n",
      "California, United States\n",
      "nan\n",
      "California, United States\n",
      "New York City\n",
      "Here And There\n",
      "Rotterdam, Zuid-Holland\n",
      "Derry, 17 \n",
      "Nowhere. Everywhere.\n",
      "Florida, USA\n",
      "Worldwide\n",
      "nan\n",
      "Nowhere. Everywhere.\n",
      "East Coast\n",
      "California\n",
      "Toronto, ON\n",
      "nan\n",
      "The Orwellion police-state\n",
      "Castaic, CA\n",
      "Helsinki, Finland\n",
      "East Kilbride\n",
      "nan\n",
      "#FLIGHTCITY UK  \n",
      "middle eastern palace\n",
      "Kent\n",
      "nan\n",
      "nan\n",
      "Nowhere. Everywhere.\n",
      "#FLIGHTCITY UK  \n",
      "Perthshire \n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "twitch.tv/naturalemblem26\n",
      "cyprus\n",
      "Memphis, TN\n",
      "nan\n",
      "Studio\n",
      "Hollywood, CA\n",
      "nan\n",
      "New York\n",
      "nan\n",
      "nan\n",
      "Pakistan\n",
      "Mexico! ^_^\n",
      "nan\n",
      "nan\n",
      "Campinas Sp\n",
      "nan\n",
      "Harlem, New York\n",
      "New York\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "Washington, DC\n",
      "Burbank,CA\n",
      "New York\n",
      "nan\n",
      "nan\n",
      "? \n",
      "nan\n",
      "Spokane, Washington\n",
      "USA\n",
      "Charlotte, NC\n",
      "Our Empire State\n",
      "nan\n",
      "nan\n",
      "Jerusalem\n",
      "Kingston, Pennsylvania\n",
      "Milwaukee, WI\n",
      "Zero Branco\n",
      "Republic of Texas\n",
      "nan\n",
      "bajaur\n",
      "USA\n",
      "Eldoret, kenya\n",
      "ยรรยรรยรร\n",
      "Jerusalem\n",
      "Miami,FL\n",
      "nan\n",
      "Los Angeles, CA\n",
      "North-East Region, Singapore\n",
      "Eldoret, kenya\n",
      "nan\n",
      "Chicago\n",
      "EARTH \n",
      "Jerusalem, Israel\n",
      "nan\n",
      "Menasha, WI\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "ss\n",
      "Atlanta\n",
      "Earth\n",
      "ss\n",
      "Worldwide\n",
      "Bleak House\n",
      "heccfidmss@gmail.com\n",
      "ss\n",
      "toronto\n",
      "[ Blonde Bi Fry. ]\n",
      "America\n",
      "NYC :) Ex- #Islamophobe\n",
      "SF Bay Area\n",
      "Orange County, California\n",
      "ss\n",
      "Winston Salem, North Carolina\n",
      "ss\n",
      "nan\n",
      "Adelaide, South Australia\n",
      "ss\n",
      "WASHINGTON,DC\n",
      "nan\n",
      "California\n",
      "ss\n",
      " snapchat // fvck_casper \n",
      "New York\n",
      "nan\n",
      "ss\n",
      "ss\n",
      "ss\n",
      "nan\n",
      "United States\n",
      "nan\n",
      "nan\n",
      "Dallas, TX\n",
      "NYC\n",
      "nan\n",
      "Location\n",
      "Selena | Britney | Hilary\n",
      "nan\n",
      "nan\n",
      "Ireland\n",
      "Freeport IL. USA\n",
      "nan\n",
      "Dubai\n",
      "Tucson, Az\n",
      "India\n",
      "Seattle WA\n",
      "nan\n",
      "Bellevue NE\n",
      "West Bank, Gaza Strip\n",
      "ยรยขFLGยรยข\n",
      "nan\n",
      "Scotland, United Kingdom\n",
      "nan\n",
      "Online 24/7. Not even kidding.\n",
      "nan\n",
      "rowyso dallas \n",
      "Halton, Ontario\n",
      "Mumbai\n",
      "portland, oregon\n",
      "FIMAK A.S Ist Bolge Muduru\n",
      "nan\n",
      "London.\n",
      "#UNITE THE BLUE  \n",
      "nan\n",
      "Dayton, Ohio\n",
      "nan\n",
      "Global\n",
      "ph\n",
      "Port Jervis, NY\n",
      "City Of Joy\n",
      "Los Angeles, CA\n",
      "Texas, USA\n",
      "1/3 of the blam squad \n",
      "CCH \n",
      "atx\n",
      "MAURITIUS\n",
      "AKRON OHIO USA\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "London\n",
      "london / st catharines ?\n",
      "nan\n",
      "Peshawar\n",
      "nan\n",
      "LEALMAN, FLORIDA\n",
      "Los Angeles, CA\n",
      "San Francisco, CA\n",
      "#GDJB #ASOT\n",
      "Groningen, Netherlands, Europe\n",
      "Livingston, IL  U.S.A.\n",
      "Arundel \n",
      "nan\n",
      "America\n",
      "Anna Maria, FL\n",
      "USA\n",
      "israel\n",
      "The Hammock, FL, USA\n",
      "??????????????????\n",
      "Sรยฃo Paulo SP,  Brasil\n",
      "in Dimitri's arms\n",
      "Oslo, Norway\n",
      "Los Angeles\n",
      "Loughton, Essex, UK\n",
      "guaravitas\n",
      "nan\n",
      "Score More Goals Buying @\n",
      "NEW YORK\n",
      "Ireland\n",
      "nan\n",
      "UK\n",
      "London, Kent & SE England.\n",
      "Score Team Goals Buying @\n",
      "nan\n",
      "New York, NY\n",
      "South Central Wales\n",
      "nan\n",
      "Jersey City, New Jersey\n",
      "nan\n",
      "nan\n",
      "Denver, CO\n",
      "Brasil\n",
      "Buy Give Me My Money \n",
      "505 W. Maple, Suite 100\n",
      "Buy Give Me My Money \n",
      "Buy Give Me My Money \n",
      "Philippines\n",
      "Freeport il \n",
      "Canada\n",
      "World\n",
      "nan\n",
      "Danville, VA\n",
      "New York\n",
      "nan\n",
      "nan\n",
      "UK Great Britain \n",
      "NYC\n",
      "Jerusalem!\n",
      "nan\n",
      "nan\n",
      "San Jose, CA\n",
      "USA\n",
      "San Francisco\n",
      "nan\n",
      "Dallas, TX\n",
      "Baton Rouge, LA\n",
      "Use #TMW in tweets get #RT\n",
      "Earth\n",
      "nan\n",
      "nan\n",
      "Utah\n",
      "Wisconsin\n",
      "West Richland, WA\n",
      "CHICAGO (312)\n",
      "New York\n",
      "USA\n",
      "Australia\n",
      "California\n",
      "Washington D.C.\n",
      "NC\n",
      "nan\n",
      "West Virginia, USA\n",
      "nan\n",
      "British girl in Texas\n",
      "Silver Spring, MD\n",
      "USA\n",
      "Phoenix, AZ\n",
      "Atlanta, GA\n",
      "Manhattan, NY\n",
      "Wilmington, DE\n",
      "nan\n",
      "Memphis\n",
      "iTunes\n",
      "nan\n",
      "United States\n",
      "Oxford, MS\n",
      "nan\n",
      "US\n",
      "Atlanta, GA\n",
      "nan\n",
      "Pelham, AL\n",
      "New York\n",
      "Atlanta, GA\n",
      "Jacksonville, FL\n",
      "Arkansas, Jonesboro\n",
      "Across the Atlantic\n",
      "Melbourne, Florida\n",
      "Worldwide\n",
      "nan\n",
      "Over the Moon...\n",
      "Extraterrestrial Highway\n",
      "nan\n",
      "Espoo, Finland\n",
      "nan\n",
      "Washington, D.C., area\n",
      "nan\n",
      "OES 4th Point. sisSTAR & TI\n",
      "Sydney, New South Wales\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "San Francisco, CA\n",
      "nan\n",
      "nan\n",
      "Netherlands,Amsterdam-Virtual \n",
      "nan\n",
      "New York City\n",
      "Hudson Valley, NY\n",
      "Philadelphia, PA\n",
      "nan\n",
      "nan\n",
      "timeline kamu\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "Budapest, Hungary\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "Searching for Bae \n",
      "Sydney, New South Wales\n",
      "California\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "Eagle Mountain, Texas \n",
      "Columbus\n",
      "nan\n",
      "Temecula, CA\n",
      "Atlanta,Ga\n",
      "Fresno, CA\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "Australia\n",
      "Raleigh Durham, NC\n",
      "ducked off . . . \n",
      "nan\n",
      "Rio de Janeiro\n",
      "nan\n",
      "302\n",
      "Columbus, OH\n",
      "PA\n",
      "nan\n",
      "Mo.City\n",
      "Tripsburg, ms.\n",
      "Durham N.C \n",
      "Delhi\n",
      "ARIZONA\n",
      "Penn Hills, PA\n",
      "Karachi \n",
      "seattle wa\n",
      "nan\n",
      "Mo.City\n",
      "nan\n",
      "SOUTHERN CALIFORNIA DESERT\n",
      "Gotham City\n",
      "nan\n",
      "My contac 27B80F7E 08170156520\n",
      "Dallas, TX\n",
      "nan\n",
      "Pig Symbol, Alabama\n",
      "Intramuros, Manila\n",
      "nan\n",
      "New York\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "THE WORLD T.G.G / M.M.M \n",
      "nan\n",
      "Between the worlds \n",
      "State of Georgia\n",
      "nan\n",
      "Your screen\n",
      "Lima-Peru\n",
      "nan\n",
      "nan\n",
      "worldwide\n",
      "nan\n",
      "nan\n",
      "Saltillo, Coahuila de Zaragoza\n",
      "nan\n",
      "nan\n",
      "Suitland\n",
      "Everywhere\n",
      "Konoha\n",
      "Swag Francisco\n",
      "Saint Marys, GA\n",
      "New York\n",
      "Nigeria\n",
      "Essex/Brighton\n",
      "nan\n",
      "Pennsylvania, PA\n",
      "nan\n",
      "Rockford, IL\n",
      "nan\n",
      "AZ\n",
      "nan\n",
      "nan\n",
      "Vero Beach , FL\n",
      "IN\n",
      "nan\n",
      "In the middle of no where\n",
      "Baltimore, MD\n",
      "Florida, USA\n",
      "dmv ?? fashion school @ KSU. \n",
      "Nice places \n",
      "nan\n",
      "Quantico, VA\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "Island Lake, IL\n",
      "Live Oak, TX\n",
      "nan\n",
      "Gages Lake, IL\n",
      "nan\n",
      "Madisonville TN\n",
      "Basketball City, USA \n",
      "AEP\n",
      "nan\n",
      "Alberta Pack\n",
      "#expelcl*y\n",
      "My heart is a ghost town!\n",
      "The Great State of Texas\n",
      "nan\n",
      "Alicante, Valencia\n",
      "nan\n",
      "Atlanta\n",
      "nan\n",
      "Greensboro, NC\n",
      " Indiana\n",
      "the local dump\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "Seattle\n",
      "#SOUTHAMPTON ENGLAND\n",
      "?205?478?\n",
      "Waterford MI\n",
      "Iowa, USA\n",
      "va\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "Florida\n",
      "nan\n",
      "california mermaid ? \n",
      "New York ? ATL\n",
      "USA/SO FLORIDA via BROOKLYN NY\n",
      "nan\n",
      "H / pez & sophia \n",
      "Brooklyn, NY\n",
      "Queens, NY\n",
      "Vancouver, BC.\n",
      "nan\n",
      "nan\n",
      "Ottawa, Canada\n",
      "wherever the $$$ at\n",
      "Purgatory, USA\n",
      "Calgary, Alberta\n",
      "Kama | 18 | France \n",
      "Sydney\n",
      "nan\n",
      "London\n",
      "Maryland, USA\n",
      "Daruka (near Tamworth) NSW\n",
      "IJmuiden, The Netherlands\n",
      "University Heights, Ohio\n",
      "Central Illinois\n",
      "Baton Rouge\n",
      "nan\n",
      "USA\n",
      "PSN: Pipbois \n",
      "Colombo,Sri Lanka.\n",
      "nan\n",
      "Johannesburg, South Africa \n",
      "Me mammy's belly\n",
      "UK & Ibiza\n",
      "nan\n",
      "Laventillemoorings \n",
      "UK\n",
      "Scotland\n",
      "London\n",
      "Vancouver, BC\n",
      "Cleveland, OH\n",
      "Kama | 18 | France \n",
      "Kama | 18 | France \n",
      "UK\n",
      "Detroit, MI, United States\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "Guelph Ontario Canada\n",
      "Waterfront\n",
      "columbus ohio\n",
      "Canada\n",
      "Waukesha, WI\n",
      "Sydney\n",
      "nan\n",
      "Himalayan Mountains\n",
      "nan\n",
      "Colorado/WorldWide\n",
      "nan\n",
      "Ontario Canada\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "California, USA\n",
      "nan\n",
      "nan\n",
      "Houston,  TX\n",
      "lakewood colorado\n",
      "THE 6IX\n",
      "Washington, USA\n",
      "The ?? below ???\n",
      "United States\n",
      "nan\n",
      "nan\n",
      "Ontario Canada\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "Ideally under a big tree\n",
      "United States\n",
      "nan\n",
      "nan\n",
      "Conversing In Janet's Cafรยฌ\n",
      "nan\n",
      "nan\n",
      "???????, ??'??????\n",
      "nan\n",
      "Dime's Palace\n",
      "Cairo, Egypt\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "Seattle, WA\n",
      "nan\n",
      "Bug Forest\n",
      "nan\n",
      "canberra\n",
      "nan\n",
      "International\n",
      "nan\n",
      "The World\n",
      "Htx\n",
      "nan\n",
      "nan\n",
      "PunPunlรยขndia\n",
      "???\n",
      "Itirapina, Sรยฃo Paulo\n",
      "North Jersey\n",
      "Ewa Beach, HI\n",
      "Biloxi, Mississippi\n",
      "nan\n",
      "Buenos Aires\n",
      "Indonesia\n",
      "Brecksville, OH\n",
      "Walthamstow, London\n",
      "nan\n",
      "Malaysia\n",
      "Ivano-Frankivsk\n",
      "Sunshine Coast, Queensland\n",
      "Dime's Palace\n",
      "England,UK,Europe,Sol 3.\n",
      "nan\n",
      "Chicago\n",
      "nan\n",
      "Nashua NH\n",
      "Leicester\n",
      "65\n",
      "Westchester\n",
      "Lynnfield, MA\n",
      "Level 3 Garrison, Sector G\n",
      "Singapore\n",
      "Adelaide, South Australia\n",
      "nan\n",
      "Las Vegas\n",
      "Glasgow\n",
      "nan\n",
      "Shity land of Northern Ireland\n",
      "Singapore\n",
      "Christchurch New Zealand\n",
      "peekskill. new york, 10566 \n",
      "nan\n",
      "nan\n",
      "PH\n",
      "Storybrooke \n",
      "under the blanket\n",
      "Isolated City In World Perth\n",
      "Brazil \n",
      "AUS\n",
      "Santa Cruz, CA\n",
      "Inverness, Nova Scotia\n",
      "L/S/Z/L/T/H/C/H/R/A/S/C\n",
      "Oklahoma\n",
      "801 SL,UT\n",
      "nan\n",
      "fluffy cloud\n",
      "USA\n",
      "The 5th Dimension. \n",
      "Georgia\n",
      "July 11th, 2015. ?\n",
      "The Grey Area\n",
      "nan\n",
      "St Paul, MN\n",
      "Cobblestone\n",
      "รรT: 30.307558,-81.403118\n",
      "Scotland\n",
      "Cosmic Oneness\n",
      "Guildford, UK\n",
      "Nowhere Islands/Smash Manor\n",
      "kisumu\n",
      "Los Angeles\n",
      "Making Worldwide Change Near U\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "ATX\n",
      "LA/OC/Vegas\n",
      "London/Outlaw Country \n",
      "Grimsby, England\n",
      "nan\n",
      "Gotham\n",
      "nan\n",
      "Manchester, The World, England\n",
      "New York\n",
      "New York\n",
      "nan\n",
      "sitting on Eddie Vedders lap,\n",
      "nan\n",
      "New York\n",
      "New York\n",
      "NYC\n",
      "Missouri, USA\n",
      "US\n",
      "nan\n",
      "nan\n",
      "California, USA\n",
      "Greenville,SC\n",
      "New York\n",
      "New York\n",
      "nan\n",
      "Paignton\n",
      "nan\n",
      "California, USA\n",
      "New York\n",
      "nan\n",
      "Paignton\n",
      "nan\n",
      "nan\n",
      "Paignton\n",
      "nan\n",
      "New York\n",
      "New York\n",
      "Paignton\n",
      "nan\n",
      "New York\n",
      "nan\n",
      "have car; will travel\n",
      "Sydney, New South Wales\n",
      "ATL ? SEA \n",
      "รรT: 39.982988,-75.261624\n",
      "nan\n",
      "#EngleWood CHICAGO \n",
      "302???? 815\n",
      "New Your\n",
      "Texas\n",
      "New York, NY\n",
      "nan\n",
      "PURPLE BOOTH STUDIOยรฃยข\n",
      "Cloud 9\n",
      "nan\n",
      "Former Yugoslav Republic of Macedonia\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "3?3?7?SLOPelousas??2?2?5?\n",
      "nan\n",
      "316\n",
      "Global\n",
      "#WhereverI'mAt\n",
      "Huber Heights, OH\n",
      "nan\n",
      "Miami ??\n",
      "Houston, TX\n",
      "Every where\n",
      "Arizona \n",
      "nan\n",
      "401 livin'\n",
      "MI\n",
      "EPTX\n",
      "Austin, Texas\n",
      "Oklahoma City\n",
      "CA\n",
      "nan\n",
      "United Kingdom\n",
      "WESTSIDE OF PHILLY 7? BLOCK??\n",
      "CA\n",
      "Wisconsin\n",
      "Charlotte NC\n",
      "LONG ISLAND, NY\n",
      "nan\n",
      "washington, d.c.\n",
      "United Kingdom\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "WAISTDEEP, TX\n",
      "Swaning Around\n",
      "D.C. - Baltimore - Annapolis\n",
      "nan\n",
      "#937??#734\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "Fife, WA\n",
      "nan\n",
      "nan\n",
      "Bushkill pa\n",
      "nan\n",
      "In the Shadows...\n",
      "southwest, Tx\n",
      "nan\n",
      "California, USA\n",
      "nan\n",
      "nan\n",
      "ANYWEHERE !!\n",
      "Menlo Park. SFO. The World.\n",
      "nan\n",
      "Speaking the Truth in Love\n",
      "nan\n",
      "Aarhus, Central Jutland\n",
      "nan\n",
      "Lincoln, NE\n",
      "wny\n",
      "Bolton & Tewkesbury, UK\n",
      "nan\n",
      "travelling to tae's pants\n",
      "keli x\n",
      "Manchester\n",
      "Canada\n",
      "Knoxville, TN\n",
      "Australia\n",
      "ChicagoRObotz\n",
      "whs '17\n",
      "nan\n",
      "NV\n",
      "nan\n",
      "nan\n",
      "Ireland\n",
      "nan\n",
      "nan\n",
      "lagos nigeria\n",
      "Edmonton, Alberta\n",
      "[Gia.] | #KardashianEmpire\n",
      "Sunrise Manor, NV\n",
      "Oxford, OH\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "Odawara, Japan\n",
      "nan\n",
      "Chicago, IL\n",
      "Des Moines, IA\n",
      "Dundas, Ontario\n",
      "nan\n",
      "nan\n",
      "New York\n",
      "IDN\n",
      "nan\n",
      "Netherlands\n",
      "DaKounty, Pa\n",
      "nan\n",
      "Tokyo\n",
      "Dร_sseldorf, Germany\n",
      "Old Blighty\n",
      "nan\n",
      "nan\n",
      "??\n",
      "nan\n",
      "nan\n",
      "The land of New Jersey. \n",
      "Atlanta Georgia\n",
      "nan\n",
      "MY RTs ARE NOT ENDORSEMENTS\n",
      "Kabul, Tuebingen, Innsbruck\n",
      "Light and dark, form and void\n",
      "Nairobi, Kenya \n",
      "nan\n",
      "nan\n",
      "Erbil\n",
      "Stockton on tees Teesside UK\n",
      "Screwston, TX\n",
      "nan\n",
      "My old New England home\n",
      "melbourne\n",
      "Cape Town\n",
      "nan\n",
      "Ikeja, Nigeria\n",
      "Warwick, RI @Dollarocracy also\n",
      "texas a&m university\n",
      "MA\n",
      "nan\n",
      "Shipwreck Cove\n",
      "nan\n",
      "nan\n",
      "Sydney, Australia\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "WorldWide\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "Overland Park, KS\n",
      "SWMO\n",
      "Puerto Rico\n",
      "Toronto-Citizen of Canada & US\n",
      "Silicon Valley\n",
      "nan\n",
      "nan\n",
      "Washington, DC\n",
      "nan\n",
      "North East USA\n",
      "Washington, D.C.\n",
      "Singapore\n",
      "nan\n",
      "London\n",
      "United States\n",
      "Vitร_ria (ES)\n",
      "Mumbai\n",
      "nan\n",
      "nan\n",
      "New Delhi, Delhi\n",
      "Buscame EL tu Melte\n",
      "nan\n",
      "Wiltshire\n",
      "nan\n",
      "PROUD INDIANS\n",
      "nan\n",
      "Leicester\n",
      "Mumbai , India\n",
      "nan\n",
      "Pittsburgh PA\n",
      "nan\n",
      "Boston\n",
      "MUM-DEL\n",
      "Leeds, England\n",
      "US\n",
      "nan\n",
      "California\n",
      "UK\n",
      "Nigeria\n",
      "Playa del Carmen, Mexico\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "Leicester\n",
      "nan\n",
      "NY, CT & Greece\n",
      "nan\n",
      "Mumbai\n",
      "nan\n",
      "nan\n",
      "Mackay, QLD, Australia\n",
      "Quincy MA\n",
      "NJ\n",
      "nan\n",
      "Madison, GA\n",
      "nan\n",
      "KurveZ@GearHeadCentral.net\n",
      "St Charles, MD\n",
      "Denver, Colorado\n",
      "San Francisco\n",
      "Ziam af \n",
      "nan\n",
      "GO BLUE! HAIL YES!!\n",
      "Chicago, IL\n",
      "Outside The Matrix, I Think.\n",
      "nan\n",
      "In Hell\n",
      "Epic City, BB.\n",
      "somewhere over a rainbow\n",
      "Selma2Oakland\n",
      "nan\n",
      "Bombardment Bay\n",
      "nan\n",
      "Savannah, GA\n",
      "dallas\n",
      "New Orleans ,Louisiana\n",
      "England, United Kingdom\n",
      "Dublin City, Ireland\n",
      "Leeds, England\n",
      "nan\n",
      "New Hampshire\n",
      "Washington, D.C.\n",
      "nan\n",
      "World Wide\n",
      "Intermountain West\n",
      "World Wide\n",
      "New Hampshire\n",
      "Manchester, NH\n",
      "Tulsa, Oklahoma\n",
      "nan\n",
      "Auburn, AL\n",
      "nan\n",
      "nan\n",
      "taken by piper curda\n",
      "Toronto\n",
      "Scotland \n",
      "Nigeria, Global\n",
      "Fort Walton Beach, Fl\n",
      "Sweden\n",
      "Groton, CT\n",
      "Brisbane Australia\n",
      "NY Capital District\n",
      "Peoria\n",
      "Reading UK\n",
      "UK\n",
      "Concord, NH \n",
      "CORNFIELDS\n",
      "Worcester, MA\n",
      "Roanoke, VA\n",
      "nj\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n",
      "MA via PA\n",
      "Boston, MA\n",
      "World Wide\n",
      "New Hampshire\n",
      "Oklahoma City, OK\n",
      "toronto ยรยข dallas\n",
      "San Diego CA\n",
      "germany\n",
      "nan\n",
      "Massachusetts\n",
      " Nxgerxa\n",
      "Erie, PA\n",
      "nan\n",
      "Port Charlotte, FL\n",
      "Belleville, Illinois\n",
      "Alabama\n",
      "nan\n",
      "Long Island NY & San Francisco\n",
      "Canada\n",
      "Gainesville, FL\n",
      "nan\n",
      "Oakland, CA\n",
      "956\n",
      "Chicago\n",
      "nan\n",
      "Escondido, CA\n",
      "DC\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "Chicago Area\n",
      "nan\n",
      "Upper St Clair, PA\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "Cherry Creek Denver CO\n",
      "627\n",
      "nan\n",
      "Blogland\n",
      "mumbai\n",
      "Isle of Man\n",
      "nan\n",
      "Hampton Roads, VA\n",
      "nan\n",
      "Gameday\n",
      "nan\n",
      "Earthling (For now!)\n",
      "nan\n",
      "nan\n",
      "Black Canyon New River, AZ\n",
      "Caracas, Venezuela.\n",
      "nan\n",
      "NY\n",
      "Charlottetown\n",
      "nan\n",
      "nan\n",
      "Paradise, NV\n",
      "??t?a\n",
      "nan\n",
      "Liรยฌge\n",
      "Spokane, Washington 99206\n",
      "taco bell\n",
      "nan\n",
      "Australian Capital Territory\n",
      "http://www.amazon.com/dp/B00HR\n",
      "New York, NY\n",
      "[ kate + they/them + infp-t ]\n",
      "nan\n",
      "New York\n",
      "nan\n",
      "Sacramento, CA\n",
      "Mackay, QLD, Australia\n",
      "St Charles, MD\n",
      "please H? ?:??\n",
      "we?it ยรยข ixwin\n",
      "nan\n",
      "nan\n",
      "Santiago Bernabeau\n",
      "y/e/l\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "NJ\n",
      "Whiterun, Skyrim\n",
      "New Orleans ,Louisiana\n",
      "Seattle, WA\n",
      "Greenpoint, Brooklyn\n",
      "nan\n",
      "dallas\n",
      "nan\n",
      "Oklahoma City, OK\n",
      "Los Angeles\n",
      "Victoria, British Columbia\n",
      "a botanical garden probably\n",
      "nan\n",
      "Madison, GA\n",
      "Nelspruit, South Africa\n",
      "midwest\n",
      "Copenhagen, Capital Region of Denmark\n",
      "In Hell\n",
      "Spying on your thoughts\n",
      "seattle grace mercy death\n",
      "Selma2Oakland\n",
      "Tucson, Arizona \n",
      "Epic City, BB.\n",
      "Charlotte County Florida\n",
      "Head Office: United Kingdom\n",
      "Mid north coast of NSW\n",
      "iPhone: -27.499212,153.011072\n",
      "Queen Creek AZ\n",
      "nan\n",
      "Jamaica\n",
      "Trinidad and Tobago\n",
      "nan\n",
      "nan\n",
      "Melbourne Australia\n",
      "The Internet & NYC\n",
      "London/Bristol/Guildford\n",
      "Canberra, Australian Capital Territory\n",
      "nan\n",
      "beacon hills \n",
      "somewhere outside\n",
      "nan\n",
      "Wolmers Trust School for Boys \n",
      "Loughborough.\n",
      "nan\n",
      "Sydney, Australia\n",
      "nan\n",
      "Selangor\n",
      "nan\n",
      "London\n",
      "indiana\n",
      "nan\n",
      "nan\n",
      "Philippines\n",
      "nan\n",
      "Bronx, New York\n",
      "Canadian bread\n",
      "Phoenix, AZ\n",
      "Le Moyne '16\n",
      "nan\n",
      "nan\n",
      "Hoxton, London\n",
      "Las Vegas, NV USA\n",
      "Heinz Field \n",
      "Skyport de la Rosa\n",
      "USA\n",
      "The Low-Cal Calzone Zone\n",
      "Brooklyn, New York\n",
      "nan\n",
      "US\n",
      "50% Queanbeyan - 50% Sydney\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "USA\n",
      "UK\n",
      "Insula Barataria\n",
      "everywhere \n",
      "Afghanistan, USA\n",
      "Inglewood, CA\n",
      "Absecon, NJ\n",
      "Williamsbridge, Bronx, New Yor\n",
      "Northern Kentucky, USA\n",
      "nan\n",
      "Mostly Yuin.\n",
      "Nairobi\n",
      "Canada\n",
      "Rochelle, GA\n",
      "nan\n",
      "London UK\n",
      "south of heaven \n",
      "nan\n",
      "El Dorado, KS\n",
      "Santa Monica, CA\n",
      "nan\n",
      "Kenya\n",
      "1648 Queen St. West, Toronto.\n",
      "Toledo, OH\n",
      "nan\n",
      "Fairfield, California\n",
      "Trinity, Bailiwick of Jersey\n",
      "Virginia\n",
      "LIVERPOOL\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "Boulder, CO\n",
      "nan\n",
      "Washington, D.C.\n",
      "Hospital, bc of SKH vid.\n",
      "Hartford  London Hong Kong\n",
      "In @4SkinChan 's arms\n",
      "nan\n",
      "nan\n",
      "Massachusetts, USA\n",
      "Seattle\n",
      "Kansas City, MO\n",
      "nan\n",
      "nan\n",
      "Wellington, New Zealand\n",
      "West Virginia, USA\n",
      "nan\n",
      "?? ??\n",
      "nan\n",
      "Florida\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "los angeles, ca\n",
      "nan\n",
      "New Brunswick, NJ\n",
      "City of Angels, CA\n",
      "Brooklyn, NY\n",
      "Morganville, Texas.\n",
      "America | New Zealand \n",
      "nan\n",
      "nan\n",
      "Denver, CO\n",
      "nan\n",
      "nan\n",
      "Stockholm, Sweden\n",
      "Worldwide\n",
      "Azeroth\n",
      "Lytham St Anne's \n",
      "nan\n",
      "nan\n",
      "Ylisse\n",
      "Wisconsin\n",
      "Portugal\n",
      "All around the world baby\n",
      "nan\n",
      "nan\n",
      "@UntmdOutdoors #T.O.R.K \n",
      "Lima, Perรยผ\n",
      "nan\n",
      "Toronto\n",
      "Piedmont Area, North Carolina\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "Buxton, Venice, and Nottingham\n",
      "Quito, Ecuador.\n",
      "Inexpressible Island \n",
      "nan\n",
      "San Jose, CA\n",
      "New York, NY\n",
      "United States\n",
      "New York\n",
      " Bouvet Island\n",
      "Shirley, NY\n",
      "Planet Earth\n",
      "Paonia, Colorado \n",
      "nan\n",
      "nan\n",
      "Inexpressible Island \n",
      "Planet Earth\n",
      "Portland, OR\n",
      "Dublin, Ireland\n",
      "Planet Earth\n",
      "nan\n",
      "Lurking\n",
      "Leeds, England\n",
      "N?? Y???.\n",
      "nan\n",
      "Littleton, CO, USA\n",
      "Virginia, United States\n",
      "Center for Domestic Preparedness\n",
      "Ukraine and Ireland\n",
      "London\n",
      "Welt\n",
      "nan\n",
      "Wales\n",
      "http://www.amazon.com/dp/B00HR\n",
      "Seattle, Washington\n",
      "Beaumont, TX\n",
      "Annapolis, MD\n",
      "Las Vegas, Nevada\n",
      "nan\n",
      "Seattle, Washington\n",
      "Harris County, Texas\n",
      "Tyler, TX\n",
      "Evanston, IL\n",
      "North Ferriby, East Yorkshire\n",
      "Colombia\n",
      "Seattle, Washington\n",
      "Evanston, IL\n",
      "Ames, Iowa\n",
      "Seattle, Washington\n",
      "Moscow, Russia\n",
      "nan\n",
      "nan\n",
      "Jersey City, NJ\n",
      "nan\n",
      "nan\n",
      "Mankato, MN\n",
      "Orbost, Victoria, Australia\n",
      " Somewhere.\n",
      "nan\n",
      " Neverland \n",
      "nan\n",
      "London, England\n",
      "At Da Laundry Mat Wit Nivea \n",
      "norway\n",
      "nyc\n",
      "nan\n",
      "$ad $hawty\n",
      "nan\n",
      "Nigeria\n",
      "nan\n",
      "NY || live easy? \n",
      "BestCoast\n",
      "nan\n",
      "nan\n",
      "BC\n",
      "Abuja, Nigeria\n",
      "nan\n",
      "Colchester Essex \n",
      "livin life in the 610\n",
      "Suplex City\n",
      "Inside your mind.\n",
      "nan\n",
      "36 & 38\n",
      "Sydney Australia\n",
      "Lagos, Nigeria\n",
      "New York, NY \n",
      "Slappin and Smackin \n",
      "DRAW A CIRCLE THAT'S THE EARTH\n",
      "nan\n",
      "nan\n",
      "Florida, USA\n",
      "Madrid, Comunidad de Madrid\n",
      "The Netherlands\n",
      "nan\n",
      "Highland Park, CA\n",
      "Scotland\n",
      "Swan River\n",
      "Kolkata, India\n",
      "Melrose\n",
      "Jubail IC, Saudi Arabia\n",
      "New York City\n",
      "nan\n",
      "Los Angeles, CA\n",
      "Location\n",
      "Sugarhouse, UT\n",
      "lugo\n",
      "Chicago, Illinois\n",
      "Blackpool, England, UK.\n",
      "nan\n",
      "San Jose, California\n",
      "nan\n",
      "Fakefams\n",
      "In the clouds...\n",
      "India\n",
      "Sandton, South Africa\n",
      "Nigeria \n",
      "nan\n",
      "#Bummerville otw\n",
      "Europe\n",
      "Brighton and Hove\n",
      "Mumbai , India\n",
      "United Kingdom\n",
      "Pompano Beach, FL\n",
      "JKT48-Muse-A7X\n",
      "Behind The Obama Curtain\n",
      "Worldwide.\n",
      "United States\n",
      "LiVE MรยS\n",
      "USA\n",
      "nan\n",
      "Henderson, NV\n",
      "nan\n",
      "CAMARILLO, CA\n",
      "manchester, uk.\n",
      "Eugene, Oregon\n",
      "they/her\n",
      "nan\n",
      "Paris \n",
      "nan\n",
      "Tokyo\n",
      "(he/him)\n",
      "nan\n",
      "Suplex City\n",
      "Victoria, British Columbia\n",
      "Kingston, Jamaica\n",
      "USA\n",
      "nan\n",
      "Spokane, WA\n",
      "nan\n",
      "I'm standing behind you\n",
      "Alexandria, Egypt.\n",
      "instagram- Chloe_Bellx\n",
      "on the web\n",
      "San Francisco Bay Area\n",
      "#ForeverWithBAP 8 \n",
      "GOT7SupportPH\n",
      "wherever-the-fuck washington\n",
      "nan\n",
      "From NY. In Scranton, PA\n",
      "Perth, Western Australia\n",
      "nan\n",
      "Live mรรs\n",
      "nan\n",
      "nan\n",
      "Melton, GA\n",
      "nan\n",
      "Greg's place\n",
      "Viejo\n",
      "nan\n",
      "USA\n",
      "Kansas, The Free State! ~ KC\n",
      "nan\n",
      "Silang, Cavite / Paraรยฑaque\n",
      "Fort Smith, AR\n",
      "Brasil\n",
      "nan\n",
      "New York\n",
      "planeta H2o\n",
      "www.youtube.com?Malkavius2\n",
      "Michigan\n",
      "The Forever Girl\n",
      "Espร_rito Santo\n",
      "New York, USA\n",
      "Ormond By The Sea, FL\n",
      "Maryland,Baltimore\n",
      "New Orleans, LA\n",
      "Vancouver, BC\n",
      "Pennsylvania, USA\n",
      "Houston, Texas\n",
      "Yadkinville, NC\n",
      "Medford, NJ\n",
      "Austin, TX\n",
      "nan\n",
      "nan\n",
      "Malaysia\n",
      "Dallas, Texas. \n",
      "Roanoke VA\n",
      "nan\n",
      "Unnamed City\n",
      "nan\n",
      "Toronto\n",
      "Peterborough, On\n",
      "-6.152261,106.775995\n",
      "@protectingtitan's side.\n",
      "nan\n",
      "Oregon\n",
      "San Francisco\n",
      "nan\n",
      "Eau Claire, Wisconsin\n",
      "St. Joseph, Minnesota\n",
      "From a torn up town MANCHESTER\n",
      "Waco TX\n",
      "Tennessee\n",
      "Cape Cod, Massachusetts USA\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "Pakistan\n",
      "bk. \n",
      "nan\n",
      "Lansing, Michigan\n",
      "Peterborough, Ont.\n",
      "Johannesburg, South Africa\n",
      "On the court \n",
      "Cherry Creek Denver CO\n",
      "nan\n",
      "Traverse City, MI\n",
      "nan\n",
      "See the barn of bleakness\n",
      "Mumbai\n",
      "Hertfordshire \n",
      "รรT: 41.252426,-96.072013\n",
      "Nairobi, Kenya\n",
      "nan\n",
      "California \n",
      "Greeley, CO\n",
      "'soooota\n",
      "Sacramento\n",
      "nan\n",
      "nan\n",
      "Sacramento\n",
      "Ontario, Canada\n",
      "Los Angeles, CA\n",
      "NY, NY\n",
      "Arvada, CO\n",
      "Oregon\n",
      "Denver, CO\n",
      "Maryland\n",
      "Vancouver, Colombie-Britannique\n",
      "Mumbai\n",
      "Irving , Texas\n",
      "Riverside, CA\n",
      "SEATTLE, WA USA\n",
      "San Francisco, CA\n",
      "Orange County, California\n",
      "USA , AZ\n",
      "East Atlanta, Georgia\n",
      "Denver, Colorado\n",
      "Los Angeles, CA\n",
      "Saint Lucia\n",
      "Peterborough, Ontario, Canada\n",
      "nan\n",
      "Colorado\n",
      "North Highlands, CA\n",
      "Vancouver\n",
      "60th St (SS)\n",
      "San Francisco, CA\n",
      "Sacramento, CA\n",
      "Ontario, Canada\n",
      "Denver, Colorado\n",
      "Denver, Colorado\n",
      "Sacramento\n",
      "USA, WA\n",
      "btwn a rock and a hard place\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "Tennessee\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "Aix-en-Provence, France\n",
      "Liverpool\n",
      "I-75 in Florida\n",
      "nan\n",
      "21.462446,-158.022017\n",
      "Cleveland, OH\n",
      "nan\n",
      "Melbourne, Australia\n",
      "nan\n",
      "Darlington\n",
      "nan\n",
      "nan\n",
      "  Melbourne, Australia\n",
      "Kenton, Ohio\n",
      "Galatians 2:20 \n",
      "nan\n",
      "Charleston, SC\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "Lancaster, Pennsylvania, USA\n",
      "nan\n",
      "In my own world!!!\n",
      "nan\n",
      "nan\n",
      "San Francisco, CA\n",
      "Definitely NOT the stables\n",
      "Pakistan\n",
      "Cuernavaca, Morelos, Mรยฉxico.\n",
      "nan\n",
      "Scotland\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "Va Beach, Virginia\n",
      "52.479722, 62.184971\n",
      "The Pig Sty\n",
      "nan\n",
      "Bangor, Co.Down\n",
      "Weston super mare\n",
      "nan\n",
      "Victoria, Tx.\n",
      "Pakistan, Islamabad\n",
      "Lincoln, NE\n",
      "nan\n",
      "nan\n",
      "London\n",
      "Gujranwala, Pakistan\n",
      "Lindenhurst\n",
      "Islamabad\n",
      "Somewhere\n",
      "nan\n",
      "too far\n",
      "Kingswinford\n",
      "i love the smurfs 2\n",
      "nan\n",
      "nan\n",
      "International \n",
      "Viterbo BFA Acting '18\n",
      "Buenos Aires\n",
      "Kaneohe\n",
      "Houma La\n",
      "nan\n",
      "nan\n",
      "EastAtlanta ??#WestGeorgia'18\n",
      "San Antonio, TX\n",
      "Cleveland, Ohio\n",
      "nan\n",
      "nan\n",
      "Everywhere\n",
      "nan\n",
      "San Diego, Texas.\n",
      "GLOBAL\n",
      "05/04/2014 18:23 ?\n",
      "Everywhere\n",
      "nan\n",
      "nan\n",
      "Bolivar, MO\n",
      "taking pain like pleasure\n",
      "San Fransokyo\n",
      "Washington, DC NATIVE\n",
      "nan\n",
      "nan\n",
      "Everywhere\n",
      "Miami, FL\n",
      "nan\n",
      "Utah\n",
      "nan\n",
      "Everywhere\n",
      "nan\n",
      "Everywhere\n",
      "w. Nykae \n",
      "honeymoon avenue\n",
      "Toronto, Worldwide \n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "11/4/14\n",
      "wherever there's netflix\n",
      "nan\n",
      "nan\n",
      "Chicago, IL\n",
      "#HAMont\n",
      "nan\n",
      "U.K.\n",
      "ph\n",
      "nan\n",
      "nan\n",
      "online \n",
      "nan\n",
      "Conroe, TX\n",
      "Ontario, Canada. \n",
      "Guayaquil\n",
      "bahstun/porta reeko\n",
      "Bucks County, Pa\n",
      "Sunny South florida \n",
      "Rio\n",
      "Neverland\n",
      "Pennsylvania\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "Trinidad & Tobago\n",
      "Chicago - Lake Buena Vista\n",
      "Toronto, Ontario\n",
      "Liberty Lake, WA\n",
      "nan\n",
      "Ankara - Malatya - ad Orontem\n",
      "nan\n",
      "nan\n",
      "antioch, california\n",
      "State College, PA\n",
      "Chicago, IL\n",
      "nan\n",
      "somewhere in cali \n",
      "Im Around ... Jersey\n",
      "Garden Grove\n",
      "Adelaide, Australia\n",
      "nan\n",
      "nan\n",
      "BKI-KUA\n",
      "nan\n",
      "urรยฃnus\n",
      "Illinois, USA\n",
      "A.A.S my Aztec Princess\n",
      "turner fenton\n",
      "nan\n",
      "KLA,Uganda\n",
      "Uganda\n",
      "nan\n",
      "253\n",
      "Elkhart, IN\n",
      "Adelaide, Australia\n",
      "California\n",
      "Adelaide, Australia\n",
      "Pon Di Gully\n",
      "Benedict College\n",
      "HTX\n",
      "nan\n",
      "IM LOST \n",
      "Miami\n",
      "nan\n",
      "Hamilton, ON\n",
      "United Kingdom\n",
      "Des Moines, Iowa \n",
      "USA\n",
      "Rural Northern Nevada\n",
      "Republic of the Philippines\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "Philippines \n",
      "Washington, DC\n",
      "hyderabad\n",
      "Vancouver (HQ) and worldwide\n",
      "Quezon City, Philippines\n",
      "Geneva\n",
      "Hartford,  connecticut\n",
      "Sioux Falls, SD\n",
      "Cornwall\n",
      "Beside Basketball\n",
      "Rome, Italy\n",
      "Tafekop Ga-Matsepe\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "Melbourne\n",
      "Vilnius\n",
      "Geneva\n",
      "nan\n",
      "nan\n",
      "Made in America\n",
      "PS4, now stop asking\n",
      "nan\n",
      "Cheshire. London. #allover\n",
      "Texas, USA\n",
      "Bartholomew County, Indiana\n",
      "U.S\n",
      "My mind is my world\n",
      "nan\n",
      "Catskills\n",
      "nan\n",
      "Columbia, SC\n",
      "nan\n",
      "Right here\n",
      "Texas\n",
      "Lawrence, KS via Emporia, KS\n",
      "http://twitch.tv/jcmonkey\n",
      "Indonesia\n",
      "Unknown\n",
      "Unknown\n",
      "Marysville, MI\n",
      "New Haven, Connecticut\n",
      "nan\n",
      "Austin | San Diego\n",
      "??? ?? ??????? \n",
      "Pontevedra, Galicia\n",
      "Somewhere in the Canada\n",
      "London/New York\n",
      "261 5th Avenue New York, NY \n",
      "nan\n",
      "nan\n",
      "nan\n",
      "Charlotte NC\n",
      "nan\n",
      "Rockville, Maryland\n",
      "nan\n",
      "nan\n",
      "Australia\n",
      "Your Conversation\n",
      "USAoV\n",
      "Bhopal, Madhya Pradesh, India.\n",
      "Tennessee\n",
      "nan\n",
      "#LemonGang \n",
      "ayr\n",
      "nan\n",
      "Worldwide\n",
      "San Jose, California\n",
      "Worldwide\n",
      "Loughborough, England\n",
      "Killarney\n",
      "Brooklyn, NY\n",
      "Lahar & Gwalior\n",
      "St Paul, MN\n",
      "Atlanta Georgia \n",
      "nan\n",
      "Lincoln, IL\n",
      "World\n",
      "Spinning through time.\n",
      "Atlanta Georgia \n",
      "Boston, MA\n",
      "Hailing from Dayton \n",
      "Worldwide\n",
      "Newcastle Upon Tyne, England\n",
      "Atlanta Georgia \n",
      "Atlanta Georgia \n",
      "nan\n",
      "nan\n",
      "Instagram: trillrebel_\n",
      "ALWAYS DYING NEVER RESTING\n",
      "San Jose, CA\n",
      "2005 |-/\n",
      "Israel\n",
      "Uruguay / Westeros / Gallifrey\n",
      "Kansas City, MO\n",
      "Silver Spring, MD\n",
      "Jersey - C.I\n",
      "nan\n",
      "nan\n",
      "Afghanistan\n",
      "Sweden\n",
      "nan\n",
      "nan\n",
      "London\n",
      "Orlando, FL\n",
      "nan\n",
      "Atmosphere\n",
      "nan\n",
      "Glasgow\n",
      "nan\n",
      "nan\n",
      "somewhere in Portugal\n",
      "South Stand\n",
      "dundalk ireland\n",
      "nan\n",
      "nan\n",
      "Sochi, KDA, RU\n",
      "รรT: -26.695807,27.837865\n",
      "South Stand\n",
      "You're not 19 forever   \n",
      "Kettering, OH\n",
      "United States\n",
      "nan\n",
      "Planet Earth\n",
      "Spare 'Oom\n",
      "nan\n",
      "United States\n",
      "Milton Keynes \n",
      "nan\n",
      "ATL??AL??\n",
      "UPTOWN \n",
      "nan\n",
      "PROV\n",
      "Novi, MI\n",
      "New York\n",
      "Buenos Aires, Argentina\n",
      "on the go\n",
      "nan\n",
      "nan\n",
      "The UK\n",
      "nan\n",
      "nan\n",
      "mpls. \n",
      "Kensington, MD\n",
      "Buffalo/DC\n",
      "nan\n",
      "Sylacauga, Alabama\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "USA\n",
      "To The Right of You!\n",
      "Cyprus\n",
      "kansas\n",
      "Los Angeles,CA, USA\n",
      "Portland, OR\n",
      "Carry On Jutta!!!\n",
      "Home of the Takers.\n",
      "New York, New York\n",
      "nan\n",
      "?s????ss? a?????\n",
      "Chicago\n",
      "Alicante, Spain\n",
      "nan\n",
      "nan\n",
      "Mooresville, NC\n",
      "Mooseknuckle, Maine\n",
      "Palermo, Sicily\n",
      "nan\n",
      "Asunciร_n-PY / Tร_bingen-GER\n",
      "Voorhees, NJ\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "UK\n",
      "Atlanta, GA\n",
      "Planet Earth\n",
      "Wakanda\n",
      "Lagos\n",
      "Does it really matter!\n",
      "nan\n",
      "Your screen\n",
      "Phoenix, AZ\n",
      "Does it really matter!\n",
      "Edinburgh\n",
      "Weyburn\n",
      "UGA '15 Alumnus - Economics \n",
      "nan\n",
      "nan\n",
      "Blackpool\n",
      "the void, U.S.A\n",
      "london town..\n",
      "Tennessee/Gallifrey\n",
      "Amman, Jordan\n",
      "Blackpool\n",
      "Washington, D.C.\n",
      "nan\n",
      "Columbia Heights, MN\n",
      "Top Secret\n",
      "รรT: 10.614817868480726,12.195582811791382\n",
      "Top secret bunker \n",
      "Belbroughton, England\n",
      "nan\n",
      "nan\n",
      "772 Temperance Permenence\n",
      "nan\n",
      "nan\n",
      "Nigeria \n",
      "nan\n",
      "46.950109,7.439469\n",
      "nan\n",
      "nan\n",
      "In the Shadows\n",
      "nan\n",
      "Campo Grande-MS\n",
      "nbc washington\n",
      "nan\n",
      "Seattle\n",
      "Hamilton, Ontario Canada\n",
      "New York\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "Dubai, United Arab Emirates\n",
      "India\n",
      "Hong Kong\n",
      "Berlin, Germany\n",
      "Nigeria \n",
      "IRAQ\n",
      "nan\n",
      "Tampa, FL\n",
      "World news\n",
      "Hong Kong\n",
      "nan\n",
      "Hong Kong\n",
      "nan\n",
      "Bristol, UK\n",
      "labuan, malaysia\n",
      "United Kingdom\n",
      "right next to you\n",
      "CT ? NYC\n",
      "www.tmgcgart.com\n",
      "nan\n",
      "London, England\n",
      "Los Angeles, CA\n",
      "nan\n",
      "nan\n",
      "Australia\n",
      "Brisbane\n",
      "Melbourne-ish\n",
      "617-BTOWN-BEATDOWN\n",
      "Brackley Beach, PE, Canada\n",
      "London\n",
      "518\n",
      "Nottingham, England\n",
      "eARth 3\n",
      "nan\n",
      "nan\n",
      "Enniscrone & Aughris, Sligo \n",
      "Mostly Wellington, NZ \n",
      "London\n",
      "nan\n",
      "USA\n",
      "nan\n",
      "617-BTOWN-BEATDOWN\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "Atlanta, Ga\n",
      "World\n",
      "nan\n",
      "Fort Fizz, Ohio\n",
      "New York, NY\n",
      "Fort Lauderdale, FL\n",
      "Raleigh (Garner/Cleveland) NC\n",
      "nan\n",
      "Newcastle upon Tyne\n",
      "walking the tightrope\n",
      "Frascati\n",
      "College Station, TX\n",
      "Bishops Lydeard, England\n",
      "nan\n",
      "Karachi, Pakistan\n",
      "nan\n",
      "Up a hill\n",
      "nan\n",
      "nan\n",
      "London\n",
      "nan\n",
      "Clearwater, FL\n",
      "Manchester\n",
      "Wellington\n",
      "nan\n",
      "nan\n",
      "#????? Libya#\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "Newcastle\n",
      "nan\n",
      "nan\n",
      "Los Angeles, CA\n",
      "Pakistan\n",
      "balvanera\n",
      "Aro Diaspora\n",
      "Washington D.C.\n",
      "Nigeria, WORLDWIDE\n",
      "รรT: 0.0,0.0\n",
      "Eastbourne England\n",
      "State of Dreaming\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "Wema building\n",
      "Port Harcourt, Nigeria\n",
      "Fruit Bowl\n",
      "South Africa\n",
      "Hooters on Peachtree\n",
      "Otsego, MI\n",
      "nan\n",
      "MAD as Hell\n",
      "ATL, GA\n",
      "Uyo, Akwa Ibom State, Nigeria\n",
      "us-east-1a\n",
      "London\n",
      "RhodeIsland\n",
      "London, UK\n",
      "London\n",
      "Napa, CA\n",
      "Lagos, Nigeria\n",
      "nan\n",
      "KOLKATA\n",
      "golborne, north west england.\n",
      "Earth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NYHC\n",
      "sweden\n",
      "ARBAILO\n",
      "Bagalkote Karnataka \n",
      "Shrewsbury\n",
      "NJ\n",
      "Terre Haute, IN\n",
      "NH via Boston, MA\n",
      "Canada\n",
      "Glasgow, Scotland\n",
      "QUEENS.\n",
      "nan\n",
      "nan\n",
      "ATL ??\n",
      "???????????\n",
      "Beautiful British Columbia\n",
      "nan\n",
      "nan\n",
      "Newcastle\n",
      "Chicago\n",
      "nan\n",
      "Dublin, Ireland\n",
      "USA\n",
      "nan\n",
      "????\n",
      "Birmingham UK\n",
      "Medford, Oregon\n",
      "Catalonia, Spain\n",
      "Kilkenny\n",
      "nan\n",
      "nan\n",
      "Paterson, New Jersey \n",
      "USA\n",
      "nan\n",
      "Lisbon, Portugal\n",
      "MA\n",
      "Halifax, Nouvelle-รรคcosse\n",
      "nan\n",
      "nan\n",
      "Murray Hill, New Jersey\n",
      "aggressive cannoli eater \n",
      "Chicago\n",
      "nan\n",
      "nan\n",
      "11202\n",
      "??????\n",
      "nan\n",
      "San Francisco, CA\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "Atlantic, IA\n",
      "nan\n",
      "nan\n",
      "New York, USA\n",
      "รรT: 36.142163,-95.979189\n",
      "Arthas US\n",
      "Funtua, Nigeria\n",
      "everywhere\n",
      "US-PR\n",
      "Washington, DC\n",
      "Buffalo, NY\n",
      "California, USA\n",
      "nan\n",
      "San Jose\n",
      "nan\n",
      "รรธรฅรรฅ_T: 40.736324,-73.990062\n",
      " Road to the Billionaires Club\n",
      "Washington, D.C.\n",
      "Washington, DC 20009\n",
      " Road to the Billionaires Club\n",
      "nan\n",
      "Los Angeles, CA\n",
      "nan\n",
      "nan\n",
      "Washington, Krasnodar (Russia)\n",
      " Road to the Billionaires Club\n",
      "Nairobi-KENYA\n",
      "at my home\n",
      "District of Gentrification/ DC\n",
      "nan\n",
      " Road to the Billionaires Club\n",
      "USA\n",
      " Road to the Billionaires Club\n",
      "London\n",
      "Washington, D.C. \n",
      "U.S.A\n",
      "nan\n",
      "Nairobi\n",
      "#MadeInNorthumberland\n",
      "London, England\n",
      "nan\n",
      "Nairobi-KENYA\n",
      "nan\n",
      "Jamshedpur, Jharkhand\n",
      " Road to the Billionaires Club\n",
      "nan\n",
      "eritrean\n",
      "Los Angeles, CA\n",
      "Dayton, OH\n",
      "Washington, DC\n",
      "Dorset, United Kingdom\n",
      "A small federal enclave\n",
      "Washington, DC\n",
      "Pune, mostly \n",
      "Washington, DC\n",
      "nan\n",
      "Kwara, Nigeria\n",
      "The Desert of the Real\n",
      "lee london\n",
      "Anchorage, AK\n",
      "Washington, DC\n",
      "USA\n",
      "Chicago, IL\n",
      "nan\n",
      "DC, frequently NYC/San Diego\n",
      "Washington, DC\n",
      "Enterprise, Alabama\n",
      "Washington, DC\n",
      "US\n",
      "รฅรธ\\_(?)_/รฅรธ\n",
      "Washington DC / Nantes, France\n",
      "nan\n",
      "Washington, DC & Charlotte, NC\n",
      "Kalamazoo, Michigan\n",
      "DC\n",
      "SEC Country\n",
      "nan\n",
      "nan\n",
      "L'Enfant Plaza Metro Station\n",
      "nan\n",
      "Durban, South Africa\n",
      "United Kingdom\n",
      "Alexandria, VA\n",
      "NYC\n",
      "nan\n",
      "Texas-USAยรฃยข ?\n",
      "Toronto\n",
      "Arlington, VA and DC\n",
      "Headed To The Top\n",
      "Mumbai (India)\n",
      "Chicago, IL\n",
      "India\n",
      "nan\n",
      "nan\n",
      "Mumbai\n",
      "Coimbatore\n",
      "nan\n",
      "Chicago, IL 60607\n",
      "Palo Alto, California\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "India\n",
      "Chicagoland\n",
      "nan\n",
      "UK\n",
      "India\n",
      "Mumbai\n",
      "Los Angeles\n",
      "nan\n",
      "New Delhi\n",
      "India\n",
      "Mumbai\n",
      "India\n",
      "Mumbai\n",
      "nan\n",
      "India\n",
      "Chicago, IL\n",
      "nan\n",
      "Minneapolis,MN,US\n",
      "Chicago\n",
      "nan\n",
      "Chicago, IL \n",
      "Palo Alto, California\n",
      "India\n",
      "nan\n",
      "nan\n",
      "United Kingdom\n",
      "NYC area\n",
      "nan\n",
      "nan\n",
      "Philadelphia, PA \n",
      "nan\n",
      "Chicago,Illinois\n",
      "รรT: 33.209923,-87.545328\n",
      "Lahti, Finland\n",
      "Here there and everywhere\n",
      "nan\n",
      "the insane asylum. \n",
      "nan\n",
      "Oakland\n",
      "Macclesfield\n",
      "Michigan, USA\n",
      "Boulder\n",
      "nan\n",
      "Temporary Towers\n",
      "nan\n",
      "Macclesfield\n",
      "Pueblo, CO\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "the insane asylum. \n",
      "Manchester\n",
      "nan\n",
      "Stockholm, Sweden\n",
      "Birmingham, UK\n",
      "Houston, Texas ! \n",
      "New York\n",
      "North East, England\n",
      "[marvelยรยขdragon ageยรยขwicdiv]\n",
      "NBO\n",
      "Lagos Nigeria\n",
      "nan\n",
      "nan\n",
      "St. Louis Mo.\n",
      "Birdland, New Meridian, FD\n",
      "Arizona\n",
      "Monterrey, Mรยฉxico\n",
      "nan\n",
      "infj \n",
      "on twitter\n",
      "2B Hindhede Rd, Singapore\n",
      "Austin, TX\n",
      "Kalimantan Timur, Indonesia\n",
      "Prehistoric Earth\n",
      "san gabriel la union\n",
      "??????\n",
      "Manila, Philippines\n",
      "Middle Earth / Asgard / Berk\n",
      "Subconscious LA\n",
      "nan\n",
      "nan\n",
      "Singapore\n",
      "Quilmes , Arg\n",
      "nan\n",
      "Richmond, VA\n",
      "Istanbul\n",
      "Romania\n",
      "Chile\n",
      "???????? ?????????.\n",
      "New York City\n",
      "Virginia, USA\n",
      "San Diego, CA\n",
      "Johannesburg \n",
      "Japan\n",
      "Honduras\n",
      "Washington, DC\n",
      "Alvin, TX\n",
      "nan\n",
      "Wilbraham, MA\n",
      "nan\n",
      "Jerseyville, IL\n",
      "New York City\n",
      "nan\n",
      "UK\n",
      "nan\n",
      "The Citadel, Oldtown, Westeros\n",
      "nan\n",
      "Trackside California\n",
      "nan\n",
      "nan\n",
      "New England\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "Florida\n",
      "nan\n",
      "he/him or she/her (ask)\n",
      "Some pum pum\n",
      "Paulton, England\n",
      "Norway\n",
      "nan\n",
      "Thailand\n",
      "SEA Server\n",
      "Pretoria\n",
      "nan\n",
      "Nigeria\n",
      "nan\n",
      "todaysbigstock.com\n",
      "USA\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "buenos aires argentina\n",
      "everydaynigerian@gmail.com\n",
      "Surulere Lagos,Home Of Swagg\n",
      "USA\n",
      "USA\n",
      "nan\n",
      "USA\n",
      "Montrรยฉal, Quรยฉbec\n",
      "Montreal\n",
      "รรT: 6.4682,3.18287\n",
      "Live4Heed??\n",
      "Waco, Texas\n",
      "North Port, FL\n",
      "Bodรรผ, Norge\n",
      "Boise, Idaho\n",
      "In my studio\n",
      "nan\n",
      "USA\n",
      "Queens New York\n",
      "nan\n",
      "USA\n",
      "Illinois, USA\n",
      "Cuttack, Orissa\n",
      "nan\n",
      "USA\n",
      "Patra-Greece.\n",
      "All Around the World\n",
      "nan\n",
      "Valle Del Sol\n",
      "Georgia, USA\n",
      "New York NYC\n",
      "nan\n",
      "Hickville, USA\n",
      "Silesia, Poland\n",
      "Moscow\n",
      "Houston, TX\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "Balikesir - Eskisehir\n",
      "Manchester, England\n",
      "Maldives\n",
      "??????? ??????? ????????\n",
      "nan\n",
      "nan\n",
      "Soul Somalia/Body Montreal\n",
      "Manchester, UK\n",
      "nan\n",
      "Yooooooo\n",
      "New York City\n",
      "nan\n",
      "Jersey\n",
      "รรT: 19.123127,72.825133\n",
      "London\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "denver colorado\n",
      "Morioh, Japan\n",
      "nan\n",
      "Milton/Tallahassee\n",
      "Paname City\n",
      "Winston-Salem, NC\n",
      "Morioh, Japan\n",
      "nan\n",
      "Kรยฆln, Nordrhein-Westfalen\n",
      "Broomfield, CO\n",
      "Morioh, Japan\n",
      "Brasil,SP\n",
      "Ottawa,Ontario Canada\n",
      "Colorado\n",
      "nan\n",
      "Memphis,TN/ World Wide\n",
      "Brasil\n",
      "Worldwide\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "Morioh, Japan\n",
      "In My Lab Creating \n",
      "Bronx NYC / M-City NY\n",
      "nan\n",
      "Bikini bottom\n",
      "back in japan ??????????\n",
      "nan\n",
      "Sharkatraz/Bindle's Cleft, PA\n",
      "Morioh, Japan\n",
      "Worldwide\n",
      "Northern Ireland\n",
      "New York, NY\n",
      "San Diego, CA\n",
      "Morioh, Japan\n",
      "Orlando, FL\n",
      "Sydney, Australia\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "Bangalore. India\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "New York\n",
      "nan\n",
      "nan\n",
      "Bangkok Thailand\n",
      "nan\n",
      "New York\n",
      "nan\n",
      "nan\n",
      "New York\n",
      "nan\n",
      "Brisbane.\n",
      "New York\n",
      "Geneva\n",
      "nan\n",
      "nan\n",
      "Manchester\n",
      "PG Chillin!\n",
      "Manchester\n",
      "nan\n",
      "nan\n",
      "REPUBLICA DOMINICANA\n",
      "nan\n",
      "nan\n",
      "FOLLOWS YOU everywhere you go\n",
      "Birmingham\n",
      "-?s?s?j??s-\n",
      "London\n",
      "nan\n",
      "nan\n",
      "Indonesia\n",
      "\n",
      "The Meadow\n",
      "Bournemouth\n",
      "Brum/Lestah \n",
      "Dorset, UK\n",
      "nan\n",
      "Chester \n",
      "nan\n",
      "nan\n",
      "Banbridge\n",
      "Port Orange, FL\n",
      "nan\n",
      "probably petting an animal\n",
      "Chicago, IL\n",
      "nan\n",
      "Varies \n",
      "ACCRA GHANA\n",
      "Columbus\n",
      "nan\n",
      "St Joseph de Beauce\n",
      "nan\n",
      "Wasington, DC\n",
      "uk\n",
      "Seattle\n",
      "  News\n",
      "nan\n",
      "Newport, Wales, UK\n",
      "nan\n",
      "nan\n",
      "United States\n",
      "iTunes - RSS\n",
      "contactSimpleNews@gmail.com\n",
      "nan\n",
      "Washington, DC\n",
      "Upper manhattan, New York\n",
      "Las Vegas, Nevada\n",
      "Victoria, British Columbia\n",
      "Vancouver BC\n",
      "Devon/London \n",
      "Brasil\n",
      "ITALY\n",
      "Atlanta g.a.\n",
      "nan\n",
      "nan\n",
      "EVERYWHERE\n",
      "Washington DC\n",
      "Denver, CO\n",
      "nan\n",
      "nan\n",
      "Washington DC\n",
      "Mount Vernon, NY\n",
      "nan\n",
      "Jackson TN\n",
      "Los Angeles, London, Kent\n",
      "USA, Haiti, Nepal\n",
      "Portoviejo-Manabi-Ecuador\n",
      "nan\n",
      "nan\n",
      "chillin at ceder rapids\n",
      "nan\n",
      "Manila, Philippines\n",
      "en el pais de los arrechos\n",
      "nan\n",
      "Nigeria\n",
      "nan\n",
      "San Francisco\n",
      "Hinton, W.Va.\n",
      "nan\n",
      "Philadelphia, PA USA\n",
      "nan\n",
      "Naperville\n",
      "nan\n",
      "Lima, Peru\n",
      "In Your Notifications \n",
      "nan\n",
      "Fort Worth, Texas\n",
      "Dappar (Mohali) Punjab\n",
      "nan\n",
      "London\n",
      "los angeles\n",
      "Alexandria, VA\n",
      "Calgary, AB\n",
      "nearest trash can \n",
      "USA \n",
      "Atlanta\n",
      "nan\n",
      "Los Angeles, California\n",
      "Morocco\n",
      "Pedophile hunting ground\n",
      "nan\n",
      "U.S.\n",
      "Visit our  dedicated website @\n",
      "Asia Pacific   \n",
      "North Carolina\n",
      "nan\n",
      "Ojodu,Lagos\n",
      "New York City\n",
      "Seattle\n",
      "nan\n",
      "Kelowna, BC\n",
      "nan\n",
      "48.870833,2.399227\n",
      "Pedophile hunting ground\n",
      "Pedophile hunting ground\n",
      "Pedophile hunting ground\n",
      "Pedophile hunting ground\n",
      "San Francisco\n",
      "Oakland, CA\n",
      "Nigeria\n",
      "UK  & Germany\n",
      "nan\n",
      "Manila\n",
      "Magnolia\n",
      "Pedophile hunting ground\n",
      "New York\n",
      "nan\n",
      "nan\n",
      "(Spain)\n",
      "Nigeria\n",
      "new york\n",
      "Canada\n",
      "Palestine \n",
      "Na:tinixw / Hoopa, Berkeley\n",
      "United Kingdom\n",
      "USA (Formerly @usNOAAgov)\n",
      "nan\n",
      "Chappaqua NY and Redlands CA\n",
      "Spokane, WA\n",
      "NYC-LA-MIAMI\n",
      "nan\n",
      "Nigeria\n",
      "Okanagan Valley, BC\n",
      "San Francisco, CA\n",
      "Caribbean\n",
      "nan\n",
      "Canada\n",
      "San Francisco , CA\n",
      "Canada\n",
      "nan\n",
      "Charlotte, NC\n",
      "Orlando, FL\n",
      "USA (Formerly @usNOAAgov)\n",
      "Las Cruces, NM\n",
      "Meereen \n",
      "Football Field\n",
      "i luv raquel\n",
      "nan\n",
      "austin, texas\n",
      "nan\n",
      "Rock Hill, SC\n",
      "Philadelphia\n",
      "Macon, Georgia\n",
      "Ashxjonespr@gmail.com\n",
      "miami\n",
      "Los Angeles\n",
      "Abuja,Nigeria\n",
      "nan\n",
      "Los Angeles, CA\n",
      "At Work\n",
      "nan\n",
      "nan\n",
      "Waialua, Hawaii\n",
      "nan\n",
      "East Coast\n",
      "Coconut Creek, Florida\n",
      "nan\n",
      "icon: cheese3d\n",
      "@notoriousD12\n",
      "new york\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "Morris, IL\n",
      "nan\n",
      "nan\n",
      "Portugal\n",
      "Lynwood, CA\n",
      "Where the money at\n",
      "nan\n",
      "mi\n",
      "Gainesville/Tampa, FL\n",
      "Saint Louis, Missouri\n",
      "nan\n",
      "it's a journey \n",
      "nan\n",
      "Jonesboro, Arkansas USA\n",
      "Layang-Layang, Perak\n",
      "somewhere in Indiana \n",
      "Inside your webcam. Stop that.\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "Financial News and Views\n",
      "R'lyeh, South Pacific\n",
      "taking bath do not disturb\n",
      "Alberta, VA\n",
      "The Howling\n",
      "United Kingdom,Fraserburgh\n",
      "nan\n",
      "nan\n",
      "WorldWideWeb\n",
      "Dreieich, Germany\n",
      "Pembroke NH\n",
      "Bayonne, NJ\n",
      "Colorado Springs\n",
      "Dubai\n",
      "United States\n",
      "nan\n",
      "IG: AyshBanaysh\n",
      "fl\n",
      "nan\n",
      "Halfrica\n",
      "nan\n",
      "nan\n",
      "Jacksonville Beach, FL\n",
      "Melbourne\n",
      "nan\n",
      "San Francisco, CA\n",
      "Richmond Heights, OH\n",
      "New Hampshire\n",
      "U.S.A. - Global Members Site\n",
      "nan\n",
      "India\n",
      "nan\n",
      "Tampa\n",
      "All around the world!\n",
      "In The Mansion\n",
      "nan\n",
      "New York\n",
      "nan\n",
      "USA\n",
      "Numa casa de old yellow bricks\n",
      "Chicago, IL\n",
      "Coventry\n",
      "94123\n",
      "nan\n",
      "nan\n",
      "Hendersonville, NC\n",
      "New Jersey/ D.R.\n",
      "CT & NY\n",
      "nan\n",
      "scandinavia\n",
      "los angeles, ca\n",
      "South Korea GMT+9\n",
      "Baltimore, MD\n",
      "Hughes, AR\n",
      "Madison, WI\n",
      "nan\n",
      "nan\n",
      "University of Chicago\n",
      "Virginia, USA\n",
      "New York\n",
      "yorkshire\n",
      "\n",
      "Coasts of Maine & California\n",
      "nan\n",
      "nan\n",
      "Pittsburgh \n",
      "nan\n",
      "nan\n",
      "Pennsylvania, USA\n",
      "San Diego, California\n",
      "Idaho\n",
      "Beirut, Lebanon\n",
      "nan\n",
      "El Paso, Texas\n",
      "qosqo\n",
      "Lubbock, TX\n",
      "CA via Brum\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "Lizzy's Knee\n",
      "nan\n",
      "nan\n",
      "D(M)V  \n",
      "Marrakech Mรยฉdina, Marrakech - Tensift - Al Haouz\n",
      "Ellensburg to Spokane\n",
      "nan\n",
      "SD |Norway| KSA\n",
      "nan\n",
      "nan\n",
      "A sofa\n",
      "CA via Brum\n",
      "ยรยข5ยรยข12ยรยข14ยรยข | รฅร#SaviourSquadรฅร\n",
      "Pocatello, ID\n",
      "Room 234\n",
      "nan\n",
      "Near Yosemite\n",
      "nan\n",
      "The Harbinger.\n",
      "Atlanta, GA\n",
      "CA via Brum\n",
      "Worldwide\n",
      "Atlanta, GA\n",
      "chicago\n",
      "nan\n",
      "Dutch/English/German\n",
      "ARGENTINA\n",
      "Earth\n",
      "nan\n",
      "Sydney\n",
      "nan\n",
      "nan\n",
      "California, USA\n",
      "oklahoma\n",
      "One World\n",
      "Desde Republica Argentina\n",
      "California, USA\n",
      "Earth\n",
      "Oklahoma City, OK\n",
      "nan\n",
      "Tรรchira - Venezuela\n",
      "California, USA\n",
      "a box\n",
      "Melbourne, Australia\n",
      "Barcelona, Spain\n",
      "Hawaii, USA\n",
      "New Zealand\n",
      "nan\n",
      "Alaska, USA\n",
      "ARGENTINA\n",
      "nan\n",
      "Okuma Town, Fukushima\n",
      "nan\n",
      "rzl ?\n",
      "nan\n",
      "nan\n",
      "Desde Republica Argentina\n",
      "Orm\n",
      "in the Word of God\n",
      "#keepthefaith J&J\n",
      "Global Edition\n",
      "London\n",
      "Seattle, WA\n",
      "world\n",
      "Saline, MI\n",
      "Here.\n",
      "nan\n",
      "nan\n",
      "BOT ACCOUNT\n",
      "nan\n",
      "#otrakansascity\n",
      "Dalkeith, Scotland\n",
      "Durand, MI\n",
      "Naperville\n",
      "nan\n",
      "Forging my Story\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "Mass\n",
      "14/cis/istj \n",
      "nan\n",
      "nan\n",
      "Michigan \n",
      "nan\n",
      "nan\n",
      "Here.\n",
      "where I'm supposed to be\n",
      "Kutztown, PA\n",
      "London\n",
      "Budapest, Hungary\n",
      "Texas, USA\n",
      "CA\n",
      "Houston, TX\n",
      "Cairo, Egypt.\n",
      "South Africa\n",
      "nan\n",
      "nan\n",
      "Karachi Pakistan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "New York\n",
      "Edinburgh\n",
      "HTX\n",
      "nan\n",
      "USA\n",
      "not so cool KY\n",
      "nan\n",
      "North Carolina\n",
      "nan\n",
      "Hampshire UK\n",
      "Oblivion?\n",
      "South West, England\n",
      "nan\n",
      "?????\n",
      "Redondo Beach, CA\n",
      "Newcastle, England\n",
      "nyc\n",
      "nan\n",
      "Planet Eyal, Shandral System\n",
      "Karachi Pakistan\n",
      "nan\n",
      "Mumbai, Maharashtra\n",
      "Atlanta\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "Michigan\n",
      "New York\n",
      "nan\n",
      "New Orleans, LA\n",
      "New York\n",
      "Columbus ?? North Carolina\n",
      "USA\n",
      "Phoenix\n",
      "denmark\n",
      "Davao City\n",
      "Based out of Portland, Oregon\n",
      "Kuwait\n",
      "Wildomar, CA\n",
      "nan\n",
      "buhh\n",
      "New York\n",
      "Indianapolis, IN\n",
      "We are global!\n",
      "nan\n",
      "nan\n",
      "Adelaide\n",
      "Anchorage, AK\n",
      "World\n",
      "nan\n",
      "University of Limerick\n",
      "nan\n",
      "Sacae Plains\n",
      "Melbourne, Australia\n",
      "Southern Maine\n",
      "nan\n",
      "Five down from the Coffeeshop\n",
      "Renfrew, Scotland\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "Kuala Lumpur, Malaysia\n",
      "nan\n",
      "Antioch, CA \n",
      "nan\n",
      "Alexandria, VA, USA\n",
      "Ireland\n",
      "Calgary, AB\n",
      "Reddit\n",
      "Nashville, TN\n",
      "Bakersfield, CA\n",
      "Calgary, Alberta, Canada\n",
      "Somewhere in the Canada\n",
      "Nagpur\n",
      "Los Angeles, CA\n",
      "Surrey & Manchester\n",
      "Calgary,AB, Canada\n",
      "nan\n",
      "Calgary\n",
      "Fort Myers, Florida\n",
      "Atlanta, GA\n",
      "Im In Route \n",
      "Madison, WI\n",
      "Kansas City\n",
      "North Hastings Ontario\n",
      "U.S. Northern Virginia\n",
      "M!$$!$$!PP!\n",
      "Indiana\n",
      "Los Angeles\n",
      "Cochrane, Alberta, Canada\n",
      "Leduc, Alberta, Canada\n",
      "nan\n",
      "Chapel Hill, NC\n",
      "nan\n",
      "204, 555 11 Ave. S.W.\n",
      "Calgary\n",
      "Augusta, GA\n",
      "Dallas, TX\n",
      "nan\n",
      "London\n",
      "nan\n",
      "Torrance, CA\n",
      "USA, Alabama\n",
      "Olympia, WA\n",
      "London, UK\n",
      "nan\n",
      "nan\n",
      "CA, AZ & NV\n",
      "British Columbia, Canada\n",
      "Auckland\n",
      "nan\n",
      "Orange County, NY\n",
      "Henderson, Nevada\n",
      "Los Angeles, CA\n",
      "Vancouver, British Columbia\n",
      "London, England\n",
      "USA\n",
      "Nevada, USA\n",
      "nan\n",
      "Alaska\n",
      "Whippany, NJ\n",
      "Seattle, WA\n",
      "Sydney, New South Wales\n",
      "Olympia, WA\n",
      "Kodiak, AK\n",
      "Los Angeles, CA\n",
      "?????\n",
      "Sydney\n",
      "Birmingham, England\n",
      "Park Ridge, Illinois\n",
      "Los Angeles, CA\n",
      "Fredonia,NY\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "Glendale, CA\n",
      "nan\n",
      "Kuwait \n",
      "nan\n",
      "Fleet/Oxford, UK\n",
      "nan\n",
      "UK\n",
      "Bahrain\n",
      "Coventry\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "Newcastle\n",
      "@ ForSL/RP\n",
      "Rochester, NY\n",
      "nan\n",
      "California\n",
      "nan\n",
      "Nevada (wishing for Colorado)\n",
      "Kokomo, In\n",
      "nan\n",
      "london\n",
      "nan\n",
      "USA\n",
      "Kenosha, WI 53143\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "Coventry\n",
      "London\n",
      "nan\n",
      "London/Surrey \n",
      "nan\n",
      "Charlotte, NC\n",
      "San Diego\n",
      "Charlotte, NC\n",
      "Lagos\n",
      "CLT\n",
      "Africa\n",
      "San Diego\n",
      "nan\n",
      "nan\n",
      "Cydia\n",
      "nan\n",
      "nan\n",
      "Tallahassee Florida\n",
      "nan\n",
      "New England\n",
      "Waterloo, Ont\n",
      "nan\n",
      "nan\n",
      "Worldwide\n",
      "17-Feb\n",
      "Brisbane\n",
      "New Britain, CT\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "Chevy Chase, MD\n",
      "LA - everywhere\n",
      "wrapped arnd hyuk's finger\n",
      "Sevier County.\n",
      "nan\n",
      "Rochester\n",
      "nan\n",
      "Gold Coast, Qld, Australia\n",
      "nan\n",
      "nan\n",
      "London UK\n",
      "Nashville\n",
      "FLYEST HIPPIE YOU KNOW \n",
      "U.S.A\n",
      "An eight-sided polygon\n",
      "nan\n",
      "Konoha Village\n",
      "St. Catharines, Ontario\n",
      "//??//\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FSC '19\n",
      "ยรยข901ยรยข\n",
      "London, England\n",
      "nan\n",
      "nan\n",
      "Chicago, IL\n",
      "Gold Coast\n",
      "Portland, Oregon\n",
      "Portland, Ore. \n",
      "Florida\n",
      "nan\n",
      "North West England UK\n",
      "Denver, Colorado\n",
      "Manchester, UK\n",
      "Midwest\n",
      "Manchester UK\n",
      "TV5, Philippines\n",
      "Hackney, London\n",
      "Nashville, TN\n",
      "nan\n",
      "Queensland, Australia\n",
      "nan\n",
      "Harpurhey, Manchester, UK\n",
      "Portland, Oregon\n",
      "Benton City, Washington\n",
      "Manchester\n",
      "Chicago, but Philly is home\n",
      "nan\n",
      "nan\n",
      "Breaking News\n",
      "seoul\n",
      "nan\n",
      "WA State\n",
      "Hensley Street, Portland\n",
      "nan\n",
      "MIchigan\n",
      "West\n",
      "nan\n",
      "Everywhere\n",
      "Gold Coast, Australia\n",
      "nan\n",
      "Portland, Oregon\n",
      "nan\n",
      "Washington\n",
      "Eureka, California, USA\n",
      "Lagos, Nigeria\n",
      "nan\n",
      "nan\n",
      "Sydney Australia\n",
      "Bend, Oregon\n",
      "Bend, Oregon\n",
      "nan\n",
      "Chevy Chase, MD\n",
      "Na:tinixw / Hoopa, Berkeley\n",
      "USA\n",
      "Minna, Nigeria\n",
      "EIU  Chucktown/LaSalle IL\n",
      "UK\n",
      "Northern California U.S.A.\n",
      "Yellowknife, NT\n",
      "Bend, Oregon\n",
      "Renfrew, Scotland\n",
      "sydney, australia\n",
      "United States\n",
      "รรT: 43.631838,-79.55807\n",
      "Yellowknife\n",
      "US: 44.414510,8.942499\n",
      "Tri-Cities, Wash.\n",
      "Bend, Oregon\n",
      "Portland, Ore. \n",
      "nan\n",
      "nan\n",
      "The Empire/First Order\n",
      "Brisbane, Queensland\n",
      "USA\n",
      "Moncton, New Brunswick\n",
      " |IG: imaginedragoner\n",
      "nan\n",
      "nan\n",
      "London / Berlin / Online\n",
      "Williamsburg, VA\n",
      "nan\n",
      "emily | helen | shelley \n",
      "New Orleans, Louisiana\n",
      "Yamaku Academy, Class 3-4\n",
      "nan\n",
      "Australia\n",
      "Spring Grove, IL\n",
      "The Windy City\n",
      "nan\n",
      "Washington, D.C.\n",
      "Oklahoma City, OK\n",
      "Bloomington, IN\n",
      "nan\n",
      "Yamaku Academy, Class 3-4\n",
      "United States\n",
      "Whitby, ON\n",
      "nan\n",
      "Kajang ? UiTM Puncak Alam\n",
      "Dallas, TX\n",
      "nan\n",
      "nan\n",
      "? Philly Baby ?\n",
      "Cleveland, TN\n",
      "they/them\n",
      "nan\n",
      "my deli\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "Winnipeg\n",
      "New Hampshire\n",
      "nan\n",
      "nan\n",
      "Trost District\n",
      "?????? in Yokohama Japan\n",
      "London\n",
      "LFC x GSW\n",
      "nan\n",
      "nan\n",
      "Chicago, IL\n",
      "Elmwood Park, NJ\n",
      "USA\n",
      "nan\n",
      "Australia\n",
      "South east of U.K\n",
      "nan\n",
      "nan\n",
      "USA\n",
      "nan\n",
      "nan\n",
      "lrhcthband;four - bournemouth\n",
      "Oakland, Ca\n",
      "Antigua ?? NYC \n",
      "Jamaica\n",
      "zboyer@washingtontimes.com\n",
      "nan\n",
      "Wonderlandยรร ?????? ???? ??????\n",
      "nan\n",
      "nan\n",
      "elizabeth king\n",
      "Germany\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "New York\n",
      "London, UK\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "London, United Kingdom\n",
      "Germany\n",
      "nan\n",
      "Chicago Heights, IL\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "S.F. Bay area\n",
      "nan\n",
      " Blood Indian Reserve\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "Columbus, OH\n",
      "Chicago, IL\n",
      "On the toilet having a dump \n",
      "nan\n",
      "City of London, London\n",
      "nan\n",
      "nan\n",
      "New York\n",
      "New York\n",
      "nan\n",
      "nan\n",
      "Florida\n",
      "Long Island, NY\n",
      "nan\n",
      "nan\n",
      "New York\n",
      "Palestine Texas\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "Kauai, Hawaii\n",
      "nan\n",
      "East London. \n",
      "nan\n",
      "Rhode Island\n",
      "Jammu and Kashmir\n",
      "canada\n",
      "nan\n",
      "United States\n",
      "nan\n",
      "Terlingua, Texas\n",
      "Chicago, IL\n",
      "Somewhere in Jersey\n",
      "Stay Fly?\n",
      "nan\n",
      "nan\n",
      "india\n",
      "Philippines\n",
      "Orlando, Fl\n",
      "Bakersfield, California\n",
      "Washington, DC\n",
      "Los Angeles... CA... USA\n",
      "nan\n",
      "nan\n",
      "UK\n",
      "nan\n",
      "Pennsylvania\n",
      "nan\n",
      "South Africa\n",
      "nan\n",
      "Orlando, Fl\n",
      "New York, NY\n",
      "South Africa\n",
      "nan\n",
      "Florida but I wanna be n Texas\n",
      "New York, USA\n",
      "Kyiv, Ukraine\n",
      "nan\n",
      "nan\n",
      "JamDung\n",
      "Panamรร \n",
      "nan\n",
      "nan\n",
      "Washington, D.C.\n",
      "Earth, Milky Way, Universe\n",
      "nan\n",
      "nan\n",
      "Edappally,Kochi\n",
      "Texas\n",
      "In #Fairie, where else? ;-)\n",
      "nan\n",
      "Massachusetts\n",
      "nan\n",
      "Ukraine\n",
      "nan\n",
      "???? ???????\n",
      "nan\n",
      "United States\n",
      "Ireland\n",
      "San Francisco\n",
      "nan\n",
      "Rogersville, MO\n",
      "Tampa, FL\n",
      "nan\n",
      "Ukraine\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "Telangana\n",
      "Chatham, IL\n",
      "nan\n",
      "Charter Member of the VRWC\n",
      "nan\n",
      "Wood Buffalo, Alberta\n",
      "รรT: 10.614817868480726,12.195582811791382\n",
      "nan\n",
      "Anchorage, AK\n",
      "Nakhon Si Thammarat\n",
      "Somewhere with Clyde\n",
      "Buffalo, NY\n",
      "Quincy\n",
      "Winnipeg\n",
      "Fresno, CA\n",
      "Varanasi\n",
      "Laredo, TX\n",
      "Thane\n",
      "New South Wales, Australia\n",
      "nan\n",
      "Poconos\n",
      "Sacramento, CA\n",
      "Haddonfield, NJ\n",
      "nan\n",
      "Southern California\n",
      "nan\n",
      "nan\n",
      "Jaipur, Rajasthan, India\n",
      "??? ???? ??????\n",
      "nan\n",
      "Bangalore\n",
      "nan\n",
      "Also follow ?\n",
      "nan\n",
      "Play For Ryan ??\n",
      "nan\n",
      "Dimapur\n",
      "nan\n",
      "In the potters hands\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "New York\n",
      "New York City ,NY\n",
      "nan\n",
      "Corpus Christi, Texas\n",
      "Cape Town\n",
      "nan\n",
      "Wolverhampton/Brum/Jersey\n",
      "Chamblee, Georgia\n",
      "Official Website\n",
      "#WashingtonState #Seattle\n",
      "TechFish \n",
      "Washington, DC & Charlotte, NC\n",
      "nan\n",
      "United States where it's warm\n",
      "nan\n",
      "Washington, DC\n",
      "Hope Road, Jamaica \n",
      "jersey \n",
      "nan\n",
      "WestEnd, Puritan Ave \n",
      "nan\n",
      "Youngstown, OH\n",
      "nan\n",
      "Nantes, France\n",
      "St. John's, NL, Canada\n",
      "Caserta-Roma, Italy \n",
      "PekanbaruรฅยกBatam IslandรฅยกMedan\n",
      "oman muscat al seeb \n",
      "San Francisco\n",
      "Wisconsin\n",
      "nan\n",
      "Jogja, Indonesia Slowly Asia\n",
      "Vancouver BC\n",
      "nan\n",
      "USA\n",
      "nan\n",
      "Avon, OH\n",
      "nan\n",
      "The North\n",
      "Philadelphia, PA\n",
      "Just Happy to Be Anywhere\n",
      "nan\n",
      "Lima, Ohio\n",
      "Ireland\n",
      "Lowell, MA\n",
      "playing soccer & eating pizza\n",
      "nan\n",
      "NY\n",
      "Hueco Mundo\n",
      "nan\n",
      "Nairobi\n",
      "Boston ยรยข Cape Cod ?\n",
      "nan\n",
      "Rafael castillo\n",
      "nan\n",
      "New Jersey\n",
      "Fort Wayne, IN\n",
      "Enterprise, NV\n",
      "nan\n",
      "nan\n",
      "434\n",
      "Grand Rapids MI\n",
      "U.S.A\n",
      "434\n",
      "New York \n",
      "Utica NY\n",
      "nan\n",
      "nan\n",
      "Nunya\n",
      "Houston, TX  \n",
      "USA\n",
      "nan\n",
      "TX\n",
      "Christiana,Tennessee\n",
      "Nashville, TN\n",
      "Bishops Stortford, England\n",
      "Earth \n",
      "Brentwood, NY\n",
      "nan\n",
      "Honduras\n",
      "nan\n",
      "Chippenham/Bath, UK\n",
      "nan\n",
      "Yewa zone\n",
      "sitting on the fence, New York\n",
      "nan\n",
      "nan\n",
      "Carregado\n",
      "Largo, MD\n",
      "nan\n",
      "mexico\n",
      "Cape Town, Khayelitsha\n",
      "Florida\n",
      "Indianapolis, IN\n",
      "nan\n",
      "nan\n",
      "Stanford University\n",
      "Midwestern USA\n",
      "Bedford IN \n",
      "nan\n",
      "Thibodaux, LA\n",
      "nan\n",
      "Brazil\n",
      "In the moment\n",
      "nan\n",
      "nan\n",
      "Alliston Ontario\n",
      "nan\n",
      "SaudI arabia - riyadh \n",
      "New York. NY\n",
      "Athens - Nicosia\n",
      "nan\n",
      "Southern California\n",
      "Bremerton, WA\n",
      "Tulalip, Washington\n",
      "Indonesia\n",
      "Halifax\n",
      "Hilton Head, SC  \n",
      "New York\n",
      "St. Louis, MO\n",
      "USA\n",
      "nan\n",
      "Sรยฃo Paulo\n",
      "nan\n",
      "nan\n",
      "2 high 2 come down \n",
      "nan\n",
      "Wrex\n",
      "nan\n",
      "big boy ยรยข 0802\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "Brooklyn\n",
      "Canada\n",
      "To The Right of You!\n",
      "St.Cloud, MN\n",
      "nan\n",
      "nan\n",
      "on a catwalk somewhere\n",
      "  รฅ_ \n",
      "Zac Newsome loves me\n",
      "louisville, kentucky\n",
      "nan\n",
      "nan\n",
      "Makai\n",
      "Fort Valley,GA/Fayetteville,AR\n",
      "Arizona\n",
      "nan\n",
      "Vancouver, BC\n",
      "Newark, NJ\n",
      "the road to success\n",
      "The Desert\n",
      "Sydney, Australia\n",
      "Yuba City, CA\n",
      "nan\n",
      "University of South Florida\n",
      "nan\n",
      "the moon\n",
      "Niagara Falls, Ontario\n",
      "Woodcreek HS, Roseville, CA\n",
      "United States\n",
      "nan\n",
      "nan\n",
      "Here & There\n",
      "District 12 - Orange County\n",
      "Plain O' Texas\n",
      "SouthEast Asia\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "Saipan, CNMI\n",
      "Orange County, Calif.\n",
      "lost in my thoughts\n",
      "nan\n",
      "Five down from the Coffeeshop\n",
      "Los Angeles\n",
      "Orange County, CA\n",
      "Five down from the Coffeeshop\n",
      "nan\n",
      "Tallahassee, FL\n",
      "nan\n",
      "Laguna Beach, Calif. \n",
      " Nevada Carson City,Freeman St\n",
      "nan\n",
      "LA ??\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "Aracaju - Sergipe\n",
      "nan\n",
      "Nashville, Tennessee\n",
      "Texas\n",
      "Long Eaton รฅรก Derbyshire รฅรก UK\n",
      "WorldWide\n",
      "nan\n",
      "Portage, IN / Worldwide\n",
      "Victoria, BC  Canada\n",
      "nan\n",
      "Roads/Trails Everywhere\n",
      "New York City\n",
      "Nashville, TN\n",
      "nan\n",
      "Liberty Township, Ohio\n",
      "Aberdeenshire\n",
      "Los Angeles, CA\n",
      "A Hoop Somewhere\n",
      "Michigan\n",
      "Bloomington, Indiana\n",
      "USA\n",
      "nan\n",
      "Washington, DC\n",
      "nan\n",
      "Basking Ridge, NJ\n",
      "Stalybridge, Tameside\n",
      "nan\n",
      "Sacramento, CA\n",
      "Franklin, TN near Nashville\n",
      "St. Louis, Missouri\n",
      "Bandung\n",
      "New York\n",
      "New York\n",
      "Louisville, KY \n",
      "nan\n",
      "Manhattan, NY\n",
      "Freeport Ny\n",
      "650/559\n",
      "nan\n",
      "dreamy lake\n",
      "nan\n",
      "houstn\n",
      "hell\n",
      "Asheville, NC\n",
      "New York\n",
      "St. Patrick's Purgatory\n",
      "nan\n",
      "@cockerelshoes\n",
      "United States\n",
      "manaus\n",
      "Sioux Falls, S.D. \n",
      "nan\n",
      "Republica Dominicana\n",
      "Eastlake, OH\n",
      "Hillsville/Lynchburg, VA\n",
      "U.S.A\n",
      "Some where\n",
      "Travelling around the world \n",
      " 616 ยรยข Kentwood , MI \n",
      "nan\n",
      "nan\n",
      "? Jet Life ?\n",
      "Fairy Tail! \n",
      "Santo Domingo Alma Rosa \n",
      "Somewhere Around You\n",
      "Houston |??| Corsicana\n",
      "Cincinnati, OH\n",
      "Bacon\n",
      "nan\n",
      "Some other mansion\n",
      "Chicago\n",
      "Astley, Manchester\n",
      "Bow, NH\n",
      "Leicester\n",
      "Frome, Somerset, England\n",
      "Mars\n",
      "Melbourne, Victoria\n",
      "UK\n",
      "Essex, England\n",
      "new york, ny\n",
      "somewhere too cold for me\n",
      "Barbados\n",
      "nan\n",
      "Leeds\n",
      "Wahpeton, ND\n",
      "nan\n",
      "Chorley, Lancashire, UK\n",
      "nan\n",
      "nan\n",
      "Northampton, MA\n",
      "Ventura\n",
      "Le Memenet\n",
      "Lubbock, Texas\n",
      "Derby\n",
      "Mississauga, Ontario\n",
      "Delhi \n",
      "Keighley, England\n",
      "Pomfret/Providence\n",
      "Baltimore, MD\n",
      "nan\n",
      "East Lansing, MI\n",
      "Rocketing through the galaxy\n",
      "New York\n",
      "boston\n",
      "United States\n",
      "New York\n",
      "New York\n",
      "Warrandyte, Australia\n",
      "British Columbia, Canada\n",
      "nan\n",
      "nan\n",
      "Aix-en-Provence/Utrecht\n",
      "Philadelphia\n",
      "New York\n",
      "Van Buren, MO\n",
      "New York\n",
      "New York\n",
      "Niagara Falls, Ontario\n",
      "USA\n",
      "nan\n",
      "Hawaii USA\n",
      "New York\n",
      "New York\n",
      "nan\n",
      "nan\n",
      "New York\n",
      "USA\n",
      "United States\n",
      "New York\n",
      "New York\n",
      "New York\n",
      "United States\n",
      "nan\n",
      "New York\n",
      "USA\n",
      "Waddesdon\n",
      "Knoxville, TN\n",
      "New York\n",
      "Rock Springs, WY\n",
      "Vietnam\n",
      "Tampa-St. Petersburg, FL\n",
      "nan\n",
      "USA\n",
      "Huntsville, Alabama\n",
      "Nova Scotia, Canada\n",
      "nan\n",
      "Metro Manila\n",
      "nan\n",
      "Breaking News\n",
      "Jakarta/Kuala Lumpur/S'pore\n",
      "UK\n",
      "nan\n",
      "Vietnam\n",
      "nan\n",
      "I ACCEPT SONG REQUESTS\n",
      "Honolulu, Hawaii\n",
      "nan\n",
      "Miami Beach, Fl\n",
      "Republic of Texas\n",
      "California, USA\n",
      "Huntsville AL\n",
      "? \n",
      "don't run\n",
      "West Midlands\n",
      "Jakarta/Kuala Lumpur/S'pore\n",
      "nan\n",
      "Global-NoLocation\n",
      "nan\n",
      "Jakarta/Kuala Lumpur/S'pore\n",
      "Ocean City, NJ\n",
      "nan\n",
      "belleville\n",
      "Kualar Lumpur, Malaysia\n",
      "Austin, TX\n",
      "nan\n",
      "Nadiad ,Gujarat , India!!\n",
      "South of D.C.\n",
      "nan\n",
      "Adventist - Lesson Sabbath\n",
      "Australia\n",
      "nan\n",
      "Passamaquoddy\n",
      "ยรยขIII.XII.MMXIยรยข\n",
      "Sherwood, Brisbane, Australia\n",
      "canada\n",
      "Michigan\n",
      "london\n",
      "nan\n",
      "California\n",
      "Global-NoLocation\n",
      "nan\n",
      "Estados Unidos\n",
      "New Jersey\n",
      "New York, NY\n",
      "Timaru District, New Zealand\n",
      "Cameroon\n",
      "St Austell, Cornwall\n",
      "Ogba, Lagos, Nigeria\n",
      "Philippines\n",
      "nan\n",
      "North America\n",
      "Earth\n",
      "nan\n",
      "nan\n",
      "New York, USA\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "21.462446,-158.022017\n",
      "Rio de Janeiro\n",
      "nan\n",
      "nan\n",
      "PDX\n",
      "Shelby County\n",
      "nan\n",
      "port matilda pa\n",
      "Concord, N.C.\n",
      "Redding, California, USA\n",
      "nan\n",
      "nan\n",
      "Slatina,Romania\n",
      "Redding, California, USA\n",
      "nan\n",
      "Asheville, NC\n",
      "nan\n",
      "nan\n",
      "Los Angeles for now\n",
      "????????????\n",
      "Western Washington\n",
      "Atlanta\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "alberta, canada\n",
      "Washington, D.C.\n",
      "nan\n",
      "nan\n",
      "Montana \n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "San Francisco\n",
      "Cape Cod\n",
      "nan\n",
      "Nicola Valley\n",
      "Tips on my blog at\n",
      "Boise, Idaho\n",
      "nan\n",
      "nan\n",
      "Duncan\n",
      "nan\n",
      "Texas \n",
      "nan\n",
      "nan\n",
      "Based in CA - Serve Nationwide\n",
      "Portland, Oregon\n",
      "Vancouver, BC\n",
      "Lansdale,Pennsylvania\n",
      "nan\n",
      "nan\n",
      "Sacramento, CA\n",
      "Carol Stream, Illinois\n",
      "nan\n",
      "nan\n",
      "Calgary/Airdrie/RedDeer/AB\n",
      "United States\n",
      "Hattiesburg, MS\n",
      "Arkansas\n",
      "United States\n",
      "nan\n",
      "Dagenham, Essex\n",
      "Inside the Beltway (DC Area)\n",
      "nan\n",
      "USA\n",
      "Florida, USA\n",
      "Windsor,Ontario\n",
      "nan\n",
      "Brasil, Fortaleza ce\n",
      "nan\n",
      "Paradise City\n",
      "Rapid City, South Dakota\n",
      "antoine fisher \n",
      "Between Dire and Radiant\n",
      "nan\n",
      "Calgary, AB\n",
      "Calgary\n",
      "nan\n",
      "Indianapolis, IN\n",
      "nan\n",
      "United States\n",
      "Bay Area, CA\n",
      "Las Vegas, Nevada\n",
      "Global\n",
      "Watch Those Videos -\n",
      "Tulsa, Oklahoma\n",
      "United States\n",
      "Calgary, AB\n",
      "USA\n",
      "Ab, Canada\n",
      "Newton Centre, Massachusetts\n",
      "Kicking Horse Pass\n",
      "nan\n",
      "far away\n",
      "Calgary, Alberta\n",
      "nan\n",
      "Calgary\n",
      "Suginami-ku, Tokyo, Japan\n",
      "Washington State\n",
      "Somewhere \n",
      "Massachusetts\n",
      "Heaven\n",
      "Calgary, Alberta\n",
      "nan\n",
      "nan\n",
      "Calgary, Canada\n",
      "Virgo Supercluster\n",
      "nan\n",
      "facebook.com/tradcatknights\n",
      "Wyoming, MI (Grand Rapids)\n",
      "Iliff,Colorado  \n",
      "Dicky Beach\n",
      "nan\n",
      "far away\n",
      "Not Los Angeles, Not New York.\n",
      "Heaven\n",
      "nan\n",
      "Calgary, Alberta\n",
      "Calgary, AB, Canada\n",
      "Global\n",
      "Massachusetts, USA\n",
      "nan\n",
      "nan\n",
      " Queensland, Australia\n",
      "Kansas City\n",
      "Cleveland, OH - San Diego, CA\n",
      "nan\n",
      "The barn\n",
      "where the wild things are\n",
      "?semekeepschanging@soyeh?\n",
      "Kansas City\n",
      "nan\n",
      "Buffalo, NY\n",
      "nan\n",
      "i love you zayn\n",
      "Gotham City\n",
      "Seattle, WA\n",
      "WORLD WIDE\n",
      "nan\n",
      "nan\n",
      "TX\n",
      "England \n",
      "Sumter, SC\n",
      "Someday I'll live in England. \n",
      "Hogsmeade\n",
      "5/5 access / rt link please x\n",
      "รฅ_: ?? รร ? : ?\n",
      "10-Jul\n",
      "nan\n",
      "Kansas City\n",
      "The South & WestCoast \n",
      "Hogwarts\n",
      "va\n",
      "nan\n",
      "Portland, OR\n",
      "nan\n",
      "?@symbolicjensen?\n",
      "nan\n",
      "Los Angeles, California\n",
      "Contoocook Valley Region of Ne\n",
      "nan\n",
      "Australia\n",
      "nan\n",
      "Alameda, CA\n",
      "London, England\n",
      "Arizona\n",
      "Lancashire, United Kingdom\n",
      "a van down by the river\n",
      "nan\n",
      "NIFC\n",
      "Giddy, Greenland\n",
      "nan\n",
      "nan\n",
      "Chicago\n",
      "Roppongi, Minato, Tokyo \n",
      "nan\n",
      "nan\n",
      "Pleasanton, CA\n",
      "nan\n",
      "Portsmouth, VA\n",
      "scandinavia\n",
      "nan\n",
      "Kenya\n",
      "Chicago, IL\n",
      "Dil's Campsite\n",
      "Massachusetts\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "UK\n",
      "nan\n",
      "ilford\n",
      "nan\n",
      "Nashville, Tn\n",
      "SWinfo@dot.state.al.us\n",
      "USA\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "United States\n",
      "Far Away From Home\n",
      "nan\n",
      "nan\n",
      "British Columbia, Canada\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "Gainesville, FL\n",
      "United States\n",
      "Muntinlupa City, Philippines\n",
      "United States\n",
      "Muntinlupa City, Philippines\n",
      "nan\n",
      "Oregon, USA\n",
      "nan\n",
      "nan\n",
      "Memphis, TN\n",
      "nan\n",
      "???\n",
      "Mysore, Karnataka\n",
      "nan\n",
      "nan\n",
      "Portland, OR\n",
      "Canada\n",
      "New Delhi, Delhi\n",
      "nan\n",
      "United States\n",
      "British Columbia, Canada\n",
      "Fort Worth,  Texas \n",
      "nan\n",
      "USA\n",
      "Greenfield, Massachusetts\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "Karachi, Pakistan\n",
      "London, Riyadh\n",
      "nan\n",
      "Crayford, London\n",
      "Arnhem, the Netherlands\n",
      "Pacific Northwest\n",
      "Somewhere in Spain\n",
      "nan\n",
      "Jackson, MS\n",
      "State College, PA\n",
      "Frisco, TX\n",
      "nan\n",
      "Malaysia/Jordan\n",
      "nan\n",
      "Somewhere out there\n",
      "nan\n",
      "USA\n",
      "International Action\n",
      "nan\n",
      "Oklahoma City, OK\n",
      "Planet of da Bathing Apes\n",
      "Maricopa, AZ\n",
      "liverpool \n",
      "Albuquerque New Mexico\n",
      "nan\n",
      "New York Brooklyn\n",
      "Nebraska, Colorado & The GLOBE\n",
      "Rheinbach / Germany\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "Riyadh\n",
      "UK\n",
      "jeddah | Khartoum\n",
      "United Hoods of the Globe\n",
      "Jubail IC, Saudi Arabia.\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "Colorado\n",
      "nan\n",
      "nan\n",
      "Denver, Colorado\n",
      "?????? ??? ?????? ????????\n",
      "Right next to Compton\n",
      "11th dimension, los angeles\n",
      "Denver, Colorado\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "570 Vanderbilt; Brooklyn, NY\n",
      "??????????? ???????????..? \n",
      "nan\n",
      "nan\n",
      "Denver, Colorado\n",
      "?????? ???? ??????\n",
      "nan\n",
      "Silvermoon or Ironforge\n",
      "Charleston, WV\n",
      "Denver, Colorado\n",
      "Colorado\n",
      "nan\n",
      "Benicia, CA \n",
      "Riyadh ')\n",
      "Houston TX\n",
      "Innerhalb der Lร_cke\n",
      "nan\n",
      "New York, NY\n",
      "Kansas City, MO\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "Nigeria\n",
      "Nigeria \n",
      "nan\n",
      "Port Harcourt, Nigeria\n",
      "eating strawberry shitsickles\n",
      "Lagos Nigeria\n",
      "nan\n",
      "Nigeria\n",
      "nan\n",
      "Maryland\n",
      "nan\n",
      "Lagos, Nigeria\n",
      "nan\n",
      "nan\n",
      "Near Richmond, VA\n",
      "Nigeria\n",
      "Worldwide\n",
      "Nigeria\n",
      "Kolkata\n",
      "am everywhere\n",
      "nan\n",
      "Miami, Florida\n",
      "nan\n",
      "NIGERIA\n",
      "Oregon, USA\n",
      "nan\n",
      "San Francisco\n",
      "San Francisco Bay Area\n",
      "Louisville, KY\n",
      "Southern California\n",
      "Sarasota, FL\n",
      "nan\n",
      "nan\n",
      "Washington state\n",
      "nan\n",
      "Fort Collins, CO\n",
      "Halifax\n",
      "nan\n",
      "USA\n",
      "nan\n",
      "nan\n",
      "West Chester, PA\n",
      "nan\n",
      "nan\n",
      "Sacramento, CA\n",
      "nan\n",
      "Free State, South Africa\n",
      "California \n",
      "nan\n",
      "California \n",
      "worldwide\n",
      "Fresno, California\n",
      "worldwide\n",
      "RSN: Tru\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "El Paso, TX\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "South Pasadena, CA\n",
      "perth, australia \n",
      "Mongolia\n",
      "brisbane, australia\n",
      "China\n",
      "World\n",
      "Chiyoda Ward, Tokyo\n",
      "rome\n",
      "Athens,Greece\n",
      "EastCarolina\n",
      "Brazil\n",
      "IN our hearts  Earth Global \n",
      "nan\n",
      "nan\n",
      "France\n",
      "nan\n",
      "nan\n",
      "tokyo\n",
      "china\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "Brazil\n",
      "between ideas & 3-5pm AEST\n",
      "nan\n",
      "Japan\n",
      "Malaysia\n",
      "Zimbabwe\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "Vancouver, British Columbia\n",
      "nan\n",
      "Starling City\n",
      "The Great State of Maine \n",
      "nan\n",
      "Glenview to Knoxville \n",
      "Indiana\n",
      "nan\n",
      "Roaming around the world\n",
      "nan\n",
      "nan\n",
      "????\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n",
      "Cape Neddick, ME\n",
      "Global\n",
      "Mariveles, Bataan\n",
      "รรT: 40.562796,-75.488849\n",
      "Roaming around the world\n",
      "nan\n",
      "Melbourne, FL\n",
      "New Chicago\n",
      "nan\n",
      "nan\n",
      "San Francisco Bay Area\n",
      "Australia \n",
      "nan\n",
      "Victorville, CA\n",
      "Washington D.C.\n",
      "nan\n",
      " Eugene, Oregon\n",
      "THANJAVUR\n",
      "Japan\n",
      "Las Vegas, NV\n",
      "nan\n",
      "Cumming, GA\n",
      "westwestwestwestwestwestwest\n",
      "nan\n",
      "Midwest\n",
      "NYC metro\n",
      "Tennessee\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "Heathrow\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "OK\n",
      "nan\n",
      "Tampa, Fl\n",
      "TonyJ@Centralizedhockey.com\n",
      "Cleveland, OH\n",
      "Pune, Maharashtra\n",
      "cuba\n",
      "china\n",
      "Rocky Mountains\n",
      "nan\n",
      "Germany\n",
      "nan\n",
      "Brazos Valley, Texas\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "EastCarolina\n",
      "china\n",
      "The Universe\n",
      "nan\n",
      "nan\n",
      "#BlackLivesMatter\n",
      " Miami Beach\n",
      "#1 Vacation Destination,HAWAII\n",
      "nan\n",
      "Haiku, Maui, Hawaii\n",
      "nan\n",
      "nan\n",
      "NAWF SIDE POKING OUT \n",
      "New York\n",
      "Berlin - Germany\n",
      "Vineyard\n",
      "nan\n",
      "nan\n",
      "@potteratthedisc\n",
      "Anderson, SC\n",
      "The Epicenter, and Beyond\n",
      "Books Published, USA\n",
      "nan\n",
      "??? ??? ????? ??? ???.\n",
      "nan\n",
      "Somewhere Powerbraking A Chevy\n",
      "Mexico City\n",
      "nan\n",
      "New York\n",
      "Berlin - Germany\n",
      "USA\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "The Globe\n",
      "Chicopee MA\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "NYC\n",
      "nan\n",
      "Srinagar,Kashmir\n",
      "DFW, Texas\n",
      "Piedmont Triad, NC\n",
      "MD\n",
      "Florida\n",
      "Paterson, New Jersey \n",
      "Kolkata\n",
      "nan\n",
      "nan\n",
      " Tropical SE FLorida\n",
      "nan\n",
      "nan\n",
      "Worldwide\n",
      "Ikorodu\n",
      "Lucknow, India\n",
      "london\n",
      "Sacramento, CA\n",
      "nan\n",
      "Doghouse\n",
      "nan\n",
      "Mumbai\n",
      "someplace living my life\n",
      "USA\n",
      "nan\n",
      "Nigeria\n",
      "Bronx NY\n",
      "World Wide Web\n",
      "Centurion \n",
      "nan\n",
      "nan\n",
      "nan\n",
      "India\n",
      "Bronx NY\n",
      "Avon\n",
      "India\n",
      "??\n",
      "Orlando,FL  USA\n",
      "North London\n",
      "come here in 20 minutes for an ass kicking\n",
      "Emirates\n",
      "Milton Keynes, England\n",
      "Scottsdale. AZ\n",
      "Tennessee\n",
      "nan\n",
      "Saskatchewan, Canada\n",
      "nan\n",
      "Toronto\n",
      "North West London\n",
      "nan\n",
      "Rockland County, NY\n",
      "Atlantic Highlands, NJ\n",
      "Sutton, London UK\n",
      "Alameda and Pleasanton, CA\n",
      "nan\n",
      "Madison, WI & St. Louis MO\n",
      "Bay Area\n",
      "California or Colorado\n",
      "Mesa, AZ\n",
      "Carterville\n",
      "Corpus - Las Vegas - Houston\n",
      "Spring Tx\n",
      "Toronto\n",
      "Georgia, U.S.A.\n",
      "Moncton, New Brunswick\n",
      "nan\n",
      "nan\n",
      "Riverview, FL \n",
      "California\n",
      "nan\n",
      "beijing .China\n",
      "nan\n",
      "nan\n",
      "Somewhere in China.\n",
      "Houston, TX\n",
      "Sacramento, CA\n",
      "Peru\n",
      "nan\n",
      "Los Angeles, California\n",
      "nan\n",
      "Cambridge, Massachusetts\n",
      "nan\n",
      "Lahore\n",
      "Saint Paul\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "Los Angeles\n",
      "Shah Alam,Malaysia\n",
      "Dubai, UAE\n",
      "รรT: 35.223347,-80.827834\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "Plano, Texas\n",
      "mnl\n",
      "nan\n",
      "nan\n",
      "Dallas, TX\n",
      "Louisiana\n",
      "Nairobi , Kenya\n",
      "California\n",
      "Dallas, TX\n",
      "nan\n",
      "Russia\n",
      "nan\n",
      "Baltimore\n",
      "Ireland\n",
      "England & Wales Border, UK\n",
      "Zeerust, South Africa\n",
      "Seattle\n",
      "Boston, Massachusetts\n",
      "Bristol\n",
      "Ireland\n",
      "The Main \n",
      "The windy plains of Denver\n",
      "Sunbury, Ohio\n",
      "Sunnyvale, CA\n",
      "Land of Lincoln\n",
      "nan\n",
      "Kenya\n",
      "Australia\n",
      "nan\n",
      "NYC&NJ\n",
      "swindon\n",
      "Croydon\n",
      "Asia\n",
      "Coventry\n",
      "nan\n",
      "Toronto, ON, Canada\n",
      "Singapore\n",
      "Manchester, England\n",
      "San Jose, CA, USA\n",
      "UK, Republic of Ireland and Australia\n",
      "Milton keynes\n",
      "Pontefract UK\n",
      "the Dirty D\n",
      "United States\n",
      "nan\n",
      "Paducah, KY\n",
      "Maryland\n",
      "nan\n",
      "Asia European Continent Korea \n",
      "nan\n",
      "nan\n",
      "Athens, Greece\n",
      "nan\n",
      "Proudly Canadian!\n",
      "Cascadia\n",
      "County Durham, United Kingdom\n",
      "nan\n",
      "nan\n",
      "Melbourne, Australia\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "Roanoke, VA\n",
      "Dundee, UK\n",
      "Texas\n",
      "New York City\n",
      "Austin, Texas\n",
      "London\n",
      "Detroit Tigers Dugout\n",
      "USA\n",
      "st.louis county missouri \n",
      "Europe\n",
      "Scotland\n",
      "Detroit/Windsor\n",
      "nan\n",
      "Dundee\n",
      "Indiana\n",
      "Edinburgh\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "UK\n",
      "EGYPT\n",
      "The Circle of Life\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "i got 1/13 menpa replies, omg\n",
      "nan\n",
      "nan\n",
      "Pocatello, Idaho\n",
      "Los Angeles, CA\n",
      "Nashville, TN\n",
      "probably watching survivor\n",
      "nan\n",
      "Medan,Indonesia\n",
      "Santa Maria, CA\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "??9?\n",
      "nan\n",
      "Colombia\n",
      "Oklahoma, USA\n",
      "nan\n",
      "Jakarta\n",
      "nan\n",
      "Newark, NJ\n",
      "USA\n",
      "Chicago, Il\n",
      "LA\n",
      "Clayton, NC\n",
      "nan\n",
      "Bandar Lampung, Indonesia\n",
      "Venezuela\n",
      "San Jose, CA\n",
      "Indonesia\n",
      "Madison, WI\n",
      "buffalo / madrid / granada\n",
      "USA\n",
      "HIยรยขUTยรยขAS\n",
      "di langit 7 bidadari (^,^ )\n",
      "nan\n",
      "Vancouver, BC\n",
      "nan\n",
      "Coventry, UK\n",
      "Seattle, WA\n",
      "Georgia\n",
      "Norman, Oklahoma\n",
      "Rapid City, Black Hills, SD\n",
      "USA\n",
      "Memphis, in the Tennessees\n",
      "Greensboro, North Carolina\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "Leesburg, FL\n",
      "Rotterdam, The Netherlands\n",
      "Greensboro, North Carolina\n",
      "Victoria, BC\n",
      "winston-salem north carolina\n",
      "nan\n",
      "nan\n",
      "Leesburg, FL\n",
      "Elchilicitanierraversal \n",
      "(RP)\n",
      " 45รฅยก 5'12.53N   14รฅยก 7'24.93E\n",
      "nan\n",
      "USA\n",
      "nan\n",
      "Reddit \n",
      "Asheboro, NC\n",
      "Holland MI via Houston, CLE\n",
      "Reddit \n",
      "State College, PA\n",
      "SoDak\n",
      "Waverly, IA\n",
      "Kenya\n",
      "Kenya\n",
      "nan\n",
      "nan\n",
      "Kenya\n",
      "Why should you know?\n",
      "Kenya\n",
      "Kenya\n",
      "Kenya\n",
      "nan\n",
      "nan\n",
      "Kenya\n",
      "Philadelphia, PA \n",
      "Photo : Blue Mountains \n",
      "Bedford, England\n",
      "Kenya\n",
      "Aperture Science Test Facility\n",
      "Wandsworth, London\n",
      "Kenya\n",
      "Kenya\n",
      "im definitely taller than you.\n",
      "Kenya\n",
      "Kenya\n",
      "london essex england uk\n",
      "Kenya\n",
      "{Detailed}\n",
      "Kenya\n",
      "nan\n",
      "Fairgrounds Resident\n",
      "English Midlands\n",
      "Kenya\n",
      "nan\n",
      "Kenya\n",
      "nan\n",
      "Victoria, Australia, Earth\n",
      "Antarctica\n",
      "Magnolia, Fiore \n",
      "New Sweden\n",
      "Victoria, Australia, Earth\n",
      "New York\n",
      "cereal aisle #17:i4\n",
      "Los Angeles\n",
      "i'm a Citizen of the World\n",
      "nan\n",
      "Leicester, England\n",
      "nan\n",
      "Melbourne, Australia\n",
      "nan\n",
      "nan\n",
      "Anonymous\n",
      "nan\n",
      " Queensland, Australia\n",
      "Everywhere\n",
      "nan\n",
      "Anonymous\n",
      "Realville\n",
      "Anonymous\n",
      "New Sweden\n",
      "by a piano probably. \n",
      "Fiore, Lamia Scale\n",
      "nan\n",
      "Chattanooga TN\n",
      "Nashville, TN\n",
      "Birmingham, England\n",
      "Huntsville, AL\n",
      "nan\n",
      "Auckland\n",
      "Haysville, KS\n",
      "nan\n",
      "Seattle\n",
      "nan\n",
      "Earth-616\n",
      "USA\n",
      "California, USA\n",
      "wisco\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "San Diego, CA\n",
      "nan\n",
      "Leaving Bikini Bottom\n",
      "Tennessee, USA\n",
      "nan\n",
      "Haysville, KS\n",
      "Eww, I'm not Paul Elam\n",
      "Huntsville, AL\n",
      "Hemel Hempstead\n",
      "Chester Football Club\n",
      "Fresno\n",
      "Earth\n",
      "West Hollywood, CA\n",
      "nan\n",
      "ANDY 4 LEADER X\n",
      "nan\n",
      "Hell\n",
      "Haysville, KS\n",
      "Tama, Iowa\n",
      "nan\n",
      "Dundee\n",
      "nan\n",
      "nan\n",
      "Minneapolis - St. Paul\n",
      "Las Vegas aka Hell\n",
      "nan\n",
      "Ashburn, VA\n",
      "St. Louis, Mo\n",
      "Vancouver Canada\n",
      "nan\n",
      "nan\n",
      "Colorado\n",
      "Kingston, Jamaica\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "Nottingham, England\n",
      " The World\n",
      "Own planet!!\n",
      "nan\n",
      "Vancouver, BC, Canada\n",
      "nan\n",
      "Ireland\n",
      "Ecuador\n",
      "London, UK\n",
      "nan\n",
      "Stay Tuned ;) \n",
      "Cimerak - Pangandaran\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "Cottonwood Arizona\n",
      "nan\n",
      "nan\n",
      "London\n",
      "India\n",
      "Norway\n",
      "nan\n",
      "Wausau, Wisconsin\n",
      "nan\n",
      "Orlando, FL\n",
      "Raleigh, NC\n",
      "BOSTON-LONDON\n",
      "Boston, MA\n",
      "GLOBAL/WORLDWIDE\n",
      "nan\n",
      "nan\n",
      "North Dartmouth, Massachusetts\n",
      "Lynchburg, VA\n",
      "107-18 79TH STREET\n",
      "Boston, MA\n",
      "The Waystone Inn\n",
      "nan\n",
      "?? Made in the Philippines ??\n",
      "Boston/Montreal \n",
      "Manavadar, Gujarat\n",
      "Pueblo, Colorado\n",
      "PG County, MD\n",
      "nan\n",
      "Somewhere between Chicago & Milwaukee\n",
      "nan\n",
      "InterplanetaryZone\n",
      "WORLDWIDE-BOSTON\n",
      "London, UK\n",
      "Jersey Shore\n",
      "Detroit, Michigan\n",
      "107-18 79TH STREET\n",
      "The Universe\n",
      "laying on the bass\n",
      "nan\n",
      "Cleveland, OH\n",
      "nan\n",
      "Hustletown\n",
      "? miranda ? 521 mi\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "Oldenburg // London\n",
      "#partsunknown\n",
      "Proudly frozen Canuck eh !!\n",
      "IL\n",
      "KSU 2017\n",
      "nan\n",
      "Colorado, USA\n",
      "Everywhere\n",
      "Storybrooke / The Moors\n",
      "NOLA ?? TX\n",
      "nan\n",
      "neil's kitchen  |  32215\n",
      "The shores of Lake Kilby\n",
      "nan\n",
      "Leeds, England\n",
      "Two Up Two Down\n",
      "nan\n",
      "Canada\n",
      "The Internet\n",
      "Washington, DC\n",
      "Flushing, Queens\n",
      "nan\n",
      " Quantico Marine Base, VA.\n",
      "nan\n",
      "canada\n",
      "nan\n",
      "nan\n",
      "Alaska\n",
      "nan\n",
      "nan\n",
      "Bozeman, Montana\n",
      "nan\n",
      "Hearts & Minds\n",
      "USA\n",
      "somewhere USA \n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "Ewa Beach, HI\n",
      "Texas\n",
      "Boston MA\n",
      "Memphis, TN\n",
      "Virginia, USA\n",
      "NY\n",
      "302\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "Paterson, New Jersey \n",
      "highlands&slands scotland\n",
      "nan\n",
      "somewhere USA \n",
      "Crouch End, London\n",
      "Ealing, London\n",
      "Malibu/SantaFe/Winning!\n",
      "London, Greater London, UK\n",
      "Edinburgh, Scotland\n",
      "Wales\n",
      "Nottingham\n",
      "London\n",
      "Tring \n",
      "Co. Tyrone Northern Ireland\n",
      "Birmingham & Bristol\n",
      "nan\n",
      "nan\n",
      "IUPUI '19\n",
      "potters bar\n",
      "Chiswick, London\n",
      "London\n",
      "London\n",
      "the burrow\n",
      "nan\n",
      "plymouth\n",
      "The Pumpkin Carriage of Dreams\n",
      "nan\n",
      "London\n",
      "Edinburgh\n",
      "Holly Springs, NC \n",
      "nan\n",
      "London, England\n",
      "Memphis, TN\n",
      "nan\n",
      "nan\n",
      "Ireland\n",
      "Notts\n",
      "nan\n",
      "nan\n",
      "South, England\n",
      "nan\n",
      "home \n",
      "Orlando/Cocoa Beach, FL\n",
      "Birmingham and the Marches\n",
      "Los Angeles\n",
      "Mรยฉxico D.F.\n",
      "Canada\n",
      "New York\n",
      "Greenwich Meridian\n",
      "nan\n",
      "USA\n",
      "nan\n",
      "Newcastle Upon Tyne, England\n",
      "nan\n",
      "liaยรยขdaniยรยขlaura\n",
      "Oneonta, NY/ Staten Island, NY\n",
      "in my own personal hell (:\n",
      "Jonesboro, AR MO, IOWA USA\n",
      "sรยฃo luis\n",
      "Aurora, IL\n",
      "America of Founding Fathers\n",
      "nan\n",
      "on to the next adventure\n",
      "nan\n",
      "nan\n",
      "Suburban Detroit, Michigan\n",
      "America of Founding Fathers\n",
      "nan\n",
      "nan\n",
      "Pennsylvania, USA\n",
      "Guatemala\n",
      "nan\n",
      "America of Founding Fathers\n",
      "Leitchfield Kentucky\n",
      "Littleton, CO\n",
      "nan\n",
      "Austin TX\n",
      "nan\n",
      "nan\n",
      "The Netherlands\n",
      "nan\n",
      "New York, NY\n",
      "Marbella. Spain\n",
      "Hammersmith, London\n",
      "nan\n",
      "Inexpressible Island \n",
      "Ashford, Kent, United Kingdom\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "Dhaka, Bangladsh\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "Sacramento, California\n",
      "#goingdownthetoilet Illinois\n",
      "San Francisco Bay Area\n",
      "nan\n",
      "nan\n",
      "Washington, DC\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "US\n",
      "Under Santa Barbara Skies\n",
      "nan\n",
      "nan\n",
      "Fukushima city Fukushima.pref\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "Washington, D.C.\n",
      "Norwalk, CT\n",
      "USA, North Dakota\n",
      "nan\n",
      "nan\n",
      "japon\n",
      "Eaubonne, 95, France\n",
      "Pluto\n",
      "nan\n",
      "nan\n",
      "Anywhere I like\n",
      "Japan\n",
      "Denver, CO\n",
      "New York, New York\n",
      "Phoenix, Arizona, USA\n",
      "nan\n",
      "Paris (France)\n",
      "Johannesburg, South Africa\n",
      "nan\n",
      "nan\n",
      "Den Helder, Rijkswerf\n",
      "SE London(heart is by the sea)\n",
      "Warsaw\n",
      "Aztec NM\n",
      "Denver, CO\n",
      "Washington, D.C.\n",
      "World\n",
      "nan\n",
      "nan\n",
      "Fort Calhoun, NE\n",
      "Bournemouth, Dorset, UK\n",
      "Virginia\n",
      "Somecity, Somerset, MD\n",
      "Dover, DE\n",
      "Cavite, Philippines\n",
      "Korea\n",
      "Seattle\n",
      "M!A: None\n",
      "United Kingdom\n",
      "Purfleet\n",
      "nan\n",
      "The Memesphere\n",
      "21 | PNW\n",
      "Miami via Lima\n",
      "Cymru araul\n",
      "EIC\n",
      "NEWCASTLE\n",
      "United Kingdom\n",
      "nan\n",
      "UK\n",
      "Jump City\n",
      "Dudetown\n",
      "nan\n",
      "nan\n",
      "Ondo\n",
      "nan\n",
      "Bhubneshwar\n",
      "Toronto\n",
      "cedar rapids ia\n",
      "EIC\n",
      "nan\n",
      "nan\n",
      "Texas\n",
      "London\n",
      "satan's colon\n",
      "Valparaiso \n",
      "New York\n",
      "nan\n",
      "nan\n",
      "prob turning up with sheen\n",
      "nan\n",
      "#freegucci\n",
      "Tennessee\n",
      "Waterloo, ON\n",
      "nan\n",
      "The dark\n",
      "Reading MA\n",
      "nan\n",
      "Wynne, AR\n",
      "Dublin, Ireland\n",
      "The Wood\n",
      "Palmyra, NJ\n",
      "Tennessee\n",
      "1313 W.Patrick St, Frederick\n",
      "Danbury, CT\n",
      "Canada\n",
      "Miami, FL\n",
      "Mid West\n",
      "nan\n",
      "Upstairs.\n",
      "Winnipeg, Manitoba\n",
      "leyland\n",
      "Birmingham, England\n",
      "don't buy the s*n\n",
      "nan\n",
      "London UK\n",
      "Overton NV\n",
      "nan\n",
      "Born in Baltimore Living in PA\n",
      "Elizabeth, NJ\n",
      "Canada \n",
      "a feminist, modernist hag.\n",
      "Glasgow, Scotland\n",
      "Maryland\n",
      "kissimmee,fl.\n",
      "nan\n",
      "New Orleans ,Louisiana\n",
      "828/704(Soufside)/while looking goofy in NJ\n",
      "Merica!\n",
      "nan\n",
      "nan\n",
      "DC Metro area\n",
      "Indiana\n",
      "nan\n",
      "India\n",
      "Federal Capital Territory\n",
      "nan\n",
      "Illinois\n",
      "nan\n",
      "Turkmenistan\n",
      "nan\n",
      "India\n",
      "23 countries and counting!\n",
      "Ontario\n",
      "Montgomery, AL\n",
      "New York, NY\n",
      "nan\n",
      "United Kingdom\n",
      "Amarillo\n",
      "California\n",
      "Los Angeles, CA\n",
      "Las Vegas, Nevada\n",
      "nan\n",
      "California\n",
      "Sydney, NSW\n",
      "Central Coast, California\n",
      "nan\n",
      "Clean World\n",
      "NYC :) Ex- #Islamophobe\n",
      "Canada\n",
      "nan\n",
      "Street of Dallas\n",
      "nan\n",
      "Goa, India\n",
      "nan\n",
      "England, Great Britain.\n",
      "San Luis Obispo, CA\n",
      "Kamloops, BC\n",
      "Lyallpur, Pakistan\n",
      "nan\n",
      "nan\n",
      "Los Angeles, CA\n",
      "Street of Dallas\n",
      "Karachi Pakistan\n",
      "nan\n",
      "nan\n",
      "Lyallpur, Pakistan\n",
      "Financial News and Views\n",
      "nan\n",
      "United Kingdom\n",
      "Pensacola, FL\n",
      "Corpus Christi\n",
      "Chile\n",
      "nan\n",
      "nan\n",
      "Espaรยฑa, Spain\n",
      "New York City, NY\n",
      "Indonesia\n",
      "nan\n",
      "LAGOS\n",
      "NJ/NYC\n",
      "??????\n",
      "???\n",
      "nan\n",
      "nan\n",
      "Bandung\n",
      "United States\n",
      "Italy\n",
      "nan\n",
      "Canada\n",
      "Stockholm, Sweden\n",
      "Los Angeles, CA\n",
      "nan\n",
      "Eagle River Alaska\n",
      "รรT: 6.488400524109015,3.352798039832285\n",
      "Nottingham, United Kingdom\n",
      "Akure city in ondo state \n",
      "The Netherlands\n",
      "nan\n",
      "Anywhere\n",
      "FCT, Abuja \n",
      "Los Angeles, CA\n",
      "Finland\n",
      "Sรยฃo Paulo\n",
      "Ile-Ife,Osun state, Nigeria\n",
      "Fukuoka, Japan\n",
      "kano\n",
      "?? ?+254? ? \\??รฅยก_??รฅยก_???รฅยก_?/??\n",
      "nan\n",
      "Dammam- KSA\n",
      "New York, NY\n",
      "Pro-American and Anti-#Occupy\n",
      "nan\n",
      "Nigeria\n",
      "Durham, NC\n",
      "The Kingdom of Fife, Scotland\n",
      "Dallas Fort-Worth\n",
      "Scotland\n",
      "Everett, WA\n",
      "Mumbai, India\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "??????\n",
      "The P (South Philly)\n",
      "VONT ISLAND, LAGOS\n",
      "Houston TX\n",
      "nan\n",
      "nan\n",
      "Durham, NC\n",
      "nan\n",
      "Leeds, UK\n",
      "www.facebook.com/stuntfm\n",
      "Lagos\n",
      "Dallas Fort-Worth\n",
      "Toronto, Canada\n",
      "Miami?Gainesville\n",
      "illinois. united state \n",
      "nan\n",
      "Los Angeles\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "London, England\n",
      "Royton\n",
      "the Refrigerator \n",
      "Everywhere\n",
      "California\n",
      "nan\n",
      "Brasil\n",
      "Leeds, United Kingdom\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "Toronto, Canada\n",
      "Everywhere\n",
      "Linton Hall, VA\n",
      "The Internetz\n",
      "Charleston, IL\n",
      "Narnia\n",
      "nan\n",
      "Charlotte, NC | Kรยฆln, NRW\n",
      "nan\n",
      "Topeka, KS\n",
      "Philadelphia, PA\n",
      "Palm Bay, FL (Kissimmee)\n",
      "Narnia\n",
      "Manhattan\n",
      "elena's bed // info on link\n",
      "Elsewhere, NZ\n",
      "518 รฅรก NY\n",
      "The Shady Hyenatown of Finland\n",
      "Maryland \n",
      "nan\n",
      "worldwide\n",
      "East TN.\n",
      "nan\n",
      "Milwaukee WI\n",
      "Toronto\n",
      "Macon, GA\n",
      "Narnia\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "Torry Alvarez love forever ? ?\n",
      "Detroit\n",
      "Petaluma, CA\n",
      "nan\n",
      "?^รฅรก??รฅรก?^?? ??\n",
      "nan\n",
      "?\n",
      "Singapore\n",
      "Oxford / bristol\n",
      "PARACHUTE\n",
      "Near Warrington\n",
      "nan\n",
      "|-/\n",
      "nan\n",
      "Pawnee\n",
      "nan\n",
      "Melbourne, Victoria\n",
      "UK\n",
      "nan\n",
      "Manchester\n",
      "Georgia\n",
      "UK\n",
      "nan\n",
      "Derbyshire, United Kingdom\n",
      "nan\n",
      "VCU\n",
      "McLean, VA\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "South Florida\n",
      "nan\n",
      "Positive 852\n",
      "Manchester, England\n",
      "San Francisco\n",
      "Kansas City, Mo.\n",
      "New York, NY\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "Indonesia\n",
      "nan\n",
      "nan\n",
      "Massachusetts, USA\n",
      "anzio,italy\n",
      "nan\n",
      "Nigeria\n",
      "US, PA\n",
      "nan\n",
      "h+l\n",
      "Republic of Texas\n",
      "Portland, OR\n",
      "Florida\n",
      "nan\n",
      "USA\n",
      "UK\n",
      "SoCal\n",
      "Bangalore, India\n",
      "Indonesia\n",
      "nan\n",
      "Milwaukee, WI\n",
      "Mesa, AZ\n",
      "Stratford, CT\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "MI,USA\n",
      "Toronto\n",
      "Houston \n",
      "Indonesia\n",
      "Los Angeles \n",
      "missouri USA\n",
      "PA\n",
      "South africa\n",
      "Bucharest\n",
      "nan\n",
      "Mumbai, Maharashtra\n",
      "Joshua Tree, CA\n",
      "Area 8 \n",
      "nan\n",
      "Bangalore, INDIA\n",
      "nan\n",
      "รรT: 40.707762,-74.014213\n",
      "nan\n",
      "nan\n",
      "mumbai\n",
      "Killa Hill, CO\n",
      "Southern Califorina\n",
      "all over the world\n",
      "San Diego, Calif.\n",
      "nan\n",
      "Geneva. And beyond. \n",
      "nan\n",
      "nan\n",
      "NYC, New York\n",
      "USA\n",
      "Vร_a Lรรctea\n",
      "New York, United States\n",
      "nan\n",
      "nan\n",
      "London\n",
      "San Diego, CA\n",
      "Some Where in this World\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "Cumming, GA\n",
      "Korea\n",
      "South Africa\n",
      "San Francisco\n",
      "russia\n",
      "California\n",
      "nan\n",
      "Sarasota, FL\n",
      "nan\n",
      "Melbourne\n",
      "Livonia, MI\n",
      "nan\n",
      "nan\n",
      "USA\n",
      "Karolinska vร_gen 18, Solna\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "China\n",
      "nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I Heard #2MBikers\n",
      "nan\n",
      "Dunwoody, GA\n",
      "nan\n",
      "nan\n",
      "Ebola\n",
      "Mongolia\n",
      "nan\n",
      "Broadview Heights, Ohio\n",
      "nan\n",
      "nan\n",
      "china\n",
      "Poplar, London\n",
      "rome\n",
      "China\n",
      "LP, MN USA\n",
      "nan\n",
      "nan\n",
      "Burlington, VT\n",
      "La Grange Park, IL\n",
      "Warszawa\n",
      "Washington, DC\n",
      "nan\n",
      "#MayGodHelpUS\n",
      "Darnley, Prince Edward Island\n",
      "Gloucester, MA\n",
      "vancouver usa\n",
      "Alberta \n",
      "Yobe State\n",
      "Coventry, Rhode Island\n",
      "nan\n",
      "United States of America\n",
      "nan\n",
      "Federal Capital Territory\n",
      "La Puente, CA\n",
      "music.\n",
      "online \n",
      "Memphis, TN\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "Thailand Malaysia Indonesia \n",
      "Earth\n",
      "Pennsylvania, USA\n",
      "vancouver usa\n",
      "Fairfax, VA\n",
      "Memphis, TN\n",
      "Seattle\n",
      "Sicamous, British Columbia\n",
      "North Memphis/Global Citizen\n",
      "Bridport, England\n",
      "Porto Alegre, Rio Grande do Sul\n",
      "North Vancouver, BC\n",
      "Seattle\n",
      "Pioneer Village, KY\n",
      "Port Harcourt, Nigeria\n",
      "ECSU16\n",
      "UK\n",
      "nan\n",
      "nan\n",
      "Nigeria\n",
      "Tulsa, OK\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "WorldWide\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "North\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "Ames, IA\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "San Diego California 92101\n",
      "Nairobi-KENYA\n",
      "Erie, PA\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "Nairobi-KENYA\n",
      "nan\n",
      "nan\n",
      "Augusta, Maine, 04330\n",
      "nan\n",
      "Ad Majorem Dei Glorium\n",
      "Aurora, Ontario \n",
      " Baku & Erzurum \n",
      "Statute Of Limitations_\n",
      "paradise\n",
      " BC, US, Asia or Europe.\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "St PetersburgFL\n",
      "Espaรยฑa\n",
      "nan\n",
      "Tarragona\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "Warri\n",
      "Melbourne Australia\n",
      "nan\n",
      "21.462446,-158.022017\n",
      "nan\n",
      "nan\n",
      "Bangalore, India\n",
      "Skyhold\n",
      "Australia\n",
      "Amman,Jordan\n",
      "tripoli international airport\n",
      "nan\n",
      "nan\n",
      "Auckland, New Zealand\n",
      "nan\n",
      "NYC,US - Cali, Colombia\n",
      "nan\n",
      "QLD Australia\n",
      "Geneva, Switzerland\n",
      "Trinidad & Tobago\n",
      "SF Bay Area\n",
      "#HarleyChick#PJNT#RunBenRun\n",
      "Toronto, Ontario\n",
      "New Jersey\n",
      "Big NorthEast Litter Box\n",
      "Sand springs oklahoma\n",
      "nan\n",
      "nan\n",
      "Surry Hills, Sydney\n",
      "Sand springs oklahoma\n",
      "#HarleyChick#PJNT#RunBenRun\n",
      "nan\n",
      "USA\n",
      "Thornton  Colorado\n",
      "Wanderlust\n",
      "Toronto, Ontario\n",
      "Karachi\n",
      "nan\n",
      "nan\n",
      "India\n",
      "U.S.A.   FEMA Region 5\n",
      "United States\n",
      "nan\n",
      "Nigeria\n",
      "Jakarta\n",
      "Ohio, USA\n",
      "watford\n",
      "West Hollywood\n",
      "Kent\n",
      "Canada\n",
      "The Multiverse\n",
      "USA\n",
      "Bournemouth\n",
      "Jammu | Kashmir | Delhi\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "southern california\n",
      "Philadelphia, Pennsylvania USA\n",
      "nan\n",
      "Winston-Salem, NC\n",
      "Huntley, IL\n",
      "NIGERIA\n",
      "The green and pleasant land.\n",
      "Earth\n",
      "nan\n",
      "nan\n",
      "Leeds, U.K.\n",
      "Ireland\n",
      "nan\n",
      "nan\n",
      "Pennsylvania, USA\n",
      "Boston MA\n",
      "Canada\n",
      "Jakarta/Kuala Lumpur/S'pore\n",
      "Cape Town\n",
      "Nigeria\n",
      "Westerland\n",
      "Kuala Lumpur\n",
      "Surabaya\n",
      "#iminchina\n",
      "nan\n",
      "Africa\n",
      "nan\n",
      "Nigeria\n",
      "Birmingham\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "London\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "?????????????, Thailand \n",
      "nan\n",
      "17th Dimension\n",
      "nan\n",
      "Washington\n",
      "USA - Canada - Europe - Asia\n",
      "worldwide\n",
      "Phoenix\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "Paris, France\n",
      "nan\n",
      "Here, there and everywhere\n",
      "Belgrade\n",
      "India\n",
      "Cardiff, UK\n",
      "nan\n",
      "nan\n",
      "Bossland\n",
      "World Wide Web\n",
      "Uppsala, Sweden\n",
      "East London\n",
      "Detroit, MI\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "Los Angeles, CA\n",
      "nan\n",
      "nan\n",
      "Montana, USA\n",
      "Utah, USA\n",
      "Frankfort, KY\n",
      "Los Angeles\n",
      "United Kingdom\n",
      "In Space\n",
      "nan\n",
      "Sligo and Galway, Ireland\n",
      "Los Angeles, CA\n",
      "Seattle\n",
      "Tipperary (Long Way) \n",
      "Mumbai\n",
      "Malang\n",
      "nan\n",
      "San Francisco, CA\n",
      "hertfordshire.\n",
      "Dublin\n",
      "nan\n",
      "Spare 'Oom\n",
      "SURROUNDED BY WEEABOOS\n",
      "nan\n",
      "Australia\n",
      "nan\n",
      "NYC\n",
      "Vidalia GA\n",
      "Vista, CA\n",
      "Serva Fidem\n",
      "nan\n",
      "Bellville, Ohio\n",
      "trapped in America\n",
      "A little house in the outback.\n",
      "tri state\n",
      "Upstate New York\n",
      "nan\n",
      "nan\n",
      "Scotland, United Kingdom\n",
      "nan\n",
      "nan\n",
      "Chicora ?? Oakland\n",
      "nan\n",
      "nan\n",
      "heart of darkness, unholy ?\n",
      "Colonial Heights, VA\n",
      "nan\n",
      "India\n",
      "The Weird Part of Wonderland\n",
      "nan\n",
      "Alabama\n",
      "A little house in the outback.\n",
      "Cassadaga Florida\n",
      "Columbus, Georgia\n",
      "Minna, Nigeria\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "Accra,Ghana\n",
      "World\n",
      "Dubai, UAE\n",
      "nan\n",
      "Phoenix Az\n",
      "Italy\n",
      "London\n",
      "nan\n",
      "ON\n",
      "St. Louis, MO\n",
      "United States\n",
      "Spain - China - Latin America.\n",
      "Chester, IL\n",
      "West Africa\n",
      "Calgary, AB, Canada\n",
      "California\n",
      "Made Here In Detroit \n",
      "Portland, OR\n",
      "nan\n",
      "Dallas, Tejas\n",
      "ATLANTA , GEORGIA \n",
      "| CA ยรยข GA  |\n",
      "Nashville, TN\n",
      "nan\n",
      "Depok\n",
      "London / Birmingham\n",
      "texas\n",
      "Yulee, FL\n",
      "austin tx\n",
      "nan\n",
      "nan\n",
      "Garrett\n",
      "followurDREAMS(& my instagram)\n",
      "nan\n",
      "sheffield // rotherham\n",
      "Wailuku, Maui\n",
      "nan\n",
      "Lima, OH\n",
      "Long Beach, CA\n",
      "Belfast\n",
      "fujo garbage heaven \n",
      "New York - Connecticut\n",
      "nan\n",
      "snapchat~ maddzz_babby \n",
      "2,360 miles away\n",
      "nan\n",
      "Winnipeg, Manitoba\n",
      "nan\n",
      "Boston\n",
      "Florida Forever\n",
      "Sacramento, CA\n",
      "#RedSoxNation\n",
      "MNL\n",
      "youtube.com/channel/UCHWTLC9B4ZjUGh7yDlb55Iw\n",
      "nan\n",
      "CA ??DC\n",
      "Des Moines, IA\n",
      "Mรยฉrida, Yucatรรn\n",
      "Scituate, MA\n",
      "USA\n",
      "nan\n",
      "United States\n",
      "nan\n",
      "United States\n",
      "USA\n",
      "hkXfYMhEx\n",
      "US\n",
      "USA\n",
      "USA\n",
      "nan\n",
      "Hamilton County, IN\n",
      "United States\n",
      "USA\n",
      "USA\n",
      "USA\n",
      "nan\n",
      "Dakar\n",
      "United States\n",
      "USA\n",
      "USA\n",
      "USA\n",
      "USA\n",
      "In your head\n",
      "USA\n",
      "nan\n",
      "North Carolina, USA\n",
      "USA\n",
      "USA\n",
      "USA\n",
      "WA State\n",
      "USA\n",
      "The Sanctuary Network, Rome\n",
      "Tractor land aka Bristol\n",
      "nan\n",
      "USA\n",
      "nan\n",
      "Adventuring in Narnia\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "Asgard\n",
      "San Diego, CA\n",
      "Unite. Bless. Wallahi \n",
      "nan\n",
      "sneaking glances at Thancred\n",
      "ljp/4\n",
      "Charlotte, North Carolina\n",
      "nan\n",
      "nan\n",
      "Nirvana\n",
      "nan\n",
      "ny\n",
      "Mentor OH\n",
      "Trumann, Arkansas\n",
      "18 ยรยข CC\n",
      "North Carolina, USA\n",
      "nan\n",
      "nan\n",
      "1D | 5SOS | AG\n",
      "PA.USA\n",
      "m3, k, a, d\n",
      "pissing off antis\n",
      "'SAN ANTONIOOOOO'\n",
      "nan\n",
      "sisterhood\n",
      "3000 miles from everyone\n",
      "livin in a plastic world\n",
      "with Doflamingo\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "21, Porto\n",
      "access to njh/5 and cth/4\n",
      "tx\n",
      "nan\n",
      "nan\n",
      "tx\n",
      "Massachusetts \n",
      "rio de janeiro | brazil\n",
      "nan\n",
      "Moore, OK\n",
      "ยรยขOlderCandyBloomยรยข\n",
      "nan\n",
      "Justin and Ariana follow\n",
      "[@blackparavde is my frankie]\n",
      "Daddy Kink Central\n",
      "nan\n",
      "Wakefield, West Yorkshire\n",
      "Pittsburgh\n",
      "Namjoon's pants\n",
      "Mogadishu, New Jersey\n",
      "All around the world\n",
      " Jariana Town\n",
      "NYC\n",
      "Honeymoon รยฃve.\n",
      "Aveiro, Portugal\n",
      "nan\n",
      "nan\n",
      "justin & ari follow || tvd\n",
      "UK\n",
      "nan\n",
      "9/1/13\n",
      "JDB/LJC/AGB/TW/PLL\n",
      "nap queen\n",
      "amsterdayum 120615 062415\n",
      "lesa * she/her\n",
      "Griffin :3\n",
      "xiumin's nonexistent solos\n",
      "nan\n",
      "marvel | books | hp | tmr\n",
      "Pennsylvania, USA\n",
      "blackfalds.\n",
      "San Juan, Puerto Rico\n",
      "nan\n",
      "New Jersey\n",
      "nan\n",
      "Freddy Fazbears pizzeria\n",
      "nan\n",
      "Brasร_lia\n",
      "POFFIN\n",
      "#Gladiator ยรยข860ยรยข757ยรยข\n",
      "W.I.T.S Academy\n",
      "nan\n",
      "Where ever i please\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "Melbourne, Australia.\n",
      "Sheffield/Leeds\n",
      "PLFD cuh..\n",
      "lost in history\n",
      "Ontario, Canada\n",
      "nan\n",
      "5-Feb\n",
      "United States\n",
      "Florida, USA\n",
      "labyrinthia\n",
      "??+ ... ??+\n",
      "texas\n",
      "garowe puntland somalia\n",
      "Oakland, CA\n",
      "nan\n",
      "Third rock from the Sun\n",
      "Lives in London\n",
      "nan\n",
      "Somalia\n",
      "nan\n",
      "Lakewood, Tennessee\n",
      "Smash Manor/Kanto\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "??????\n",
      "nan\n",
      "nan\n",
      "Somalia\n",
      "Bokaro Steel City, Jharkhand\n",
      "nan\n",
      "Hatteras, North Carolina\n",
      "412 NW 5th Ave. Portland OR\n",
      "Madison, Wisconsin, USA\n",
      "Mogadishu, Somalia\n",
      "the azure cloud\n",
      "Perenjori, WA\n",
      "UK\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "Unknown \n",
      "nan\n",
      "Atlanta(ish), GA\n",
      "Trinidad and Tobago\n",
      "Above the snake line - #YoNews\n",
      "San Diego\n",
      "Haddonfield, NJ\n",
      "nan\n",
      "Canada\n",
      "NY\n",
      "ill yorker\n",
      "Newcastle\n",
      "Above the snake line - #YoNews\n",
      "White Plains, NY\n",
      "nan\n",
      "B&B near Alton Towers\n",
      "nan\n",
      "Alger-New York-San Francisco\n",
      "nan\n",
      "nan\n",
      "ยรwagger!รยominicanรรท\n",
      "Haddonfield, NJ\n",
      "USA\n",
      "nan\n",
      "hell\n",
      "Texas af\n",
      "San Diego California 92101\n",
      "San Diego, CA\n",
      "Hinterestland\n",
      "Evansville, IN\n",
      "Greenpoint\n",
      "In the potters hands\n",
      "Haddonfield, NJ\n",
      "รรT: 42.910975,-78.865828\n",
      "Newcastle\n",
      "nan\n",
      "San Diego, CA\n",
      "nan\n",
      "nan\n",
      "Cypress, CA 90630\n",
      "Sacramento\n",
      "nan\n",
      "Vancouver, British Columbia\n",
      "nan\n",
      "North East Unsigned Radio\n",
      "Every Where in the World\n",
      "Memphis\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "Fountain Valley, CA\n",
      "Canada\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "Michigan, USA\n",
      "nan\n",
      "Sacramento, CA\n",
      "Liverpool\n",
      "Haarlem\n",
      "nan\n",
      "Queensland\n",
      "HOMRA.\n",
      "nan\n",
      "Ciudad Autร_noma de Buenos Aires, Argentina\n",
      "nan\n",
      "Not where I want to be, yet\n",
      "London\n",
      "Duval, WV 25573, USA ?\n",
      "nan\n",
      "nan\n",
      "London\n",
      "Rhyme Or Reason?\n",
      "MA\n",
      "hey Georgia\n",
      "Coventry\n",
      "nc\n",
      "nan\n",
      "kenya\n",
      "My subconscious\n",
      "London\n",
      " New England\n",
      "nan\n",
      "nan\n",
      "UAE,Sharjah/ AbuDhabi\n",
      "Tampa, FL\n",
      "New York\n",
      "The Web\n",
      "Paris.\n",
      "Am International\n",
      "Team Slytherin\n",
      "Michel Delving.\n",
      "Hame\n",
      "Brizzle City !\n",
      "nan\n",
      "???  Dreamz\n",
      "???  Dreamz\n",
      "nan\n",
      "texasss\n",
      "Diamondville\n",
      "Massachusetts, USA\n",
      "nan\n",
      "The TARDIS\n",
      "nan\n",
      "nan\n",
      "California, USA\n",
      "nan\n",
      "Honolulu,Hawaii \n",
      "Everywhere\n",
      "nan\n",
      "Flipadelphia\n",
      "nan\n",
      "nan\n",
      "The American Wasteland (MV)\n",
      "Charlotte, NC\n",
      "high way 99\n",
      "nan\n",
      "... -.- -.--\n",
      "Canada Eh! \n",
      "Ohio\n",
      "828??864??803\n",
      "Nomad, USA\n",
      "they/them \n",
      "Greater Los Angeles Bearia\n",
      "nan\n",
      "victoria mozรยฃo \n",
      "Denver, CO\n",
      "Seattle, WA\n",
      "they/them \n",
      "Sydney\n",
      "Hollywood\n",
      "miami x dallas \n",
      "Hamburg, DE\n",
      "Nanaimo, BC, Canada\n",
      "nan\n",
      "Decatur, GA\n",
      "Los Angles, CA\n",
      "az\n",
      "Crato - CE \n",
      "Philippines\n",
      "Australia\n",
      "nan\n",
      "nan\n",
      "Ventura, Ca\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "Arlington, TX\n",
      "Rio de Janeiro\n",
      "Medellร_n, Antioquia\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "BrowardCounty // Florida \n",
      "nan\n",
      "nan\n",
      "nan\n",
      "Houston, TX\n",
      "nan\n",
      "Soufside\n",
      "nan\n",
      "nan\n",
      "Dalston, Hackney\n",
      "nan\n",
      "nan\n",
      "WORLDWI$E \n",
      "atlanta\n",
      "nan\n",
      "nan\n",
      "3.28.15|7.20.15|7.25.15\n",
      "cigarknub@gmail.com\n",
      "Ktx\n",
      "nan\n",
      "PSA Nursing \n",
      "Narnia, Maryland\n",
      "nan\n",
      "Winnipeg, MB, Canada\n",
      "Indonesia\n",
      "Bronx, NY\n",
      "Dallas, TX\n",
      "Wakefield MA\n",
      "nan\n",
      "New Jersey\n",
      "Huntington, WV\n",
      "South, USA\n",
      "Manchester\n",
      "Hampshire, UK\n",
      "Louisiana, USA\n",
      "New York\n",
      "Australia\n",
      "South, USA\n",
      "nan\n",
      "London\n",
      "This Is Paradise. Relax. \n",
      "Deployed in the Middle East\n",
      "Brooklyn, NY\n",
      "Raleigh, NC\n",
      "South, USA\n",
      "nan\n",
      "teh internets\n",
      "Neath, South Wales\n",
      "Mountains\n",
      "Porthcawl\n",
      "In the spirit world\n",
      "Western New York\n",
      "Mpela'zwe \n",
      "Austin, TX\n",
      "Italy\n",
      "nan\n",
      "nan\n",
      "State College, Pa\n",
      "nan\n",
      "Somewhere Only We Know ?\n",
      "nan\n",
      "Chicago, IL\n",
      "nan\n",
      "mind ya business\n",
      "nor*cal\n",
      "nan\n",
      "nan\n",
      "West Palm Beach, Florida\n",
      "Atlanta, GA\n",
      "philly\n",
      "nan\n",
      "Mackem in Bolton\n",
      "nan\n",
      "Wilmington, NC\n",
      "Johns Creek, GA\n",
      " New Delhi \n",
      "nan\n",
      "Raleigh, NC\n",
      " Alberta\n",
      "#BossNation!\n",
      "pittsboro\n",
      "Charlotte, NC\n",
      "NC || OR\n",
      "Espaรยฑa - Spain - Espagne\n",
      "#PhanTrash\n",
      "Desert Storm?? |BCHS|\n",
      "Cleveland, OH - San Diego, CA\n",
      "Ottawa, Ontario\n",
      "Santiago de Chile\n",
      "nan\n",
      "Camaquรยฃ/Pelotas\n",
      "nan\n",
      "nan\n",
      "Taylor Swift\n",
      "Oklahoma City\n",
      "NYC\n",
      "Austin/Los Angeles\n",
      "Florida\n",
      "Taylor Swift\n",
      "U.S.A and Canada\n",
      "In a crazy genius mind\n",
      "hatena bookmark\n",
      "nan\n",
      "nan\n",
      "Florida\n",
      "nan\n",
      "nan\n",
      "Docker container\n",
      "nan\n",
      "Asheville, NC\n",
      "Taylor Swift\n",
      "DMV\n",
      "philly \n",
      "south africa eastern cape\n",
      "Amazon Seller , Propagandist\n",
      "??\n",
      "nan\n",
      "nan\n",
      "NJ/NY/NM/NE/ND\n",
      "Florida\n",
      "nan\n",
      "nan\n",
      "3???2???????\n",
      "Berlin, Germany\n",
      "Canada\n",
      "San Francisco, CA\n",
      "nan\n",
      "nan\n",
      "Cimahi,West Java,Indonesia\n",
      "Bukittinggi  ?? Sumatera Barat\n",
      "Asia\n",
      "nan\n",
      "Stateless Global Citizen\n",
      "india\n",
      "USA\n",
      "San Francisco\n",
      "nan\n",
      "Halton Region\n",
      "Asia\n",
      "Los Angeles, Calif.\n",
      "Indonesia\n",
      "nan\n",
      "Bukittinggi  ?? Sumatera Barat\n",
      "nan\n",
      "nan\n",
      "Washington, D.C.\n",
      "nan\n",
      "Warm Heart Of Africa\n",
      "รรT: 27.9136024,-81.6078532\n",
      "nan\n",
      "nan\n",
      "Vร_sterรยดs, Sweden\n",
      "Reality\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "Chicago, IL\n",
      "nan\n",
      "nan\n",
      "Roadside\n",
      "Nigeria\n",
      "London/Lagos/FL รรT: 6.6200132,\n",
      "nan\n",
      "Nigeria\n",
      "nan\n",
      "Lagos\n",
      "Nigeria\n",
      "nan\n",
      "The Land of MAss Stupidity\n",
      "dorito land\n",
      "Nigeria\n",
      "Gidi\n",
      "Email: Lovethterry@gmail.com\n",
      "nan\n",
      "Nigeria\n",
      "Nigeria\n",
      "nan\n",
      "nan\n",
      "Homs- Syria\n",
      "lagos. Unilag\n",
      "nan\n",
      "Did anybody see me here ??\n",
      "Worldwide\n",
      "nan\n",
      "Na waffi\n",
      "nan\n",
      "nan\n",
      "GLOBAL\n",
      "Nigeria\n",
      "Nigeria\n",
      "Worldwide\n",
      "Nigeria\n",
      "WorldWide\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "Helsinki\n",
      "Saudi Arabia\n",
      "nan\n",
      "Garden City, NY\n",
      "nan\n",
      "Worldwide\n",
      "19.600858, -99.047821\n",
      "United States\n",
      "nan\n",
      "nan\n",
      "Indonesia\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "africa\n",
      "Moscow\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "Birmingham, United Kingdom\n",
      "nigeria\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "Australia\n",
      "Minority Privilege, USA\n",
      "England\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "Paris\n",
      "USA\n",
      "nan\n",
      "Istanbul\n",
      "nan\n",
      "Chicago IL\n",
      "nan\n",
      "Principality of Zeron\n",
      "nan\n",
      "USA\n",
      "GCC\n",
      "On a beach \n",
      "Belgium\n",
      "nan\n",
      "Australia\n",
      "Memphis\n",
      "EARTH\n",
      "nan\n",
      "Lagos\n",
      "USA\n",
      "Tokyo & Osaka\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "mainly California\n",
      "nan\n",
      "18 | 509 \n",
      "nan\n",
      "glasgow\n",
      "New York\n",
      "nan\n",
      "nan\n",
      "Bon Temps Louisiana\n",
      "nan\n",
      "my house\n",
      "nan\n",
      "Beacon Hills\n",
      "Derby\n",
      "West Coast, USA\n",
      "Gaborone, Botswana\n",
      "Bracknell\n",
      "London, England\n",
      "{GoT | Modern AU | Lizz}\n",
      "Cardiff, Wales\n",
      "nan\n",
      "NAIROBI  KENYA \n",
      "nan\n",
      "In your mind\n",
      "nan\n",
      "Virginia\n",
      "Essex\n",
      "oxford\n",
      "New York\n",
      "Wisconsin\n",
      "NYC\n",
      "nan\n",
      "Gloucester\n",
      "San Jose, CA\n",
      "i beg vines sorry \n",
      "nan\n",
      "nan\n",
      "Ashburn, VA\n",
      "nan\n",
      "toledo\n",
      "Semarang, Indonesia\n",
      "nan\n",
      "nan\n",
      "United States\n",
      "Lincoln City Oregon\n",
      "nan\n",
      "nan\n",
      "Fountain City, IN \n",
      "gaffney, sc \n",
      "nan\n",
      "nan\n",
      "???????, Texas\n",
      "United States\n",
      "nan\n",
      "Camberwell, Melbourne\n",
      "Kansas\n",
      "ava\n",
      "nan\n",
      "nj/ny\n",
      "Barbados\n",
      "Back East in PA\n",
      "? icon by @Hashiren_3 ?\n",
      "EveryWhere\n",
      "Gotham City\n",
      "nan\n",
      "nan\n",
      "Trapped in my Conscience \n",
      "nan\n",
      "death star\n",
      "nan\n",
      "nan\n",
      "London\n",
      "nan\n",
      "nan\n",
      "Texas, USA\n",
      "Brentwood,TN\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Auburn \n",
      "NYC\n",
      "Harlem, NY or Chocolate City\n",
      "Mumbai, Maharashtra\n",
      "?\n",
      "nan\n",
      "Washington, USA\n",
      "nan\n",
      "nan\n",
      "Polmont \n",
      "Port Harcourt, Nigeria\n",
      "Predjama, Eslovenia.\n",
      "Pakistan\n",
      "Tn\n",
      "nan\n",
      "nan\n",
      "South Asia\n",
      "nan\n",
      "Reading a romance novel\n",
      "Glasgow\n",
      "Bulgaria\n",
      "U.K.\n",
      "Scotland\n",
      "nan\n",
      "Puerto Rico\n",
      "nan\n",
      "Austin, TX\n",
      "nan\n",
      "AUSTRALIA-SOUTHAFRICA-CAMBODIA\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "Port Williams NS\n",
      "nan\n",
      "nan\n",
      "Chicago, IL\n",
      "Dallas, TX \n",
      "Sรยฃo Paulo, Brasil\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "WORLD\n",
      "Stowmarket\n",
      "Philadelphia, Pennsylvania\n",
      "Texas\n",
      "Lake Monticello, VA\n",
      "Sao Paulo\n",
      "Upstairs.\n",
      "WV, love the blue and gold\n",
      "Berlin, NY, DC, Malibu\n",
      "nan\n",
      "Shanghai\n",
      "Upstairs.\n",
      "Anywhere Safe\n",
      "Marietta, GA\n",
      "nan\n",
      "Numenor\n",
      "nan\n",
      "nan\n",
      "everywhere\n",
      "nan\n",
      "nan\n",
      "USA\n",
      "Salt Lake City, UT\n",
      "Winston-Salem, NC\n",
      "nan\n",
      "nan\n",
      "Earth 0\n",
      "New Jersey, USA\n",
      "nan\n",
      "Lancaster, CA\n",
      "nan\n",
      "nan\n",
      "Melbourne\n",
      "nan\n",
      "nan\n",
      "Planet Earth\n",
      "Reston, VA, USA\n",
      "Jeddah_Saudi Arabia.\n",
      "Tampa, FL\n",
      "Riyadh\n",
      "toronto, ontario\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "Bangalore City, India\n",
      "nan\n",
      "Iraq|Afghanistan| RSA |Baghdad\n",
      "peshawar pakistan \n",
      "nan\n",
      "nan\n",
      "world\n",
      "nan\n",
      "nan\n",
      "Beit El - Israel\n",
      "nan\n",
      "New Delhi, India\n",
      "nan\n",
      "Iraq|Afghanistan| RSA |Baghdad\n",
      "Sanganer, Rajasthan\n",
      "Loading...\n",
      "Chennai\n",
      "BILASPUR,CHHATTISGARH,495001\n",
      "Niall's arms\n",
      "Canada\n",
      "proudly South African\n",
      "nan\n",
      "MAD as Hell\n",
      "nan\n",
      "nan\n",
      "MAD as Hell\n",
      "????? ???? ????\n",
      "Florida\n",
      "Charleston S.C.\n",
      "MAD as Hell\n",
      "nan\n",
      "Iraq|Afghanistan| RSA |Baghdad\n",
      "Everywhere\n",
      "nan\n",
      "Hyrule\n",
      "front row at a show\n",
      "Ohio, USA\n",
      "London, UK\n",
      "Arlington, VA\n",
      "God.Family.Money\n",
      "Everywhere\n",
      "Kwajalein/Virginia/Dayton, OH\n",
      "Biรยฑan,Laguna\n",
      "Enfield, UK\n",
      "Atlanta, GA\n",
      "South Carolina, USA\n",
      "nan\n",
      "Enfield, UK\n",
      "Decatur, GA\n",
      "nan\n",
      "London\n",
      "Atlanta, Georgia USA\n",
      "Gander NF\n",
      "Leeds, England\n",
      "nan\n",
      "nan\n",
      "New York, USA\n",
      "nan\n",
      "nowhere\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "Palm Desert, CA\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "Palma, Islas Baleares\n",
      "nan\n",
      "??\n",
      "IndiLand \n",
      "Accra,Ghana\n",
      "Baydestrian\n",
      "Macon, GA\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "gamertag: bexrayandvav \n",
      "Tulsa, Oklahoma\n",
      "nan\n",
      "nan\n",
      "Jupiter\n",
      "nan\n",
      "United States\n",
      "Jupiter\n",
      "USA\n",
      "nan\n",
      "Sydney\n",
      "El Dorado, Arkansas\n",
      "East Coast\n",
      "Asheville, NC\n",
      "Oklahoma City, OK\n",
      "WORDLDWIDE\n",
      "73101\n",
      "nan\n",
      "nan\n",
      "Oklahoma City\n",
      "nan\n",
      "South Carolina\n",
      "Tornado Alley, USA \n",
      "nan\n",
      "Nicoma Park, OK\n",
      "Memphis, TN\n",
      "Lethbridge, Alberta, Canada\n",
      "nan\n",
      "Helsinki\n",
      "Killafornia made me \n",
      "Lethbridge, AB, Canada\n",
      "El Dorado, Arkansas\n",
      "NC\n",
      "nan\n",
      "God is Love. \n",
      "cognitive dissonance town\n",
      "I O W A\n",
      "Gurgaon, Haryana. \n",
      "Wherever I'm sent\n",
      "nan\n",
      "nan\n",
      "Austin\n",
      "Midwest\n",
      "Canada\n",
      "nan\n",
      "Midwest\n",
      "Asheville, NC\n",
      "Fort Knox, KY 40121\n",
      "Los Angeles\n",
      "california\n",
      "Toronto\n",
      "San Antonio, TX\n",
      "Providence RI / Lisnaskea \n",
      "Dindigul,TamilNadu.\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "canada\n",
      "khartoum sudan\n",
      "Naperville\n",
      "  \n",
      "nan\n",
      "nan\n",
      "Kentucky, USA\n",
      "nan\n",
      "New Delhi, India\n",
      "nan\n",
      "nan\n",
      "New Jersey/New York\n",
      "#SandraBland\n",
      "nan\n",
      "nan\n",
      "'Merica\n",
      "nan\n",
      "Pennsylvania, USA\n",
      "Harper Woods, MI\n",
      "India\n",
      "San Diego, CA\n",
      "nan\n",
      "dubai \n",
      "nan\n",
      "nan\n",
      "staggering on tenement roofs\n",
      "nan\n",
      "Canada\n",
      "Silicon Valley\n",
      "nan\n",
      "houston\n",
      "nan\n",
      "Orlando \n",
      "nan\n",
      "nan\n",
      "Noida, NCR, India\n",
      "Mรยฉxico\n",
      "India\n",
      "NYC / International\n",
      "nan\n",
      "nan\n",
      "Jamaica\n",
      "Los Angeles, CA\n",
      "America\n",
      "PA, USA\n",
      "London, England\n",
      "call me peach or sam lo\n",
      "LITTLETON, CO, USA, TERRAN\n",
      "Orlando\n",
      "shoujo hell \n",
      "10 Steps Ahead.  Cloud 9\n",
      "central chazifornia\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "New York City\n",
      "Utah\n",
      "????s ?? ????รรธ????รยกa\n",
      "T E X A S | wwat 8.24.14\n",
      "nan\n",
      "worldwide\n",
      "876 Jamrock.\n",
      "nan\n",
      "Puerto Rico\n",
      "nan\n",
      "Georgia, USA\n",
      "รฅ_รฅ_Los Mina Cityยรฃยข\n",
      "nan\n",
      "St Louis, MO\n",
      "nan\n",
      "WORLDWIDE!\n",
      "Greensburg, PA\n",
      "nan\n",
      "Like us on Face \n",
      "nan\n",
      "Florida\n",
      "nan\n",
      "Montgomery County, MD\n",
      "Cambridge, Massachusetts, U.S.\n",
      "Detroit, MI\n",
      "LOCAL ATLANTA NEWS 4/28/00 - 4/28/15 FREELANCER\n",
      "Your notifications\n",
      "nan\n",
      "Methville, CA\n",
      "I rap to burn shame.\n",
      "nan\n",
      "Houston, TX\n",
      "nan\n",
      "nan\n",
      "Los Angeles New York\n",
      "www.aprylpooley.com\n",
      "nan\n",
      " Little Rock, AR\n",
      "The Jewfnited State\n",
      "nan\n",
      "Minneapolis, MN\n",
      "Gumptown\n",
      "nan\n",
      "Nashville, TN\n",
      "World\n",
      "nan\n",
      "The Triskelion\n",
      "Texas, USA\n",
      "Minneapolis/St. Paul\n",
      "nan\n",
      "Chicago, Illinois\n",
      "Colorado\n",
      "London\n",
      "Ireland\n",
      "Kirkwall\n",
      "nan\n",
      "uk\n",
      "Sweden\n",
      "Portsmouth, UK\n",
      "cork\n",
      "Scotland\n",
      "dublin \n",
      "ELVY\n",
      "Hackney, London\n",
      "nan\n",
      "nan\n",
      "South Africa\n",
      "Tunbridge Wells\n",
      "lowestoft\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "North East / Middlesbrough \n",
      "nan\n",
      "London\n",
      "nan\n",
      "Londonstan\n",
      "nan\n",
      "Tring, UK\n",
      "Stage with Trey Songz\n",
      "nan\n",
      "nan\n",
      "Hampstead, London.\n",
      "Tamworth\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "Kawartha Lakes, Ontario, Canad\n",
      "Gold Coast, Australia\n",
      "nan\n",
      "YA MOTHA BED\n",
      "Atlanta\n",
      "Canada\n",
      "Indiana, USA\n",
      "nan\n",
      "Illumination \n",
      "Bathtub de Bett \n",
      "Chasing My Dreams w/Jass??\n",
      "Boston, MA\n",
      "NYC\n",
      "nan\n",
      "Rochester Hills, MI\n",
      "Davis, California\n",
      "Displaced Son of TEXAS!\n",
      "nan\n",
      "?\n",
      "on twitter \n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "Wolverhampton\n",
      "North Carolina \n",
      "Manila City\n",
      "Palo Alto, CA\n",
      "USA\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "Washington, DC\n",
      "nan\n",
      "nan\n",
      "in the Word of God\n",
      "in the Word of God\n",
      "Washington, DC\n",
      "nan\n",
      "Louavul, KY\n",
      "nan\n",
      "nan\n",
      "IG : Sincerely_TSUNAMI\n",
      "Kleenex factory\n",
      "??????????????\n",
      "nan\n",
      "nan\n",
      "Land Of The Kings\n",
      "Winter Park, Colorado\n",
      "nan\n",
      "The Netherlands\n",
      "#ODU\n",
      "ona block w/ my BOY ??\n",
      "East Islip, NY\n",
      "but i love kaylen ??\n",
      "COMING SOON\n",
      "in the Word of God\n",
      "nan\n",
      "Austin, TX\n",
      "Gotham City,USA\n",
      "nan\n",
      "nan\n",
      "Hawaii\n",
      "BROKE NIGGAS DREAM!!\n",
      "in the Word of God\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "Calgary, Alberta\n",
      "nan\n",
      "Everywhere\n",
      "Las Vegas, NV \n",
      "United States\n",
      "nan\n",
      "Detroit\n",
      "Geneva\n",
      "nan\n",
      "nan\n",
      "London\n",
      "รฅยกรฅยกMidwest ยรยขยรยข\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "Riverdale, GA \n",
      "nan\n",
      "nan\n",
      "Calgary, Alberta\n",
      "NY\n",
      "Plano,TX\n",
      "nan\n",
      "Midwest City, OK\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "Lisbon, Portugal\n",
      "Galapa / Atlรรntico\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "STL ?NOLA\n",
      "nan\n",
      "instagram: bribriony\n",
      "Long Island\n",
      "Downtown Oklahoma City\n",
      "nan\n",
      "london\n",
      "Seattle native in Prescott, AZ\n",
      "nan\n",
      "Savage States of America\n",
      "Calgary, AB, Canada\n",
      "Whole World \n",
      "Ibadan,Oyo state\n",
      "USA\n",
      "ngapain?\n",
      "Tema,Accra\n",
      "phuket thailand\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "USA\n",
      "Houston, TX\n",
      "Unites States\n",
      "nan\n",
      "REPUBLICA DOMINICANA\n",
      "New York \n",
      "Santiago,Repรยผblica Dominicana\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "Wilmington, Delaware\n",
      "nan\n",
      "iamdigitalent.com\n",
      "Evergreen Colorado\n",
      "Washington, DC\n",
      "india\n",
      "SWMO\n",
      "anzio,italy\n",
      "nan\n",
      "nan\n",
      "The Peach State\n",
      "nan\n",
      "NYC :) Ex- #Islamophobe\n",
      "Dhaka\n",
      "Halifax, Nova Scotia\n",
      "Deadend, UK\n",
      "Oregon\n",
      "Washington, D.C.\n",
      "nan\n",
      "nan\n",
      "Jamaica\n",
      "USA\n",
      "Chester\n",
      "eBooks, North America\n",
      "Sydney & Worldwide\n",
      "Auckland, New Zealand\n",
      "NYC\n",
      "Scotts Valley, CA\n",
      "Oregon\n",
      "IG/SC:bjfordiani\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "Wisconsin, USA\n",
      "nan\n",
      "East Aurora, NY\n",
      "Woosley\n",
      "nan\n",
      "London, UK\n",
      "Costa Rica\n",
      "Hamilton, Ontario CA\n",
      "maryland\n",
      "Attock\n",
      "Atlanta, GA\n",
      "INDIA\n",
      "Milwaukee, WI\n",
      "nan\n",
      "CPT & JHB, South Africa\n",
      "London, UK\n",
      "Oregon and Washington\n",
      "Karachi\n",
      "Connecticut\n",
      "Perth, Western Australia\n",
      "UK\n",
      "Yuuko-san's shop\n",
      "Watertown, Mass.\n",
      "Toronto\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "Worldwide\n",
      "nan\n",
      "Chicago, IL\n",
      "Milky Way galaxy \n",
      "nan\n",
      "Worldwide\n",
      "us\n",
      "??? ???? ?f glory. ?\n",
      "Your Six\n",
      "South Africa\n",
      "Costa Rica\n",
      "nan\n",
      "nan\n",
      "รรT: 1.50225,103.742992\n",
      "Newcastle, OK\n",
      "United Kingdom\n",
      "nan\n",
      "Oshawa/Toronto\n",
      "Very SW CA, USA....Draenor\n",
      "nan\n",
      "New Zealand\n",
      "Kenya\n",
      "nan\n",
      "Barbados\n",
      "Amsterdam & Worldwide\n",
      "3rd Eye Chakra\n",
      "Perth, Australia\n",
      " ? ??????? ? ( ?? รฅยก ? ? ? รฅยก)\n",
      "USA\n",
      "ARGENTINA\n",
      "nan\n",
      "Earth\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "Hawaii, USA\n",
      "Hawaii, USA\n",
      "nan\n",
      "nan\n",
      "Paris\n",
      "nan\n",
      "California, USA\n",
      "Ted&Qz Inc, Ireland, Europe\n",
      "Santiago de Cmpostela Galicia\n",
      "Massachusetts\n",
      "cleveland, oh\n",
      "Northern Colorado\n",
      "West Coast, Cali USA\n",
      "Indonesia\n",
      "nan\n",
      "right here\n",
      "nan\n",
      "nan\n",
      "nashville, tn \n",
      "Incognito\n",
      "nan\n",
      "Somewhere else...\n",
      "Columbia, SC\n",
      "In a graveyard \n",
      "Downtown Churubusco, Indiana\n",
      "California, USA\n",
      "nan\n",
      "Louisiana\n",
      "nan\n",
      "Host of #MindMoversPodcast\n",
      "The D\n",
      "Still. ??S.A.N.D.O.S??\n",
      "nan\n",
      "We're All Mad Here\n",
      "Southern California\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "New Hampshire, USA\n",
      "ca(NADA) \n",
      "T-Ville\n",
      "Pennsylvania\n",
      "//RP\\ ot @Mort3mer\\\\\n",
      "?????\n",
      "BROOKLYN, NYC\n",
      "nan\n",
      "nan\n",
      "รฅร(?ยรยข`?ยรยขรฅยซ)??\n",
      "New York 2099\n",
      "?\n",
      "nan\n",
      "Massachusetts\n",
      "nan\n",
      "Halifax, NS, Canada\n",
      "UK,singer,songwriter,?2 act\n",
      "kediri,,jawa timur\n",
      "MI - CA\n",
      "Central Florida\n",
      "Des Moines, IA\n",
      "nan\n",
      "nan\n",
      "Washington DC\n",
      "nan\n",
      "California, United States\n",
      "?????\n",
      "nan\n",
      "CT, USA\n",
      "nan\n",
      "www.twitch.tv/PKSparkxx\n",
      "statesboro/vidalia\n",
      "New Jersey, usually\n",
      "Manchester, England\n",
      "Haveli, Maharashtra\n",
      "Slateport City, Hoenn\n",
      "nan\n",
      "jayankondacholapuram.tamilnadu\n",
      "New York, NY\n",
      "Denver Colorado. Fun Times\n",
      "nan\n",
      "Not Of This World\n",
      "ohio\n",
      "California, United States\n",
      "Proud @BuckMasonUSA supporter!\n",
      "Odawara, Japan\n",
      "Houston, TX\n",
      "Ely, Cambridgeshire\n",
      "nan\n",
      "GrC Founder, 8,000 Subscribers\n",
      "Beirut/Toronto\n",
      "Los Angeles \n",
      "nan\n",
      "England\n",
      "Incognito\n",
      "nan\n",
      "Regalo Island\n",
      "nan\n",
      "nan\n",
      "california | oregon | peru |\n",
      "N. California USA\n",
      "Multinational *****\n",
      "nan\n",
      "rural ohio (fuck)\n",
      "??????\n",
      "West\n",
      "Vermont, USA\n",
      "nan\n",
      "Hawthorne, NE\n",
      "nan\n",
      "??? ?????????????\n",
      "Rocky Mountains\n",
      "( ?รฅยก ?? ?รฅยก), \n",
      "nan\n",
      "Kernow\n",
      "nan\n",
      "London\n",
      "Nottingham, England\n",
      "California\n",
      "The Netherlands\n",
      "St. Louis\n",
      "Holly, MI\n",
      "Somewhere between here & there\n",
      "CHICAGO\n",
      "Frostburg\n",
      "nan\n",
      "Thailand\n",
      "Canterbury kent\n",
      "nan\n",
      "The Sun's Corona\n",
      "140920-21 & 150718-19 BEIJING\n",
      "London\n",
      "World\n",
      "Harbour Heights, FL\n",
      "New York\n",
      "Sheff/Bangor/Salamanca/Madrid\n",
      "Stamford & Cork (& Shropshire)\n",
      "nan\n",
      "140920-21 & 150718-19 BEIJING\n",
      "#KaumElite;#F?VOR;#SMOFC\n",
      "Orlando, FL\n",
      "Nairobi, Kenya\n",
      "Richardson TX\n",
      "nan\n",
      "London, Sydney\n",
      "Las Vegas\n",
      "nan\n",
      "Internet\n",
      "Florida\n",
      "Pittsburgh\n",
      "Where I Need To Be\n",
      "Here, unless there.  \n",
      "Manchester\n",
      "NEPA/570\n",
      "nan\n",
      "pettyville, usa\n",
      "Phila.\n",
      "brooklyn, NYC\n",
      "Atlanta - FAU class of '18\n",
      "Bristol, England\n",
      "Houston\n",
      "Twitterville\n",
      "nan\n",
      "Eastern Iowa\n",
      "Los Angeles, CA\n",
      "nan\n",
      "nan\n",
      "Fort Walton Beach, FL\n",
      "USA\n",
      "Dallas, TX\n",
      "Toronto, Bob-Lo, Miami Beach\n",
      "North Carolina, USA\n",
      "planet earth\n",
      "Los Angeles, CA\n",
      "nap central\n",
      "Olathe, KS\n",
      "Madrid\n",
      "Johannesburg, South Africa\n",
      "Hartford, Connecticut\n",
      "New Jersey \n",
      "Cedar Island, Clinton CT 06413\n",
      "nan\n",
      "nan\n",
      "West Vancouver, B.C.\n",
      "London, UK\n",
      "Indiana\n",
      "nan\n",
      "nan\n",
      "Canada\n",
      "nan\n",
      "United States of America\n",
      "Tucson, AZ\n",
      "nan\n",
      "Get our App\n",
      "nan\n",
      "nan\n",
      "Vail Valley\n",
      "Amsterdam | San Francisco\n",
      "Australia\n",
      "nan\n",
      "Ashland, Oregon\n",
      "Worldwide\n",
      "nan\n",
      "Riverside, California.\n",
      "Oakland, CA\n",
      "nan\n",
      "Ottawa, Canada\n",
      "USA\n",
      "Columbus, OH\n",
      "Around the world\n",
      "Washington State\n",
      "nan\n",
      "California, USA\n",
      "nan\n",
      "Lancaster California\n",
      "nan\n",
      "nan\n",
      "?? Cloud Mafia ??\n",
      "nan\n",
      "nan\n",
      "Eddyville, Oregon 97343\n",
      "nan\n",
      "USA\n",
      "Bakersfield, California\n",
      "Galveston, Texas\n",
      "nan\n",
      "Chicago\n",
      "Palm Beach County, FL\n",
      "nan\n",
      "nan\n",
      "Victoria, BC\n",
      "nan\n",
      "Georgia ? Tennessee\n",
      "nan\n",
      "Lagos, Nigeria\n",
      "Puerto Rico\n",
      "nan\n",
      "Gettysburg, PA\n",
      "Austin, Texas\n",
      "nan\n",
      "Florida USA\n",
      "Hermitage, PA\n",
      "nan\n",
      "Jakarta, Indonesia\n",
      "Puerto Rico\n",
      "San Diego, CA\n",
      "calgary,ab\n",
      "Newton, NJ 07860\n",
      "nan\n",
      "she/her/your majesty/empress\n",
      "nan\n",
      "nan\n",
      "Houston\n",
      "nan\n",
      "LYNBROOK\n",
      "Argus Industries \\m/666\\m/\n",
      "nan\n",
      "Friendswood, TX\n",
      "Sugar Land, TX\n",
      "(a) property of the universe\n",
      "Chicago\n",
      "Home is where we park it!\n",
      "Webster, TX\n",
      "nan\n",
      "nan\n",
      "Maracay y Nirgua, Venezuela\n",
      "North Cack/919\n",
      "Scottsdale, AZ\n",
      "United States\n",
      "Paterson, New Jersey \n",
      "New York\n",
      "Venezuela\n",
      "Victoria, Canada\n",
      "FILM OUT LATE 2015\n",
      "Miami,Fla\n",
      "santo domingo\n",
      "Illinois, USA\n",
      "Worldwide - Global\n",
      "nan\n",
      "USA\n",
      "Suva, Fiji Islands.\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "Americas Newsroom\n",
      "nan\n",
      "USA\n",
      "Yogya Berhati Nyaman\n",
      "Washington, D.C.\n",
      "nan\n",
      "Brooklyn, NY\n",
      "worldwide\n",
      "nan\n",
      "nan\n",
      "Fredericksburg, Virginia\n",
      "England, United Kingdom\n",
      "USA\n",
      "nan\n",
      "Wherever I'm needed\n",
      "Yogya Berhati Nyaman\n",
      "Paterson, New Jersey \n",
      "Charlotte \n",
      "Sale, England\n",
      "Newcastle, England \n",
      "Kashmir!\n",
      "nan\n",
      "Kashmir!\n",
      "in my head\n",
      "Earth\n",
      "nan\n",
      "Rutherfordton, NC\n",
      "Lake Highlands\n",
      "Charlotte, N.C.\n",
      "iPhone: 33.104393,-96.628624\n",
      "nan\n",
      "United States\n",
      "The American Wasteland (MV)\n",
      "nan\n",
      "Cleveland, OH\n",
      "Cape Town\n",
      "nan\n",
      "KADUNA, NIGERIA\n",
      "Shady Pines \n",
      "Wales, United Kingdom\n",
      " Alex/Mika/Leo|18|he/she/they\n",
      "North Carolina\n",
      "?Gangsta OC / MV RP; 18+.?\n",
      "Earth: Senseless nonsense\n",
      "Tampa, FL\n",
      "Not Steven Yeun / AMC.\n",
      "cody, austin follows ?*?\n",
      "Paterson, New Jersey \n",
      "nan\n",
      "Charlotte\n",
      "moss chamber b\n",
      "nan\n",
      "nan\n",
      "1/10 Taron squad\n",
      "nan\n",
      "Argentina\n",
      " ?currently writing a book?\n",
      "Lebanon, Tennessee\n",
      "Pratt-on-Wye\n",
      "Somerset, UK\n",
      "Victoria, BC\n",
      "United States\n",
      "nan\n",
      "nan\n",
      "Atlanta, Georgia\n",
      "SF Bay Area, California / Greater Phoenix, AZ\n",
      "Baltimore\n",
      "Alabama, USA\n",
      "nan\n",
      "nan\n",
      "new york\n",
      "Primum non nocere\n",
      "Orlando, FL\n",
      "nan\n",
      "Arlington, TX\n",
      "Gwersyllt, Wales\n",
      "Greenville\n",
      "Greenville, S.C.\n",
      "United States\n",
      "Innsmouth, Mass.\n",
      "nan\n",
      "709?\n",
      "Norwich\n",
      "nan\n",
      "Canada BC\n",
      "Plano, IL\n",
      "scumbernauld\n",
      "Mumbai\n",
      "Tokyo\n",
      "nan\n",
      "Sydney\n",
      "nan\n",
      "WorldWide\n",
      "Mumbai\n",
      "khanna\n",
      "nan\n",
      "nan\n",
      "Africa\n",
      "nan\n",
      "Mumbai, Maharashtra\n",
      "Bangkok\n",
      "Mumbai\n",
      "Mumbai\n",
      "Punjab\n",
      "nan\n",
      "Southern California\n",
      "nan\n",
      "nan\n",
      "Dublin City, Ireland\n",
      "iTunes\n",
      "Mumbai\n",
      "uk\n",
      "nan\n",
      "No ID, No VOTE!!!\n",
      "iTunes\n",
      "Mumbai\n",
      "Mumbai\n",
      "nan\n",
      "Mumbai\n",
      "India\n",
      "nan\n",
      "Maharashtra\n",
      "Mumbai\n",
      "New Delhi,India\n",
      "Xi'an, China\n",
      "Mumbai\n",
      "Brooklyn, NY\n",
      "Robin Hood's County \n",
      "nan\n",
      "United States\n",
      "Pennsylvania\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "Santa Cruz, CA\n",
      "nan\n",
      "nan\n",
      "Milwaukee County\n",
      "Click the link below, okay \n",
      "nan\n",
      "probably not home\n",
      "nan\n",
      "nan\n",
      "6\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "United States\n",
      "At your back\n",
      "nan\n",
      "Livingston, MT\n",
      "California\n",
      "  Glasgow \n",
      "Manhattan, NY\n",
      "Denton, Texas\n",
      "Global\n",
      "TN\n",
      "nan\n",
      "#NewcastleuponTyne #UK\n",
      "nan\n",
      "Vancouver, Canada\n",
      "London \n",
      "Lincoln\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(train['location'])):\n",
    "    print(train['location'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
