{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from functools import partial\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import re\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "#from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Dropout, Flatten, MaxPooling2D\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import KFold\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('sparse.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.2932           30.55m\n",
      "         2           1.2373           33.68m\n",
      "         3           1.1949           33.42m\n",
      "         4           1.1652           32.68m\n",
      "         5           1.1407           32.67m\n",
      "         6           1.1205           30.46m\n",
      "         7           1.1042           29.57m\n",
      "         8           1.0905           28.24m\n",
      "         9           1.0771           26.82m\n",
      "        10           1.0635           26.42m\n",
      "        20           0.9727           22.15m\n",
      "        30           0.9143           20.07m\n",
      "        40           0.8743           18.44m\n",
      "        50           0.8380           17.56m\n",
      "        60           0.8059           16.82m\n",
      "        70           0.7793           16.23m\n",
      "        80           0.7564           15.76m\n",
      "        90           0.7355           15.35m\n",
      "       100           0.7139           14.99m\n",
      "       200           0.5775           12.32m\n",
      "       300           0.4842           10.45m\n",
      "       400           0.4177            8.87m\n",
      "       500           0.3685            7.30m\n",
      "       600           0.3256            5.80m\n",
      "       700           0.2911            4.32m\n",
      "       800           0.2613            2.86m\n",
      "       900           0.2335            1.43m\n",
      "      1000           0.2085            0.00s\n",
      "[[374  64]\n",
      " [103 221]]\n",
      "Accuracy: 0.781%\n",
      "Precision: 0.784%\n",
      "Recall: 0.854%\n",
      "F: 0.817\n"
     ]
    }
   ],
   "source": [
    "# (old) Gradient boosting classifier\n",
    "# Lots of parameters, but let's just see how the default does first...\n",
    "gbc = GradientBoostingClassifier(learning_rate=0.25, n_estimators=1000, max_depth=5, verbose=1)\n",
    "gbc.fit(train_x, train_y)\n",
    "preds = gbc.predict(test_x)\n",
    "gbc_cm = confusion_matrix(test_y, preds)\n",
    "print(gbc_cm)\n",
    "tp, fn, fp, tn = gbc_cm.ravel()\n",
    "accuracy = (tp+tn)/(tp+fn+fp+tn)\n",
    "precision = tp/(tp + fp)\n",
    "recall = tp/(tp + fn)\n",
    "F = 2*precision*recall/(precision + recall)\n",
    "print('Accuracy: {:.3f}%\\nPrecision: {:.3f}%\\nRecall: {:.3f}%\\nF: {:.3f}'.format(accuracy, precision, recall, F))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the approaches that will be tested:\n",
    "\n",
    "Preprocessing applied to both training and test data:\n",
    "\n",
    "* Tokenize with a special tokenizer designed for tweets\n",
    "* Make all text lowercase\n",
    "* Remove tokens that only contain punctuation or numbers\n",
    "* Remove mentions and URLs\n",
    "* Remove tokens with certain special characters\n",
    "* Remove stopwords\n",
    "* Lemmatize tokens\n",
    "* Remove tokens that only occur once\n",
    "* For the test data: Remove tokens that don't occur in the training data (this could be a problem)\n",
    "\n",
    "Options that were not pursued:\n",
    "\n",
    "* Add in the location and keyword data, or maybe just whether or not they are present\n",
    "* Add in the sentiment data\n",
    "\n",
    "Minimal model: Take the set intersection between the unique tokens from the training and test datasets, and use those as the variables. This leaves 2618 variables.\n",
    "\n",
    "Further preprocessing options:\n",
    "\n",
    "* Use the raw counts of each word in each document\n",
    "* Use the document-length normalized frequencies\n",
    "* Use the TF-IDF transformed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the train and test data\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "# Tokenizer for Twitter derived tweetmotif from the ARK, developed at CMU\n",
    "tweetMotif = r''' (?x)\t# set flag to allow verbose regexps\n",
    "      (?:https?://|www)\\S+      # simple URLs\n",
    "      | (?::-\\)|;-\\))\t\t# small list of emoticons\n",
    "      | &(?:amp|lt|gt|quot);    # XML or HTML entity\n",
    "      | \\#\\w+                 # hashtags\n",
    "      | @\\w+                  # mentions   \n",
    "      | \\d+:\\d+               # timelike pattern\n",
    "      | \\d+\\.\\d+              # number with a decimal\n",
    "      | (?:\\d+,)+?\\d{3}(?=(?:[^,]|$))   # number with a comma\n",
    "      | (?:[A-Z]\\.)+                    # simple abbreviations\n",
    "      | (?:--+)               # multiple dashes\n",
    "      | \\w+(?:-\\w+)*          # words with internal hyphens or apostrophes\n",
    "      | ['\\\".?!,:;/]+         # special characters\n",
    "      '''\n",
    "\n",
    "# Read in stopwords and add in some special ones\n",
    "fstop = open('smart.english.stop', 'r')\n",
    "stoptext = fstop.read()\n",
    "fstop.close()\n",
    "stopwords = nltk.word_tokenize(stoptext)\n",
    "stopwords.extend(['&amp;', '&lt;', '&gt;', 'as', 'ur', 'isn', 'don', 'wa'])\n",
    "\n",
    "# Initializer the lemmatizer\n",
    "wnl = nltk.WordNetLemmatizer()\n",
    "\n",
    "# Filters out tokens consisting only of punctuation and/or numbers\n",
    "def alpha_filter(w):\n",
    "    pattern = re.compile('^[^a-z]+$')\n",
    "    return pattern.match(w)\n",
    "\n",
    "# Applies the previous filter, and also removes mentions, URLs, tokens with special characters, and stopwords\n",
    "def word_filter(word):\n",
    "    return not (alpha_filter(word) or '@' in word or '//' in word or 'รฅ' in word or 'รป' in word or word in stopwords)\n",
    "\n",
    "# Applies tokenizaion, lemmatization, and filters\n",
    "def document_analyze(document):\n",
    "    return list(filter(word_filter, map(wnl.lemmatize, nltk.regexp_tokenize(document, pattern=tweetMotif))))\n",
    "\n",
    "def POS_process(word):\n",
    "    if word == 'i':\n",
    "        return 'I'\n",
    "    # In case hashtags are actual words, this may help\n",
    "    # If they are made from multiple words, that could be a problem\n",
    "    if word.startswith('#'):\n",
    "        return word[1:]\n",
    "    # Leave mentions unchanged\n",
    "    # Replace links with a placeholder\n",
    "    if '//' in word:\n",
    "        return 'LINK' # Maybe?\n",
    "    return word\n",
    "\n",
    "def POS_translate(document):\n",
    "    return [word + '/' + tag for (word, tag) in nltk.pos_tag(document)]\n",
    "\n",
    "def POS_analyze(document):\n",
    "    return POS_translate(list(map(POS_process, nltk.regexp_tokenize(document, pattern=tweetMotif))))\n",
    "\n",
    "# Creates the term-document matrix\n",
    "def vectorize_data(data, prepro=True):\n",
    "    # Make all the text lowercase\n",
    "    documents = [text.lower() for text in data['text']]\n",
    "\n",
    "    # Run the vectorization process with the custom analyzer\n",
    "    vec = None\n",
    "    if prepro:\n",
    "        vec = CountVectorizer(analyzer = document_analyze)\n",
    "    else:\n",
    "        # No preprocessing\n",
    "        vec = CountVectorizer(analyzer = partial(nltk.regexp_tokenize, pattern=tweetMotif))\n",
    "    cv = vec.fit_transform(documents)\n",
    "    df = pd.DataFrame(cv.toarray(), columns=vec.get_feature_names())\n",
    "\n",
    "    if prepro:\n",
    "        # Remove words that only occur once\n",
    "        toDrop = []\n",
    "        for i in range(len(df.columns)):\n",
    "            if np.sum(df[df.columns[i]] > 0) < 2: # Threshold for minimum number of documents with this word\n",
    "                toDrop.append(df.columns[i])\n",
    "        df = df.drop(columns=toDrop)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def POS_vectorize_data(data):\n",
    "    documents = [text.lower() for text in data['text']]\n",
    "    vec = CountVectorizer(analyzer = POS_analyze)\n",
    "    cv = vec.fit_transform(documents)\n",
    "    df = pd.DataFrame(cv.toarray(), columns = vec.get_feature_names())\n",
    "     # Remove word-tag pairs that only occur once\n",
    "    toDrop = []\n",
    "    for i in range(len(df.columns)):\n",
    "        if np.sum(df[df.columns[i]] > 0) < 2: # Threshold for minimum number of documents with this word\n",
    "            toDrop.append(df.columns[i])\n",
    "    df = df.drop(columns=toDrop)\n",
    "    return df\n",
    "\n",
    "def ngram_vectorize_data(data, n=2):\n",
    "    documents = [text.lower() for text in data['text']]\n",
    "    vec = CountVectorizer(ngram_range = (1,n), analyzer = partial(nltk.regexp_tokenize, pattern=tweetMotif))\n",
    "    cv = vec.fit_transform(documents)\n",
    "    df = pd.DataFrame(cv.toarray(), columns = vec.get_feature_names())\n",
    "    toDrop = []\n",
    "    for i in range(len(df.columns)):\n",
    "        if np.sum(df[df.columns[i]] > 0) < 2: # Threshold for minimum number of documents with this word\n",
    "            toDrop.append(df.columns[i])\n",
    "    df = df.drop(columns=toDrop)\n",
    "    return df\n",
    "\n",
    "# Vectorize training set\n",
    "dftrain = vectorize_data(train)\n",
    "dftrain_noprep = vectorize_data(train, prepro=False)\n",
    "# Add back in the target variable\n",
    "dftrain['_target'] = train['target']\n",
    "dftrain_noprep['_target'] = train['target']\n",
    "\n",
    "# Vectorize test set\n",
    "dftest = vectorize_data(test)\n",
    "dftest_noprep = vectorize_data(test, prepro=False)\n",
    "\n",
    "# Vectorize with POS tagging\n",
    "dftrain_pos = POS_vectorize_data(train)\n",
    "dftest_pos = POS_vectorize_data(test)\n",
    "\n",
    "# Vectorize with bigrams, no preprocessing (since that seems to work better in other situations)\n",
    "dftrain_bi = ngram_vectorize_data(train)\n",
    "dftest_bi = ngram_vectorize_data(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_frequency_transformations(dftrain, dftest):\n",
    "    # Find the tokens in both training and testing sets\n",
    "    combined = set.intersection(set(dftrain.columns), set(dftest.columns))\n",
    "\n",
    "    # minimal: only include features which are in both\n",
    "    dftrain_minimal = dftrain.copy()\n",
    "    dftest_minimal = dftest.copy()\n",
    "\n",
    "    # Drop the columns not in the intersection (including _target, which will be added back in at the end)\n",
    "    toDrop = []\n",
    "    for col in dftrain.columns:\n",
    "        if col not in combined:\n",
    "            toDrop.append(col)\n",
    "    dftrain_minimal = dftrain_minimal.drop(columns=toDrop)\n",
    "\n",
    "    toDrop = []\n",
    "    for col in dftest.columns:\n",
    "        if col not in combined:\n",
    "            toDrop.append(col)\n",
    "    dftest_minimal = dftest_minimal.drop(columns=toDrop)\n",
    "\n",
    "    # Frequency normalization (normalize by the total length in terms of tokens remaining, so excluding things like stopwords)\n",
    "\n",
    "    # Count the number of tokens left in each document, and prevent division by 0\n",
    "    train_document_lengths = np.array(np.sum(dftrain_minimal, axis=1)).reshape(-1,1)\n",
    "    train_document_lengths[train_document_lengths == 0] = 1\n",
    "    test_document_lengths = np.array(np.sum(dftest_minimal, axis=1)).reshape(-1,1)\n",
    "    test_document_lengths[test_document_lengths == 0] = 1\n",
    "\n",
    "    # Make normalized frequencies\n",
    "    dftrain_min_norm = dftrain_minimal.copy()\n",
    "    dftest_min_norm = dftest_minimal.copy()\n",
    "\n",
    "    # Divide by the document lengths, row-wise\n",
    "    dftrain_min_norm /= train_document_lengths\n",
    "    dftest_min_norm /= test_document_lengths\n",
    "\n",
    "    # TF-IDF transform\n",
    "\n",
    "    # Note: The training and testing dataframes have the same columns.\n",
    "    # When computing inverse document frequency, use the combined data from both training and test sets.\n",
    "\n",
    "    # Find the inverse document frequency\n",
    "    #log(train.shape[0]/count)\n",
    "    # Because we removed columns with only one occurrence for both the train and test sets, these counts will always be 4 or more.\n",
    "    counts = np.array(np.sum(dftrain_minimal > 0, axis=0)) + np.array(np.sum(dftest_minimal > 0, axis=0))\n",
    "\n",
    "    # Take the log of the inverse document frequency\n",
    "    idf_vals = np.log10((train.shape[0]+test.shape[0])/counts)\n",
    "\n",
    "    dftrain_min_tfidf = dftrain_min_norm.copy()\n",
    "    dftest_min_tfidf = dftest_min_norm.copy()\n",
    "\n",
    "    # Multiply by the IDF values, column-wise\n",
    "    dftrain_min_tfidf *= idf_vals\n",
    "    dftest_min_tfidf *= idf_vals\n",
    "\n",
    "    # Add back in the _target variable to these three preprocessing variants\n",
    "    dftrain_minimal['_target'] = train['target']\n",
    "    dftrain_min_norm['_target'] = train['target']\n",
    "    dftrain_min_tfidf['_target'] = train['target']\n",
    "    \n",
    "    return dftrain_minimal, dftest_minimal, dftrain_min_norm, dftest_min_norm, dftrain_min_tfidf, dftest_min_tfidf\n",
    "\n",
    "dftrain_minimal_noprep, dftest_minimal_noprep, dftrain_min_norm_noprep, dftest_min_norm_noprep, dftrain_min_tfidf_noprep, dftest_min_tfidf_noprep = apply_frequency_transformations(dftrain_noprep, dftest_noprep)\n",
    "dftrain_minimal, dftest_minimal, dftrain_min_norm, dftest_min_norm, dftrain_min_tfidf, dftest_min_tfidf = apply_frequency_transformations(dftrain, dftest)\n",
    "dftrain_pos_minimal, dftest_pos_minimal, dftrain_pos_min_norm, dftest_pos_min_norm, dftrain_pos_min_tfidf, dftest_pos_min_tfidf = apply_frequency_transformations(dftrain_pos, dftest_pos)\n",
    "dftrain_bi_minimal, dftest_bi_minimal, dftrain_bi_min_norm, dftest_bi_min_norm, dftrain_bi_min_tfidf, dftest_bi_min_tfidf = apply_frequency_transformations(dftrain_bi, dftest_bi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# e.g. split=0.3, trees=500, quick evaluation\n",
    "def run_randomForest(dataset, split, trees):\n",
    "    t0 = time.time()\n",
    "    np.random.seed(1)\n",
    "    train, test = train_test_split(dataset, test_size=split)\n",
    "    train_x = train.drop('_target', axis=1)\n",
    "    train_y = train['_target']\n",
    "    test_x = test.drop('_target', axis=1)\n",
    "    test_y = test['_target']\n",
    "    print('Training...')\n",
    "    rft = RandomForestClassifier(n_estimators=trees)\n",
    "    rft.fit(train_x, train_y)\n",
    "    print('Predicting...')\n",
    "    preds = rft.predict(test_x)\n",
    "    cm = confusion_matrix(test_y, preds)\n",
    "    print(cm)\n",
    "    tp, fn, fp, tn = cm.ravel()\n",
    "    accuracy = (tp+tn)/(tp+fn+fp+tn)\n",
    "    precision = tp/(tp+fp)\n",
    "    recall = tp/(tp+fn)\n",
    "    F = 2*precision*recall/(precision+recall)\n",
    "    print('Accuracy: {:.2f}%\\nPrecision: {:.2f}%\\nRecall: {:.2f}%\\nF: {:.2f}'.format(100*accuracy, 100*precision, 100*recall, 100*F))\n",
    "    t1 = time.time()\n",
    "    print('(Took {:.3f} sec)'.format(t1-t0))\n",
    "    return rft\n",
    "\n",
    "# 5-fold cross-validation, all metrics\n",
    "def cross_validate_random_forest(dataset, trees):\n",
    "    t_0 = time.time()\n",
    "    # Fixed at 5-fold cross-validation for now\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "    X = np.array(dataset.drop('_target', axis=1))\n",
    "    y = np.array(dataset['_target'])\n",
    "    accs = []\n",
    "    precs_p = []\n",
    "    precs_n = []\n",
    "    recs_p = []\n",
    "    recs_n = []\n",
    "    Fs_p = []\n",
    "    Fs_n = []\n",
    "    mFs = [] # Mean F score, this is the actual competition metric\n",
    "    cm = np.zeros((2,2))\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        t0 = time.time()\n",
    "        train_x, test_x = X[train_index], X[test_index]\n",
    "        train_y, test_y = y[train_index], y[test_index]\n",
    "        #print('Training...')\n",
    "        rft = RandomForestClassifier(n_estimators=trees)\n",
    "        rft.fit(train_x, train_y)\n",
    "        #print('Predicting...')\n",
    "        preds = rft.predict(test_x)\n",
    "        cm_batch = confusion_matrix(test_y, preds)\n",
    "        cm += np.array(cm_batch)\n",
    "        tp, fn, fp, tn = cm_batch.ravel()\n",
    "        acc = (tp+tn)/(tp+fn+fp+tn)\n",
    "        accs.append( acc )\n",
    "        prec_p = tp/(tp+fp)\n",
    "        precs_p.append( prec_p )\n",
    "        prec_n = tn/(tn+fn)\n",
    "        precs_n.append( prec_n )\n",
    "        rec_p = tp/(tp+fn)\n",
    "        recs_p.append( rec_p )\n",
    "        rec_n = tn/(tn+fp)\n",
    "        recs_n.append( rec_n )\n",
    "        F_p = 2*prec_p*rec_p/(prec_p+rec_p)\n",
    "        Fs_p.append( F_p )\n",
    "        F_n = 2*prec_n*rec_n/(prec_n+rec_n)\n",
    "        Fs_n.append( F_n )\n",
    "        mF = (F_p + F_n)/2.0\n",
    "        mFs.append( mF )\n",
    "        t1 = time.time()\n",
    "        print('(Took {:.3f} sec)'.format(t1-t0))\n",
    "    t_1 = time.time()\n",
    "    print('Combined confusion matrix:')\n",
    "    print(cm)\n",
    "    print('(Overall, took {:.3f} sec)'.format(t_1-t_0))\n",
    "    return [accs, precs_p, precs_n, recs_p, recs_n, Fs_p, Fs_n, mFs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Predicting...\n",
      "[[204  15]\n",
      " [ 66  96]]\n",
      "Accuracy: 78.74%\n",
      "Precision: 75.56%\n",
      "Recall: 93.15%\n",
      "F: 83.44\n",
      "(Took 134.297 sec)\n"
     ]
    }
   ],
   "source": [
    "# Test model to see feature importances\n",
    "rf = run_randomForest(dftrain_minimal_noprep, 0.05, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA8MAAAOjCAYAAACfgdZsAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAgAElEQVR4nOzdebRdd3nn6e8by4lAxrKLweBCICAIOpjBZZHghKSARjRQCcOCNCmcECCFIAQMooHyCsUUQgEJCxdTQqloyhloIMxuQiAizCQOluM5BKeYmmJQM9gyCGzAvP3H3e5cX64sCUv3nHt/z7OWlvbde5993nPMWvjj377nVHcHAAAARvITsx4AAAAAVpoYBgAAYDhiGAAAgOGIYQAAAIYjhgEAABiOGAYAAGA4YhgADlFV/euq+tuq+lZVvXjW8xyKqvqpqvp2VZ0461kAYJbEMABrwhR41/75YVV9d9HPpx3mp3tyks939026+zk35EJV9eaq+k+Haa4D6u6ru/uY7v7ySj3n/lTV+qrqqrr1rGcBYDzrZj0AABwO3X3MtdtV9fkk/6G7P3CEnu62Sf7xCF37kFTVuu7+waznOFRV5d9BAJgpK8MADKGqblRVr62qr1TV/6yqP6yqo6djD6yq/1FVL6yqb1bVZ6vqV/dznTcleVSS506rzr9YVUdV1XOnx329qt5YVcdN56+rqrdX1Z6quqKqPlRVd5qOnZ7kEYuu9dblVksXrx4vmvW5VbUnyR9P+x9eVRdNz/GxqvqZ/cx/netP135lVe2qqn1V9eGqukVV/dF0rUur6q6LHv/Vqnp2Vf3T9F7trKqfWnT8d6rqM1X1jap6R1WdsOR5f7uqPpPkkiQfnR726en1P6yqbl5Vf1VVX5uu/+6qutWi659TVc+f/r6yqt5bVccvOn6f6djeqvp/qurRi/75/5eq+uL0Gl597dxVdcuqet/0er9RVR888P+iAFjtxDAAo3hhkrsluWuSU5LcJ8mzFx3fnOQnk9wyyfYkf1JVt1t6ke7+90nenuRF0+3GH0vyrCQPSHLvJLdO8v0kZy562NlJ7jBd+5+S/Ml0rVctudayAb6MzUmOTrIpyelVda8kf5TkcUlumuTPkrzrEFZfH5XkmUluloW7xs5J8pHpWu9N8gdLzv/3Se6X5E5JTs7C609VPTjJc5M8PMm/TvL1JH++5LG/nIX3/+QkvzTtu9P0+t+VhX83eV2S2yS59v0/c8k1Hp3ktCS3SnJckqdNz//TSd6T5A+n2U9Jcumia9w6C//875RkS5IzpmP/Mcmnp9d/qyQv2N8bBcDaIYYBGMVpSZ7f3V/v7j1Jfj/Jbyw6/oMkL+zu7023V38gySMP8tpPTHJGd3+5u6/KQng/qqqqu3/Q3X/S3d9edOxnq2r9DXgtV2choL/X3d+dnv813X1ed1/T3TuT/FQWYvBgvLW7L5yu9e4ke7v7Ld19TZK/yEK4LvbK6bV+LclLshDHycJ7vLO7L5pe67OT/K9VdctFj31xd18xPdeP6O493f3u7v5ud++drv9vl5z237r7M929L8nbktxj2v8bSf7v7n779L5/rbsvnP6jwOOTPG167r1JXprk16bHfT/JiUluM72nHw0Aa54YBmDNq6rKwqrsFxbt/kIWVi+v9bUp4BYfP+AnLk/X3pTkvdNttlckOT8L/x970+k26ZdPt1BfmYWV4crCyuWP66vd/f1FP982ye9e+/zTDDdf8vquz55F299d5udjrnt6vrhoe/H7dGIWvcfdfUWSK5fMsfixP6KqblJVb5hucb4yyV9nYcV2sa8u2v7Oovk2JfnMMpc9MQsr6Zcuen/eleQW0/EXJ/lykg9Nt6A/4/pmBGBtEMMArHnd3VkIqNsu2n2bJF9a9PPNlqzW3iYLgXQw1/5Skvt193GL/qzv7q9n4dblByS5b5KNSe48PbSuvcSSS34vCyuVN16075ZLzln6mC8med6S579xd7/jQPP/mDYt2l78Pn05i97jqtqY5Nhc933u/Wxf64ws3M58z+4+NgvvXS1z3nK+mIXb0Zf6ShZW/u+w6P3Z2N03TZLu3tvdT+vu22bhd7j/U1X9wkE+JwCrlBgGYBRvSvL8qrppVd0iyXNy3d9nPToLH2T1k1V1vyTbsvD7vAfjdUleWlWbkmT6AKpfmY7dJMlVSb6RZEMWbs9ebE+S21/7Q3f/MMnFSU6rhQ/m+pUkpx7g+XcmeWpVba0Fx1TVQ6rqxgd43I/r9Kq6VVXdLAvx+pZp/5uSPKGqTpr+w8LLknywu7+63EW6++oke7Po9Wfh/fpOkium6x/K1079aZJfnj5M7Kjpw7juNq2ivyHJK6vqZtN7tKmqtiXJ9F7dblrl35vkmukPAGuYGAZgFM/LwtchXZrkgiSfyHU/GOrzWVg9/GoWwulx3f3Zg7z2H2Thd4w/WFXfSvK3Sf7NdOz/TPK16boXJ/n4ksfuTHLP6fbdN0/7npKFD7W6PAsfRvWe63vy7v5EktOT/NckVyS5LAsfMrXcyuvh8OYkH0ryz1l4TX8wzfGeLPyO79lZWCW+Za77e9nLeV6St06v/yFJXp6F26K/kYX36r0HO1R3fybJQ5P8bhbeu91J7jIdfvo00+4sBO/7kvz0dOx/SfLhJN/Kwidcv7y7zznY5wVgdaqFu7sAYFxV9cAsfADVTx/w5MFV1VeTPLK7l0Y9AKwqVoYBAAAYjhgGAABgOG6TBgAAYDhWhgEAABiOGAYAAGA462Y9wKzd7GY3682bN896DAAAAI6A88477+vdffOl+4eP4c2bN2f37t2zHgMAAIAjoKq+sNx+t0kDAAAwHDEMAADAcMQwAAAAwxHDAAAADEcMAwAAMBwxDAAAwHDEMAAAAMMRwwAAAAxHDAMAADAcMQwAAMBwxDAAAADDEcMAAAAMRwwDAAAwHDEMAADAcMQwAAAAwxHDAAAADEcMAwAAMBwxDAAAwHDEMAAAAMMRwwAAAAxHDAMAADAcMQwAAMBwxDAAAADDEcMAAAAMRwwDAAAwHDEMAADAcMQwAAAAwxHDAAAADEcMAwAAMBwxDAAAwHDEMAAAAMMRwwAAAAxHDAMAADAcMQwAAMBwxDAAAADDEcMAAAAMRwwDAAAwHDEMAADAcNbNeoBZ23PlVTlz12WzHgPWpB3btsx6BAAAWJaVYQAAAIYjhgEAABiOGAYAAGA4YhgAAIDhiGEAAACGI4YBAAAYjhgGAABgOGIYAACA4YhhAAAAhiOGAQAAGI4YBgAAYDhzH8NV9atV9amq+lBV3aOqHjzrmQAAAFjd5j6Gk/xWkid3932T3CPJIcVwVa07IlMBAACwas1VKFbVu5JsSrI+ySuT3DLJvZPcrqrem+QRSW5UVfdO8pIk70ny6iR3zcJreUF3v7uqHpvk303X2ZDkfiv8UgAAAJhjcxXDSR7f3d+sqhslOTfJv81CyD6zu3dX1YVJtnb3U5Kkqv5zkg929+Or6rgkn6yqD0zXOjXJ3br7mzN4HQAAAMyxeYvh06vq4dP2piR3PMD5D0jykKp65vTz+iS3mbZ37S+Eq2p7ku1JcvwtTrxhEwMAALDqzE0MV9V9ktw/yand/Z2q+nAW4vZ6H5bkEd396SXX+rkk+/b3oO7emWRnkmzaclLfgLEBAABYhebpA7Q2Jrl8CuE7J7nXMud8K8lNFv38/iRPrapKkqo6+ciPCQAAwGo3TzH8viTrquqiJC9Kcs4y53woyc9U1QVV9ajpvKOTXFRVl0w/AwAAwPWam9uku/vqJA9a5tB9Fp3zzST3XHL8ictc66wkZx2+6QAAAFhL5mllGAAAAFaEGAYAAGA4YhgAAIDhiGEAAACGI4YBAAAYjhgGAABgOGIYAACA4YhhAAAAhiOGAQAAGI4YBgAAYDhiGAAAgOGsm/UAs3bCseuzY9uWWY8BAADACrIyDAAAwHDEMAAAAMMRwwAAAAxHDAMAADAcMQwAAMBwxDAAAADDEcMAAAAMZ/jvGd5z5VU5c9dlsx4DOEx8bzgAAAfDyjAAAADDEcMAAAAMRwwDAAAwHDEMAADAcMQwAAAAwxHDAAAADEcMAwAAMBwxDAAAwHDEMAAAAMMRwwAAAAxHDAMAADCcNR3DVfXtWc8AAADA/FnTMQwAAADLmfsYrqp3VdV5VXVpVW2f9n27ql5cVRdW1TlVdcK0/3ZV9XdVdW5VvWi2kwMAADCv5j6Gkzy+u09JsjXJ6VV10yQbkpzT3XdP8tEkT5jOfWWSP+7ueyb56v4uWFXbq2p3Ve3et/fyIzw+AAAA82Y1xPDpVXVhknOSbEpyxyTfS/Ke6fh5STZP27+Q5E3T9p/t74LdvbO7t3b31g0bjz8iQwMAADC/1s16gOtTVfdJcv8kp3b3d6rqw0nWJ/l+d/d02jW57uvoAAAAwPWY95XhjUkun0L4zknudYDzP5Hk16bt047oZAAAAKxa8x7D70uyrqouSvKiLNwqfX2eluR3qurcLIQ0AAAA/Ii5vk26u69O8qBlDh2z6Jy3JXnbtP25JKcuOu+lR3RAAAAAVqV5XxkGAACAw04MAwAAMBwxDAAAwHDEMAAAAMMRwwAAAAxHDAMAADAcMQwAAMBwxDAAAADDEcMAAAAMRwwDAAAwnHWzHmDWTjh2fXZs2zLrMQAAAFhBVoYBAAAYjhgGAABgOGIYAACA4YhhAAAAhiOGAQAAGI4YBgAAYDhiGAAAgOEM/z3De668KmfuumzWYwCHke8OBwDgQKwMAwAAMBwxDAAAwHDEMAAAAMMRwwAAAAxHDAMAADAcMQwAAMBwxDAAAADDEcMAAAAMRwwDAAAwHDEMAADAcMQwAAAAwxHDAAAADGdVx3BVHVdVT571HAAAAKwuqzqGkxyXRAwDAABwSNbNeoAb6KVJ7lBVFyTZNe17UJJO8vvd/ZaZTQYAAMDcWu0rw2ck+Ux33yPJOUnukeTuSe6f5A+r6lbLPaiqtlfV7qravW/v5Ss3LQAAAHNhtcfwYvdO8qbuvqa79yT5SJJ7Lndid+/s7q3dvXXDxuNXdEgAAABmby3FcM16AAAAAFaH1R7D30pyk2n7o0keVVVHVdXNk/xSkk/ObDIAAADm1qr+AK3u/kZVfaKqLknyV0kuSnJhFj5A69nd/dWZDggAAMBcWtUxnCTd/eglu541k0EAAABYNVb7bdIAAABwyMQwAAAAwxHDAAAADEcMAwAAMBwxDAAAwHDEMAAAAMMRwwAAAAxHDAMAADAcMQwAAMBwxDAAAADDWTfrAWbthGPXZ8e2LbMeAwAAgBVkZRgAAIDhiGEAAACGI4YBAAAYjhgGAABgOGIYAACA4YhhAAAAhiOGAQAAGM7w3zO858qrcuauy2Y9BrCK+G5yAIDVz8owAAAAwxHDAAAADEcMAwAAMBwxDAAAwHDEMAAAAMMRwwAAAAxHDAMAADAcMQwAAMBwxDAAAADDEcMAAAAMRwwDAAAwHDEMAADAcA4Yw1W1uaouWWb/71XV/W/oAFX1gqp65n6O/e0NvT4AAAAste7HfWB3P2+5/VV1VHdf8+OPdJ3n+PnDcR0AAABY7GBvkz6qqv5bVV1aVX9dVTeqqrOq6pFJUlWfr6rnVdXHk/xqVd2jqs6pqouq6p1Vdfx03ulV9Y/T/jcvuv7PVNWHq+qzVXX6tTur6tvT3/epqo9U1V9U1WVV9dKqOq2qPllVF1fVHabzfqWq/r6qzq+qD1TVCYflXQIAAGBNOdgYvmOS13b3XZJckeQRy5xzVXffu7vfnORPk/zH7r5bkouTPH8654wkJ0/7n7TosXdO8r8l+dkkz6+qo5e5/t2TPC3JXZP8RpIt3f2zSV6f5KnTOR9Pcq/uPjnJm5M8e7kXU1Xbq2p3Ve3et/fyg3sHAAAAWDMONoY/190XTNvnJdm8zDlvSZKq2pjkuO7+yLT/T5L80rR9UZI3VtWvJ/nBosf+ZXdf3d1fT/L/JlluRffc7v5Kd1+d5DNJ/nraf/GieW6d5P1VdXGSZyW5y3Ivprt3dvfW7t66YePx1/OyAQAAWIsONoavXrR9TZb/XeN9B3Gdf5fktUlOSXJeVV17nYO5/uJzfrjo5x8uOv/VSV7T3XdN8sQk6w9iJgAAAAZz2L9aqbv3Jrm8qn5x2vUbST5SVT+RZFN3fygLty8fl+SYw/z0G5N8adr+zcN8bQAAANaIH/vTpA/gN5O8rqpunOSzSR6X5Kgkfz7dRl1JzuzuK6rqcD7vC5K8taq+lOScJLc7nBcHAABgbajunvUMM7Vpy0n9jNe+Y9ZjAKvIjm1bZj0CAAAHqarO6+6tS/cf9tukAQAAYN6JYQAAAIYjhgEAABiOGAYAAGA4YhgAAIDhiGEAAACGI4YBAAAYjhgGAABgOGIYAACA4YhhAAAAhrNu1gPM2gnHrs+ObVtmPQYAAAAryMowAAAAwxHDAAAADEcMAwAAMBwxDAAAwHDEMAAAAMMRwwAAAAxHDAMAADCc4b9neM+VV+XMXZfNegxglfH95AAAq5uVYQAAAIYjhgEAABiOGAYAAGA4YhgAAIDhiGEAAACGI4YBAAAYjhgGAABgOGIYAACA4YhhAAAAhiOGAQAAGI4YBgAAYDgzjeGq2lxVlyzZt7WqXjVtP7aqXjNtv6CqnnmI1//24ZsWAACAtWLdrAdYqrt3J9k96zkAAABYu+bmNumqun1VnV9Vz6qq9xzg3DtU1fuq6ryq+lhV3Xnaf7uq+ruqOreqXrQykwMAALDazEUMV9Wdkrw9yeOSnHsQD9mZ5KndfUqSZyb5o2n/K5P8cXffM8lXj8SsAAAArH7zEMM3T/LuJL/e3Rcc6OSqOibJzyd5a1VdkOS/JrnVdPgXkrxp2v6z67nG9qraXVW79+29/AYNDwAAwOozD78zvDfJF7MQspcexPk/keSK7r7Hfo73gS7Q3TuzsLqcTVtOOuD5AAAArC3zsDL8vSQPS/KYqnr0gU7u7iuTfK6qfjVJasHdp8OfSPJr0/ZpR2JYAAAAVr95iOF0974kv5xkR5KNB/GQ05L8VlVdmIXV5IdO+5+W5Heq6tyDvA4AAAADqu6x7xLetOWkfsZr3zHrMYBVZse2LbMeAQCAg1BV53X31qX752JlGAAAAFaSGAYAAGA4YhgAAIDhiGEAAACGI4YBAAAYjhgGAABgOGIYAACA4YhhAAAAhiOGAQAAGI4YBgAAYDhiGAAAgOGsm/UAs3bCseuzY9uWWY8BAADACrIyDAAAwHDEMAAAAMMRwwAAAAxHDAMAADAcMQwAAMBwxDAAAADDEcMAAAAMZ/jvGd5z5VU5c9dlsx4DWAN8ZzkAwOphZRgAAIDhiGEAAACGI4YBAAAYjhgGAABgOGIYAACA4YhhAAAAhiOGAQAAGI4YBgAAYDhiGAAAgOGIYQAAAIYjhgEAABjOXMRwVT2kqs44wDl/u5/9Z1XVI4/MZAAAAKxF62Y9QJJ099lJzj7AOT+/QuMAAACwxh2xleGq2lBVf1lVF1bVJVX1qKr6fFXdbDq+tao+PG0/tqpeM22fUFXvnB53YVX9/LT/29PfVVWvqap/rKq/THKLRc95SlV9pKrOq6r3V9WtjtTrAwAAYPU6krdJPzDJl7v77t19UpL3HeTjXpXkI9199yT/JsmlS44/PMmdktw1yROSXBvLRyd5dZJHdvcpSd6Q5MU3+FUAAACw5hzJ26QvTvLyqnpZkvd098eq6mAed78kj0mS7r4myd4lx38pyZumY1+uqg9O+++U5KQku6bnOSrJV5Z7gqranmR7khx/ixMP5TUBAACwBhyxGO7uy6rqlCQPTvKSqvrrJD/Iv6xGr78hl19mXyW5tLtPPYjZdibZmSSbtpy03LUAAABYw47k7wyfmOQ73f3nSV6ehVueP5/klOmUR+znoX+T5LenaxxVVccuOf7RJL82HbtVkvtO+z+d5OZVder02KOr6i6H6/UAAACwdhzJ3xm+a5JPVtUFSZ6T5PeTvDDJK6vqY0mu2c/jnpbkvlV1cZLzkiwN2ncm+ecs3Ib9x0k+kiTd/b0kj0zysqq6MMkFmX6fGAAAABY7krdJvz/J+5c5tGWZc89Kcta0vSfJQ5c555jp707ylP085wVZ+J1iAAAA2K8juTIMAAAAc0kMAwAAMBwxDAAAwHDEMAAAAMMRwwAAAAxHDAMAADAcMQwAAMBwxDAAAADDEcMAAAAMRwwDAAAwHDEMAADAcNbNeoBZO+HY9dmxbcusxwAAAGAFWRkGAABgOGIYAACA4YhhAAAAhiOGAQAAGI4YBgAAYDhiGAAAgOGIYQAAAIYz/PcM77nyqpy567JZjwEMyvecAwDMhpVhAAAAhiOGAQAAGI4YBgAAYDhiGAAAgOGIYQAAAIYjhgEAABiOGAYAAGA4YhgAAIDhiGEAAACGI4YBAAAYjhgGAABgOHMXw1V1elV9qqour6ozZj0PAAAAa8+6WQ+wjCcneVB3f265g1W1rrt/sMIzAQAAsIbM1cpwVb0uye2TnF1VO6rqNdP+s6rqFVX1oSQvq6oNVfWGqjq3qs6vqodO592lqj5ZVRdU1UVVdccZvhwAAADm1FytDHf3k6rqgUnum+SXlxzekuT+3X1NVf3nJB/s7sdX1XFJPllVH0jypCSv7O43VtVPJjlqRV8AAAAAq8JcxfABvLW7r5m2H5DkIVX1zOnn9Uluk+Tvkjynqm6d5B3d/c/LXaiqtifZniTH3+LEIzs1AAAAc2eubpM+gH2LtivJI7r7HtOf23T3p7r7/0rykCTfTfL+qrrfchfq7p3dvbW7t27YePwKjA4AAMA8WU0xvNj7kzy1qipJqurk6e/bJ/lsd78qydlJ7ja7EQEAAJhXqzWGX5Tk6CQXVdUl089J8qgkl1TVBUnunORPZzQfAAAAc2zufme4uzdPm2dNf9Ldj11yzneTPHGZx74kyUuO5HwAAACsfqt1ZRgAAAB+bGIYAACA4YhhAAAAhiOGAQAAGI4YBgAAYDhiGAAAgOGIYQAAAIYjhgEAABiOGAYAAGA4YhgAAIDhiGEAAACGs27WA8zaCceuz45tW2Y9BgAAACvIyjAAAADDEcMAAAAMRwwDAAAwHDEMAADAcMQwAAAAwxHDAAAADEcMAwAAMJzhv2d4z5VX5cxdl816DGBgvuscAGDlWRkGAABgOGIYAACA4YhhAAAAhiOGAQAAGI4YBgAAYDhiGAAAgOGIYQAAAIYjhgEAABiOGAYAAGA4YhgAAIDhiGEAAACGs2ZjuKpOr6pPVdUbZz0LAAAA82XdrAc4gp6c5EHd/blZDwIAAMB8WRMrw1X1jKq6ZPrz9Kp6XZLbJzm7qnbMej4AAADmy6pfGa6qU5I8LsnPJakkf5/k15M8MMl9u/vrMxwPAACAObQWVobvneSd3b2vu7+d5B1JfvH6HlBV26tqd1Xt3rf38hUZEgAAgPmxFmK4DvUB3b2zu7d299YNG48/EjMBAAAwx9ZCDH80ycOq6sZVtSHJw5N8bMYzAQAAMMdW/e8Md/c/VNVZST457Xp9d59fdcgLxgAAAAxi1cdwknT3K5K8Ysm+zbOZBgAAgHm3Fm6TBgAAgEMihgEAABiOGAYAAGA4YhgAAIDhiGEAAACGI4YBAAAYjhgGAABgOGIYAACA4YhhAAAAhiOGAQAAGI4YBgAAYDjrZj3ArJ1w7Prs2LZl1mMAAACwgqwMAwAAMBwxDAAAwHDEMAAAAMMRwwAAAAxHDAMAADAcMQwAAMBwxDAAAADDGf57hvdceVXO3HXZrMcA+LH5rnQAgENnZRgAAIDhiGEAAACGI4YBAAAYjhgGAABgOGIYAACA4YhhAAAAhiOGAQAAGI4YBgAAYDhiGAAAgOGIYQAAAIYjhgEAABiOGAYAAGA4YhgAAIDhrLkYrqp3VdV5VXVpVW2f9TwAAADMn3WzHuAIeHx3f7OqbpTk3Kp6e3d/Y9ZDAQAAMD/W3MpwktOr6sIk5yTZlOSOS0+oqu1Vtbuqdu/be/mKDwgAAMBsrakYrqr7JLl/klO7++5Jzk+yful53b2zu7d299YNG49f4SkBAACYtTUVw0k2Jrm8u79TVXdOcq9ZDwQAAMD8WWsx/L4k66rqoiQvysKt0gAAAHAda+oDtLr76iQPmvUcAAAAzLe1tjIMAAAABySGAQAAGI4YBgAAYDhiGAAAgOGIYQAAAIYjhgEAABiOGAYAAGA4YhgAAIDhiGEAAACGI4YBAAAYjhgGAABgOOtmPcCsnXDs+uzYtmXWYwAAALCCrAwDAAAwHDEMAADAcMQwAAAAwxHDAAAADEcMAwAAMBwxDAAAwHDEMAAAAMMZ/nuG91x5Vc7cddmsxwBY03yfOwAwb6wMAwAAMBwxDAAAwHDEMAAAAMMRwwAAAAxHDAMAADAcMQwAAMBwxDAAAADDEcMAAAAMRwwDAAAwHDEMAADAcMQwAAAAwzliMVxVj62q10zbT6qqx0zbd66qC6rq/Kq6w2F4nv//2gAAAHAw1q3Ek3T36xb9+LAk7+7u5x/MY6uqklR3//Agrg0AAAAHdMgrw1X1mKq6qKourKo/q6pfqaq/n1Z6P1BVJyzzmBdU1TOr6sFJnp7kP1TVh6Zjz6iqS6Y/T5/2ba6qT1XVHyX5hySbqurbVfXi6XnPufZ5rr32tP2Eqjp3OuftVXXjH/+tAQAAYK06pBiuqrskeU6S+3X33ZM8LcnHk9yru09O8uYkz97f47v7vUlel+TM7r5vVZ2S5HFJfi7JvZI8oapOnk6/U5I/7e6Tu/sLSTYkOWd63o8mecIyT/GO7r7ndM6nkvzWobw+AAAAxnCot0nfL8nbuvvrSdLd36yquyZ5S1XdKslPJvncIVzv3kne2d37kqSq3pHkF5OcneQL3X3OonO/l+Q90/Z5SbYtc72Tqur3kxyX5Jgk71/uSatqe5LtSXL8LU48hHEBAABYCw71NulK0kv2vTrJa7r7rkmemGT9IV5vf/Yt+fn73X3tc1+T5UP+rCRPmWZ54f5m6e6d3b21u7du2Hj8IYwLAADAWnCoMfw3Sf73qrppklTVv0qyMcmXpuO/eUTzoSsAABxVSURBVIjX+2iSh1XVjatqQ5KHJ/nYIV5jsZsk+UpVHZ3ktBtwHQAAANawQ7pNursvraoXJ/lIVV2T5PwkL0jy1qr6UpJzktzuEK73D1V1VpJPTrte393nV9XmQ5lrkecm+fskX0hycRbiGAAAAK6j/uXO4zFt2nJSP+O175j1GABr2o5tW2Y9AgAwqKo6r7u3Lt1/yF+tBAAAAKudGAYAAGA4YhgAAIDhiGEAAACGI4YBAAAYjhgGAABgOGIYAACA4YhhAAAAhiOGAQAAGI4YBgAAYDhiGAAAgOGsm/UAs3bCseuzY9uWWY8BAADACrIyDAAAwHDEMAAAAMMRwwAAAAxHDAMAADAcMQwAAMBwxDAAAADDEcMAAAAMZ/jvGd5z5VU5c9dlsx4DYFi+6x0AmAUrwwAAAAxHDAMAADAcMQwAAMBwxDAAAADDEcMAAAAMRwwDAAAwHDEMAADAcMQwAAAAwxHDAAAADEcMAwAAMBwxDAAAwHBWRQxX1elV9amquryqzpj1PAAAAKxu62Y9wEF6cpIHdffnljtYVeu6+wcrPBMAAACr1NyvDFfV65LcPsnZVbWjql4z7T+rql5RVR9K8rKq2lBVb6iqc6vq/Kp66EwHBwAAYG7NfQx395OSfDnJfZNcvuTwliT37+7/I8lzknywu+85nfuHVbVhRYcFAABgVZj7GD6At3b3NdP2A5KcUVUXJPlwkvVJbrPcg6pqe1Xtrqrd+/Yu7WsAAADWutXyO8P7s2/RdiV5RHd/+kAP6u6dSXYmyaYtJ/URmg0AAIA5tdpXhhd7f5KnVlUlSVWdPON5AAAAmFNrKYZflOToJBdV1SXTzwAAAPAjVsVt0t29edo8a/qT7n7sknO+m+SJKzgWAAAAq9RaWhkGAACAgyKGAQAAGI4YBgAAYDhiGAAAgOGIYQAAAIYjhgEAABiOGAYAAGA4YhgAAIDhiGEAAACGI4YBAAAYjhgGAABgOOtmPcCsnXDs+uzYtmXWYwAAALCCrAwDAAAwHDEMAADAcMQwAAAAwxHDAAAADEcMAwAAMBwxDAAAwHDEMAAAAMMZ/nuG91x5Vc7cddmsxwAYmu97BwBWmpVhAAAAhiOGAQAAGI4YBgAAYDhiGAAAgOGIYQAAAIYjhgEAABiOGAYAAGA4YhgAAIDhiGEAAACGI4YBAAAYjhgGAABgOGIYAACA4YhhAAAAhiOGAQAAGI4YBgAAYDhDxnBVba+q3VW1e9/ey2c9DgAAACtsyBju7p3dvbW7t27YePysxwEAAGCFDRnDAAAAjG3Vx3BVvbeqTqyq36uqh0z7HlJVvzfr2QAAAJhP62Y9wA3V3Q+eNp+3aN/ZSc6ezUQAAADMu1W/MgwAAACHSgwDAAAwHDEMAADAcMQwAAAAwxHDAAAADEcMAwAAMBwxDAAAwHDEMAAAAMMRwwAAAAxHDAMAADAcMQwAAMBw1s16gFk74dj12bFty6zHAAAAYAVZGQYAAGA4YhgAAIDhiGEAAACGI4YBAAAYjhgGAABgOGIYAACA4YhhAAAAhjP89wzvufKqnLnrslmPAcB++C54AOBIsDIMAADAcMQwAAAAwxHDAAAADEcMAwAAMBwxDAAAwHDEMAAAAMMRwwAAAAxHDAMAADAcMQwAAMBwxDAAAADDEcMAAAAMZ1XEcFUdV1VPnrbvU1XvmfVMAAAArF6rIoaTHJfkybMeAgAAgLVh3awHOEgvTXKHqrogyfeT7KuqtyU5Kcl5SX69u7uqTknyiiTHJPl6ksd291dmNTQAAADzabWsDJ+R5DPdfY8kz0pycpKnJ/mZJLdP8gtVdXSSVyd5ZHefkuQNSV48o3kBAACYY6tlZXipT3b3/0ySabV4c5IrsrBSvKuqkuSoJMuuClfV9iTbk+T4W5y4AuMCAAAwT1ZrDF+9aPuaLLyOSnJpd596oAd3984kO5Nk05aT+ohMCAAAwNxaLbdJfyvJTQ5wzqeT3LyqTk2Sqjq6qu5yxCcDAABg1VkVK8Pd/Y2q+kRVXZLku0n2LHPO96rqkUleVVUbs/Da/kuSS1d2WgAAAObdqojhJOnuR+9n/1MWbV+Q5JdWbCgAAABWpdVymzQAAAAcNmIYAACA4YhhAAAAhiOGAQAAGI4YBgAAYDhiGAAAgOGIYQAAAIYjhgEAABiOGAYAAGA4YhgAAIDhiGEAAACGs27WA8zaCceuz45tW2Y9BgAAACvIyjAAAADDEcMAAAAMRwwDAAAwHDEMAADAcMQwAAAAwxHDAAAADEcMAwAAMJzhv2d4z5VX5cxdl816DABWCd9NDwBrg5VhAAAAhiOGAQAAGI4YBgAAYDhiGAAAgOGIYQAAAIYjhgEAABiOGAYAAGA4YhgAAIDhiGEAAACGI4YBAAAYjhgGAABgOGIYAACA4YhhAAAAhrNqYriqNlfVP1XV66vqkqp6Y1Xdv6o+UVX/XFU/O/198+n8n6iq/1FVN5v17AAAAMyXVRPDk59O8sokd0ty5ySPTnLvJM9M8rtJ/jzJadO5909yYXd/fQZzAgAAMMdWWwx/rrsv7u4fJrk0yd90dye5OMnmJG9I8pjp3Mcn+e/LXaSqtlfV7qravW/v5SswNgAAAPNktcXw1Yu2f7jo5x8mWdfdX0yyp6rul+TnkvzVchfp7p3dvbW7t27YePwRHRgAAID5s9pi+GC8Pgu3S/9Fd18z62EAAACYP2sxhs9Ockz2c4s0AAAArJv1AAeruz+f5KRFPz92P8funoUPzvqnFRwPAACAVWTVxPDBqKozkvx2/uUTpQEAAOBHrKnbpLv7pd192+7++KxnAQAAYH6tqRgGAACAgyGGAQAAGI4YBgAAYDhiGAAAgOGIYQAAAIYjhgEAABiOGAYAAGA4YhgAAIDhiGEAAACGs27WA8zaCceuz45tW2Y9BgAAACvIyjAAAADDEcMAAAAMRwwDAAAwHDEMAADAcMQwAAAAwxHDAAAADEcMAwAAMJzhv2d4z5VX5cxdl816DABWMd9XDwCrj5VhAAAAhiOGAQAAGI4YBgAAYDhiGAAAgOGIYQAAAIYjhgEAABiOGAYAAGA4YhgAAIDhiGEAAACGI4YBAAAYjhgGAABgOGIYAACA4YhhAAAAhiOGAQAAGI4YBgAAYDhDxnBVba+q3VW1e9/ey2c9DgAAACtsyBju7p3dvbW7t27YePysxwEAAGCFDRnDAAAAjG1Nx3BVvbeqTpz1HAAAAMyXdbMe4Ejq7gfPegYAAADmz5peGQYAAIDliGEAAACGI4YBAAAYjhgG/r/27j/osruuD/j7YxaysLBLRpkIooYgW0xbSXRxCDAaC6sWrTAdGRhra2bqZKw/JizTMnX8R8TOOGPb+CMVzYTWH0UEI7RMtAMZDFqgY1gQSJQkEECNhBRL3JDQTST5+Mc9yTxsdpclz73Puc/9vl4zZ/Y+555z7ue7n7n3ue/ne+65AAAwHGEYAACA4QjDAAAADEcYBgAAYDjCMAAAAMMRhgEAABiOMAwAAMBwhGEAAACGs2fuAuZ27v69OXL44NxlAAAAsIPMDAMAADAcYRgAAIDhCMMAAAAMRxgGAABgOMIwAAAAwxGGAQAAGI4wDAAAwHCG/57hO+8+niuuu3XuMgAgSXLk8MG5SwCAIZgZBgAAYDjCMAAAAMMRhgEAABiOMAwAAMBwhGEAAACGIwwDAAAwHGEYAACA4QjDAAAADEcYBgAAYDjCMAAAAMMRhgEAABiOMAwAAMBwNjoMV9Un564BAACA9bPRYRgAAABOZtPD8GfmLgAAAID1s9FhuLufc7L1VXVZVR2tqqP3Hrtrp8sCAABgZhsdhk+lu6/q7kPdfWjfgXPmLgcAAIAdNmQYBgAAYGzCMAAAAMMRhgEAABiOMAwAAMBwhGEAAACGIwwDAAAwHGEYAACA4QjDAAAADEcYBgAAYDjCMAAAAMMRhgEAABiOMAwAAMBwhGEAAACGIwwDAAAwnD1zFzC3c/fvzZHDB+cuAwAAgB1kZhgAAIDhCMMAAAAMRxgGAABgOMIwAAAAwxGGAQAAGI4wDAAAwHCEYQAAAIYz/PcM33n38Vxx3a1zlwEAsC1HDh+cuwSAXcXMMAAAAMMRhgEAABiOMAwAAMBwhGEAAACGIwwDAAAwHGEYAACA4QjDAAAADEcYBgAAYDjCMAAAAMMRhgEAABiOMAwAAMBwhGEAAACGIwwDAAAwnLUOw1X12qq6fMvP/6GqLq+qn6+qm6rqxqp6+XTfJVV17ZZtr6yqS2coGwAAgDW31mE4yeuT/FCSVNVXJHlFktuTXJjk2UlelOTnq+opX85Bq+qyqjpaVUfvPXbXkksGAABg3a11GO7uTyb5f1V1UZLvTPKnSV6Q5I3d/UB335nkj5I858s87lXdfai7D+07cM6yywYAAGDN7Zm7gDNwdZJLk3x1kv+aRSg+mS/ki8P93tWWBQAAwG611jPDk7cm+e4sZn/fnuSPk7y8qs6qqicn+bYkNyT5iyQXVNXZVXUgyQvnKhgAAID1tvYzw919f1Vdn+Rvu/uBqnprkouTfChJJ3l1d386SarqzUk+nOSjWZxSDQAAAI+w9mF4unDWc5O8LEm6u5P8u2n5It396iSv3tECAQAA2HXW+jTpqrogyceSvLO7Pzp3PQAAAGyGtZ4Z7u4/T3L+3HUAAACwWdZ6ZhgAAABWQRgGAABgOMIwAAAAwxGGAQAAGI4wDAAAwHCEYQAAAIYjDAMAADAcYRgAAIDh7Jm7gLmdu39vjhw+OHcZAAAA7CAzwwAAAAxHGAYAAGA4wjAAAADDEYYBAAAYjjAMAADAcIRhAAAAhiMMAwAAMJzhv2f4zruP54rrbp27DACAYR05fHDuEoABmRkGAABgOMIwAAAAwxGGAQAAGI4wDAAAwHCEYQAAAIYjDAMAADAcYRgAAIDhCMMAAAAMRxgGAABgOMIwAAAAwxGGAQAAGI4wDAAAwHCEYQAAAIYjDAMAADAcYRgAAIDhbHQYrqo/qKqnnmT9ZVV1tKqO3nvsrjlKAwAAYEYbHYa7+8Xd/amTrL+quw9196F9B86ZozQAAABmtNFhGAAAAE5GGAYAAGA4Gx2GT/WZYQAAAMa2Z+4CVqm7Xzx3DQAAAKyfjZ4ZBgAAgJMRhgEAABiOMAwAAMBwhGEAAACGIwwDAAAwHGEYAACA4QjDAAAADEcYBgAAYDjCMAAAAMMRhgEAABjOnrkLmNu5+/fmyOGDc5cBAADADjIzDAAAwHCEYQAAAIYjDAMAADAcYRgAAIDhCMMAAAAMRxgGAABgOMIwAAAAwxn+e4bvvPt4rrju1rnLAAAABnDk8MG5S2BiZhgAAIDhCMMAAAAMRxgGAABgOMIwAAAAwxGGAQAAGI4wDAAAwHCEYQAAAIYjDAMAADAcYRgAAIDhCMMAAAAMRxgGAABgOMIwAAAAw9mIMFxV7527BgAAAHaPjQjD3f28uWsAAABg99iIMFxV90z/XlJV76qqa6rq5qp6Q1XV3PUBAACwXjYiDJ/goiSvTHJBkvOTPP/EDarqsqo6WlVH7z12107XBwAAwMw2MQzf0N23d/eDST6Y5LwTN+juq7r7UHcf2nfgnB0vEAAAgHltYhi+b8vtB5LsmasQAAAA1tMmhmEAAAA4LWEYAACA4WzEKcTd/YTp33cledeW9T8+U0kAAACsMTPDAAAADEcYBgAAYDjCMAAAAMMRhgEAABiOMAwAAMBwhGEAAACGIwwDAAAwHGEYAACA4QjDAAAADEcYBgAAYDh75i5gbufu35sjhw/OXQYAAAA7yMwwAAAAwxGGAQAAGI4wDAAAwHCEYQAAAIYjDAMAADAcYRgAAIDhCMMAAAAMZ/jvGb7z7uO54rpb5y4DAABg1zhy+ODcJWybmWEAAACGIwwDAAAwHGEYAACA4QjDAAAADEcYBgAAYDjCMAAAAMMRhgEAABiOMAwAAMBwhGEAAACGIwwDAAAwHGEYAACA4Wx0GK6q985dAwAAAOtno8Nwdz9v7hoAAABYPxsdhqvqnrlrAAAAYP1sdBgGAACAkxkyDFfVZVV1tKqO3nvsrrnLAQAAYIcNGYa7+6ruPtTdh/YdOGfucgAAANhhQ4ZhAAAAxiYMAwAAMJyNDsPd/YS5awAAAGD9bHQYBgAAgJMRhgEAABiOMAwAAMBwhGEAAACGIwwDAAAwHGEYAACA4QjDAAAADEcYBgAAYDjCMAAAAMMRhgEAABiOMAwAAMBw9sxdwNzO3b83Rw4fnLsMAAAAdpCZYQAAAIYjDAMAADAcYRgAAIDhCMMAAAAMRxgGAABgOMIwAAAAwxGGAQAAGI4wDAAAwHCEYQAAAIYjDAMAADAcYRgAAIDhCMMAAAAMRxgGAABgOMIwAAAAwxGGAQAAGI4wDAAAwHCEYQAAAIYjDAMAADAcYRgAAIDhCMMAAAAMRxgGAABgOMIwAAAAwxGGAQAAGI4wDAAAwHCEYQAAAIYjDAMAADAcYRgAAIDhCMMAAAAMRxgGAABgOMIwAAAAwxGGAQAAGI4wDAAAwHCEYQAAAIYjDAMAADAcYRgAAIDhCMMAAAAMRxgGAABgOMIwAAAAwxGGAQAAGE5199w1zKqqPpfklrnrYFu+KsnfzF0E26aPu58ebgZ93Az6uPvp4WbQx/Xw9d395BNX7pmjkjVzS3cfmrsIHr2qOqqHu58+7n56uBn0cTPo4+6nh5tBH9eb06QBAAAYjjAMAADAcITh5Kq5C2Db9HAz6OPup4ebQR83gz7ufnq4GfRxjQ1/AS0AAADGY2YYAACA4WxUGK6q766qW6rqY1X1709y/9lV9abp/j+pqvO23PeT0/pbquq7zvSYLN+y+1hVX1tV11fVR6rqz6rq8p0bzZhW8Vyc7jurqv60qq5d/ShY0Wvqk6rqmqq6eXpOXrwzoxnXivp4ZHo9vamq3lhVe3dmNGN6tD2sqq+cfv/dU1VXnrDPt1TVjdM+v1RVtTOjGdey+1hVj6+q359eT/+sqn5u50YzplU8F7fs+7aqumm1I+ARunsjliRnJbktyflJHpvkQ0kuOGGbH03yq9PtVyR503T7gmn7s5M8fTrOWWdyTMuu6ONTknzztM0Tk9yqj7urh1v2e1WS305y7dzj3PRlVX1M8htJfni6/dgkT5p7rJu8rOg19WuSfCLJ46bt3pzk0rnHuqnLNnu4L8kLkvxIkitP2OeGJBcnqST/K8k/nXusm7ysoo9JHp/kO6bbj03yv/Vxd/Vwy37/fHp/c9Pc4xxt2aSZ4W9N8rHu/nh335/kd5K85IRtXpLFG7EkuSbJC6e/hL4kye90933d/YkkH5uOdybHZLmW3sfuvqO7P5Ak3f25JB/J4s0cq7GK52Kq6mlJvifJ1TswBlbQx6ran+Tbkrw+Sbr7/u7+2x0Yy8hW8nxMsifJ46pqTxZvyD+14nGM7FH3sLvv7e53Jzm+deOqekqS/d39f3rxTvw3k7x0paNg6X3s7s939/XT7fuTfCDJ01Y5iMEtvYdJUlVPyOKP/T+7utI5lU0Kw1+T5K+2/Hx7Hhl4Ht6mu7+Q5FiSrzzNvmdyTJZrFX182HS6ykVJ/mSJNfPFVtXDX0jy6iQPLr9kTmIVfTw/yWeS/LfpdPerq2rfaspnsvQ+dvdfJ/mPSf4yyR1JjnX3O1ZSPcn2eni6Y97+JY7Jcq2ijw+rqicl+WdJ3rntSjmVVfXwtUn+U5LPL6dMvhybFIZP9lmXEy+Vfaptvtz1rM4q+rjYafGXt99L8sruvvtRV8iXsvQeVtX3Jvm/3f3+7RbHGVvFc3FPkm9O8rruvijJvUlci2G1VvF8PCeL2Y+nJ3lqkn1V9YPbqpLT2U4Pt3NMlmsVfVzstDhD441Jfqm7P/4oauPMLL2HVXVhkm/o7rdupzAevU0Kw7cn+dotPz8tjzxt6+FtpheOA0k+e5p9z+SYLNcq+piqekwWQfgN3f2WlVTOQ1bRw+cn+b6q+mQWpyX9k6r676sonoet6jX19u5+6MyMa7IIx6zOKvr4oiSf6O7PdPffJXlLkuetpHqS7fXwdMfcejqt9zert4o+PuSqJB/t7l9YQp2c2ip6eHGSb5ne37w7ycGqeteS6uUMbFIYfl+SZ1bV06vqsVl8aP1tJ2zztiQ/NN3+/iR/OH1W5m1JXjFdAe7pSZ6ZxYUlzuSYLNfS+zh99u31ST7S3f95R0YxtqX3sLt/sruf1t3nTcf7w+42E7Vaq+jjp5P8VVX9g2mfFyb581UPZHCr+N34l0meO13JtrLo40d2YCyj2k4PT6q770jyuap67tTDf5Xkfy6/dLZYeh+TpKp+NovA9col18sjreK5+Lrufur0/uYFSW7t7kuWXjmnNvcVvJa5JHlxFlcKvi3JT03rfibJ90239yb53SwuAnJDkvO37PtT0363ZMuV+E52TMvu6mMWLy6d5MNJPjgtL557nJu8rOK5uOX+S+Jq0ru2j0kuTHJ0ej7+jyTnzD3OTV9W1MfXJLk5yU1JfivJ2XOPc5OXbfbwk1nMTN2TxazVBdP6Q1P/bktyZZKae5ybviy7j1nMTHYWf4x66P3ND889zk1eVvFc3HL/eXE16R1favrPBwAAgGFs0mnSAAAAcEaEYQAAAIYjDAMAADAcYRgAAIDhCMMAAAAMRxgGgJlV1T07/HjnVdUP7ORjAsC6EYYBYCBVtSeL77MUhgEY2p65CwAAFqrqkiSvSXJnkguTvCXJjUkuT/K4JC/t7tuq6teTHE/yD5Ocm+RV3X1tVe1N8rokh5J8YVp/fVVdmuR7kuxNsi/J45N8Y1V9MMlvJHlrkt+a7kuSH+/u9071/HSSv0nyj5K8P8kPdndX1XOS/OK0z31JXpjk80l+LsklSc5O8l+6+9eW/f8EAMsgDAPAenl2km9M8tkkH09ydXd/a1VdnuQnkrxy2u68JN+e5BlJrq+qb0jyY0nS3f+4qp6V5B1VdXDa/uIk39Tdn51C7r/t7u9Nkqp6fJLD3X28qp6Z5I1ZBOokuSiL0P2pJO9J8vyquiHJm5K8vLvfV1X7k/z/JP86ybHufk5VnZ3kPVX1ju7+xAr+nwBgW4RhAFgv7+vuO5Kkqm5L8o5p/Y1JvmPLdm/u7geTfLSqPp7kWUlekOSXk6S7b66qv0jyUBi+rrs/e4rHfEySK6vqwiQPbNknSW7o7tunej6YRQg/luSO7n7f9Fh3T/d/Z5Jvqqrvn/Y9kOSZSYRhANaOMAwA6+W+Lbcf3PLzg/ni39t9wn6dpE5z3HtPc9+RLE7NfnYW1xM5fop6HphqqJM8fqb1P9Hdbz/NYwHAWnABLQDYnV5WVV9RVc9Icn6SW5L8cZJ/kSTT6dFfN60/0eeSPHHLzweymOl9MMm/THLWl3jsm5M8dfrccKrqidOFud6e5N9U1WMeqqGq9p3mOAAwGzPDALA73ZLkj7K4gNaPTJ/3/ZUkv1pVN2ZxAa1Lu/u+qkdMGH84yReq6kNJfj3JryT5vap6WZLrc/pZ5HT3/VX18iS/XFWPy+Lzwi9KcnUWp1F/oBYP+pkkL13GYAFg2ar7ZGc5AQDrarqa9LXdfc3ctQDAbuU0aQAAAIZjZhgAAIDhmBkGAABgOMIwAAAAwxGGAQAAGI4wDAAAwHCEYQAAAIYjDAMAADCcvwd3hOTnrtEfAgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 1152x1152 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "features = {dftrain_minimal_noprep.columns[i]:rf.feature_importances_[i] for i in range(len(rf.feature_importances_))}\n",
    "top_features = sorted(features.items(), key=lambda x: x[1], reverse=True)[0:100]\n",
    "topfeatures = pd.DataFrame(columns = ['feature', 'importance'])\n",
    "topfeatures['feature'] = [top_features[i][0] for i in range(100)]\n",
    "topfeatures['importance'] = [top_features[i][1] for i in range(100)]\n",
    "\n",
    "plt.figure(figsize=(16,16))\n",
    "plt.barh([i for i in range(20)], topfeatures['importance'][0:20], align='center', alpha=0.5)\n",
    "plt.yticks([i for i in range(20)], topfeatures['feature'][0:20])\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Top feature importances')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Critical t value for n = 5 and 95% confidence is 2.78\n",
    "# ddof=1 gives the sample standard deviation\n",
    "# Assumes len(scores) = 5\n",
    "def make_CI(name, scores):\n",
    "    mean_score = np.mean(scores)\n",
    "    ci_val = 2.78*np.std(scores, ddof=1)/np.sqrt(5)\n",
    "    print('{}: {:.2f}% +/- {:.2f}%'.format(name, 100*mean_score, 100*ci_val))\n",
    "\n",
    "def all_metrics(scores):\n",
    "    make_CI('Accuracy', scores[0])\n",
    "    make_CI('Precision for positive class', scores[1])\n",
    "    make_CI('Precision for negative class', scores[2])\n",
    "    make_CI('Recall for positive class', scores[3])\n",
    "    make_CI('Recall for negative class', scores[4])\n",
    "    make_CI('F for positive class', scores[5])\n",
    "    make_CI('F for negative class', scores[6])\n",
    "    make_CI('Mean F score', scores[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Took 83.597 sec)\n",
      "(Took 84.683 sec)\n",
      "(Took 87.339 sec)\n",
      "(Took 84.173 sec)\n",
      "(Took 83.563 sec)\n",
      "Combined confusion matrix:\n",
      "[[3918.  424.]\n",
      " [1250. 2021.]]\n",
      "(Overall, took 423.794 sec)\n",
      "Accuracy: 78.01% +/- 0.71%\n",
      "Precision for positive class: 75.82% +/- 1.75%\n",
      "Precision for negative class: 82.67% +/- 2.21%\n",
      "Recall for positive class: 90.25% +/- 1.23%\n",
      "Recall for negative class: 61.80% +/- 2.17%\n",
      "F for positive class: 82.39% +/- 0.71%\n",
      "F for negative class: 70.70% +/- 1.13%\n",
      "Mean F score: 76.55% +/- 0.74%\n",
      "(Took 84.540 sec)\n",
      "(Took 86.329 sec)\n",
      "(Took 88.001 sec)\n",
      "(Took 84.978 sec)\n",
      "(Took 81.386 sec)\n",
      "Combined confusion matrix:\n",
      "[[3940.  402.]\n",
      " [1315. 1956.]]\n",
      "(Overall, took 425.446 sec)\n",
      "Accuracy: 77.45% +/- 1.06%\n",
      "Precision for positive class: 74.98% +/- 1.96%\n",
      "Precision for negative class: 82.96% +/- 1.40%\n",
      "Recall for positive class: 90.74% +/- 0.94%\n",
      "Recall for negative class: 59.82% +/- 2.31%\n",
      "F for positive class: 82.10% +/- 1.06%\n",
      "F for negative class: 69.49% +/- 1.23%\n",
      "Mean F score: 75.80% +/- 1.00%\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'dftrain_pos_min_tdidf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-c45c3bac6f4c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mall_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcross_validate_random_forest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdftrain_pos_min_norm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m500\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# POS tagging, TF-IDF\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mall_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcross_validate_random_forest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdftrain_pos_min_tdidf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m500\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'dftrain_pos_min_tdidf' is not defined"
     ]
    }
   ],
   "source": [
    "# POS tagging, raw counts\n",
    "all_metrics(cross_validate_random_forest(dftrain_pos_minimal, 500))\n",
    "# POS tagging, normalized\n",
    "all_metrics(cross_validate_random_forest(dftrain_pos_min_norm, 500))\n",
    "# Typo, woops. Don't want to rerun these"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Took 81.309 sec)\n",
      "(Took 78.752 sec)\n",
      "(Took 85.863 sec)\n",
      "(Took 83.712 sec)\n",
      "(Took 84.529 sec)\n",
      "Combined confusion matrix:\n",
      "[[3929.  413.]\n",
      " [1311. 1960.]]\n",
      "(Overall, took 414.357 sec)\n",
      "Accuracy: 77.35% +/- 0.71%\n",
      "Precision for positive class: 74.98% +/- 1.71%\n",
      "Precision for negative class: 82.61% +/- 1.81%\n",
      "Recall for positive class: 90.50% +/- 1.02%\n",
      "Recall for negative class: 59.94% +/- 1.88%\n",
      "F for positive class: 82.00% +/- 0.80%\n",
      "F for negative class: 69.45% +/- 0.90%\n",
      "Mean F score: 75.72% +/- 0.64%\n"
     ]
    }
   ],
   "source": [
    "# POS tagging, TF-IDF\n",
    "all_metrics(cross_validate_random_forest(dftrain_pos_min_tfidf, 500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Took 94.395 sec)\n",
      "(Took 94.816 sec)\n",
      "(Took 95.213 sec)\n",
      "(Took 95.183 sec)\n",
      "(Took 91.817 sec)\n",
      "Combined confusion matrix:\n",
      "[[3925.  417.]\n",
      " [1215. 2056.]]\n",
      "(Overall, took 471.786 sec)\n",
      "Accuracy: 78.56% +/- 0.81%\n",
      "Precision for positive class: 76.36% +/- 1.67%\n",
      "Precision for negative class: 83.16% +/- 1.90%\n",
      "Recall for positive class: 90.40% +/- 1.34%\n",
      "Recall for negative class: 62.87% +/- 1.86%\n",
      "F for positive class: 82.78% +/- 0.95%\n",
      "F for negative class: 71.58% +/- 0.79%\n",
      "Mean F score: 77.18% +/- 0.66%\n"
     ]
    }
   ],
   "source": [
    "# Bigrams, no preprocessing\n",
    "all_metrics(cross_validate_random_forest(dftrain_bi_minimal, 500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Took 103.832 sec)\n",
      "(Took 99.711 sec)\n",
      "(Took 100.042 sec)\n",
      "(Took 99.642 sec)\n",
      "(Took 104.247 sec)\n",
      "Combined confusion matrix:\n",
      "[[3934.  408.]\n",
      " [1267. 2004.]]\n",
      "(Overall, took 507.804 sec)\n",
      "Accuracy: 78.00% +/- 0.93%\n",
      "Precision for positive class: 75.64% +/- 1.91%\n",
      "Precision for negative class: 83.11% +/- 2.07%\n",
      "Recall for positive class: 90.61% +/- 1.32%\n",
      "Recall for negative class: 61.29% +/- 1.97%\n",
      "F for positive class: 82.44% +/- 0.99%\n",
      "F for negative class: 70.53% +/- 0.77%\n",
      "Mean F score: 76.48% +/- 0.79%\n",
      "(Took 102.143 sec)\n",
      "(Took 98.834 sec)\n",
      "(Took 103.584 sec)\n",
      "(Took 101.021 sec)\n",
      "(Took 102.712 sec)\n",
      "Combined confusion matrix:\n",
      "[[3929.  413.]\n",
      " [1251. 2020.]]\n",
      "(Overall, took 508.609 sec)\n",
      "Accuracy: 78.14% +/- 0.82%\n",
      "Precision for positive class: 75.85% +/- 1.66%\n",
      "Precision for negative class: 83.05% +/- 2.22%\n",
      "Recall for positive class: 90.49% +/- 1.43%\n",
      "Recall for negative class: 61.77% +/- 1.55%\n",
      "F for positive class: 82.52% +/- 0.94%\n",
      "F for negative class: 70.83% +/- 0.65%\n",
      "Mean F score: 76.67% +/- 0.66%\n"
     ]
    }
   ],
   "source": [
    "# Bigrams, normalized\n",
    "all_metrics(cross_validate_random_forest(dftrain_bi_min_norm, 500))\n",
    "# Bigrams, TF-IDF\n",
    "all_metrics(cross_validate_random_forest(dftrain_bi_min_tfidf, 500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Took 175.586 sec)\n",
      "(Took 166.540 sec)\n",
      "(Took 178.447 sec)\n",
      "(Took 172.954 sec)\n",
      "(Took 175.714 sec)\n",
      "Combined confusion matrix:\n",
      "[[3967.  375.]\n",
      " [1252. 2019.]]\n",
      "(Overall, took 870.133 sec)\n",
      "Accuracy: 78.63% +/- 0.32%\n",
      "Precision for positive class: 76.01% +/- 1.34%\n",
      "Precision for negative class: 84.37% +/- 2.26%\n",
      "Recall for positive class: 91.37% +/- 1.48%\n",
      "Recall for negative class: 61.72% +/- 1.95%\n",
      "F for positive class: 82.98% +/- 0.62%\n",
      "F for negative class: 71.26% +/- 0.86%\n",
      "Mean F score: 77.12% +/- 0.24%\n",
      "(Took 167.798 sec)\n",
      "(Took 161.820 sec)\n",
      "(Took 176.910 sec)\n",
      "(Took 177.370 sec)\n",
      "(Took 168.292 sec)\n",
      "Combined confusion matrix:\n",
      "[[3976.  366.]\n",
      " [1284. 1987.]]\n",
      "(Overall, took 852.725 sec)\n",
      "Accuracy: 78.33% +/- 0.99%\n",
      "Precision for positive class: 75.59% +/- 1.84%\n",
      "Precision for negative class: 84.48% +/- 2.08%\n",
      "Recall for positive class: 91.57% +/- 1.39%\n",
      "Recall for negative class: 60.77% +/- 1.70%\n",
      "F for positive class: 82.81% +/- 1.09%\n",
      "F for negative class: 70.67% +/- 0.56%\n",
      "Mean F score: 76.74% +/- 0.78%\n",
      "(Took 170.507 sec)\n",
      "(Took 165.736 sec)\n",
      "(Took 166.746 sec)\n",
      "(Took 165.870 sec)\n",
      "(Took 165.094 sec)\n",
      "Combined confusion matrix:\n",
      "[[3978.  364.]\n",
      " [1287. 1984.]]\n",
      "(Overall, took 834.439 sec)\n",
      "Accuracy: 78.31% +/- 0.73%\n",
      "Precision for positive class: 75.55% +/- 1.46%\n",
      "Precision for negative class: 84.53% +/- 2.24%\n",
      "Recall for positive class: 91.62% +/- 1.49%\n",
      "Recall for negative class: 60.66% +/- 1.35%\n",
      "F for positive class: 82.80% +/- 0.94%\n",
      "F for negative class: 70.61% +/- 0.40%\n",
      "Mean F score: 76.71% +/- 0.46%\n",
      "(Took 232.670 sec)\n",
      "(Took 244.208 sec)\n",
      "(Took 236.544 sec)\n",
      "(Took 236.417 sec)\n",
      "(Took 233.768 sec)\n",
      "Combined confusion matrix:\n",
      "[[3578.  764.]\n",
      " [1045. 2226.]]\n",
      "(Overall, took 1183.808 sec)\n",
      "Accuracy: 76.24% +/- 0.66%\n",
      "Precision for positive class: 77.43% +/- 2.11%\n",
      "Precision for negative class: 74.50% +/- 2.68%\n",
      "Recall for positive class: 82.42% +/- 2.38%\n",
      "Recall for negative class: 68.08% +/- 3.31%\n",
      "F for positive class: 79.82% +/- 0.68%\n",
      "F for negative class: 71.09% +/- 1.34%\n",
      "Mean F score: 75.45% +/- 0.75%\n",
      "(Took 272.014 sec)\n",
      "(Took 272.352 sec)\n",
      "(Took 274.971 sec)\n",
      "(Took 292.892 sec)\n",
      "(Took 453.295 sec)\n",
      "Combined confusion matrix:\n",
      "[[3685.  657.]\n",
      " [1024. 2247.]]\n",
      "(Overall, took 1565.736 sec)\n",
      "Accuracy: 77.92% +/- 1.01%\n",
      "Precision for positive class: 78.28% +/- 2.24%\n",
      "Precision for negative class: 77.41% +/- 2.59%\n",
      "Recall for positive class: 84.88% +/- 2.15%\n",
      "Recall for negative class: 68.71% +/- 3.60%\n",
      "F for positive class: 81.42% +/- 0.88%\n",
      "F for negative class: 72.75% +/- 1.75%\n",
      "Mean F score: 77.09% +/- 1.13%\n",
      "(Took 471.777 sec)\n",
      "(Took 289.937 sec)\n",
      "(Took 287.464 sec)\n",
      "(Took 286.283 sec)\n",
      "(Took 278.269 sec)\n",
      "Combined confusion matrix:\n",
      "[[3675.  667.]\n",
      " [1020. 2251.]]\n",
      "(Overall, took 1614.060 sec)\n",
      "Accuracy: 77.84% +/- 1.06%\n",
      "Precision for positive class: 78.31% +/- 2.15%\n",
      "Precision for negative class: 77.19% +/- 2.89%\n",
      "Recall for positive class: 84.65% +/- 2.49%\n",
      "Recall for negative class: 68.83% +/- 3.61%\n",
      "F for positive class: 81.33% +/- 0.96%\n",
      "F for negative class: 72.72% +/- 1.78%\n",
      "Mean F score: 77.02% +/- 1.15%\n"
     ]
    }
   ],
   "source": [
    "# Cross-validated random forest models (all have 500 trees)\n",
    "# No prep, raw counts\n",
    "all_metrics(cross_validate_random_forest(dftrain_minimal_noprep, 500))\n",
    "# No prep, normalized\n",
    "all_metrics(cross_validate_random_forest(dftrain_min_norm_noprep, 500))\n",
    "# No prep, TF-IDF\n",
    "all_metrics(cross_validate_random_forest(dftrain_min_tfidf_noprep, 500))\n",
    "# Preprocessed, raw counts\n",
    "all_metrics(cross_validate_random_forest(dftrain_minimal, 500))\n",
    "# Preprocessed, normalized\n",
    "all_metrics(cross_validate_random_forest(dftrain_min_norm, 500))\n",
    "# Preprocessed, TF-IDF\n",
    "all_metrics(cross_validate_random_forest(dftrain_min_tfidf, 500))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other random forest models were tried before doing cross-validation, but these were not left in the file.\n",
    "Adjusting the number of trees, 500 was found to work decently well.\n",
    "Modifying the criterion between entropy and Gini did not produce better models.\n",
    "Raising the minimum document threshold for a word decreased scores.\n",
    "Overall, the best models ended up being the ones that didn't do any real preprocessing, which is kind of ironic given that I tried all these preprocessing options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regularization did not help at all, sadly\n",
    "def run_logistic(dataset, split, epochs, batch_size=32, reg='', patience=1):\n",
    "    t0 = time.time()\n",
    "    np.random.seed(1)\n",
    "    train, test = train_test_split(dataset, test_size=split)\n",
    "    train_x = np.array(train.drop('_target', axis=1))\n",
    "    train_y = np.array(train['_target'])\n",
    "    test_x = np.array(test.drop('_target', axis=1))\n",
    "    test_y = np.array(test['_target'])\n",
    "    print('Training...')\n",
    "    nn = Sequential()\n",
    "    if reg == '':\n",
    "        nn.add(Dense(1, activation='sigmoid', input_shape=(train_x.shape[1],)))\n",
    "    else:\n",
    "        nn.add(Dense(1, activation='sigmoid', kernel_regularizer='l1', input_shape=(train_x.shape[1],)))\n",
    "    nn.compile(optimizer='adam', loss='binary_crossentropy', metrics=[])\n",
    "    es = EarlyStopping(monitor='val_loss', patience=patience)\n",
    "    h = nn.fit(train_x, train_y, batch_size = batch_size, epochs=epochs, validation_data=(test_x, test_y), callbacks=[es])\n",
    "    plt.plot(h.history['val_loss'])\n",
    "    print('Predicting...')\n",
    "    preds = 1*(nn.predict(test_x) > 0.5)\n",
    "    cm = confusion_matrix(test_y, preds)\n",
    "    print(cm)\n",
    "    tp, fn, fp, tn = cm.ravel()\n",
    "    accuracy = (tp+tn)/(tp+fn+fp+tn)\n",
    "    precision = tp/(tp+fp)\n",
    "    recall = tp/(tp+fn)\n",
    "    F = 2*precision*recall/(precision+recall)\n",
    "    print('Accuracy: {:.2f}%\\nPrecision: {:.2f}%\\nRecall: {:.2f}%\\nF: {:.2f}'.format(100*accuracy, 100*precision, 100*recall, 100*F))\n",
    "    t1 = time.time()\n",
    "    print('(Took {:.3f} sec)'.format(t1-t0))\n",
    "    return nn\n",
    "\n",
    "# 5-fold cross-validation, all metrics\n",
    "def cross_validate_logistic(dataset, epochs, batch_size=32):\n",
    "    t_0 = time.time()\n",
    "    # Fixed at 5-fold cross-validation for now\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "    X = np.array(dataset.drop('_target', axis=1))\n",
    "    y = np.array(dataset['_target'])\n",
    "    accs = []\n",
    "    precs_p = []\n",
    "    precs_n = []\n",
    "    recs_p = []\n",
    "    recs_n = []\n",
    "    Fs_p = []\n",
    "    Fs_n = []\n",
    "    mFs = [] # Mean F score, this is the actual competition metric\n",
    "    cm = np.zeros((2,2))\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        t0 = time.time()\n",
    "        train_x, test_x = X[train_index], X[test_index]\n",
    "        train_y, test_y = y[train_index], y[test_index]\n",
    "        #print('Training...')\n",
    "        nn = Sequential()\n",
    "        nn.add(Dense(1, activation='sigmoid', input_shape=(train_x.shape[1],)))\n",
    "        nn.compile(optimizer='adam', loss='binary_crossentropy', metrics=[])\n",
    "        es = EarlyStopping(monitor='val_loss', patience=1)\n",
    "        h = nn.fit(train_x, train_y, batch_size = batch_size, epochs=epochs, validation_data=(test_x, test_y), callbacks=[es])\n",
    "        #plt.plot(h.history['val_loss'])\n",
    "        print('Predicting...')\n",
    "        preds = 1*(nn.predict(test_x) > 0.5)\n",
    "        cm_batch = confusion_matrix(test_y, preds)\n",
    "        cm += np.array(cm_batch)\n",
    "        tp, fn, fp, tn = cm_batch.ravel()\n",
    "        acc = (tp+tn)/(tp+fn+fp+tn)\n",
    "        accs.append( acc )\n",
    "        prec_p = tp/(tp+fp)\n",
    "        precs_p.append( prec_p )\n",
    "        prec_n = tn/(tn+fn)\n",
    "        precs_n.append( prec_n )\n",
    "        rec_p = tp/(tp+fn)\n",
    "        recs_p.append( rec_p )\n",
    "        rec_n = tn/(tn+fp)\n",
    "        recs_n.append( rec_n )\n",
    "        F_p = 2*prec_p*rec_p/(prec_p+rec_p)\n",
    "        Fs_p.append( F_p )\n",
    "        F_n = 2*prec_n*rec_n/(prec_n+rec_n)\n",
    "        Fs_n.append( F_n )\n",
    "        mF = (F_p + F_n)/2.0\n",
    "        mFs.append( mF )\n",
    "        t1 = time.time()\n",
    "        print('(Took {:.3f} sec)'.format(t1-t0))\n",
    "    t_1 = time.time()\n",
    "    print('Combined confusion matrix:')\n",
    "    print(cm)\n",
    "    print('(Overall, took {:.3f} sec)'.format(t_1-t_0))\n",
    "    return [accs, precs_p, precs_n, recs_p, recs_n, Fs_p, Fs_n, mFs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/100\n",
      "6090/6090 [==============================] - 3s 488us/sample - loss: 0.6500 - val_loss: 0.6065\n",
      "Epoch 2/100\n",
      "6090/6090 [==============================] - 1s 159us/sample - loss: 0.5783 - val_loss: 0.5613\n",
      "Epoch 3/100\n",
      "6090/6090 [==============================] - 1s 155us/sample - loss: 0.5338 - val_loss: 0.5327\n",
      "Epoch 4/100\n",
      "6090/6090 [==============================] - 1s 159us/sample - loss: 0.5017 - val_loss: 0.5133\n",
      "Epoch 5/100\n",
      "6090/6090 [==============================] - 1s 159us/sample - loss: 0.4768 - val_loss: 0.4986\n",
      "Epoch 6/100\n",
      "6090/6090 [==============================] - 1s 148us/sample - loss: 0.4561 - val_loss: 0.4876\n",
      "Epoch 7/100\n",
      "6090/6090 [==============================] - 1s 142us/sample - loss: 0.4386 - val_loss: 0.4788\n",
      "Epoch 8/100\n",
      "6090/6090 [==============================] - 1s 152us/sample - loss: 0.4233 - val_loss: 0.4718\n",
      "Epoch 9/100\n",
      "6090/6090 [==============================] - 1s 132us/sample - loss: 0.4098 - val_loss: 0.4659\n",
      "Epoch 10/100\n",
      "6090/6090 [==============================] - 1s 136us/sample - loss: 0.3977 - val_loss: 0.4613\n",
      "Epoch 11/100\n",
      "6090/6090 [==============================] - 1s 152us/sample - loss: 0.3868 - val_loss: 0.4574\n",
      "Epoch 12/100\n",
      "6090/6090 [==============================] - 1s 150us/sample - loss: 0.3768 - val_loss: 0.4540\n",
      "Epoch 13/100\n",
      "6090/6090 [==============================] - 1s 150us/sample - loss: 0.3675 - val_loss: 0.4513\n",
      "Epoch 14/100\n",
      "6090/6090 [==============================] - 1s 153us/sample - loss: 0.3589 - val_loss: 0.4491\n",
      "Epoch 15/100\n",
      "6090/6090 [==============================] - 1s 132us/sample - loss: 0.3511 - val_loss: 0.4471\n",
      "Epoch 16/100\n",
      "6090/6090 [==============================] - 1s 141us/sample - loss: 0.3437 - val_loss: 0.4456\n",
      "Epoch 17/100\n",
      "6090/6090 [==============================] - 1s 143us/sample - loss: 0.3366 - val_loss: 0.4443\n",
      "Epoch 18/100\n",
      "6090/6090 [==============================] - 1s 153us/sample - loss: 0.3301 - val_loss: 0.4434\n",
      "Epoch 19/100\n",
      "6090/6090 [==============================] - 1s 147us/sample - loss: 0.3238 - val_loss: 0.4426\n",
      "Epoch 20/100\n",
      "6090/6090 [==============================] - 1s 167us/sample - loss: 0.3180 - val_loss: 0.4420\n",
      "Epoch 21/100\n",
      "6090/6090 [==============================] - 1s 147us/sample - loss: 0.3124 - val_loss: 0.4417\n",
      "Epoch 22/100\n",
      "6090/6090 [==============================] - 1s 144us/sample - loss: 0.3070 - val_loss: 0.4413\n",
      "Epoch 23/100\n",
      "6090/6090 [==============================] - 1s 161us/sample - loss: 0.3019 - val_loss: 0.4414\n",
      "Predicting...\n",
      "(Took 26.054 sec)\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/100\n",
      "6090/6090 [==============================] - 1s 208us/sample - loss: 0.6470 - val_loss: 0.6094\n",
      "Epoch 2/100\n",
      "6090/6090 [==============================] - 1s 166us/sample - loss: 0.5754 - val_loss: 0.5686\n",
      "Epoch 3/100\n",
      "6090/6090 [==============================] - 1s 153us/sample - loss: 0.5310 - val_loss: 0.5437\n",
      "Epoch 4/100\n",
      "6090/6090 [==============================] - 1s 158us/sample - loss: 0.4988 - val_loss: 0.5267\n",
      "Epoch 5/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.4737 - val_loss: 0.5140\n",
      "Epoch 6/100\n",
      "6090/6090 [==============================] - 1s 157us/sample - loss: 0.4533 - val_loss: 0.5036\n",
      "Epoch 7/100\n",
      "6090/6090 [==============================] - 1s 156us/sample - loss: 0.4358 - val_loss: 0.4957\n",
      "Epoch 8/100\n",
      "6090/6090 [==============================] - 1s 154us/sample - loss: 0.4206 - val_loss: 0.4893\n",
      "Epoch 9/100\n",
      "6090/6090 [==============================] - 1s 161us/sample - loss: 0.4071 - val_loss: 0.4835\n",
      "Epoch 10/100\n",
      "6090/6090 [==============================] - 1s 157us/sample - loss: 0.3951 - val_loss: 0.4792\n",
      "Epoch 11/100\n",
      "6090/6090 [==============================] - 1s 154us/sample - loss: 0.3844 - val_loss: 0.4755\n",
      "Epoch 12/100\n",
      "6090/6090 [==============================] - 1s 153us/sample - loss: 0.3745 - val_loss: 0.4722\n",
      "Epoch 13/100\n",
      "6090/6090 [==============================] - 1s 148us/sample - loss: 0.3654 - val_loss: 0.4699\n",
      "Epoch 14/100\n",
      "6090/6090 [==============================] - 1s 145us/sample - loss: 0.3570 - val_loss: 0.4677\n",
      "Epoch 15/100\n",
      "6090/6090 [==============================] - 1s 143us/sample - loss: 0.3492 - val_loss: 0.4661\n",
      "Epoch 16/100\n",
      "6090/6090 [==============================] - 1s 147us/sample - loss: 0.3418 - val_loss: 0.4646\n",
      "Epoch 17/100\n",
      "6090/6090 [==============================] - 1s 150us/sample - loss: 0.3349 - val_loss: 0.4634\n",
      "Epoch 18/100\n",
      "6090/6090 [==============================] - 1s 144us/sample - loss: 0.3284 - val_loss: 0.4629\n",
      "Epoch 19/100\n",
      "6090/6090 [==============================] - 1s 152us/sample - loss: 0.3221 - val_loss: 0.4619\n",
      "Epoch 20/100\n",
      "6090/6090 [==============================] - 1s 146us/sample - loss: 0.3165 - val_loss: 0.4616\n",
      "Epoch 21/100\n",
      "6090/6090 [==============================] - 1s 140us/sample - loss: 0.3108 - val_loss: 0.4614\n",
      "Epoch 22/100\n",
      "6090/6090 [==============================] - 1s 148us/sample - loss: 0.3056 - val_loss: 0.4618\n",
      "Predicting...\n",
      "(Took 21.571 sec)\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/100\n",
      "6090/6090 [==============================] - 1s 191us/sample - loss: 0.6437 - val_loss: 0.6130\n",
      "Epoch 2/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.5740 - val_loss: 0.5715\n",
      "Epoch 3/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.5301 - val_loss: 0.5458\n",
      "Epoch 4/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.4982 - val_loss: 0.5273\n",
      "Epoch 5/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.4734 - val_loss: 0.5136\n",
      "Epoch 6/100\n",
      "6090/6090 [==============================] - 1s 146us/sample - loss: 0.4529 - val_loss: 0.5028\n",
      "Epoch 7/100\n",
      "6090/6090 [==============================] - 1s 147us/sample - loss: 0.4356 - val_loss: 0.4947\n",
      "Epoch 8/100\n",
      "6090/6090 [==============================] - 1s 146us/sample - loss: 0.4205 - val_loss: 0.4879\n",
      "Epoch 9/100\n",
      "6090/6090 [==============================] - 1s 144us/sample - loss: 0.4073 - val_loss: 0.4822\n",
      "Epoch 10/100\n",
      "6090/6090 [==============================] - 1s 145us/sample - loss: 0.3954 - val_loss: 0.4776\n",
      "Epoch 11/100\n",
      "6090/6090 [==============================] - 1s 143us/sample - loss: 0.3845 - val_loss: 0.4740\n",
      "Epoch 12/100\n",
      "6090/6090 [==============================] - 1s 140us/sample - loss: 0.3746 - val_loss: 0.4708\n",
      "Epoch 13/100\n",
      "6090/6090 [==============================] - 1s 141us/sample - loss: 0.3654 - val_loss: 0.4682\n",
      "Epoch 14/100\n",
      "6090/6090 [==============================] - 1s 140us/sample - loss: 0.3569 - val_loss: 0.4658\n",
      "Epoch 15/100\n",
      "6090/6090 [==============================] - 1s 141us/sample - loss: 0.3490 - val_loss: 0.4638\n",
      "Epoch 16/100\n",
      "6090/6090 [==============================] - 1s 141us/sample - loss: 0.3417 - val_loss: 0.4626\n",
      "Epoch 17/100\n",
      "6090/6090 [==============================] - 1s 141us/sample - loss: 0.3347 - val_loss: 0.4616\n",
      "Epoch 18/100\n",
      "6090/6090 [==============================] - 1s 146us/sample - loss: 0.3281 - val_loss: 0.4611\n",
      "Epoch 19/100\n",
      "6090/6090 [==============================] - 1s 147us/sample - loss: 0.3219 - val_loss: 0.4601\n",
      "Epoch 20/100\n",
      "6090/6090 [==============================] - 1s 146us/sample - loss: 0.3160 - val_loss: 0.4602\n",
      "Predicting...\n",
      "(Took 18.485 sec)\n",
      "Train on 6091 samples, validate on 1522 samples\n",
      "Epoch 1/100\n",
      "6091/6091 [==============================] - 1s 198us/sample - loss: 0.6496 - val_loss: 0.6124\n",
      "Epoch 2/100\n",
      "6091/6091 [==============================] - 1s 165us/sample - loss: 0.5771 - val_loss: 0.5689\n",
      "Epoch 3/100\n",
      "6091/6091 [==============================] - 1s 168us/sample - loss: 0.5328 - val_loss: 0.5408\n",
      "Epoch 4/100\n",
      "6091/6091 [==============================] - 1s 163us/sample - loss: 0.5009 - val_loss: 0.5214\n",
      "Epoch 5/100\n",
      "6091/6091 [==============================] - 1s 162us/sample - loss: 0.4760 - val_loss: 0.5068\n",
      "Epoch 6/100\n",
      "6091/6091 [==============================] - 1s 164us/sample - loss: 0.4554 - val_loss: 0.4954\n",
      "Epoch 7/100\n",
      "6091/6091 [==============================] - 1s 163us/sample - loss: 0.4380 - val_loss: 0.4860\n",
      "Epoch 8/100\n",
      "6091/6091 [==============================] - 1s 137us/sample - loss: 0.4228 - val_loss: 0.4787\n",
      "Epoch 9/100\n",
      "6091/6091 [==============================] - 1s 137us/sample - loss: 0.4091 - val_loss: 0.4725\n",
      "Epoch 10/100\n",
      "6091/6091 [==============================] - 1s 135us/sample - loss: 0.3970 - val_loss: 0.4673\n",
      "Epoch 11/100\n",
      "6091/6091 [==============================] - 1s 137us/sample - loss: 0.3861 - val_loss: 0.4629\n",
      "Epoch 12/100\n",
      "6091/6091 [==============================] - 1s 151us/sample - loss: 0.3760 - val_loss: 0.4596\n",
      "Epoch 13/100\n",
      "6091/6091 [==============================] - 1s 137us/sample - loss: 0.3667 - val_loss: 0.4563\n",
      "Epoch 14/100\n",
      "6091/6091 [==============================] - 1s 133us/sample - loss: 0.3582 - val_loss: 0.4540\n",
      "Epoch 15/100\n",
      "6091/6091 [==============================] - 1s 139us/sample - loss: 0.3502 - val_loss: 0.4519\n",
      "Epoch 16/100\n",
      "6091/6091 [==============================] - 1s 146us/sample - loss: 0.3427 - val_loss: 0.4500\n",
      "Epoch 17/100\n",
      "6091/6091 [==============================] - 1s 140us/sample - loss: 0.3357 - val_loss: 0.4488\n",
      "Epoch 18/100\n",
      "6091/6091 [==============================] - 1s 137us/sample - loss: 0.3290 - val_loss: 0.4479\n",
      "Epoch 19/100\n",
      "6091/6091 [==============================] - 1s 137us/sample - loss: 0.3228 - val_loss: 0.4473\n",
      "Epoch 20/100\n",
      "6091/6091 [==============================] - 1s 137us/sample - loss: 0.3168 - val_loss: 0.4465\n",
      "Epoch 21/100\n",
      "6091/6091 [==============================] - 1s 137us/sample - loss: 0.3112 - val_loss: 0.4464\n",
      "Epoch 22/100\n",
      "6091/6091 [==============================] - 1s 137us/sample - loss: 0.3057 - val_loss: 0.4460\n",
      "Epoch 23/100\n",
      "6091/6091 [==============================] - 1s 139us/sample - loss: 0.3005 - val_loss: 0.4461\n",
      "Predicting...\n",
      "(Took 21.526 sec)\n",
      "Train on 6091 samples, validate on 1522 samples\n",
      "Epoch 1/100\n",
      "6091/6091 [==============================] - 1s 193us/sample - loss: 0.6443 - val_loss: 0.6106\n",
      "Epoch 2/100\n",
      "6091/6091 [==============================] - 1s 138us/sample - loss: 0.5748 - val_loss: 0.5697\n",
      "Epoch 3/100\n",
      "6091/6091 [==============================] - 1s 138us/sample - loss: 0.5313 - val_loss: 0.5430\n",
      "Epoch 4/100\n",
      "6091/6091 [==============================] - 1s 137us/sample - loss: 0.4997 - val_loss: 0.5248\n",
      "Epoch 5/100\n",
      "6091/6091 [==============================] - 1s 136us/sample - loss: 0.4747 - val_loss: 0.5098\n",
      "Epoch 6/100\n",
      "6091/6091 [==============================] - 1s 134us/sample - loss: 0.4541 - val_loss: 0.4990\n",
      "Epoch 7/100\n",
      "6091/6091 [==============================] - 1s 135us/sample - loss: 0.4367 - val_loss: 0.4902\n",
      "Epoch 8/100\n",
      "6091/6091 [==============================] - 1s 137us/sample - loss: 0.4214 - val_loss: 0.4828\n",
      "Epoch 9/100\n",
      "6091/6091 [==============================] - 1s 135us/sample - loss: 0.4079 - val_loss: 0.4769\n",
      "Epoch 10/100\n",
      "6091/6091 [==============================] - 1s 137us/sample - loss: 0.3958 - val_loss: 0.4721\n",
      "Epoch 11/100\n",
      "6091/6091 [==============================] - 1s 139us/sample - loss: 0.3848 - val_loss: 0.4679\n",
      "Epoch 12/100\n",
      "6091/6091 [==============================] - 1s 137us/sample - loss: 0.3748 - val_loss: 0.4643\n",
      "Epoch 13/100\n",
      "6091/6091 [==============================] - 1s 136us/sample - loss: 0.3656 - val_loss: 0.4614\n",
      "Epoch 14/100\n",
      "6091/6091 [==============================] - 1s 134us/sample - loss: 0.3570 - val_loss: 0.4589\n",
      "Epoch 15/100\n",
      "6091/6091 [==============================] - 1s 138us/sample - loss: 0.3490 - val_loss: 0.4572\n",
      "Epoch 16/100\n",
      "6091/6091 [==============================] - 1s 148us/sample - loss: 0.3417 - val_loss: 0.4553\n",
      "Epoch 17/100\n",
      "6091/6091 [==============================] - 1s 139us/sample - loss: 0.3346 - val_loss: 0.4541\n",
      "Epoch 18/100\n",
      "6091/6091 [==============================] - 1s 138us/sample - loss: 0.3281 - val_loss: 0.4529\n",
      "Epoch 19/100\n",
      "6091/6091 [==============================] - 1s 137us/sample - loss: 0.3219 - val_loss: 0.4520\n",
      "Epoch 20/100\n",
      "6091/6091 [==============================] - 1s 139us/sample - loss: 0.3160 - val_loss: 0.4510\n",
      "Epoch 21/100\n",
      "6091/6091 [==============================] - 1s 136us/sample - loss: 0.3104 - val_loss: 0.4507\n",
      "Epoch 22/100\n",
      "6091/6091 [==============================] - 1s 136us/sample - loss: 0.3050 - val_loss: 0.4505\n",
      "Epoch 23/100\n",
      "6091/6091 [==============================] - 1s 137us/sample - loss: 0.2999 - val_loss: 0.4499\n",
      "Epoch 24/100\n",
      "6091/6091 [==============================] - 1s 137us/sample - loss: 0.2950 - val_loss: 0.4500\n",
      "Predicting...\n",
      "(Took 21.269 sec)\n",
      "Combined confusion matrix:\n",
      "[[3831.  511.]\n",
      " [ 961. 2310.]]\n",
      "(Overall, took 109.351 sec)\n",
      "Accuracy: 80.66% +/- 0.54%\n",
      "Precision for positive class: 79.95% +/- 1.50%\n",
      "Precision for negative class: 81.89% +/- 2.03%\n",
      "Recall for positive class: 88.24% +/- 1.27%\n",
      "Recall for negative class: 70.62% +/- 2.17%\n",
      "F for positive class: 83.88% +/- 0.51%\n",
      "F for negative class: 75.82% +/- 1.20%\n",
      "Mean F score: 79.85% +/- 0.66%\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/100\n",
      "6090/6090 [==============================] - 1s 203us/sample - loss: 0.6863 - val_loss: 0.6784\n",
      "Epoch 2/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.6743 - val_loss: 0.6682\n",
      "Epoch 3/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.6650 - val_loss: 0.6602\n",
      "Epoch 4/100\n",
      "6090/6090 [==============================] - 1s 136us/sample - loss: 0.6570 - val_loss: 0.6532\n",
      "Epoch 5/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.6496 - val_loss: 0.6468\n",
      "Epoch 6/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.6425 - val_loss: 0.6406\n",
      "Epoch 7/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.6358 - val_loss: 0.6349\n",
      "Epoch 8/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.6293 - val_loss: 0.6293\n",
      "Epoch 9/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.6230 - val_loss: 0.6240\n",
      "Epoch 10/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.6169 - val_loss: 0.6188\n",
      "Epoch 11/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.6110 - val_loss: 0.6139\n",
      "Epoch 12/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.6053 - val_loss: 0.6090\n",
      "Epoch 13/100\n",
      "6090/6090 [==============================] - 1s 140us/sample - loss: 0.5998 - val_loss: 0.6045\n",
      "Epoch 14/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.5944 - val_loss: 0.6001\n",
      "Epoch 15/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.5892 - val_loss: 0.5958\n",
      "Epoch 16/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.5842 - val_loss: 0.5916\n",
      "Epoch 17/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.5793 - val_loss: 0.5877\n",
      "Epoch 18/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.5746 - val_loss: 0.5838\n",
      "Epoch 19/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.5700 - val_loss: 0.5800\n",
      "Epoch 20/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.5655 - val_loss: 0.5764\n",
      "Epoch 21/100\n",
      "6090/6090 [==============================] - 1s 143us/sample - loss: 0.5612 - val_loss: 0.5729\n",
      "Epoch 22/100\n",
      "6090/6090 [==============================] - 1s 134us/sample - loss: 0.5570 - val_loss: 0.5696\n",
      "Epoch 23/100\n",
      "6090/6090 [==============================] - 1s 135us/sample - loss: 0.5529 - val_loss: 0.5663\n",
      "Epoch 24/100\n",
      "6090/6090 [==============================] - 1s 134us/sample - loss: 0.5489 - val_loss: 0.5632\n",
      "Epoch 25/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.5450 - val_loss: 0.5602\n",
      "Epoch 26/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.5412 - val_loss: 0.5572\n",
      "Epoch 27/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.5376 - val_loss: 0.5543\n",
      "Epoch 28/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.5340 - val_loss: 0.5516\n",
      "Epoch 29/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.5305 - val_loss: 0.5489\n",
      "Epoch 30/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.5271 - val_loss: 0.5463\n",
      "Epoch 31/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.5238 - val_loss: 0.5438\n",
      "Epoch 32/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.5205 - val_loss: 0.5414\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.5174 - val_loss: 0.5390\n",
      "Epoch 34/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.5143 - val_loss: 0.5367\n",
      "Epoch 35/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.5113 - val_loss: 0.5344\n",
      "Epoch 36/100\n",
      "6090/6090 [==============================] - 1s 136us/sample - loss: 0.5083 - val_loss: 0.5322\n",
      "Epoch 37/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.5054 - val_loss: 0.5301\n",
      "Epoch 38/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.5026 - val_loss: 0.5281\n",
      "Epoch 39/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.4999 - val_loss: 0.5261\n",
      "Epoch 40/100\n",
      "6090/6090 [==============================] - 1s 136us/sample - loss: 0.4972 - val_loss: 0.5242\n",
      "Epoch 41/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.4945 - val_loss: 0.5224\n",
      "Epoch 42/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.4919 - val_loss: 0.5205\n",
      "Epoch 43/100\n",
      "6090/6090 [==============================] - 1s 136us/sample - loss: 0.4894 - val_loss: 0.5188\n",
      "Epoch 44/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.4869 - val_loss: 0.5171\n",
      "Epoch 45/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.4845 - val_loss: 0.5154\n",
      "Epoch 46/100\n",
      "6090/6090 [==============================] - 1s 141us/sample - loss: 0.4821 - val_loss: 0.5138\n",
      "Epoch 47/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.4798 - val_loss: 0.5122\n",
      "Epoch 48/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.4775 - val_loss: 0.5107\n",
      "Epoch 49/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.4753 - val_loss: 0.5092\n",
      "Epoch 50/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.4731 - val_loss: 0.5077\n",
      "Epoch 51/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.4709 - val_loss: 0.5063\n",
      "Epoch 52/100\n",
      "6090/6090 [==============================] - 1s 148us/sample - loss: 0.4688 - val_loss: 0.5049\n",
      "Epoch 53/100\n",
      "6090/6090 [==============================] - 1s 152us/sample - loss: 0.4667 - val_loss: 0.5036\n",
      "Epoch 54/100\n",
      "6090/6090 [==============================] - 1s 136us/sample - loss: 0.4646 - val_loss: 0.5023\n",
      "Epoch 55/100\n",
      "6090/6090 [==============================] - 1s 133us/sample - loss: 0.4626 - val_loss: 0.5010\n",
      "Epoch 56/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.4607 - val_loss: 0.4997\n",
      "Epoch 57/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.4587 - val_loss: 0.4986\n",
      "Epoch 58/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.4568 - val_loss: 0.4974\n",
      "Epoch 59/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.4549 - val_loss: 0.4962\n",
      "Epoch 60/100\n",
      "6090/6090 [==============================] - 1s 136us/sample - loss: 0.4530 - val_loss: 0.4952\n",
      "Epoch 61/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.4512 - val_loss: 0.4940\n",
      "Epoch 62/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.4495 - val_loss: 0.4929\n",
      "Epoch 63/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.4476 - val_loss: 0.4919\n",
      "Epoch 64/100\n",
      "6090/6090 [==============================] - 1s 142us/sample - loss: 0.4459 - val_loss: 0.4909\n",
      "Epoch 65/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.4442 - val_loss: 0.4899\n",
      "Epoch 66/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.4425 - val_loss: 0.4890\n",
      "Epoch 67/100\n",
      "6090/6090 [==============================] - 1s 142us/sample - loss: 0.4408 - val_loss: 0.4880\n",
      "Epoch 68/100\n",
      "6090/6090 [==============================] - 1s 144us/sample - loss: 0.4391 - val_loss: 0.4871\n",
      "Epoch 69/100\n",
      "6090/6090 [==============================] - 1s 140us/sample - loss: 0.4376 - val_loss: 0.4862\n",
      "Epoch 70/100\n",
      "6090/6090 [==============================] - 1s 140us/sample - loss: 0.4359 - val_loss: 0.4853\n",
      "Epoch 71/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.4344 - val_loss: 0.4844\n",
      "Epoch 72/100\n",
      "6090/6090 [==============================] - 1s 142us/sample - loss: 0.4328 - val_loss: 0.4836\n",
      "Epoch 73/100\n",
      "6090/6090 [==============================] - 1s 136us/sample - loss: 0.4313 - val_loss: 0.4828\n",
      "Epoch 74/100\n",
      "6090/6090 [==============================] - 1s 135us/sample - loss: 0.4298 - val_loss: 0.4820\n",
      "Epoch 75/100\n",
      "6090/6090 [==============================] - 1s 136us/sample - loss: 0.4283 - val_loss: 0.4812\n",
      "Epoch 76/100\n",
      "6090/6090 [==============================] - 1s 136us/sample - loss: 0.4268 - val_loss: 0.4804\n",
      "Epoch 77/100\n",
      "6090/6090 [==============================] - 1s 145us/sample - loss: 0.4254 - val_loss: 0.4797\n",
      "Epoch 78/100\n",
      "6090/6090 [==============================] - 1s 148us/sample - loss: 0.4239 - val_loss: 0.4790\n",
      "Epoch 79/100\n",
      "6090/6090 [==============================] - 1s 149us/sample - loss: 0.4225 - val_loss: 0.4783\n",
      "Epoch 80/100\n",
      "6090/6090 [==============================] - 1s 153us/sample - loss: 0.4211 - val_loss: 0.4776\n",
      "Epoch 81/100\n",
      "6090/6090 [==============================] - 1s 148us/sample - loss: 0.4197 - val_loss: 0.4769\n",
      "Epoch 82/100\n",
      "6090/6090 [==============================] - 1s 146us/sample - loss: 0.4183 - val_loss: 0.4762\n",
      "Epoch 83/100\n",
      "6090/6090 [==============================] - 1s 135us/sample - loss: 0.4170 - val_loss: 0.4756\n",
      "Epoch 84/100\n",
      "6090/6090 [==============================] - 1s 140us/sample - loss: 0.4156 - val_loss: 0.4749\n",
      "Epoch 85/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.4143 - val_loss: 0.4743\n",
      "Epoch 86/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.4130 - val_loss: 0.4737\n",
      "Epoch 87/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.4117 - val_loss: 0.4731\n",
      "Epoch 88/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.4105 - val_loss: 0.4725\n",
      "Epoch 89/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.4092 - val_loss: 0.4719\n",
      "Epoch 90/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.4079 - val_loss: 0.4714\n",
      "Epoch 91/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.4067 - val_loss: 0.4708\n",
      "Epoch 92/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.4055 - val_loss: 0.4703\n",
      "Epoch 93/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.4043 - val_loss: 0.4698\n",
      "Epoch 94/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.4031 - val_loss: 0.4693\n",
      "Epoch 95/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.4019 - val_loss: 0.4687\n",
      "Epoch 96/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.4007 - val_loss: 0.4683\n",
      "Epoch 97/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.3995 - val_loss: 0.4678\n",
      "Epoch 98/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.3984 - val_loss: 0.4673\n",
      "Epoch 99/100\n",
      "6090/6090 [==============================] - 1s 136us/sample - loss: 0.3973 - val_loss: 0.4669\n",
      "Epoch 100/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.3961 - val_loss: 0.4664\n",
      "Predicting...\n",
      "(Took 85.644 sec)\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/100\n",
      "6090/6090 [==============================] - 1s 180us/sample - loss: 0.6859 - val_loss: 0.6770\n",
      "Epoch 2/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.6741 - val_loss: 0.6670\n",
      "Epoch 3/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.6652 - val_loss: 0.6589\n",
      "Epoch 4/100\n",
      "6090/6090 [==============================] - 1s 140us/sample - loss: 0.6573 - val_loss: 0.6520\n",
      "Epoch 5/100\n",
      "6090/6090 [==============================] - 1s 140us/sample - loss: 0.6500 - val_loss: 0.6456\n",
      "Epoch 6/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.6429 - val_loss: 0.6396\n",
      "Epoch 7/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.6361 - val_loss: 0.6339\n",
      "Epoch 8/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.6296 - val_loss: 0.6285\n",
      "Epoch 9/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.6233 - val_loss: 0.6233\n",
      "Epoch 10/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.6172 - val_loss: 0.6183\n",
      "Epoch 11/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.6112 - val_loss: 0.6135\n",
      "Epoch 12/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.6055 - val_loss: 0.6088\n",
      "Epoch 13/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.6000 - val_loss: 0.6043\n",
      "Epoch 14/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.5946 - val_loss: 0.6000\n",
      "Epoch 15/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.5894 - val_loss: 0.5959\n",
      "Epoch 16/100\n",
      "6090/6090 [==============================] - 1s 136us/sample - loss: 0.5844 - val_loss: 0.5919\n",
      "Epoch 17/100\n",
      "6090/6090 [==============================] - 1s 136us/sample - loss: 0.5795 - val_loss: 0.5880\n",
      "Epoch 18/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.5747 - val_loss: 0.5842\n",
      "Epoch 19/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.5701 - val_loss: 0.5806\n",
      "Epoch 20/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.5656 - val_loss: 0.5772\n",
      "Epoch 21/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.5613 - val_loss: 0.5737\n",
      "Epoch 22/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.5571 - val_loss: 0.5705\n",
      "Epoch 23/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.5529 - val_loss: 0.5673\n",
      "Epoch 24/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.5490 - val_loss: 0.5641\n",
      "Epoch 25/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.5450 - val_loss: 0.5613\n",
      "Epoch 26/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.5412 - val_loss: 0.5583\n",
      "Epoch 27/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.5375 - val_loss: 0.5555\n",
      "Epoch 28/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.5340 - val_loss: 0.5528\n",
      "Epoch 29/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.5304 - val_loss: 0.5503\n",
      "Epoch 30/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.5270 - val_loss: 0.5478\n",
      "Epoch 31/100\n",
      "6090/6090 [==============================] - 1s 140us/sample - loss: 0.5237 - val_loss: 0.5453\n",
      "Epoch 32/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.5204 - val_loss: 0.5429\n",
      "Epoch 33/100\n",
      "6090/6090 [==============================] - 1s 146us/sample - loss: 0.5173 - val_loss: 0.5407\n",
      "Epoch 34/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.5142 - val_loss: 0.5386\n",
      "Epoch 35/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.5111 - val_loss: 0.5363\n",
      "Epoch 36/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.5082 - val_loss: 0.5342\n",
      "Epoch 37/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.5053 - val_loss: 0.5321\n",
      "Epoch 38/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.5025 - val_loss: 0.5301\n",
      "Epoch 39/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.4997 - val_loss: 0.5281\n",
      "Epoch 40/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.4971 - val_loss: 0.5263\n",
      "Epoch 41/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.4944 - val_loss: 0.5245\n",
      "Epoch 42/100\n",
      "6090/6090 [==============================] - 1s 140us/sample - loss: 0.4918 - val_loss: 0.5228\n",
      "Epoch 43/100\n",
      "6090/6090 [==============================] - 1s 140us/sample - loss: 0.4893 - val_loss: 0.5211\n",
      "Epoch 44/100\n",
      "6090/6090 [==============================] - 1s 149us/sample - loss: 0.4868 - val_loss: 0.5194\n",
      "Epoch 45/100\n",
      "6090/6090 [==============================] - 1s 142us/sample - loss: 0.4843 - val_loss: 0.5178\n",
      "Epoch 46/100\n",
      "6090/6090 [==============================] - 1s 144us/sample - loss: 0.4819 - val_loss: 0.5162\n",
      "Epoch 47/100\n",
      "6090/6090 [==============================] - 1s 141us/sample - loss: 0.4796 - val_loss: 0.5146\n",
      "Epoch 48/100\n",
      "6090/6090 [==============================] - 1s 140us/sample - loss: 0.4773 - val_loss: 0.5132\n",
      "Epoch 49/100\n",
      "6090/6090 [==============================] - 1s 136us/sample - loss: 0.4750 - val_loss: 0.5117\n",
      "Epoch 50/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.4728 - val_loss: 0.5103\n",
      "Epoch 51/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.4706 - val_loss: 0.5089\n",
      "Epoch 52/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.4685 - val_loss: 0.5074\n",
      "Epoch 53/100\n",
      "6090/6090 [==============================] - 1s 140us/sample - loss: 0.4664 - val_loss: 0.5062\n",
      "Epoch 54/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.4643 - val_loss: 0.5048\n",
      "Epoch 55/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.4623 - val_loss: 0.5037\n",
      "Epoch 56/100\n",
      "6090/6090 [==============================] - 1s 140us/sample - loss: 0.4603 - val_loss: 0.5024\n",
      "Epoch 57/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.4584 - val_loss: 0.5014\n",
      "Epoch 58/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.4565 - val_loss: 0.5002\n",
      "Epoch 59/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.4546 - val_loss: 0.4990\n",
      "Epoch 60/100\n",
      "6090/6090 [==============================] - 1s 141us/sample - loss: 0.4527 - val_loss: 0.4980\n",
      "Epoch 61/100\n",
      "6090/6090 [==============================] - 1s 140us/sample - loss: 0.4509 - val_loss: 0.4969\n",
      "Epoch 62/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.4491 - val_loss: 0.4959\n",
      "Epoch 63/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.4473 - val_loss: 0.4948\n",
      "Epoch 64/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.4455 - val_loss: 0.4938\n",
      "Epoch 65/100\n",
      "6090/6090 [==============================] - 1s 141us/sample - loss: 0.4438 - val_loss: 0.4928\n",
      "Epoch 66/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.4421 - val_loss: 0.4920\n",
      "Epoch 67/100\n",
      "6090/6090 [==============================] - 1s 140us/sample - loss: 0.4404 - val_loss: 0.4910\n",
      "Epoch 68/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.4388 - val_loss: 0.4901\n",
      "Epoch 69/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.4372 - val_loss: 0.4891\n",
      "Epoch 70/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.4355 - val_loss: 0.4883\n",
      "Epoch 71/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.4340 - val_loss: 0.4874\n",
      "Epoch 72/100\n",
      "6090/6090 [==============================] - 1s 140us/sample - loss: 0.4324 - val_loss: 0.4866\n",
      "Epoch 73/100\n",
      "6090/6090 [==============================] - 1s 141us/sample - loss: 0.4308 - val_loss: 0.4858\n",
      "Epoch 74/100\n",
      "6090/6090 [==============================] - 1s 140us/sample - loss: 0.4294 - val_loss: 0.4850\n",
      "Epoch 75/100\n",
      "6090/6090 [==============================] - 1s 140us/sample - loss: 0.4279 - val_loss: 0.4842\n",
      "Epoch 76/100\n",
      "6090/6090 [==============================] - 1s 141us/sample - loss: 0.4264 - val_loss: 0.4836\n",
      "Epoch 77/100\n",
      "6090/6090 [==============================] - 1s 141us/sample - loss: 0.4249 - val_loss: 0.4828\n",
      "Epoch 78/100\n",
      "6090/6090 [==============================] - 1s 142us/sample - loss: 0.4235 - val_loss: 0.4820\n",
      "Epoch 79/100\n",
      "6090/6090 [==============================] - 1s 141us/sample - loss: 0.4220 - val_loss: 0.4813\n",
      "Epoch 80/100\n",
      "6090/6090 [==============================] - 1s 140us/sample - loss: 0.4206 - val_loss: 0.4807\n",
      "Epoch 81/100\n",
      "6090/6090 [==============================] - 1s 141us/sample - loss: 0.4193 - val_loss: 0.4800\n",
      "Epoch 82/100\n",
      "6090/6090 [==============================] - 1s 140us/sample - loss: 0.4179 - val_loss: 0.4793\n",
      "Epoch 83/100\n",
      "6090/6090 [==============================] - 1s 140us/sample - loss: 0.4165 - val_loss: 0.4786\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.4152 - val_loss: 0.4779\n",
      "Epoch 85/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.4139 - val_loss: 0.4774\n",
      "Epoch 86/100\n",
      "6090/6090 [==============================] - 1s 146us/sample - loss: 0.4126 - val_loss: 0.4767\n",
      "Epoch 87/100\n",
      "6090/6090 [==============================] - 1s 140us/sample - loss: 0.4113 - val_loss: 0.4762\n",
      "Epoch 88/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.4100 - val_loss: 0.4756\n",
      "Epoch 89/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.4087 - val_loss: 0.4749\n",
      "Epoch 90/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.4075 - val_loss: 0.4744\n",
      "Epoch 91/100\n",
      "6090/6090 [==============================] - 1s 140us/sample - loss: 0.4062 - val_loss: 0.4739\n",
      "Epoch 92/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.4050 - val_loss: 0.4734\n",
      "Epoch 93/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.4038 - val_loss: 0.4729\n",
      "Epoch 94/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.4026 - val_loss: 0.4723\n",
      "Epoch 95/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.4014 - val_loss: 0.4718\n",
      "Epoch 96/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.4002 - val_loss: 0.4713\n",
      "Epoch 97/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.3991 - val_loss: 0.4709\n",
      "Epoch 98/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.3980 - val_loss: 0.4704\n",
      "Epoch 99/100\n",
      "6090/6090 [==============================] - 1s 140us/sample - loss: 0.3968 - val_loss: 0.4699\n",
      "Epoch 100/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.3957 - val_loss: 0.4695\n",
      "Predicting...\n",
      "(Took 85.877 sec)\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/100\n",
      "6090/6090 [==============================] - 1s 190us/sample - loss: 0.6853 - val_loss: 0.6808\n",
      "Epoch 2/100\n",
      "6090/6090 [==============================] - 1s 140us/sample - loss: 0.6728 - val_loss: 0.6727\n",
      "Epoch 3/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.6634 - val_loss: 0.6660\n",
      "Epoch 4/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.6553 - val_loss: 0.6598\n",
      "Epoch 5/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.6479 - val_loss: 0.6539\n",
      "Epoch 6/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.6409 - val_loss: 0.6483\n",
      "Epoch 7/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.6341 - val_loss: 0.6428\n",
      "Epoch 8/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.6276 - val_loss: 0.6373\n",
      "Epoch 9/100\n",
      "6090/6090 [==============================] - 1s 140us/sample - loss: 0.6213 - val_loss: 0.6322\n",
      "Epoch 10/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.6152 - val_loss: 0.6271\n",
      "Epoch 11/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.6092 - val_loss: 0.6224\n",
      "Epoch 12/100\n",
      "6090/6090 [==============================] - 1s 140us/sample - loss: 0.6035 - val_loss: 0.6177\n",
      "Epoch 13/100\n",
      "6090/6090 [==============================] - 1s 140us/sample - loss: 0.5980 - val_loss: 0.6134\n",
      "Epoch 14/100\n",
      "6090/6090 [==============================] - 1s 140us/sample - loss: 0.5926 - val_loss: 0.6090\n",
      "Epoch 15/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.5874 - val_loss: 0.6048\n",
      "Epoch 16/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.5824 - val_loss: 0.6008\n",
      "Epoch 17/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.5775 - val_loss: 0.5971\n",
      "Epoch 18/100\n",
      "6090/6090 [==============================] - 1s 140us/sample - loss: 0.5727 - val_loss: 0.5932\n",
      "Epoch 19/100\n",
      "6090/6090 [==============================] - 1s 143us/sample - loss: 0.5681 - val_loss: 0.5896\n",
      "Epoch 20/100\n",
      "6090/6090 [==============================] - 1s 147us/sample - loss: 0.5636 - val_loss: 0.5862\n",
      "Epoch 21/100\n",
      "6090/6090 [==============================] - 1s 145us/sample - loss: 0.5592 - val_loss: 0.5828\n",
      "Epoch 22/100\n",
      "6090/6090 [==============================] - 1s 140us/sample - loss: 0.5550 - val_loss: 0.5796\n",
      "Epoch 23/100\n",
      "6090/6090 [==============================] - 1s 144us/sample - loss: 0.5508 - val_loss: 0.5766\n",
      "Epoch 24/100\n",
      "6090/6090 [==============================] - 1s 150us/sample - loss: 0.5468 - val_loss: 0.5735\n",
      "Epoch 25/100\n",
      "6090/6090 [==============================] - 1s 140us/sample - loss: 0.5429 - val_loss: 0.5706\n",
      "Epoch 26/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.5391 - val_loss: 0.5679\n",
      "Epoch 27/100\n",
      "6090/6090 [==============================] - 1s 140us/sample - loss: 0.5354 - val_loss: 0.5651\n",
      "Epoch 28/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.5318 - val_loss: 0.5626\n",
      "Epoch 29/100\n",
      "6090/6090 [==============================] - 1s 140us/sample - loss: 0.5283 - val_loss: 0.5600\n",
      "Epoch 30/100\n",
      "6090/6090 [==============================] - 1s 142us/sample - loss: 0.5249 - val_loss: 0.5575\n",
      "Epoch 31/100\n",
      "6090/6090 [==============================] - 1s 140us/sample - loss: 0.5215 - val_loss: 0.5552\n",
      "Epoch 32/100\n",
      "6090/6090 [==============================] - 1s 141us/sample - loss: 0.5183 - val_loss: 0.5529\n",
      "Epoch 33/100\n",
      "6090/6090 [==============================] - 1s 141us/sample - loss: 0.5151 - val_loss: 0.5507\n",
      "Epoch 34/100\n",
      "6090/6090 [==============================] - 1s 141us/sample - loss: 0.5120 - val_loss: 0.5485\n",
      "Epoch 35/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.5090 - val_loss: 0.5463\n",
      "Epoch 36/100\n",
      "6090/6090 [==============================] - 1s 140us/sample - loss: 0.5060 - val_loss: 0.5443\n",
      "Epoch 37/100\n",
      "6090/6090 [==============================] - 1s 140us/sample - loss: 0.5031 - val_loss: 0.5424\n",
      "Epoch 38/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.5003 - val_loss: 0.5404\n",
      "Epoch 39/100\n",
      "6090/6090 [==============================] - 1s 141us/sample - loss: 0.4975 - val_loss: 0.5386\n",
      "Epoch 40/100\n",
      "6090/6090 [==============================] - 1s 141us/sample - loss: 0.4948 - val_loss: 0.5367\n",
      "Epoch 41/100\n",
      "6090/6090 [==============================] - 1s 141us/sample - loss: 0.4921 - val_loss: 0.5349\n",
      "Epoch 42/100\n",
      "6090/6090 [==============================] - 1s 141us/sample - loss: 0.4895 - val_loss: 0.5333\n",
      "Epoch 43/100\n",
      "6090/6090 [==============================] - 1s 140us/sample - loss: 0.4870 - val_loss: 0.5316\n",
      "Epoch 44/100\n",
      "6090/6090 [==============================] - 1s 142us/sample - loss: 0.4845 - val_loss: 0.5300\n",
      "Epoch 45/100\n",
      "6090/6090 [==============================] - 1s 140us/sample - loss: 0.4821 - val_loss: 0.5283\n",
      "Epoch 46/100\n",
      "6090/6090 [==============================] - 1s 141us/sample - loss: 0.4797 - val_loss: 0.5269\n",
      "Epoch 47/100\n",
      "6090/6090 [==============================] - 1s 141us/sample - loss: 0.4773 - val_loss: 0.5254\n",
      "Epoch 48/100\n",
      "6090/6090 [==============================] - 1s 143us/sample - loss: 0.4750 - val_loss: 0.5239\n",
      "Epoch 49/100\n",
      "6090/6090 [==============================] - 1s 142us/sample - loss: 0.4728 - val_loss: 0.5224\n",
      "Epoch 50/100\n",
      "6090/6090 [==============================] - 1s 142us/sample - loss: 0.4705 - val_loss: 0.5211\n",
      "Epoch 51/100\n",
      "6090/6090 [==============================] - 1s 140us/sample - loss: 0.4684 - val_loss: 0.5198\n",
      "Epoch 52/100\n",
      "6090/6090 [==============================] - 1s 141us/sample - loss: 0.4662 - val_loss: 0.5186\n",
      "Epoch 53/100\n",
      "6090/6090 [==============================] - 1s 141us/sample - loss: 0.4641 - val_loss: 0.5173\n",
      "Epoch 54/100\n",
      "6090/6090 [==============================] - 1s 141us/sample - loss: 0.4621 - val_loss: 0.5160\n",
      "Epoch 55/100\n",
      "6090/6090 [==============================] - 1s 141us/sample - loss: 0.4600 - val_loss: 0.5148\n",
      "Epoch 56/100\n",
      "6090/6090 [==============================] - 1s 140us/sample - loss: 0.4580 - val_loss: 0.5136\n",
      "Epoch 57/100\n",
      "6090/6090 [==============================] - 1s 141us/sample - loss: 0.4561 - val_loss: 0.5125\n",
      "Epoch 58/100\n",
      "6090/6090 [==============================] - 1s 149us/sample - loss: 0.4542 - val_loss: 0.5113\n",
      "Epoch 59/100\n",
      "6090/6090 [==============================] - 1s 164us/sample - loss: 0.4523 - val_loss: 0.5103\n",
      "Epoch 60/100\n",
      "6090/6090 [==============================] - 1s 161us/sample - loss: 0.4504 - val_loss: 0.5092\n",
      "Epoch 61/100\n",
      "6090/6090 [==============================] - 1s 164us/sample - loss: 0.4486 - val_loss: 0.5082\n",
      "Epoch 62/100\n",
      "6090/6090 [==============================] - 1s 162us/sample - loss: 0.4468 - val_loss: 0.5072\n",
      "Epoch 63/100\n",
      "6090/6090 [==============================] - 1s 162us/sample - loss: 0.4450 - val_loss: 0.5062\n",
      "Epoch 64/100\n",
      "6090/6090 [==============================] - 1s 160us/sample - loss: 0.4433 - val_loss: 0.5053\n",
      "Epoch 65/100\n",
      "6090/6090 [==============================] - 1s 145us/sample - loss: 0.4415 - val_loss: 0.5042\n",
      "Epoch 66/100\n",
      "6090/6090 [==============================] - 1s 140us/sample - loss: 0.4399 - val_loss: 0.5034\n",
      "Epoch 67/100\n",
      "6090/6090 [==============================] - 1s 140us/sample - loss: 0.4382 - val_loss: 0.5025\n",
      "Epoch 68/100\n",
      "6090/6090 [==============================] - 1s 140us/sample - loss: 0.4365 - val_loss: 0.5015\n",
      "Epoch 69/100\n",
      "6090/6090 [==============================] - 1s 143us/sample - loss: 0.4349 - val_loss: 0.5007\n",
      "Epoch 70/100\n",
      "6090/6090 [==============================] - 1s 148us/sample - loss: 0.4333 - val_loss: 0.4998\n",
      "Epoch 71/100\n",
      "6090/6090 [==============================] - 1s 146us/sample - loss: 0.4317 - val_loss: 0.4990\n",
      "Epoch 72/100\n",
      "6090/6090 [==============================] - 1s 172us/sample - loss: 0.4301 - val_loss: 0.4982\n",
      "Epoch 73/100\n",
      "6090/6090 [==============================] - 1s 165us/sample - loss: 0.4286 - val_loss: 0.4974\n",
      "Epoch 74/100\n",
      "6090/6090 [==============================] - 1s 172us/sample - loss: 0.4271 - val_loss: 0.4966\n",
      "Epoch 75/100\n",
      "6090/6090 [==============================] - 1s 173us/sample - loss: 0.4256 - val_loss: 0.4959\n",
      "Epoch 76/100\n",
      "6090/6090 [==============================] - 1s 169us/sample - loss: 0.4241 - val_loss: 0.4952\n",
      "Epoch 77/100\n",
      "6090/6090 [==============================] - 1s 170us/sample - loss: 0.4227 - val_loss: 0.4945\n",
      "Epoch 78/100\n",
      "6090/6090 [==============================] - 1s 168us/sample - loss: 0.4212 - val_loss: 0.4937\n",
      "Epoch 79/100\n",
      "6090/6090 [==============================] - 1s 147us/sample - loss: 0.4198 - val_loss: 0.4931\n",
      "Epoch 80/100\n",
      "6090/6090 [==============================] - 1s 161us/sample - loss: 0.4184 - val_loss: 0.4923\n",
      "Epoch 81/100\n",
      "6090/6090 [==============================] - 1s 156us/sample - loss: 0.4170 - val_loss: 0.4917\n",
      "Epoch 82/100\n",
      "6090/6090 [==============================] - 1s 155us/sample - loss: 0.4157 - val_loss: 0.4910\n",
      "Epoch 83/100\n",
      "6090/6090 [==============================] - 1s 148us/sample - loss: 0.4143 - val_loss: 0.4904\n",
      "Epoch 84/100\n",
      "6090/6090 [==============================] - 1s 145us/sample - loss: 0.4130 - val_loss: 0.4898\n",
      "Epoch 85/100\n",
      "6090/6090 [==============================] - 1s 141us/sample - loss: 0.4116 - val_loss: 0.4891\n",
      "Epoch 86/100\n",
      "6090/6090 [==============================] - 1s 141us/sample - loss: 0.4103 - val_loss: 0.4885\n",
      "Epoch 87/100\n",
      "6090/6090 [==============================] - 1s 145us/sample - loss: 0.4091 - val_loss: 0.4879\n",
      "Epoch 88/100\n",
      "6090/6090 [==============================] - 1s 146us/sample - loss: 0.4078 - val_loss: 0.4873\n",
      "Epoch 89/100\n",
      "6090/6090 [==============================] - 1s 152us/sample - loss: 0.4065 - val_loss: 0.4867\n",
      "Epoch 90/100\n",
      "6090/6090 [==============================] - 1s 141us/sample - loss: 0.4053 - val_loss: 0.4863\n",
      "Epoch 91/100\n",
      "6090/6090 [==============================] - 1s 148us/sample - loss: 0.4041 - val_loss: 0.4858\n",
      "Epoch 92/100\n",
      "6090/6090 [==============================] - 1s 149us/sample - loss: 0.4028 - val_loss: 0.4851\n",
      "Epoch 93/100\n",
      "6090/6090 [==============================] - 1s 154us/sample - loss: 0.4016 - val_loss: 0.4846\n",
      "Epoch 94/100\n",
      "6090/6090 [==============================] - 1s 159us/sample - loss: 0.4004 - val_loss: 0.4841\n",
      "Epoch 95/100\n",
      "6090/6090 [==============================] - 1s 150us/sample - loss: 0.3993 - val_loss: 0.4836\n",
      "Epoch 96/100\n",
      "6090/6090 [==============================] - 1s 152us/sample - loss: 0.3981 - val_loss: 0.4831\n",
      "Epoch 97/100\n",
      "6090/6090 [==============================] - 1s 149us/sample - loss: 0.3969 - val_loss: 0.4825\n",
      "Epoch 98/100\n",
      "6090/6090 [==============================] - 1s 147us/sample - loss: 0.3958 - val_loss: 0.4821\n",
      "Epoch 99/100\n",
      "6090/6090 [==============================] - 1s 143us/sample - loss: 0.3946 - val_loss: 0.4816\n",
      "Epoch 100/100\n",
      "6090/6090 [==============================] - 1s 140us/sample - loss: 0.3935 - val_loss: 0.4812\n",
      "Predicting...\n",
      "(Took 90.130 sec)\n",
      "Train on 6091 samples, validate on 1522 samples\n",
      "Epoch 1/100\n",
      "6091/6091 [==============================] - 1s 184us/sample - loss: 0.6859 - val_loss: 0.6792\n",
      "Epoch 2/100\n",
      "6091/6091 [==============================] - 1s 142us/sample - loss: 0.6736 - val_loss: 0.6697\n",
      "Epoch 3/100\n",
      "6091/6091 [==============================] - 1s 141us/sample - loss: 0.6641 - val_loss: 0.6620\n",
      "Epoch 4/100\n",
      "6091/6091 [==============================] - 1s 141us/sample - loss: 0.6561 - val_loss: 0.6554\n",
      "Epoch 5/100\n",
      "6091/6091 [==============================] - 1s 142us/sample - loss: 0.6487 - val_loss: 0.6493\n",
      "Epoch 6/100\n",
      "6091/6091 [==============================] - 1s 142us/sample - loss: 0.6416 - val_loss: 0.6434\n",
      "Epoch 7/100\n",
      "6091/6091 [==============================] - 1s 142us/sample - loss: 0.6348 - val_loss: 0.6377\n",
      "Epoch 8/100\n",
      "6091/6091 [==============================] - 1s 144us/sample - loss: 0.6282 - val_loss: 0.6324\n",
      "Epoch 9/100\n",
      "6091/6091 [==============================] - 1s 143us/sample - loss: 0.6219 - val_loss: 0.6271\n",
      "Epoch 10/100\n",
      "6091/6091 [==============================] - 1s 143us/sample - loss: 0.6158 - val_loss: 0.6221\n",
      "Epoch 11/100\n",
      "6091/6091 [==============================] - 1s 143us/sample - loss: 0.6099 - val_loss: 0.6173\n",
      "Epoch 12/100\n",
      "6091/6091 [==============================] - 1s 143us/sample - loss: 0.6042 - val_loss: 0.6127\n",
      "Epoch 13/100\n",
      "6091/6091 [==============================] - 1s 144us/sample - loss: 0.5986 - val_loss: 0.6081\n",
      "Epoch 14/100\n",
      "6091/6091 [==============================] - 1s 141us/sample - loss: 0.5933 - val_loss: 0.6039\n",
      "Epoch 15/100\n",
      "6091/6091 [==============================] - 1s 142us/sample - loss: 0.5881 - val_loss: 0.5997\n",
      "Epoch 16/100\n",
      "6091/6091 [==============================] - 1s 146us/sample - loss: 0.5831 - val_loss: 0.5957\n",
      "Epoch 17/100\n",
      "6091/6091 [==============================] - 1s 145us/sample - loss: 0.5782 - val_loss: 0.5918\n",
      "Epoch 18/100\n",
      "6091/6091 [==============================] - 1s 143us/sample - loss: 0.5735 - val_loss: 0.5881\n",
      "Epoch 19/100\n",
      "6091/6091 [==============================] - 1s 146us/sample - loss: 0.5689 - val_loss: 0.5845\n",
      "Epoch 20/100\n",
      "6091/6091 [==============================] - 1s 147us/sample - loss: 0.5644 - val_loss: 0.5810\n",
      "Epoch 21/100\n",
      "6091/6091 [==============================] - 1s 141us/sample - loss: 0.5601 - val_loss: 0.5776\n",
      "Epoch 22/100\n",
      "6091/6091 [==============================] - 1s 146us/sample - loss: 0.5559 - val_loss: 0.5743\n",
      "Epoch 23/100\n",
      "6091/6091 [==============================] - 1s 143us/sample - loss: 0.5518 - val_loss: 0.5711\n",
      "Epoch 24/100\n",
      "6091/6091 [==============================] - 1s 144us/sample - loss: 0.5478 - val_loss: 0.5681\n",
      "Epoch 25/100\n",
      "6091/6091 [==============================] - 1s 143us/sample - loss: 0.5439 - val_loss: 0.5651\n",
      "Epoch 26/100\n",
      "6091/6091 [==============================] - 1s 144us/sample - loss: 0.5402 - val_loss: 0.5623\n",
      "Epoch 27/100\n",
      "6091/6091 [==============================] - 1s 144us/sample - loss: 0.5365 - val_loss: 0.5595\n",
      "Epoch 28/100\n",
      "6091/6091 [==============================] - 1s 144us/sample - loss: 0.5329 - val_loss: 0.5568\n",
      "Epoch 29/100\n",
      "6091/6091 [==============================] - 1s 146us/sample - loss: 0.5295 - val_loss: 0.5542\n",
      "Epoch 30/100\n",
      "6091/6091 [==============================] - 1s 143us/sample - loss: 0.5260 - val_loss: 0.5516\n",
      "Epoch 31/100\n",
      "6091/6091 [==============================] - 1s 143us/sample - loss: 0.5227 - val_loss: 0.5492\n",
      "Epoch 32/100\n",
      "6091/6091 [==============================] - 1s 145us/sample - loss: 0.5195 - val_loss: 0.5468\n",
      "Epoch 33/100\n",
      "6091/6091 [==============================] - 1s 144us/sample - loss: 0.5163 - val_loss: 0.5445\n",
      "Epoch 34/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6091/6091 [==============================] - 1s 144us/sample - loss: 0.5132 - val_loss: 0.5422\n",
      "Epoch 35/100\n",
      "6091/6091 [==============================] - 1s 142us/sample - loss: 0.5103 - val_loss: 0.5400\n",
      "Epoch 36/100\n",
      "6091/6091 [==============================] - 1s 141us/sample - loss: 0.5073 - val_loss: 0.5379\n",
      "Epoch 37/100\n",
      "6091/6091 [==============================] - 1s 142us/sample - loss: 0.5045 - val_loss: 0.5358\n",
      "Epoch 38/100\n",
      "6091/6091 [==============================] - 1s 141us/sample - loss: 0.5017 - val_loss: 0.5338\n",
      "Epoch 39/100\n",
      "6091/6091 [==============================] - 1s 142us/sample - loss: 0.4989 - val_loss: 0.5319\n",
      "Epoch 40/100\n",
      "6091/6091 [==============================] - 1s 144us/sample - loss: 0.4962 - val_loss: 0.5300\n",
      "Epoch 41/100\n",
      "6091/6091 [==============================] - 1s 141us/sample - loss: 0.4936 - val_loss: 0.5282\n",
      "Epoch 42/100\n",
      "6091/6091 [==============================] - 1s 142us/sample - loss: 0.4910 - val_loss: 0.5264\n",
      "Epoch 43/100\n",
      "6091/6091 [==============================] - 1s 142us/sample - loss: 0.4886 - val_loss: 0.5246\n",
      "Epoch 44/100\n",
      "6091/6091 [==============================] - 1s 143us/sample - loss: 0.4861 - val_loss: 0.5229\n",
      "Epoch 45/100\n",
      "6091/6091 [==============================] - 1s 144us/sample - loss: 0.4837 - val_loss: 0.5213\n",
      "Epoch 46/100\n",
      "6091/6091 [==============================] - 1s 143us/sample - loss: 0.4813 - val_loss: 0.5197\n",
      "Epoch 47/100\n",
      "6091/6091 [==============================] - 1s 143us/sample - loss: 0.4790 - val_loss: 0.5181\n",
      "Epoch 48/100\n",
      "6091/6091 [==============================] - 1s 141us/sample - loss: 0.4767 - val_loss: 0.5165\n",
      "Epoch 49/100\n",
      "6091/6091 [==============================] - 1s 141us/sample - loss: 0.4745 - val_loss: 0.5150\n",
      "Epoch 50/100\n",
      "6091/6091 [==============================] - 1s 141us/sample - loss: 0.4723 - val_loss: 0.5137\n",
      "Epoch 51/100\n",
      "6091/6091 [==============================] - 1s 140us/sample - loss: 0.4702 - val_loss: 0.5122\n",
      "Epoch 52/100\n",
      "6091/6091 [==============================] - 1s 142us/sample - loss: 0.4681 - val_loss: 0.5108\n",
      "Epoch 53/100\n",
      "6091/6091 [==============================] - 1s 142us/sample - loss: 0.4660 - val_loss: 0.5095\n",
      "Epoch 54/100\n",
      "6091/6091 [==============================] - 1s 142us/sample - loss: 0.4639 - val_loss: 0.5081\n",
      "Epoch 55/100\n",
      "6091/6091 [==============================] - 1s 142us/sample - loss: 0.4619 - val_loss: 0.5068\n",
      "Epoch 56/100\n",
      "6091/6091 [==============================] - 1s 143us/sample - loss: 0.4599 - val_loss: 0.5056\n",
      "Epoch 57/100\n",
      "6091/6091 [==============================] - 1s 143us/sample - loss: 0.4580 - val_loss: 0.5044\n",
      "Epoch 58/100\n",
      "6091/6091 [==============================] - 1s 143us/sample - loss: 0.4561 - val_loss: 0.5032\n",
      "Epoch 59/100\n",
      "6091/6091 [==============================] - 1s 143us/sample - loss: 0.4542 - val_loss: 0.5020\n",
      "Epoch 60/100\n",
      "6091/6091 [==============================] - 1s 142us/sample - loss: 0.4524 - val_loss: 0.5008\n",
      "Epoch 61/100\n",
      "6091/6091 [==============================] - 1s 143us/sample - loss: 0.4506 - val_loss: 0.4997\n",
      "Epoch 62/100\n",
      "6091/6091 [==============================] - 1s 142us/sample - loss: 0.4488 - val_loss: 0.4986\n",
      "Epoch 63/100\n",
      "6091/6091 [==============================] - 1s 143us/sample - loss: 0.4470 - val_loss: 0.4975\n",
      "Epoch 64/100\n",
      "6091/6091 [==============================] - 1s 144us/sample - loss: 0.4453 - val_loss: 0.4965\n",
      "Epoch 65/100\n",
      "6091/6091 [==============================] - 1s 142us/sample - loss: 0.4436 - val_loss: 0.4954\n",
      "Epoch 66/100\n",
      "6091/6091 [==============================] - 1s 147us/sample - loss: 0.4419 - val_loss: 0.4944\n",
      "Epoch 67/100\n",
      "6091/6091 [==============================] - 1s 158us/sample - loss: 0.4402 - val_loss: 0.4934\n",
      "Epoch 68/100\n",
      "6091/6091 [==============================] - 1s 149us/sample - loss: 0.4386 - val_loss: 0.4925\n",
      "Epoch 69/100\n",
      "6091/6091 [==============================] - 1s 146us/sample - loss: 0.4370 - val_loss: 0.4915\n",
      "Epoch 70/100\n",
      "6091/6091 [==============================] - 1s 147us/sample - loss: 0.4354 - val_loss: 0.4905\n",
      "Epoch 71/100\n",
      "6091/6091 [==============================] - 1s 145us/sample - loss: 0.4338 - val_loss: 0.4897\n",
      "Epoch 72/100\n",
      "6091/6091 [==============================] - 1s 143us/sample - loss: 0.4323 - val_loss: 0.4887\n",
      "Epoch 73/100\n",
      "6091/6091 [==============================] - 1s 144us/sample - loss: 0.4307 - val_loss: 0.4879\n",
      "Epoch 74/100\n",
      "6091/6091 [==============================] - 1s 143us/sample - loss: 0.4292 - val_loss: 0.4870\n",
      "Epoch 75/100\n",
      "6091/6091 [==============================] - 1s 143us/sample - loss: 0.4277 - val_loss: 0.4862\n",
      "Epoch 76/100\n",
      "6091/6091 [==============================] - 1s 143us/sample - loss: 0.4263 - val_loss: 0.4854\n",
      "Epoch 77/100\n",
      "6091/6091 [==============================] - 1s 144us/sample - loss: 0.4248 - val_loss: 0.4846\n",
      "Epoch 78/100\n",
      "6091/6091 [==============================] - 1s 143us/sample - loss: 0.4234 - val_loss: 0.4838\n",
      "Epoch 79/100\n",
      "6091/6091 [==============================] - 1s 143us/sample - loss: 0.4220 - val_loss: 0.4830\n",
      "Epoch 80/100\n",
      "6091/6091 [==============================] - 1s 143us/sample - loss: 0.4206 - val_loss: 0.4823\n",
      "Epoch 81/100\n",
      "6091/6091 [==============================] - 1s 145us/sample - loss: 0.4192 - val_loss: 0.4815\n",
      "Epoch 82/100\n",
      "6091/6091 [==============================] - 1s 142us/sample - loss: 0.4178 - val_loss: 0.4808\n",
      "Epoch 83/100\n",
      "6091/6091 [==============================] - 1s 144us/sample - loss: 0.4165 - val_loss: 0.4801\n",
      "Epoch 84/100\n",
      "6091/6091 [==============================] - 1s 143us/sample - loss: 0.4152 - val_loss: 0.4794\n",
      "Epoch 85/100\n",
      "6091/6091 [==============================] - 1s 142us/sample - loss: 0.4139 - val_loss: 0.4786\n",
      "Epoch 86/100\n",
      "6091/6091 [==============================] - 1s 143us/sample - loss: 0.4125 - val_loss: 0.4780\n",
      "Epoch 87/100\n",
      "6091/6091 [==============================] - 1s 144us/sample - loss: 0.4113 - val_loss: 0.4773\n",
      "Epoch 88/100\n",
      "6091/6091 [==============================] - 1s 143us/sample - loss: 0.4100 - val_loss: 0.4767\n",
      "Epoch 89/100\n",
      "6091/6091 [==============================] - 1s 142us/sample - loss: 0.4087 - val_loss: 0.4760\n",
      "Epoch 90/100\n",
      "6091/6091 [==============================] - 1s 145us/sample - loss: 0.4075 - val_loss: 0.4754\n",
      "Epoch 91/100\n",
      "6091/6091 [==============================] - 1s 145us/sample - loss: 0.4062 - val_loss: 0.4748\n",
      "Epoch 92/100\n",
      "6091/6091 [==============================] - 1s 144us/sample - loss: 0.4050 - val_loss: 0.4742\n",
      "Epoch 93/100\n",
      "6091/6091 [==============================] - 1s 143us/sample - loss: 0.4038 - val_loss: 0.4736\n",
      "Epoch 94/100\n",
      "6091/6091 [==============================] - 1s 143us/sample - loss: 0.4026 - val_loss: 0.4730\n",
      "Epoch 95/100\n",
      "6091/6091 [==============================] - 1s 144us/sample - loss: 0.4015 - val_loss: 0.4724\n",
      "Epoch 96/100\n",
      "6091/6091 [==============================] - 1s 143us/sample - loss: 0.4003 - val_loss: 0.4718\n",
      "Epoch 97/100\n",
      "6091/6091 [==============================] - 1s 143us/sample - loss: 0.3991 - val_loss: 0.4712\n",
      "Epoch 98/100\n",
      "6091/6091 [==============================] - 1s 145us/sample - loss: 0.3980 - val_loss: 0.4706\n",
      "Epoch 99/100\n",
      "6091/6091 [==============================] - 1s 143us/sample - loss: 0.3969 - val_loss: 0.4701\n",
      "Epoch 100/100\n",
      "6091/6091 [==============================] - 1s 145us/sample - loss: 0.3958 - val_loss: 0.4696\n",
      "Predicting...\n",
      "(Took 88.620 sec)\n",
      "Train on 6091 samples, validate on 1522 samples\n",
      "Epoch 1/100\n",
      "6091/6091 [==============================] - 1s 196us/sample - loss: 0.6853 - val_loss: 0.6805\n",
      "Epoch 2/100\n",
      "6091/6091 [==============================] - 1s 150us/sample - loss: 0.6726 - val_loss: 0.6724\n",
      "Epoch 3/100\n",
      "6091/6091 [==============================] - 1s 143us/sample - loss: 0.6632 - val_loss: 0.6656\n",
      "Epoch 4/100\n",
      "6091/6091 [==============================] - 1s 147us/sample - loss: 0.6551 - val_loss: 0.6595\n",
      "Epoch 5/100\n",
      "6091/6091 [==============================] - 1s 144us/sample - loss: 0.6477 - val_loss: 0.6535\n",
      "Epoch 6/100\n",
      "6091/6091 [==============================] - 1s 142us/sample - loss: 0.6406 - val_loss: 0.6477\n",
      "Epoch 7/100\n",
      "6091/6091 [==============================] - 1s 144us/sample - loss: 0.6338 - val_loss: 0.6422\n",
      "Epoch 8/100\n",
      "6091/6091 [==============================] - 1s 144us/sample - loss: 0.6272 - val_loss: 0.6368\n",
      "Epoch 9/100\n",
      "6091/6091 [==============================] - 1s 142us/sample - loss: 0.6209 - val_loss: 0.6316\n",
      "Epoch 10/100\n",
      "6091/6091 [==============================] - 1s 143us/sample - loss: 0.6148 - val_loss: 0.6266\n",
      "Epoch 11/100\n",
      "6091/6091 [==============================] - 1s 142us/sample - loss: 0.6088 - val_loss: 0.6218\n",
      "Epoch 12/100\n",
      "6091/6091 [==============================] - 1s 142us/sample - loss: 0.6031 - val_loss: 0.6172\n",
      "Epoch 13/100\n",
      "6091/6091 [==============================] - 1s 142us/sample - loss: 0.5975 - val_loss: 0.6127\n",
      "Epoch 14/100\n",
      "6091/6091 [==============================] - 1s 144us/sample - loss: 0.5922 - val_loss: 0.6083\n",
      "Epoch 15/100\n",
      "6091/6091 [==============================] - 1s 141us/sample - loss: 0.5870 - val_loss: 0.6042\n",
      "Epoch 16/100\n",
      "6091/6091 [==============================] - 1s 143us/sample - loss: 0.5819 - val_loss: 0.6002\n",
      "Epoch 17/100\n",
      "6091/6091 [==============================] - 1s 142us/sample - loss: 0.5770 - val_loss: 0.5962\n",
      "Epoch 18/100\n",
      "6091/6091 [==============================] - 1s 142us/sample - loss: 0.5723 - val_loss: 0.5925\n",
      "Epoch 19/100\n",
      "6091/6091 [==============================] - 1s 145us/sample - loss: 0.5677 - val_loss: 0.5890\n",
      "Epoch 20/100\n",
      "6091/6091 [==============================] - 1s 143us/sample - loss: 0.5632 - val_loss: 0.5854\n",
      "Epoch 21/100\n",
      "6091/6091 [==============================] - 1s 144us/sample - loss: 0.5588 - val_loss: 0.5821\n",
      "Epoch 22/100\n",
      "6091/6091 [==============================] - 1s 143us/sample - loss: 0.5546 - val_loss: 0.5788\n",
      "Epoch 23/100\n",
      "6091/6091 [==============================] - 1s 154us/sample - loss: 0.5505 - val_loss: 0.5756\n",
      "Epoch 24/100\n",
      "6091/6091 [==============================] - 1s 168us/sample - loss: 0.5465 - val_loss: 0.5726\n",
      "Epoch 25/100\n",
      "6091/6091 [==============================] - 1s 142us/sample - loss: 0.5426 - val_loss: 0.5696\n",
      "Epoch 26/100\n",
      "6091/6091 [==============================] - 1s 140us/sample - loss: 0.5389 - val_loss: 0.5668\n",
      "Epoch 27/100\n",
      "6091/6091 [==============================] - 1s 140us/sample - loss: 0.5352 - val_loss: 0.5640\n",
      "Epoch 28/100\n",
      "6091/6091 [==============================] - 1s 147us/sample - loss: 0.5316 - val_loss: 0.5613\n",
      "Epoch 29/100\n",
      "6091/6091 [==============================] - 1s 148us/sample - loss: 0.5281 - val_loss: 0.5587\n",
      "Epoch 30/100\n",
      "6091/6091 [==============================] - 1s 145us/sample - loss: 0.5247 - val_loss: 0.5561\n",
      "Epoch 31/100\n",
      "6091/6091 [==============================] - 1s 148us/sample - loss: 0.5214 - val_loss: 0.5537\n",
      "Epoch 32/100\n",
      "6091/6091 [==============================] - 1s 142us/sample - loss: 0.5181 - val_loss: 0.5513\n",
      "Epoch 33/100\n",
      "6091/6091 [==============================] - 1s 144us/sample - loss: 0.5150 - val_loss: 0.5490\n",
      "Epoch 34/100\n",
      "6091/6091 [==============================] - 1s 144us/sample - loss: 0.5119 - val_loss: 0.5468\n",
      "Epoch 35/100\n",
      "6091/6091 [==============================] - 1s 146us/sample - loss: 0.5089 - val_loss: 0.5447\n",
      "Epoch 36/100\n",
      "6091/6091 [==============================] - 1s 146us/sample - loss: 0.5059 - val_loss: 0.5425\n",
      "Epoch 37/100\n",
      "6091/6091 [==============================] - 1s 146us/sample - loss: 0.5031 - val_loss: 0.5405\n",
      "Epoch 38/100\n",
      "6091/6091 [==============================] - 1s 144us/sample - loss: 0.5003 - val_loss: 0.5385\n",
      "Epoch 39/100\n",
      "6091/6091 [==============================] - 1s 144us/sample - loss: 0.4975 - val_loss: 0.5366\n",
      "Epoch 40/100\n",
      "6091/6091 [==============================] - 1s 156us/sample - loss: 0.4948 - val_loss: 0.5347\n",
      "Epoch 41/100\n",
      "6091/6091 [==============================] - 1s 153us/sample - loss: 0.4922 - val_loss: 0.5329\n",
      "Epoch 42/100\n",
      "6091/6091 [==============================] - 1s 150us/sample - loss: 0.4896 - val_loss: 0.5311\n",
      "Epoch 43/100\n",
      "6091/6091 [==============================] - 1s 153us/sample - loss: 0.4871 - val_loss: 0.5294\n",
      "Epoch 44/100\n",
      "6091/6091 [==============================] - 1s 146us/sample - loss: 0.4846 - val_loss: 0.5277\n",
      "Epoch 45/100\n",
      "6091/6091 [==============================] - 1s 144us/sample - loss: 0.4822 - val_loss: 0.5262\n",
      "Epoch 46/100\n",
      "6091/6091 [==============================] - 1s 145us/sample - loss: 0.4799 - val_loss: 0.5246\n",
      "Epoch 47/100\n",
      "6091/6091 [==============================] - 1s 144us/sample - loss: 0.4776 - val_loss: 0.5230\n",
      "Epoch 48/100\n",
      "6091/6091 [==============================] - 1s 144us/sample - loss: 0.4752 - val_loss: 0.5215\n",
      "Epoch 49/100\n",
      "6091/6091 [==============================] - 1s 145us/sample - loss: 0.4730 - val_loss: 0.5200\n",
      "Epoch 50/100\n",
      "6091/6091 [==============================] - 1s 144us/sample - loss: 0.4708 - val_loss: 0.5186\n",
      "Epoch 51/100\n",
      "6091/6091 [==============================] - 1s 144us/sample - loss: 0.4687 - val_loss: 0.5172\n",
      "Epoch 52/100\n",
      "6091/6091 [==============================] - 1s 143us/sample - loss: 0.4666 - val_loss: 0.5159\n",
      "Epoch 53/100\n",
      "6091/6091 [==============================] - 1s 147us/sample - loss: 0.4645 - val_loss: 0.5146\n",
      "Epoch 54/100\n",
      "6091/6091 [==============================] - 1s 150us/sample - loss: 0.4624 - val_loss: 0.5133\n",
      "Epoch 55/100\n",
      "6091/6091 [==============================] - 1s 146us/sample - loss: 0.4604 - val_loss: 0.5120\n",
      "Epoch 56/100\n",
      "6091/6091 [==============================] - 1s 145us/sample - loss: 0.4585 - val_loss: 0.5108\n",
      "Epoch 57/100\n",
      "6091/6091 [==============================] - 1s 150us/sample - loss: 0.4565 - val_loss: 0.5096\n",
      "Epoch 58/100\n",
      "6091/6091 [==============================] - 1s 146us/sample - loss: 0.4546 - val_loss: 0.5085\n",
      "Epoch 59/100\n",
      "6091/6091 [==============================] - 1s 145us/sample - loss: 0.4528 - val_loss: 0.5073\n",
      "Epoch 60/100\n",
      "6091/6091 [==============================] - 1s 144us/sample - loss: 0.4509 - val_loss: 0.5062\n",
      "Epoch 61/100\n",
      "6091/6091 [==============================] - 1s 146us/sample - loss: 0.4491 - val_loss: 0.5051\n",
      "Epoch 62/100\n",
      "6091/6091 [==============================] - 1s 146us/sample - loss: 0.4473 - val_loss: 0.5041\n",
      "Epoch 63/100\n",
      "6091/6091 [==============================] - 1s 143us/sample - loss: 0.4455 - val_loss: 0.5030\n",
      "Epoch 64/100\n",
      "6091/6091 [==============================] - 1s 145us/sample - loss: 0.4438 - val_loss: 0.5020\n",
      "Epoch 65/100\n",
      "6091/6091 [==============================] - 1s 146us/sample - loss: 0.4421 - val_loss: 0.5010\n",
      "Epoch 66/100\n",
      "6091/6091 [==============================] - 1s 146us/sample - loss: 0.4404 - val_loss: 0.5001\n",
      "Epoch 67/100\n",
      "6091/6091 [==============================] - 1s 147us/sample - loss: 0.4388 - val_loss: 0.4991\n",
      "Epoch 68/100\n",
      "6091/6091 [==============================] - 1s 146us/sample - loss: 0.4371 - val_loss: 0.4982\n",
      "Epoch 69/100\n",
      "6091/6091 [==============================] - 1s 147us/sample - loss: 0.4355 - val_loss: 0.4973\n",
      "Epoch 70/100\n",
      "6091/6091 [==============================] - 1s 146us/sample - loss: 0.4339 - val_loss: 0.4964\n",
      "Epoch 71/100\n",
      "6091/6091 [==============================] - 1s 145us/sample - loss: 0.4324 - val_loss: 0.4955\n",
      "Epoch 72/100\n",
      "6091/6091 [==============================] - 1s 145us/sample - loss: 0.4308 - val_loss: 0.4947\n",
      "Epoch 73/100\n",
      "6091/6091 [==============================] - 1s 146us/sample - loss: 0.4293 - val_loss: 0.4938\n",
      "Epoch 74/100\n",
      "6091/6091 [==============================] - 1s 146us/sample - loss: 0.4278 - val_loss: 0.4931\n",
      "Epoch 75/100\n",
      "6091/6091 [==============================] - 1s 142us/sample - loss: 0.4263 - val_loss: 0.4923\n",
      "Epoch 76/100\n",
      "6091/6091 [==============================] - 1s 143us/sample - loss: 0.4248 - val_loss: 0.4915\n",
      "Epoch 77/100\n",
      "6091/6091 [==============================] - 1s 149us/sample - loss: 0.4234 - val_loss: 0.4907\n",
      "Epoch 78/100\n",
      "6091/6091 [==============================] - 1s 145us/sample - loss: 0.4220 - val_loss: 0.4900\n",
      "Epoch 79/100\n",
      "6091/6091 [==============================] - 1s 146us/sample - loss: 0.4205 - val_loss: 0.4893\n",
      "Epoch 80/100\n",
      "6091/6091 [==============================] - 1s 147us/sample - loss: 0.4192 - val_loss: 0.4886\n",
      "Epoch 81/100\n",
      "6091/6091 [==============================] - 1s 146us/sample - loss: 0.4178 - val_loss: 0.4879\n",
      "Epoch 82/100\n",
      "6091/6091 [==============================] - 1s 146us/sample - loss: 0.4164 - val_loss: 0.4872\n",
      "Epoch 83/100\n",
      "6091/6091 [==============================] - 1s 144us/sample - loss: 0.4151 - val_loss: 0.4865\n",
      "Epoch 84/100\n",
      "6091/6091 [==============================] - 1s 145us/sample - loss: 0.4137 - val_loss: 0.4859\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85/100\n",
      "6091/6091 [==============================] - 1s 143us/sample - loss: 0.4124 - val_loss: 0.4852\n",
      "Epoch 86/100\n",
      "6091/6091 [==============================] - 1s 143us/sample - loss: 0.4111 - val_loss: 0.4846\n",
      "Epoch 87/100\n",
      "6091/6091 [==============================] - 1s 143us/sample - loss: 0.4099 - val_loss: 0.4840\n",
      "Epoch 88/100\n",
      "6091/6091 [==============================] - 1s 144us/sample - loss: 0.4086 - val_loss: 0.4834\n",
      "Epoch 89/100\n",
      "6091/6091 [==============================] - 1s 146us/sample - loss: 0.4073 - val_loss: 0.4828\n",
      "Epoch 90/100\n",
      "6091/6091 [==============================] - 1s 145us/sample - loss: 0.4061 - val_loss: 0.4822\n",
      "Epoch 91/100\n",
      "6091/6091 [==============================] - 1s 146us/sample - loss: 0.4049 - val_loss: 0.4816\n",
      "Epoch 92/100\n",
      "6091/6091 [==============================] - 1s 143us/sample - loss: 0.4037 - val_loss: 0.4811\n",
      "Epoch 93/100\n",
      "6091/6091 [==============================] - 1s 146us/sample - loss: 0.4025 - val_loss: 0.4806\n",
      "Epoch 94/100\n",
      "6091/6091 [==============================] - 1s 143us/sample - loss: 0.4013 - val_loss: 0.4800\n",
      "Epoch 95/100\n",
      "6091/6091 [==============================] - 1s 143us/sample - loss: 0.4001 - val_loss: 0.4795\n",
      "Epoch 96/100\n",
      "6091/6091 [==============================] - 1s 143us/sample - loss: 0.3989 - val_loss: 0.4790\n",
      "Epoch 97/100\n",
      "6091/6091 [==============================] - 1s 143us/sample - loss: 0.3978 - val_loss: 0.4785\n",
      "Epoch 98/100\n",
      "6091/6091 [==============================] - 1s 145us/sample - loss: 0.3967 - val_loss: 0.4780\n",
      "Epoch 99/100\n",
      "6091/6091 [==============================] - 1s 143us/sample - loss: 0.3955 - val_loss: 0.4775\n",
      "Epoch 100/100\n",
      "6091/6091 [==============================] - 1s 142us/sample - loss: 0.3944 - val_loss: 0.4770\n",
      "Predicting...\n",
      "(Took 89.677 sec)\n",
      "Combined confusion matrix:\n",
      "[[3854.  488.]\n",
      " [1118. 2153.]]\n",
      "(Overall, took 440.454 sec)\n",
      "Accuracy: 78.90% +/- 0.85%\n",
      "Precision for positive class: 77.51% +/- 1.60%\n",
      "Precision for negative class: 81.53% +/- 1.28%\n",
      "Recall for positive class: 88.76% +/- 0.93%\n",
      "Recall for negative class: 65.83% +/- 0.87%\n",
      "F for positive class: 82.75% +/- 1.06%\n",
      "F for negative class: 72.84% +/- 0.31%\n",
      "Mean F score: 77.79% +/- 0.64%\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/100\n",
      "6090/6090 [==============================] - 1s 188us/sample - loss: 0.6833 - val_loss: 0.6725\n",
      "Epoch 2/100\n",
      "6090/6090 [==============================] - 1s 143us/sample - loss: 0.6646 - val_loss: 0.6578\n",
      "Epoch 3/100\n",
      "6090/6090 [==============================] - 1s 148us/sample - loss: 0.6492 - val_loss: 0.6458\n",
      "Epoch 4/100\n",
      "6090/6090 [==============================] - 1s 145us/sample - loss: 0.6353 - val_loss: 0.6348\n",
      "Epoch 5/100\n",
      "6090/6090 [==============================] - 1s 146us/sample - loss: 0.6223 - val_loss: 0.6246\n",
      "Epoch 6/100\n",
      "6090/6090 [==============================] - 1s 144us/sample - loss: 0.6101 - val_loss: 0.6152\n",
      "Epoch 7/100\n",
      "6090/6090 [==============================] - 1s 146us/sample - loss: 0.5985 - val_loss: 0.6064\n",
      "Epoch 8/100\n",
      "6090/6090 [==============================] - 1s 147us/sample - loss: 0.5875 - val_loss: 0.5980\n",
      "Epoch 9/100\n",
      "6090/6090 [==============================] - 1s 148us/sample - loss: 0.5770 - val_loss: 0.5901\n",
      "Epoch 10/100\n",
      "6090/6090 [==============================] - 1s 144us/sample - loss: 0.5670 - val_loss: 0.5827\n",
      "Epoch 11/100\n",
      "6090/6090 [==============================] - 1s 146us/sample - loss: 0.5575 - val_loss: 0.5757\n",
      "Epoch 12/100\n",
      "6090/6090 [==============================] - 1s 154us/sample - loss: 0.5484 - val_loss: 0.5690\n",
      "Epoch 13/100\n",
      "6090/6090 [==============================] - 1s 151us/sample - loss: 0.5397 - val_loss: 0.5627\n",
      "Epoch 14/100\n",
      "6090/6090 [==============================] - 1s 150us/sample - loss: 0.5314 - val_loss: 0.5568\n",
      "Epoch 15/100\n",
      "6090/6090 [==============================] - 1s 155us/sample - loss: 0.5235 - val_loss: 0.5512\n",
      "Epoch 16/100\n",
      "6090/6090 [==============================] - 1s 149us/sample - loss: 0.5159 - val_loss: 0.5458\n",
      "Epoch 17/100\n",
      "6090/6090 [==============================] - 1s 153us/sample - loss: 0.5087 - val_loss: 0.5409\n",
      "Epoch 18/100\n",
      "6090/6090 [==============================] - 1s 151us/sample - loss: 0.5017 - val_loss: 0.5360\n",
      "Epoch 19/100\n",
      "6090/6090 [==============================] - 1s 151us/sample - loss: 0.4950 - val_loss: 0.5315\n",
      "Epoch 20/100\n",
      "6090/6090 [==============================] - 1s 150us/sample - loss: 0.4886 - val_loss: 0.5272\n",
      "Epoch 21/100\n",
      "6090/6090 [==============================] - 1s 147us/sample - loss: 0.4824 - val_loss: 0.5231\n",
      "Epoch 22/100\n",
      "6090/6090 [==============================] - 1s 147us/sample - loss: 0.4765 - val_loss: 0.5193\n",
      "Epoch 23/100\n",
      "6090/6090 [==============================] - 1s 149us/sample - loss: 0.4707 - val_loss: 0.5156\n",
      "Epoch 24/100\n",
      "6090/6090 [==============================] - 1s 146us/sample - loss: 0.4653 - val_loss: 0.5121\n",
      "Epoch 25/100\n",
      "6090/6090 [==============================] - 1s 147us/sample - loss: 0.4600 - val_loss: 0.5088\n",
      "Epoch 26/100\n",
      "6090/6090 [==============================] - 1s 145us/sample - loss: 0.4548 - val_loss: 0.5057\n",
      "Epoch 27/100\n",
      "6090/6090 [==============================] - 1s 145us/sample - loss: 0.4499 - val_loss: 0.5027\n",
      "Epoch 28/100\n",
      "6090/6090 [==============================] - 1s 145us/sample - loss: 0.4451 - val_loss: 0.4999\n",
      "Epoch 29/100\n",
      "6090/6090 [==============================] - 1s 145us/sample - loss: 0.4405 - val_loss: 0.4972\n",
      "Epoch 30/100\n",
      "6090/6090 [==============================] - 1s 146us/sample - loss: 0.4360 - val_loss: 0.4946\n",
      "Epoch 31/100\n",
      "6090/6090 [==============================] - 1s 145us/sample - loss: 0.4317 - val_loss: 0.4922\n",
      "Epoch 32/100\n",
      "6090/6090 [==============================] - 1s 146us/sample - loss: 0.4275 - val_loss: 0.4899\n",
      "Epoch 33/100\n",
      "6090/6090 [==============================] - 1s 145us/sample - loss: 0.4234 - val_loss: 0.4877\n",
      "Epoch 34/100\n",
      "6090/6090 [==============================] - 1s 148us/sample - loss: 0.4195 - val_loss: 0.4857\n",
      "Epoch 35/100\n",
      "6090/6090 [==============================] - 1s 147us/sample - loss: 0.4157 - val_loss: 0.4837\n",
      "Epoch 36/100\n",
      "6090/6090 [==============================] - 1s 146us/sample - loss: 0.4119 - val_loss: 0.4818\n",
      "Epoch 37/100\n",
      "6090/6090 [==============================] - 1s 146us/sample - loss: 0.4083 - val_loss: 0.4801\n",
      "Epoch 38/100\n",
      "6090/6090 [==============================] - 1s 147us/sample - loss: 0.4048 - val_loss: 0.4784\n",
      "Epoch 39/100\n",
      "6090/6090 [==============================] - 1s 146us/sample - loss: 0.4014 - val_loss: 0.4768\n",
      "Epoch 40/100\n",
      "6090/6090 [==============================] - 1s 146us/sample - loss: 0.3981 - val_loss: 0.4753\n",
      "Epoch 41/100\n",
      "6090/6090 [==============================] - 1s 147us/sample - loss: 0.3949 - val_loss: 0.4738\n",
      "Epoch 42/100\n",
      "6090/6090 [==============================] - 1s 145us/sample - loss: 0.3917 - val_loss: 0.4724\n",
      "Epoch 43/100\n",
      "6090/6090 [==============================] - 1s 149us/sample - loss: 0.3887 - val_loss: 0.4711\n",
      "Epoch 44/100\n",
      "6090/6090 [==============================] - 1s 145us/sample - loss: 0.3857 - val_loss: 0.4699\n",
      "Epoch 45/100\n",
      "6090/6090 [==============================] - 1s 146us/sample - loss: 0.3828 - val_loss: 0.4687\n",
      "Epoch 46/100\n",
      "6090/6090 [==============================] - 1s 146us/sample - loss: 0.3799 - val_loss: 0.4677\n",
      "Epoch 47/100\n",
      "6090/6090 [==============================] - 1s 147us/sample - loss: 0.3772 - val_loss: 0.4666\n",
      "Epoch 48/100\n",
      "6090/6090 [==============================] - 1s 144us/sample - loss: 0.3744 - val_loss: 0.4655\n",
      "Epoch 49/100\n",
      "6090/6090 [==============================] - 1s 145us/sample - loss: 0.3718 - val_loss: 0.4646\n",
      "Epoch 50/100\n",
      "6090/6090 [==============================] - 1s 144us/sample - loss: 0.3692 - val_loss: 0.4637\n",
      "Epoch 51/100\n",
      "6090/6090 [==============================] - 1s 144us/sample - loss: 0.3666 - val_loss: 0.4628\n",
      "Epoch 52/100\n",
      "6090/6090 [==============================] - 1s 148us/sample - loss: 0.3642 - val_loss: 0.4621\n",
      "Epoch 53/100\n",
      "6090/6090 [==============================] - 1s 146us/sample - loss: 0.3617 - val_loss: 0.4613\n",
      "Epoch 54/100\n",
      "6090/6090 [==============================] - 1s 147us/sample - loss: 0.3594 - val_loss: 0.4606\n",
      "Epoch 55/100\n",
      "6090/6090 [==============================] - 1s 148us/sample - loss: 0.3570 - val_loss: 0.4599\n",
      "Epoch 56/100\n",
      "6090/6090 [==============================] - 1s 145us/sample - loss: 0.3547 - val_loss: 0.4593\n",
      "Epoch 57/100\n",
      "6090/6090 [==============================] - 1s 146us/sample - loss: 0.3525 - val_loss: 0.4587\n",
      "Epoch 58/100\n",
      "6090/6090 [==============================] - 1s 145us/sample - loss: 0.3503 - val_loss: 0.4581\n",
      "Epoch 59/100\n",
      "6090/6090 [==============================] - 1s 144us/sample - loss: 0.3482 - val_loss: 0.4576\n",
      "Epoch 60/100\n",
      "6090/6090 [==============================] - 1s 143us/sample - loss: 0.3461 - val_loss: 0.4571\n",
      "Epoch 61/100\n",
      "6090/6090 [==============================] - 1s 143us/sample - loss: 0.3440 - val_loss: 0.4567\n",
      "Epoch 62/100\n",
      "6090/6090 [==============================] - 1s 143us/sample - loss: 0.3420 - val_loss: 0.4563\n",
      "Epoch 63/100\n",
      "6090/6090 [==============================] - 1s 145us/sample - loss: 0.3400 - val_loss: 0.4558\n",
      "Epoch 64/100\n",
      "6090/6090 [==============================] - 1s 145us/sample - loss: 0.3380 - val_loss: 0.4555\n",
      "Epoch 65/100\n",
      "6090/6090 [==============================] - 1s 143us/sample - loss: 0.3361 - val_loss: 0.4551\n",
      "Epoch 66/100\n",
      "6090/6090 [==============================] - 1s 144us/sample - loss: 0.3342 - val_loss: 0.4549\n",
      "Epoch 67/100\n",
      "6090/6090 [==============================] - 1s 143us/sample - loss: 0.3323 - val_loss: 0.4545\n",
      "Epoch 68/100\n",
      "6090/6090 [==============================] - 1s 142us/sample - loss: 0.3305 - val_loss: 0.4543\n",
      "Epoch 69/100\n",
      "6090/6090 [==============================] - 1s 144us/sample - loss: 0.3287 - val_loss: 0.4540\n",
      "Epoch 70/100\n",
      "6090/6090 [==============================] - 1s 147us/sample - loss: 0.3269 - val_loss: 0.4538\n",
      "Epoch 71/100\n",
      "6090/6090 [==============================] - 1s 146us/sample - loss: 0.3252 - val_loss: 0.4536\n",
      "Epoch 72/100\n",
      "6090/6090 [==============================] - 1s 145us/sample - loss: 0.3235 - val_loss: 0.4534\n",
      "Epoch 73/100\n",
      "6090/6090 [==============================] - 1s 145us/sample - loss: 0.3218 - val_loss: 0.4533\n",
      "Epoch 74/100\n",
      "6090/6090 [==============================] - 1s 145us/sample - loss: 0.3201 - val_loss: 0.4531\n",
      "Epoch 75/100\n",
      "6090/6090 [==============================] - 1s 144us/sample - loss: 0.3185 - val_loss: 0.4531\n",
      "Epoch 76/100\n",
      "6090/6090 [==============================] - 1s 145us/sample - loss: 0.3169 - val_loss: 0.4529\n",
      "Epoch 77/100\n",
      "6090/6090 [==============================] - 1s 146us/sample - loss: 0.3153 - val_loss: 0.4529\n",
      "Epoch 78/100\n",
      "6090/6090 [==============================] - 1s 146us/sample - loss: 0.3137 - val_loss: 0.4528\n",
      "Epoch 79/100\n",
      "6090/6090 [==============================] - 1s 146us/sample - loss: 0.3122 - val_loss: 0.4528\n",
      "Epoch 80/100\n",
      "6090/6090 [==============================] - 1s 147us/sample - loss: 0.3107 - val_loss: 0.4527\n",
      "Epoch 81/100\n",
      "6090/6090 [==============================] - 1s 146us/sample - loss: 0.3092 - val_loss: 0.4527\n",
      "Predicting...\n",
      "(Took 73.346 sec)\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/100\n",
      "6090/6090 [==============================] - 1s 192us/sample - loss: 0.6842 - val_loss: 0.6728\n",
      "Epoch 2/100\n",
      "6090/6090 [==============================] - 1s 144us/sample - loss: 0.6655 - val_loss: 0.6570\n",
      "Epoch 3/100\n",
      "6090/6090 [==============================] - 1s 155us/sample - loss: 0.6500 - val_loss: 0.6445\n",
      "Epoch 4/100\n",
      "6090/6090 [==============================] - 1s 150us/sample - loss: 0.6360 - val_loss: 0.6332\n",
      "Epoch 5/100\n",
      "6090/6090 [==============================] - 1s 154us/sample - loss: 0.6230 - val_loss: 0.6230\n",
      "Epoch 6/100\n",
      "6090/6090 [==============================] - 1s 151us/sample - loss: 0.6107 - val_loss: 0.6135\n",
      "Epoch 7/100\n",
      "6090/6090 [==============================] - 1s 149us/sample - loss: 0.5991 - val_loss: 0.6046\n",
      "Epoch 8/100\n",
      "6090/6090 [==============================] - 1s 146us/sample - loss: 0.5881 - val_loss: 0.5964\n",
      "Epoch 9/100\n",
      "6090/6090 [==============================] - 1s 142us/sample - loss: 0.5776 - val_loss: 0.5886\n",
      "Epoch 10/100\n",
      "6090/6090 [==============================] - 1s 141us/sample - loss: 0.5676 - val_loss: 0.5813\n",
      "Epoch 11/100\n",
      "6090/6090 [==============================] - 1s 147us/sample - loss: 0.5580 - val_loss: 0.5744\n",
      "Epoch 12/100\n",
      "6090/6090 [==============================] - 1s 144us/sample - loss: 0.5489 - val_loss: 0.5679\n",
      "Epoch 13/100\n",
      "6090/6090 [==============================] - 1s 146us/sample - loss: 0.5402 - val_loss: 0.5617\n",
      "Epoch 14/100\n",
      "6090/6090 [==============================] - 1s 147us/sample - loss: 0.5319 - val_loss: 0.5558\n",
      "Epoch 15/100\n",
      "6090/6090 [==============================] - 1s 145us/sample - loss: 0.5240 - val_loss: 0.5504\n",
      "Epoch 16/100\n",
      "6090/6090 [==============================] - 1s 144us/sample - loss: 0.5163 - val_loss: 0.5450\n",
      "Epoch 17/100\n",
      "6090/6090 [==============================] - 1s 143us/sample - loss: 0.5090 - val_loss: 0.5400\n",
      "Epoch 18/100\n",
      "6090/6090 [==============================] - 1s 143us/sample - loss: 0.5020 - val_loss: 0.5354\n",
      "Epoch 19/100\n",
      "6090/6090 [==============================] - 1s 146us/sample - loss: 0.4953 - val_loss: 0.5309\n",
      "Epoch 20/100\n",
      "6090/6090 [==============================] - 1s 148us/sample - loss: 0.4889 - val_loss: 0.5267\n",
      "Epoch 21/100\n",
      "6090/6090 [==============================] - 1s 149us/sample - loss: 0.4827 - val_loss: 0.5228\n",
      "Epoch 22/100\n",
      "6090/6090 [==============================] - 1s 147us/sample - loss: 0.4767 - val_loss: 0.5190\n",
      "Epoch 23/100\n",
      "6090/6090 [==============================] - 1s 147us/sample - loss: 0.4710 - val_loss: 0.5154\n",
      "Epoch 24/100\n",
      "6090/6090 [==============================] - 1s 146us/sample - loss: 0.4655 - val_loss: 0.5120\n",
      "Epoch 25/100\n",
      "6090/6090 [==============================] - 1s 147us/sample - loss: 0.4602 - val_loss: 0.5087\n",
      "Epoch 26/100\n",
      "6090/6090 [==============================] - 1s 147us/sample - loss: 0.4550 - val_loss: 0.5057\n",
      "Epoch 27/100\n",
      "6090/6090 [==============================] - 1s 152us/sample - loss: 0.4501 - val_loss: 0.5028\n",
      "Epoch 28/100\n",
      "6090/6090 [==============================] - 1s 146us/sample - loss: 0.4453 - val_loss: 0.5001\n",
      "Epoch 29/100\n",
      "6090/6090 [==============================] - 1s 147us/sample - loss: 0.4406 - val_loss: 0.4974\n",
      "Epoch 30/100\n",
      "6090/6090 [==============================] - 1s 147us/sample - loss: 0.4362 - val_loss: 0.4949\n",
      "Epoch 31/100\n",
      "6090/6090 [==============================] - 1s 148us/sample - loss: 0.4319 - val_loss: 0.4925\n",
      "Epoch 32/100\n",
      "6090/6090 [==============================] - 1s 147us/sample - loss: 0.4277 - val_loss: 0.4903\n",
      "Epoch 33/100\n",
      "6090/6090 [==============================] - 1s 146us/sample - loss: 0.4235 - val_loss: 0.4882\n",
      "Epoch 34/100\n",
      "6090/6090 [==============================] - 1s 145us/sample - loss: 0.4196 - val_loss: 0.4860\n",
      "Epoch 35/100\n",
      "6090/6090 [==============================] - 1s 147us/sample - loss: 0.4158 - val_loss: 0.4841\n",
      "Epoch 36/100\n",
      "6090/6090 [==============================] - 1s 144us/sample - loss: 0.4121 - val_loss: 0.4823\n",
      "Epoch 37/100\n",
      "6090/6090 [==============================] - 1s 147us/sample - loss: 0.4085 - val_loss: 0.4806\n",
      "Epoch 38/100\n",
      "6090/6090 [==============================] - 1s 148us/sample - loss: 0.4049 - val_loss: 0.4789\n",
      "Epoch 39/100\n",
      "6090/6090 [==============================] - 1s 147us/sample - loss: 0.4015 - val_loss: 0.4774\n",
      "Epoch 40/100\n",
      "6090/6090 [==============================] - 1s 148us/sample - loss: 0.3982 - val_loss: 0.4760\n",
      "Epoch 41/100\n",
      "6090/6090 [==============================] - 1s 146us/sample - loss: 0.3950 - val_loss: 0.4746\n",
      "Epoch 42/100\n",
      "6090/6090 [==============================] - 1s 145us/sample - loss: 0.3918 - val_loss: 0.4732\n",
      "Epoch 43/100\n",
      "6090/6090 [==============================] - 1s 148us/sample - loss: 0.3888 - val_loss: 0.4719\n",
      "Epoch 44/100\n",
      "6090/6090 [==============================] - 1s 149us/sample - loss: 0.3857 - val_loss: 0.4707\n",
      "Epoch 45/100\n",
      "6090/6090 [==============================] - 1s 160us/sample - loss: 0.3828 - val_loss: 0.4695\n",
      "Epoch 46/100\n",
      "6090/6090 [==============================] - 1s 150us/sample - loss: 0.3800 - val_loss: 0.4685\n",
      "Epoch 47/100\n",
      "6090/6090 [==============================] - 1s 145us/sample - loss: 0.3772 - val_loss: 0.4674\n",
      "Epoch 48/100\n",
      "6090/6090 [==============================] - 1s 161us/sample - loss: 0.3745 - val_loss: 0.4665\n",
      "Epoch 49/100\n",
      "6090/6090 [==============================] - 1s 182us/sample - loss: 0.3718 - val_loss: 0.4656\n",
      "Epoch 50/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6090/6090 [==============================] - 1s 193us/sample - loss: 0.3692 - val_loss: 0.4647\n",
      "Epoch 51/100\n",
      "6090/6090 [==============================] - 1s 246us/sample - loss: 0.3667 - val_loss: 0.4639\n",
      "Epoch 52/100\n",
      "6090/6090 [==============================] - 1s 186us/sample - loss: 0.3642 - val_loss: 0.4631\n",
      "Epoch 53/100\n",
      "6090/6090 [==============================] - 1s 176us/sample - loss: 0.3618 - val_loss: 0.4624\n",
      "Epoch 54/100\n",
      "6090/6090 [==============================] - 1s 165us/sample - loss: 0.3594 - val_loss: 0.4617\n",
      "Epoch 55/100\n",
      "6090/6090 [==============================] - 1s 169us/sample - loss: 0.3570 - val_loss: 0.4610\n",
      "Epoch 56/100\n",
      "6090/6090 [==============================] - 1s 163us/sample - loss: 0.3548 - val_loss: 0.4604\n",
      "Epoch 57/100\n",
      "6090/6090 [==============================] - 1s 161us/sample - loss: 0.3525 - val_loss: 0.4599\n",
      "Epoch 58/100\n",
      "6090/6090 [==============================] - 1s 166us/sample - loss: 0.3503 - val_loss: 0.4594\n",
      "Epoch 59/100\n",
      "6090/6090 [==============================] - 1s 173us/sample - loss: 0.3482 - val_loss: 0.4589\n",
      "Epoch 60/100\n",
      "6090/6090 [==============================] - 1s 141us/sample - loss: 0.3461 - val_loss: 0.4585\n",
      "Epoch 61/100\n",
      "6090/6090 [==============================] - 1s 145us/sample - loss: 0.3440 - val_loss: 0.4580\n",
      "Epoch 62/100\n",
      "6090/6090 [==============================] - 1s 153us/sample - loss: 0.3420 - val_loss: 0.4576\n",
      "Epoch 63/100\n",
      "6090/6090 [==============================] - 1s 148us/sample - loss: 0.3400 - val_loss: 0.4572\n",
      "Epoch 64/100\n",
      "6090/6090 [==============================] - 1s 180us/sample - loss: 0.3380 - val_loss: 0.4568\n",
      "Epoch 65/100\n",
      "6090/6090 [==============================] - 1s 141us/sample - loss: 0.3361 - val_loss: 0.4565\n",
      "Epoch 66/100\n",
      "6090/6090 [==============================] - 1s 154us/sample - loss: 0.3342 - val_loss: 0.4563\n",
      "Epoch 67/100\n",
      "6090/6090 [==============================] - 1s 136us/sample - loss: 0.3323 - val_loss: 0.4559\n",
      "Epoch 68/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.3305 - val_loss: 0.4557\n",
      "Epoch 69/100\n",
      "6090/6090 [==============================] - 1s 141us/sample - loss: 0.3287 - val_loss: 0.4555\n",
      "Epoch 70/100\n",
      "6090/6090 [==============================] - 1s 155us/sample - loss: 0.3269 - val_loss: 0.4553\n",
      "Epoch 71/100\n",
      "6090/6090 [==============================] - 1s 141us/sample - loss: 0.3251 - val_loss: 0.4552\n",
      "Epoch 72/100\n",
      "6090/6090 [==============================] - 1s 144us/sample - loss: 0.3235 - val_loss: 0.4550\n",
      "Epoch 73/100\n",
      "6090/6090 [==============================] - 1s 152us/sample - loss: 0.3218 - val_loss: 0.4549\n",
      "Epoch 74/100\n",
      "6090/6090 [==============================] - 1s 159us/sample - loss: 0.3201 - val_loss: 0.4548\n",
      "Epoch 75/100\n",
      "6090/6090 [==============================] - 1s 151us/sample - loss: 0.3185 - val_loss: 0.4547\n",
      "Epoch 76/100\n",
      "6090/6090 [==============================] - 1s 145us/sample - loss: 0.3168 - val_loss: 0.4546\n",
      "Epoch 77/100\n",
      "6090/6090 [==============================] - 1s 146us/sample - loss: 0.3153 - val_loss: 0.4545\n",
      "Epoch 78/100\n",
      "6090/6090 [==============================] - 1s 150us/sample - loss: 0.3137 - val_loss: 0.4545\n",
      "Predicting...\n",
      "(Took 73.555 sec)\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/100\n",
      "6090/6090 [==============================] - 1s 199us/sample - loss: 0.6830 - val_loss: 0.6759\n",
      "Epoch 2/100\n",
      "6090/6090 [==============================] - 1s 155us/sample - loss: 0.6635 - val_loss: 0.6629\n",
      "Epoch 3/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.6478 - val_loss: 0.6518\n",
      "Epoch 4/100\n",
      "6090/6090 [==============================] - 1s 150us/sample - loss: 0.6338 - val_loss: 0.6417\n",
      "Epoch 5/100\n",
      "6090/6090 [==============================] - 1s 149us/sample - loss: 0.6209 - val_loss: 0.6321\n",
      "Epoch 6/100\n",
      "6090/6090 [==============================] - 1s 146us/sample - loss: 0.6087 - val_loss: 0.6231\n",
      "Epoch 7/100\n",
      "6090/6090 [==============================] - 1s 153us/sample - loss: 0.5972 - val_loss: 0.6145\n",
      "Epoch 8/100\n",
      "6090/6090 [==============================] - 1s 162us/sample - loss: 0.5862 - val_loss: 0.6064\n",
      "Epoch 9/100\n",
      "6090/6090 [==============================] - 1s 158us/sample - loss: 0.5757 - val_loss: 0.5987\n",
      "Epoch 10/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.5657 - val_loss: 0.5915\n",
      "Epoch 11/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.5562 - val_loss: 0.5846\n",
      "Epoch 12/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.5471 - val_loss: 0.5781\n",
      "Epoch 13/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.5385 - val_loss: 0.5720\n",
      "Epoch 14/100\n",
      "6090/6090 [==============================] - 1s 150us/sample - loss: 0.5301 - val_loss: 0.5662\n",
      "Epoch 15/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.5222 - val_loss: 0.5606\n",
      "Epoch 16/100\n",
      "6090/6090 [==============================] - 1s 157us/sample - loss: 0.5146 - val_loss: 0.5555\n",
      "Epoch 17/100\n",
      "6090/6090 [==============================] - 1s 148us/sample - loss: 0.5073 - val_loss: 0.5505\n",
      "Epoch 18/100\n",
      "6090/6090 [==============================] - 1s 154us/sample - loss: 0.5003 - val_loss: 0.5458\n",
      "Epoch 19/100\n",
      "6090/6090 [==============================] - 1s 148us/sample - loss: 0.4936 - val_loss: 0.5414\n",
      "Epoch 20/100\n",
      "6090/6090 [==============================] - 1s 141us/sample - loss: 0.4872 - val_loss: 0.5373\n",
      "Epoch 21/100\n",
      "6090/6090 [==============================] - 1s 145us/sample - loss: 0.4810 - val_loss: 0.5333\n",
      "Epoch 22/100\n",
      "6090/6090 [==============================] - 1s 172us/sample - loss: 0.4751 - val_loss: 0.5295\n",
      "Epoch 23/100\n",
      "6090/6090 [==============================] - 1s 142us/sample - loss: 0.4693 - val_loss: 0.5259\n",
      "Epoch 24/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.4638 - val_loss: 0.5225\n",
      "Epoch 25/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.4585 - val_loss: 0.5193\n",
      "Epoch 26/100\n",
      "6090/6090 [==============================] - 1s 152us/sample - loss: 0.4534 - val_loss: 0.5163\n",
      "Epoch 27/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.4484 - val_loss: 0.5133\n",
      "Epoch 28/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.4436 - val_loss: 0.5105\n",
      "Epoch 29/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.4390 - val_loss: 0.5079\n",
      "Epoch 30/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.4345 - val_loss: 0.5054\n",
      "Epoch 31/100\n",
      "6090/6090 [==============================] - 1s 136us/sample - loss: 0.4302 - val_loss: 0.5029\n",
      "Epoch 32/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.4260 - val_loss: 0.5006\n",
      "Epoch 33/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.4219 - val_loss: 0.4985\n",
      "Epoch 34/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.4179 - val_loss: 0.4965\n",
      "Epoch 35/100\n",
      "6090/6090 [==============================] - 1s 136us/sample - loss: 0.4141 - val_loss: 0.4945\n",
      "Epoch 36/100\n",
      "6090/6090 [==============================] - 1s 136us/sample - loss: 0.4104 - val_loss: 0.4926\n",
      "Epoch 37/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.4067 - val_loss: 0.4908\n",
      "Epoch 38/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.4032 - val_loss: 0.4892\n",
      "Epoch 39/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.3998 - val_loss: 0.4876\n",
      "Epoch 40/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.3964 - val_loss: 0.4860\n",
      "Epoch 41/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.3932 - val_loss: 0.4845\n",
      "Epoch 42/100\n",
      "6090/6090 [==============================] - 1s 136us/sample - loss: 0.3901 - val_loss: 0.4832\n",
      "Epoch 43/100\n",
      "6090/6090 [==============================] - 1s 135us/sample - loss: 0.3870 - val_loss: 0.4819\n",
      "Epoch 44/100\n",
      "6090/6090 [==============================] - 1s 136us/sample - loss: 0.3840 - val_loss: 0.4806\n",
      "Epoch 45/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.3810 - val_loss: 0.4794\n",
      "Epoch 46/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.3782 - val_loss: 0.4783\n",
      "Epoch 47/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.3754 - val_loss: 0.4772\n",
      "Epoch 48/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.3726 - val_loss: 0.4762\n",
      "Epoch 49/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.3700 - val_loss: 0.4752\n",
      "Epoch 50/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.3674 - val_loss: 0.4743\n",
      "Epoch 51/100\n",
      "6090/6090 [==============================] - 1s 136us/sample - loss: 0.3648 - val_loss: 0.4733\n",
      "Epoch 52/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.3623 - val_loss: 0.4725\n",
      "Epoch 53/100\n",
      "6090/6090 [==============================] - 1s 135us/sample - loss: 0.3599 - val_loss: 0.4718\n",
      "Epoch 54/100\n",
      "6090/6090 [==============================] - 1s 136us/sample - loss: 0.3575 - val_loss: 0.4710\n",
      "Epoch 55/100\n",
      "6090/6090 [==============================] - 1s 136us/sample - loss: 0.3551 - val_loss: 0.4703\n",
      "Epoch 56/100\n",
      "6090/6090 [==============================] - 1s 136us/sample - loss: 0.3529 - val_loss: 0.4697\n",
      "Epoch 57/100\n",
      "6090/6090 [==============================] - 1s 136us/sample - loss: 0.3506 - val_loss: 0.4690\n",
      "Epoch 58/100\n",
      "6090/6090 [==============================] - 1s 136us/sample - loss: 0.3484 - val_loss: 0.4684\n",
      "Epoch 59/100\n",
      "6090/6090 [==============================] - 1s 141us/sample - loss: 0.3462 - val_loss: 0.4679\n",
      "Epoch 60/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.3441 - val_loss: 0.4673\n",
      "Epoch 61/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.3420 - val_loss: 0.4668\n",
      "Epoch 62/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.3400 - val_loss: 0.4663\n",
      "Epoch 63/100\n",
      "6090/6090 [==============================] - 1s 136us/sample - loss: 0.3380 - val_loss: 0.4659\n",
      "Epoch 64/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.3360 - val_loss: 0.4655\n",
      "Epoch 65/100\n",
      "6090/6090 [==============================] - 1s 136us/sample - loss: 0.3341 - val_loss: 0.4651\n",
      "Epoch 66/100\n",
      "6090/6090 [==============================] - 1s 136us/sample - loss: 0.3322 - val_loss: 0.4647\n",
      "Epoch 67/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.3303 - val_loss: 0.4643\n",
      "Epoch 68/100\n",
      "6090/6090 [==============================] - 1s 136us/sample - loss: 0.3285 - val_loss: 0.4640\n",
      "Epoch 69/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.3267 - val_loss: 0.4637\n",
      "Epoch 70/100\n",
      "6090/6090 [==============================] - 1s 150us/sample - loss: 0.3249 - val_loss: 0.4635\n",
      "Epoch 71/100\n",
      "6090/6090 [==============================] - 1s 144us/sample - loss: 0.3231 - val_loss: 0.4632\n",
      "Epoch 72/100\n",
      "6090/6090 [==============================] - 1s 144us/sample - loss: 0.3214 - val_loss: 0.4630\n",
      "Epoch 73/100\n",
      "6090/6090 [==============================] - 1s 141us/sample - loss: 0.3197 - val_loss: 0.4627\n",
      "Epoch 74/100\n",
      "6090/6090 [==============================] - 1s 152us/sample - loss: 0.3181 - val_loss: 0.4626\n",
      "Epoch 75/100\n",
      "6090/6090 [==============================] - 1s 145us/sample - loss: 0.3164 - val_loss: 0.4624\n",
      "Epoch 76/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.3148 - val_loss: 0.4623\n",
      "Epoch 77/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.3132 - val_loss: 0.4621\n",
      "Epoch 78/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.3116 - val_loss: 0.4620\n",
      "Epoch 79/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.3101 - val_loss: 0.4619\n",
      "Epoch 80/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.3085 - val_loss: 0.4618\n",
      "Epoch 81/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.3070 - val_loss: 0.4617\n",
      "Epoch 82/100\n",
      "6090/6090 [==============================] - 1s 135us/sample - loss: 0.3055 - val_loss: 0.4616\n",
      "Epoch 83/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.3041 - val_loss: 0.4616\n",
      "Epoch 84/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.3026 - val_loss: 0.4616\n",
      "Epoch 85/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.3012 - val_loss: 0.4616\n",
      "Predicting...\n",
      "(Took 74.180 sec)\n",
      "Train on 6091 samples, validate on 1522 samples\n",
      "Epoch 1/100\n",
      "6091/6091 [==============================] - 1s 184us/sample - loss: 0.6832 - val_loss: 0.6739\n",
      "Epoch 2/100\n",
      "6091/6091 [==============================] - 1s 138us/sample - loss: 0.6643 - val_loss: 0.6598\n",
      "Epoch 3/100\n",
      "6091/6091 [==============================] - 1s 137us/sample - loss: 0.6486 - val_loss: 0.6480\n",
      "Epoch 4/100\n",
      "6091/6091 [==============================] - 1s 138us/sample - loss: 0.6346 - val_loss: 0.6374\n",
      "Epoch 5/100\n",
      "6091/6091 [==============================] - 1s 139us/sample - loss: 0.6216 - val_loss: 0.6275\n",
      "Epoch 6/100\n",
      "6091/6091 [==============================] - 1s 138us/sample - loss: 0.6094 - val_loss: 0.6184\n",
      "Epoch 7/100\n",
      "6091/6091 [==============================] - 1s 138us/sample - loss: 0.5978 - val_loss: 0.6097\n",
      "Epoch 8/100\n",
      "6091/6091 [==============================] - 1s 138us/sample - loss: 0.5867 - val_loss: 0.6015\n",
      "Epoch 9/100\n",
      "6091/6091 [==============================] - 1s 138us/sample - loss: 0.5762 - val_loss: 0.5937\n",
      "Epoch 10/100\n",
      "6091/6091 [==============================] - 1s 154us/sample - loss: 0.5662 - val_loss: 0.5864\n",
      "Epoch 11/100\n",
      "6091/6091 [==============================] - 1s 154us/sample - loss: 0.5567 - val_loss: 0.5795\n",
      "Epoch 12/100\n",
      "6091/6091 [==============================] - 1s 152us/sample - loss: 0.5476 - val_loss: 0.5729\n",
      "Epoch 13/100\n",
      "6091/6091 [==============================] - 1s 155us/sample - loss: 0.5389 - val_loss: 0.5668\n",
      "Epoch 14/100\n",
      "6091/6091 [==============================] - 1s 153us/sample - loss: 0.5306 - val_loss: 0.5609\n",
      "Epoch 15/100\n",
      "6091/6091 [==============================] - 1s 153us/sample - loss: 0.5227 - val_loss: 0.5554\n",
      "Epoch 16/100\n",
      "6091/6091 [==============================] - 1s 156us/sample - loss: 0.5151 - val_loss: 0.5501\n",
      "Epoch 17/100\n",
      "6091/6091 [==============================] - 1s 153us/sample - loss: 0.5078 - val_loss: 0.5451\n",
      "Epoch 18/100\n",
      "6091/6091 [==============================] - 1s 142us/sample - loss: 0.5008 - val_loss: 0.5403\n",
      "Epoch 19/100\n",
      "6091/6091 [==============================] - 1s 139us/sample - loss: 0.4941 - val_loss: 0.5358\n",
      "Epoch 20/100\n",
      "6091/6091 [==============================] - 1s 139us/sample - loss: 0.4877 - val_loss: 0.5315\n",
      "Epoch 21/100\n",
      "6091/6091 [==============================] - 1s 140us/sample - loss: 0.4816 - val_loss: 0.5274\n",
      "Epoch 22/100\n",
      "6091/6091 [==============================] - 1s 139us/sample - loss: 0.4756 - val_loss: 0.5236\n",
      "Epoch 23/100\n",
      "6091/6091 [==============================] - 1s 140us/sample - loss: 0.4699 - val_loss: 0.5199\n",
      "Epoch 24/100\n",
      "6091/6091 [==============================] - 1s 141us/sample - loss: 0.4645 - val_loss: 0.5164\n",
      "Epoch 25/100\n",
      "6091/6091 [==============================] - 1s 137us/sample - loss: 0.4592 - val_loss: 0.5131\n",
      "Epoch 26/100\n",
      "6091/6091 [==============================] - 1s 138us/sample - loss: 0.4541 - val_loss: 0.5099\n",
      "Epoch 27/100\n",
      "6091/6091 [==============================] - 1s 137us/sample - loss: 0.4491 - val_loss: 0.5068\n",
      "Epoch 28/100\n",
      "6091/6091 [==============================] - 1s 137us/sample - loss: 0.4444 - val_loss: 0.5039\n",
      "Epoch 29/100\n",
      "6091/6091 [==============================] - 1s 138us/sample - loss: 0.4398 - val_loss: 0.5012\n",
      "Epoch 30/100\n",
      "6091/6091 [==============================] - 1s 139us/sample - loss: 0.4353 - val_loss: 0.4986\n",
      "Epoch 31/100\n",
      "6091/6091 [==============================] - 1s 138us/sample - loss: 0.4310 - val_loss: 0.4961\n",
      "Epoch 32/100\n",
      "6091/6091 [==============================] - 1s 138us/sample - loss: 0.4268 - val_loss: 0.4937\n",
      "Epoch 33/100\n",
      "6091/6091 [==============================] - 1s 137us/sample - loss: 0.4227 - val_loss: 0.4914\n",
      "Epoch 34/100\n",
      "6091/6091 [==============================] - 1s 138us/sample - loss: 0.4188 - val_loss: 0.4893\n",
      "Epoch 35/100\n",
      "6091/6091 [==============================] - 1s 139us/sample - loss: 0.4150 - val_loss: 0.4872\n",
      "Epoch 36/100\n",
      "6091/6091 [==============================] - 1s 139us/sample - loss: 0.4113 - val_loss: 0.4853\n",
      "Epoch 37/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6091/6091 [==============================] - 1s 137us/sample - loss: 0.4077 - val_loss: 0.4834\n",
      "Epoch 38/100\n",
      "6091/6091 [==============================] - 1s 137us/sample - loss: 0.4042 - val_loss: 0.4815\n",
      "Epoch 39/100\n",
      "6091/6091 [==============================] - 1s 138us/sample - loss: 0.4008 - val_loss: 0.4798\n",
      "Epoch 40/100\n",
      "6091/6091 [==============================] - 1s 138us/sample - loss: 0.3975 - val_loss: 0.4782\n",
      "Epoch 41/100\n",
      "6091/6091 [==============================] - 1s 137us/sample - loss: 0.3943 - val_loss: 0.4767\n",
      "Epoch 42/100\n",
      "6091/6091 [==============================] - 1s 138us/sample - loss: 0.3911 - val_loss: 0.4751\n",
      "Epoch 43/100\n",
      "6091/6091 [==============================] - 1s 138us/sample - loss: 0.3881 - val_loss: 0.4736\n",
      "Epoch 44/100\n",
      "6091/6091 [==============================] - 1s 139us/sample - loss: 0.3851 - val_loss: 0.4723\n",
      "Epoch 45/100\n",
      "6091/6091 [==============================] - 1s 138us/sample - loss: 0.3822 - val_loss: 0.4710\n",
      "Epoch 46/100\n",
      "6091/6091 [==============================] - 1s 138us/sample - loss: 0.3793 - val_loss: 0.4697\n",
      "Epoch 47/100\n",
      "6091/6091 [==============================] - 1s 137us/sample - loss: 0.3766 - val_loss: 0.4685\n",
      "Epoch 48/100\n",
      "6091/6091 [==============================] - 1s 138us/sample - loss: 0.3738 - val_loss: 0.4673\n",
      "Epoch 49/100\n",
      "6091/6091 [==============================] - 1s 140us/sample - loss: 0.3712 - val_loss: 0.4662\n",
      "Epoch 50/100\n",
      "6091/6091 [==============================] - 1s 140us/sample - loss: 0.3686 - val_loss: 0.4652\n",
      "Epoch 51/100\n",
      "6091/6091 [==============================] - 1s 137us/sample - loss: 0.3661 - val_loss: 0.4642\n",
      "Epoch 52/100\n",
      "6091/6091 [==============================] - 1s 136us/sample - loss: 0.3636 - val_loss: 0.4632\n",
      "Epoch 53/100\n",
      "6091/6091 [==============================] - 1s 138us/sample - loss: 0.3612 - val_loss: 0.4623\n",
      "Epoch 54/100\n",
      "6091/6091 [==============================] - 1s 139us/sample - loss: 0.3588 - val_loss: 0.4615\n",
      "Epoch 55/100\n",
      "6091/6091 [==============================] - 1s 139us/sample - loss: 0.3565 - val_loss: 0.4606\n",
      "Epoch 56/100\n",
      "6091/6091 [==============================] - 1s 140us/sample - loss: 0.3542 - val_loss: 0.4599\n",
      "Epoch 57/100\n",
      "6091/6091 [==============================] - 1s 140us/sample - loss: 0.3519 - val_loss: 0.4591\n",
      "Epoch 58/100\n",
      "6091/6091 [==============================] - 1s 140us/sample - loss: 0.3498 - val_loss: 0.4584\n",
      "Epoch 59/100\n",
      "6091/6091 [==============================] - 1s 139us/sample - loss: 0.3476 - val_loss: 0.4577\n",
      "Epoch 60/100\n",
      "6091/6091 [==============================] - 1s 151us/sample - loss: 0.3455 - val_loss: 0.4571\n",
      "Epoch 61/100\n",
      "6091/6091 [==============================] - 1s 146us/sample - loss: 0.3434 - val_loss: 0.4565\n",
      "Epoch 62/100\n",
      "6091/6091 [==============================] - 1s 151us/sample - loss: 0.3414 - val_loss: 0.4559\n",
      "Epoch 63/100\n",
      "6091/6091 [==============================] - 1s 141us/sample - loss: 0.3394 - val_loss: 0.4553\n",
      "Epoch 64/100\n",
      "6091/6091 [==============================] - 1s 146us/sample - loss: 0.3374 - val_loss: 0.4548\n",
      "Epoch 65/100\n",
      "6091/6091 [==============================] - 1s 142us/sample - loss: 0.3355 - val_loss: 0.4543\n",
      "Epoch 66/100\n",
      "6091/6091 [==============================] - 1s 137us/sample - loss: 0.3336 - val_loss: 0.4538\n",
      "Epoch 67/100\n",
      "6091/6091 [==============================] - 1s 137us/sample - loss: 0.3318 - val_loss: 0.4533\n",
      "Epoch 68/100\n",
      "6091/6091 [==============================] - 1s 138us/sample - loss: 0.3299 - val_loss: 0.4530\n",
      "Epoch 69/100\n",
      "6091/6091 [==============================] - 1s 139us/sample - loss: 0.3281 - val_loss: 0.4525\n",
      "Epoch 70/100\n",
      "6091/6091 [==============================] - 1s 139us/sample - loss: 0.3264 - val_loss: 0.4522\n",
      "Epoch 71/100\n",
      "6091/6091 [==============================] - 1s 139us/sample - loss: 0.3246 - val_loss: 0.4518\n",
      "Epoch 72/100\n",
      "6091/6091 [==============================] - 1s 143us/sample - loss: 0.3229 - val_loss: 0.4515\n",
      "Epoch 73/100\n",
      "6091/6091 [==============================] - 1s 138us/sample - loss: 0.3212 - val_loss: 0.4512\n",
      "Epoch 74/100\n",
      "6091/6091 [==============================] - 1s 137us/sample - loss: 0.3196 - val_loss: 0.4510\n",
      "Epoch 75/100\n",
      "6091/6091 [==============================] - 1s 138us/sample - loss: 0.3179 - val_loss: 0.4507\n",
      "Epoch 76/100\n",
      "6091/6091 [==============================] - 1s 138us/sample - loss: 0.3163 - val_loss: 0.4504\n",
      "Epoch 77/100\n",
      "6091/6091 [==============================] - 1s 138us/sample - loss: 0.3147 - val_loss: 0.4501\n",
      "Epoch 78/100\n",
      "6091/6091 [==============================] - 1s 138us/sample - loss: 0.3132 - val_loss: 0.4500\n",
      "Epoch 79/100\n",
      "6091/6091 [==============================] - 1s 137us/sample - loss: 0.3116 - val_loss: 0.4498\n",
      "Epoch 80/100\n",
      "6091/6091 [==============================] - 1s 138us/sample - loss: 0.3101 - val_loss: 0.4496\n",
      "Epoch 81/100\n",
      "6091/6091 [==============================] - 1s 139us/sample - loss: 0.3086 - val_loss: 0.4493\n",
      "Epoch 82/100\n",
      "6091/6091 [==============================] - 1s 138us/sample - loss: 0.3071 - val_loss: 0.4493\n",
      "Epoch 83/100\n",
      "6091/6091 [==============================] - 1s 139us/sample - loss: 0.3056 - val_loss: 0.4491\n",
      "Epoch 84/100\n",
      "6091/6091 [==============================] - 1s 137us/sample - loss: 0.3042 - val_loss: 0.4490\n",
      "Epoch 85/100\n",
      "6091/6091 [==============================] - 1s 139us/sample - loss: 0.3028 - val_loss: 0.4489\n",
      "Epoch 86/100\n",
      "6091/6091 [==============================] - 1s 137us/sample - loss: 0.3014 - val_loss: 0.4489\n",
      "Epoch 87/100\n",
      "6091/6091 [==============================] - 1s 136us/sample - loss: 0.3000 - val_loss: 0.4488\n",
      "Epoch 88/100\n",
      "6091/6091 [==============================] - 1s 138us/sample - loss: 0.2986 - val_loss: 0.4487\n",
      "Epoch 89/100\n",
      "6091/6091 [==============================] - 1s 137us/sample - loss: 0.2973 - val_loss: 0.4487\n",
      "Epoch 90/100\n",
      "6091/6091 [==============================] - 1s 139us/sample - loss: 0.2959 - val_loss: 0.4487\n",
      "Predicting...\n",
      "(Took 79.385 sec)\n",
      "Train on 6091 samples, validate on 1522 samples\n",
      "Epoch 1/100\n",
      "6091/6091 [==============================] - 1s 184us/sample - loss: 0.6833 - val_loss: 0.6759\n",
      "Epoch 2/100\n",
      "6091/6091 [==============================] - 1s 138us/sample - loss: 0.6639 - val_loss: 0.6628\n",
      "Epoch 3/100\n",
      "6091/6091 [==============================] - 1s 137us/sample - loss: 0.6481 - val_loss: 0.6515\n",
      "Epoch 4/100\n",
      "6091/6091 [==============================] - 1s 138us/sample - loss: 0.6341 - val_loss: 0.6412\n",
      "Epoch 5/100\n",
      "6091/6091 [==============================] - 1s 138us/sample - loss: 0.6211 - val_loss: 0.6313\n",
      "Epoch 6/100\n",
      "6091/6091 [==============================] - 1s 137us/sample - loss: 0.6088 - val_loss: 0.6221\n",
      "Epoch 7/100\n",
      "6091/6091 [==============================] - 1s 138us/sample - loss: 0.5972 - val_loss: 0.6133\n",
      "Epoch 8/100\n",
      "6091/6091 [==============================] - 1s 138us/sample - loss: 0.5862 - val_loss: 0.6050\n",
      "Epoch 9/100\n",
      "6091/6091 [==============================] - 1s 140us/sample - loss: 0.5757 - val_loss: 0.5972\n",
      "Epoch 10/100\n",
      "6091/6091 [==============================] - 1s 138us/sample - loss: 0.5657 - val_loss: 0.5899\n",
      "Epoch 11/100\n",
      "6091/6091 [==============================] - 1s 137us/sample - loss: 0.5562 - val_loss: 0.5829\n",
      "Epoch 12/100\n",
      "6091/6091 [==============================] - 1s 139us/sample - loss: 0.5471 - val_loss: 0.5764\n",
      "Epoch 13/100\n",
      "6091/6091 [==============================] - 1s 138us/sample - loss: 0.5384 - val_loss: 0.5701\n",
      "Epoch 14/100\n",
      "6091/6091 [==============================] - 1s 138us/sample - loss: 0.5301 - val_loss: 0.5641\n",
      "Epoch 15/100\n",
      "6091/6091 [==============================] - 1s 139us/sample - loss: 0.5222 - val_loss: 0.5585\n",
      "Epoch 16/100\n",
      "6091/6091 [==============================] - 1s 139us/sample - loss: 0.5146 - val_loss: 0.5531\n",
      "Epoch 17/100\n",
      "6091/6091 [==============================] - 1s 139us/sample - loss: 0.5074 - val_loss: 0.5481\n",
      "Epoch 18/100\n",
      "6091/6091 [==============================] - 1s 138us/sample - loss: 0.5004 - val_loss: 0.5434\n",
      "Epoch 19/100\n",
      "6091/6091 [==============================] - 1s 139us/sample - loss: 0.4938 - val_loss: 0.5388\n",
      "Epoch 20/100\n",
      "6091/6091 [==============================] - 1s 140us/sample - loss: 0.4873 - val_loss: 0.5346\n",
      "Epoch 21/100\n",
      "6091/6091 [==============================] - 1s 146us/sample - loss: 0.4812 - val_loss: 0.5306\n",
      "Epoch 22/100\n",
      "6091/6091 [==============================] - 1s 140us/sample - loss: 0.4752 - val_loss: 0.5267\n",
      "Epoch 23/100\n",
      "6091/6091 [==============================] - 1s 140us/sample - loss: 0.4695 - val_loss: 0.5230\n",
      "Epoch 24/100\n",
      "6091/6091 [==============================] - 1s 138us/sample - loss: 0.4640 - val_loss: 0.5196\n",
      "Epoch 25/100\n",
      "6091/6091 [==============================] - 1s 139us/sample - loss: 0.4587 - val_loss: 0.5163\n",
      "Epoch 26/100\n",
      "6091/6091 [==============================] - 1s 138us/sample - loss: 0.4536 - val_loss: 0.5131\n",
      "Epoch 27/100\n",
      "6091/6091 [==============================] - 1s 138us/sample - loss: 0.4487 - val_loss: 0.5102\n",
      "Epoch 28/100\n",
      "6091/6091 [==============================] - 1s 139us/sample - loss: 0.4439 - val_loss: 0.5074\n",
      "Epoch 29/100\n",
      "6091/6091 [==============================] - 1s 139us/sample - loss: 0.4393 - val_loss: 0.5048\n",
      "Epoch 30/100\n",
      "6091/6091 [==============================] - 1s 139us/sample - loss: 0.4348 - val_loss: 0.5022\n",
      "Epoch 31/100\n",
      "6091/6091 [==============================] - 1s 140us/sample - loss: 0.4305 - val_loss: 0.4998\n",
      "Epoch 32/100\n",
      "6091/6091 [==============================] - 1s 139us/sample - loss: 0.4263 - val_loss: 0.4975\n",
      "Epoch 33/100\n",
      "6091/6091 [==============================] - 1s 139us/sample - loss: 0.4222 - val_loss: 0.4953\n",
      "Epoch 34/100\n",
      "6091/6091 [==============================] - 1s 140us/sample - loss: 0.4183 - val_loss: 0.4932\n",
      "Epoch 35/100\n",
      "6091/6091 [==============================] - 1s 139us/sample - loss: 0.4145 - val_loss: 0.4912\n",
      "Epoch 36/100\n",
      "6091/6091 [==============================] - 1s 141us/sample - loss: 0.4108 - val_loss: 0.4893\n",
      "Epoch 37/100\n",
      "6091/6091 [==============================] - 1s 139us/sample - loss: 0.4072 - val_loss: 0.4876\n",
      "Epoch 38/100\n",
      "6091/6091 [==============================] - 1s 138us/sample - loss: 0.4037 - val_loss: 0.4859\n",
      "Epoch 39/100\n",
      "6091/6091 [==============================] - 1s 138us/sample - loss: 0.4003 - val_loss: 0.4843\n",
      "Epoch 40/100\n",
      "6091/6091 [==============================] - 1s 138us/sample - loss: 0.3970 - val_loss: 0.4827\n",
      "Epoch 41/100\n",
      "6091/6091 [==============================] - 1s 139us/sample - loss: 0.3938 - val_loss: 0.4813\n",
      "Epoch 42/100\n",
      "6091/6091 [==============================] - 1s 139us/sample - loss: 0.3906 - val_loss: 0.4799\n",
      "Epoch 43/100\n",
      "6091/6091 [==============================] - 1s 140us/sample - loss: 0.3876 - val_loss: 0.4787\n",
      "Epoch 44/100\n",
      "6091/6091 [==============================] - 1s 148us/sample - loss: 0.3846 - val_loss: 0.4774\n",
      "Epoch 45/100\n",
      "6091/6091 [==============================] - 1s 151us/sample - loss: 0.3817 - val_loss: 0.4762\n",
      "Epoch 46/100\n",
      "6091/6091 [==============================] - 1s 144us/sample - loss: 0.3788 - val_loss: 0.4751\n",
      "Epoch 47/100\n",
      "6091/6091 [==============================] - 1s 147us/sample - loss: 0.3761 - val_loss: 0.4740\n",
      "Epoch 48/100\n",
      "6091/6091 [==============================] - 1s 142us/sample - loss: 0.3734 - val_loss: 0.4730\n",
      "Epoch 49/100\n",
      "6091/6091 [==============================] - 1s 145us/sample - loss: 0.3707 - val_loss: 0.4720\n",
      "Epoch 50/100\n",
      "6091/6091 [==============================] - 1s 138us/sample - loss: 0.3681 - val_loss: 0.4711\n",
      "Epoch 51/100\n",
      "6091/6091 [==============================] - 1s 140us/sample - loss: 0.3656 - val_loss: 0.4703\n",
      "Epoch 52/100\n",
      "6091/6091 [==============================] - 1s 138us/sample - loss: 0.3631 - val_loss: 0.4695\n",
      "Epoch 53/100\n",
      "6091/6091 [==============================] - 1s 139us/sample - loss: 0.3607 - val_loss: 0.4688\n",
      "Epoch 54/100\n",
      "6091/6091 [==============================] - 1s 139us/sample - loss: 0.3583 - val_loss: 0.4681\n",
      "Epoch 55/100\n",
      "6091/6091 [==============================] - 1s 138us/sample - loss: 0.3560 - val_loss: 0.4673\n",
      "Epoch 56/100\n",
      "6091/6091 [==============================] - 1s 140us/sample - loss: 0.3537 - val_loss: 0.4667\n",
      "Epoch 57/100\n",
      "6091/6091 [==============================] - 1s 140us/sample - loss: 0.3515 - val_loss: 0.4661\n",
      "Epoch 58/100\n",
      "6091/6091 [==============================] - 1s 145us/sample - loss: 0.3493 - val_loss: 0.4656\n",
      "Epoch 59/100\n",
      "6091/6091 [==============================] - 1s 140us/sample - loss: 0.3472 - val_loss: 0.4651\n",
      "Epoch 60/100\n",
      "6091/6091 [==============================] - 1s 140us/sample - loss: 0.3450 - val_loss: 0.4646\n",
      "Epoch 61/100\n",
      "6091/6091 [==============================] - 1s 140us/sample - loss: 0.3430 - val_loss: 0.4641\n",
      "Epoch 62/100\n",
      "6091/6091 [==============================] - 1s 139us/sample - loss: 0.3410 - val_loss: 0.4637\n",
      "Epoch 63/100\n",
      "6091/6091 [==============================] - 1s 139us/sample - loss: 0.3390 - val_loss: 0.4632\n",
      "Epoch 64/100\n",
      "6091/6091 [==============================] - 1s 138us/sample - loss: 0.3370 - val_loss: 0.4629\n",
      "Epoch 65/100\n",
      "6091/6091 [==============================] - 1s 139us/sample - loss: 0.3351 - val_loss: 0.4625\n",
      "Epoch 66/100\n",
      "6091/6091 [==============================] - 1s 140us/sample - loss: 0.3332 - val_loss: 0.4622\n",
      "Epoch 67/100\n",
      "6091/6091 [==============================] - 1s 139us/sample - loss: 0.3314 - val_loss: 0.4619\n",
      "Epoch 68/100\n",
      "6091/6091 [==============================] - 1s 140us/sample - loss: 0.3295 - val_loss: 0.4616\n",
      "Epoch 69/100\n",
      "6091/6091 [==============================] - 1s 139us/sample - loss: 0.3277 - val_loss: 0.4614\n",
      "Epoch 70/100\n",
      "6091/6091 [==============================] - 1s 140us/sample - loss: 0.3260 - val_loss: 0.4612\n",
      "Epoch 71/100\n",
      "6091/6091 [==============================] - 1s 140us/sample - loss: 0.3243 - val_loss: 0.4610\n",
      "Epoch 72/100\n",
      "6091/6091 [==============================] - 1s 141us/sample - loss: 0.3225 - val_loss: 0.4607\n",
      "Epoch 73/100\n",
      "6091/6091 [==============================] - 1s 138us/sample - loss: 0.3209 - val_loss: 0.4606\n",
      "Epoch 74/100\n",
      "6091/6091 [==============================] - 1s 138us/sample - loss: 0.3192 - val_loss: 0.4605\n",
      "Epoch 75/100\n",
      "6091/6091 [==============================] - 1s 140us/sample - loss: 0.3176 - val_loss: 0.4604\n",
      "Epoch 76/100\n",
      "6091/6091 [==============================] - 1s 140us/sample - loss: 0.3160 - val_loss: 0.4602\n",
      "Epoch 77/100\n",
      "6091/6091 [==============================] - 1s 139us/sample - loss: 0.3144 - val_loss: 0.4601\n",
      "Epoch 78/100\n",
      "6091/6091 [==============================] - 1s 139us/sample - loss: 0.3128 - val_loss: 0.4601\n",
      "Epoch 79/100\n",
      "6091/6091 [==============================] - 1s 140us/sample - loss: 0.3113 - val_loss: 0.4601\n",
      "Predicting...\n",
      "(Took 68.500 sec)\n",
      "Combined confusion matrix:\n",
      "[[3841.  501.]\n",
      " [1046. 2225.]]\n",
      "(Overall, took 369.565 sec)\n",
      "Accuracy: 79.68% +/- 1.18%\n",
      "Precision for positive class: 78.61% +/- 2.20%\n",
      "Precision for negative class: 81.63% +/- 1.26%\n",
      "Recall for positive class: 88.46% +/- 1.14%\n",
      "Recall for negative class: 68.04% +/- 3.10%\n",
      "F for positive class: 83.23% +/- 1.13%\n",
      "F for negative class: 74.19% +/- 1.59%\n",
      "Mean F score: 78.71% +/- 1.20%\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/100\n",
      "6090/6090 [==============================] - 1s 152us/sample - loss: 0.6694 - val_loss: 0.6474\n",
      "Epoch 2/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.6219 - val_loss: 0.6117\n",
      "Epoch 3/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.5847 - val_loss: 0.5843\n",
      "Epoch 4/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.5548 - val_loss: 0.5624\n",
      "Epoch 5/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.5302 - val_loss: 0.5447\n",
      "Epoch 6/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.5096 - val_loss: 0.5302\n",
      "Epoch 7/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4922 - val_loss: 0.5182\n",
      "Epoch 8/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4772 - val_loss: 0.5081\n",
      "Epoch 9/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4640 - val_loss: 0.4997\n",
      "Epoch 10/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.4524 - val_loss: 0.4925\n",
      "Epoch 11/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4421 - val_loss: 0.4863\n",
      "Epoch 12/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4327 - val_loss: 0.4810\n",
      "Epoch 13/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4242 - val_loss: 0.4764\n",
      "Epoch 14/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4165 - val_loss: 0.4725\n",
      "Epoch 15/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4095 - val_loss: 0.4690\n",
      "Epoch 16/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4029 - val_loss: 0.4664\n",
      "Epoch 17/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.3968 - val_loss: 0.4638\n",
      "Epoch 18/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.3912 - val_loss: 0.4614\n",
      "Epoch 19/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.3858 - val_loss: 0.4597\n",
      "Epoch 20/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.3809 - val_loss: 0.4580\n",
      "Epoch 21/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.3762 - val_loss: 0.4567\n",
      "Epoch 22/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.3717 - val_loss: 0.4557\n",
      "Epoch 23/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.3675 - val_loss: 0.4547\n",
      "Epoch 24/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.3635 - val_loss: 0.4539\n",
      "Epoch 25/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.3598 - val_loss: 0.4533\n",
      "Epoch 26/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.3561 - val_loss: 0.4528\n",
      "Epoch 27/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.3527 - val_loss: 0.4526\n",
      "Epoch 28/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.3494 - val_loss: 0.4525\n",
      "Epoch 29/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.3462 - val_loss: 0.4523\n",
      "Epoch 30/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.3431 - val_loss: 0.4524\n",
      "Predicting...\n",
      "(Took 20.358 sec)\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/100\n",
      "6090/6090 [==============================] - 1s 158us/sample - loss: 0.6706 - val_loss: 0.6450\n",
      "Epoch 2/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.6235 - val_loss: 0.6098\n",
      "Epoch 3/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.5862 - val_loss: 0.5822\n",
      "Epoch 4/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.5563 - val_loss: 0.5610\n",
      "Epoch 5/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.5317 - val_loss: 0.5434\n",
      "Epoch 6/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.5111 - val_loss: 0.5294\n",
      "Epoch 7/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.4936 - val_loss: 0.5177\n",
      "Epoch 8/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4785 - val_loss: 0.5076\n",
      "Epoch 9/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4654 - val_loss: 0.4994\n",
      "Epoch 10/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4537 - val_loss: 0.4924\n",
      "Epoch 11/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4434 - val_loss: 0.4863\n",
      "Epoch 12/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4341 - val_loss: 0.4812\n",
      "Epoch 13/100\n",
      "6090/6090 [==============================] - 1s 122us/sample - loss: 0.4257 - val_loss: 0.4766\n",
      "Epoch 14/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.4180 - val_loss: 0.4728\n",
      "Epoch 15/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.4109 - val_loss: 0.4694\n",
      "Epoch 16/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.4045 - val_loss: 0.4665\n",
      "Epoch 17/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.3984 - val_loss: 0.4639\n",
      "Epoch 18/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.3929 - val_loss: 0.4618\n",
      "Epoch 19/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.3877 - val_loss: 0.4599\n",
      "Epoch 20/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.3828 - val_loss: 0.4585\n",
      "Epoch 21/100\n",
      "6090/6090 [==============================] - 1s 122us/sample - loss: 0.3781 - val_loss: 0.4570\n",
      "Epoch 22/100\n",
      "6090/6090 [==============================] - 1s 118us/sample - loss: 0.3738 - val_loss: 0.4558\n",
      "Epoch 23/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.3697 - val_loss: 0.4548\n",
      "Epoch 24/100\n",
      "6090/6090 [==============================] - 1s 125us/sample - loss: 0.3658 - val_loss: 0.4542\n",
      "Epoch 25/100\n",
      "6090/6090 [==============================] - 1s 118us/sample - loss: 0.3621 - val_loss: 0.4536\n",
      "Epoch 26/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.3585 - val_loss: 0.4532\n",
      "Epoch 27/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.3551 - val_loss: 0.4529\n",
      "Epoch 28/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.3519 - val_loss: 0.4527\n",
      "Epoch 29/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.3488 - val_loss: 0.4527\n",
      "Epoch 30/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.3458 - val_loss: 0.4527\n",
      "Epoch 31/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.3430 - val_loss: 0.4526\n",
      "Epoch 32/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.3402 - val_loss: 0.4527\n",
      "Predicting...\n",
      "(Took 22.407 sec)\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/100\n",
      "6090/6090 [==============================] - 1s 152us/sample - loss: 0.6708 - val_loss: 0.6488\n",
      "Epoch 2/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.6229 - val_loss: 0.6142\n",
      "Epoch 3/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.5853 - val_loss: 0.5871\n",
      "Epoch 4/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.5552 - val_loss: 0.5657\n",
      "Epoch 5/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.5305 - val_loss: 0.5484\n",
      "Epoch 6/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.5098 - val_loss: 0.5344\n",
      "Epoch 7/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4923 - val_loss: 0.5226\n",
      "Epoch 8/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.4772 - val_loss: 0.5129\n",
      "Epoch 9/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.4640 - val_loss: 0.5048\n",
      "Epoch 10/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4523 - val_loss: 0.4977\n",
      "Epoch 11/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4419 - val_loss: 0.4917\n",
      "Epoch 12/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.4325 - val_loss: 0.4866\n",
      "Epoch 13/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4240 - val_loss: 0.4825\n",
      "Epoch 14/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4163 - val_loss: 0.4788\n",
      "Epoch 15/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4091 - val_loss: 0.4756\n",
      "Epoch 16/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4026 - val_loss: 0.4727\n",
      "Epoch 17/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.3965 - val_loss: 0.4705\n",
      "Epoch 18/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.3908 - val_loss: 0.4685\n",
      "Epoch 19/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.3855 - val_loss: 0.4667\n",
      "Epoch 20/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.3805 - val_loss: 0.4655\n",
      "Epoch 21/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.3757 - val_loss: 0.4642\n",
      "Epoch 22/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.3714 - val_loss: 0.4633\n",
      "Epoch 23/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.3672 - val_loss: 0.4626\n",
      "Epoch 24/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.3631 - val_loss: 0.4619\n",
      "Epoch 25/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.3594 - val_loss: 0.4617\n",
      "Epoch 26/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.3557 - val_loss: 0.4612\n",
      "Epoch 27/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.3523 - val_loss: 0.4612\n",
      "Epoch 28/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.3489 - val_loss: 0.4611\n",
      "Epoch 29/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.3458 - val_loss: 0.4613\n",
      "Predicting...\n",
      "(Took 19.680 sec)\n",
      "Train on 6091 samples, validate on 1522 samples\n",
      "Epoch 1/100\n",
      "6091/6091 [==============================] - 1s 152us/sample - loss: 0.6694 - val_loss: 0.6463\n",
      "Epoch 2/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.6226 - val_loss: 0.6098\n",
      "Epoch 3/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.5859 - val_loss: 0.5816\n",
      "Epoch 4/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.5563 - val_loss: 0.5593\n",
      "Epoch 5/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.5318 - val_loss: 0.5411\n",
      "Epoch 6/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.5113 - val_loss: 0.5264\n",
      "Epoch 7/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.4940 - val_loss: 0.5141\n",
      "Epoch 8/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.4789 - val_loss: 0.5040\n",
      "Epoch 9/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4658 - val_loss: 0.4956\n",
      "Epoch 10/100\n",
      "6091/6091 [==============================] - 1s 114us/sample - loss: 0.4542 - val_loss: 0.4883\n",
      "Epoch 11/100\n",
      "6091/6091 [==============================] - 1s 110us/sample - loss: 0.4439 - val_loss: 0.4821\n",
      "Epoch 12/100\n",
      "6091/6091 [==============================] - 1s 110us/sample - loss: 0.4346 - val_loss: 0.4767\n",
      "Epoch 13/100\n",
      "6091/6091 [==============================] - 1s 113us/sample - loss: 0.4262 - val_loss: 0.4721\n",
      "Epoch 14/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.4185 - val_loss: 0.4683\n",
      "Epoch 15/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4115 - val_loss: 0.4650\n",
      "Epoch 16/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4049 - val_loss: 0.4621\n",
      "Epoch 17/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.3989 - val_loss: 0.4595\n",
      "Epoch 18/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.3933 - val_loss: 0.4571\n",
      "Epoch 19/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.3880 - val_loss: 0.4554\n",
      "Epoch 20/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.3831 - val_loss: 0.4539\n",
      "Epoch 21/100\n",
      "6091/6091 [==============================] - 1s 106us/sample - loss: 0.3784 - val_loss: 0.4527\n",
      "Epoch 22/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.3740 - val_loss: 0.4514\n",
      "Epoch 23/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.3699 - val_loss: 0.4505\n",
      "Epoch 24/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.3660 - val_loss: 0.4497\n",
      "Epoch 25/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.3621 - val_loss: 0.4491\n",
      "Epoch 26/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.3586 - val_loss: 0.4485\n",
      "Epoch 27/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.3551 - val_loss: 0.4481\n",
      "Epoch 28/100\n",
      "6091/6091 [==============================] - 1s 110us/sample - loss: 0.3519 - val_loss: 0.4478\n",
      "Epoch 29/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.3487 - val_loss: 0.4477\n",
      "Epoch 30/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.3457 - val_loss: 0.4476\n",
      "Epoch 31/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.3428 - val_loss: 0.4477\n",
      "Predicting...\n",
      "(Took 21.127 sec)\n",
      "Train on 6091 samples, validate on 1522 samples\n",
      "Epoch 1/100\n",
      "6091/6091 [==============================] - 1s 151us/sample - loss: 0.6692 - val_loss: 0.6481\n",
      "Epoch 2/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.6211 - val_loss: 0.6142\n",
      "Epoch 3/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.5836 - val_loss: 0.5877\n",
      "Epoch 4/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.5535 - val_loss: 0.5664\n",
      "Epoch 5/100\n",
      "6091/6091 [==============================] - 1s 106us/sample - loss: 0.5290 - val_loss: 0.5496\n",
      "Epoch 6/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.5083 - val_loss: 0.5358\n",
      "Epoch 7/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4908 - val_loss: 0.5241\n",
      "Epoch 8/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4757 - val_loss: 0.5144\n",
      "Epoch 9/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4625 - val_loss: 0.5064\n",
      "Epoch 10/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4508 - val_loss: 0.4994\n",
      "Epoch 11/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4404 - val_loss: 0.4935\n",
      "Epoch 12/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.4311 - val_loss: 0.4885\n",
      "Epoch 13/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4226 - val_loss: 0.4843\n",
      "Epoch 14/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.4149 - val_loss: 0.4805\n",
      "Epoch 15/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.4078 - val_loss: 0.4773\n",
      "Epoch 16/100\n",
      "6091/6091 [==============================] - 1s 106us/sample - loss: 0.4012 - val_loss: 0.4744\n",
      "Epoch 17/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.3951 - val_loss: 0.4721\n",
      "Epoch 18/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.3895 - val_loss: 0.4701\n",
      "Epoch 19/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.3842 - val_loss: 0.4682\n",
      "Epoch 20/100\n",
      "6091/6091 [==============================] - 1s 106us/sample - loss: 0.3793 - val_loss: 0.4666\n",
      "Epoch 21/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.3746 - val_loss: 0.4652\n",
      "Epoch 22/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.3702 - val_loss: 0.4642\n",
      "Epoch 23/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.3661 - val_loss: 0.4634\n",
      "Epoch 24/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.3621 - val_loss: 0.4627\n",
      "Epoch 25/100\n",
      "6091/6091 [==============================] - 1s 106us/sample - loss: 0.3584 - val_loss: 0.4620\n",
      "Epoch 26/100\n",
      "6091/6091 [==============================] - 1s 121us/sample - loss: 0.3548 - val_loss: 0.4616\n",
      "Epoch 27/100\n",
      "6091/6091 [==============================] - 1s 115us/sample - loss: 0.3514 - val_loss: 0.4613\n",
      "Epoch 28/100\n",
      "6091/6091 [==============================] - 1s 119us/sample - loss: 0.3482 - val_loss: 0.4608\n",
      "Epoch 29/100\n",
      "6091/6091 [==============================] - 1s 120us/sample - loss: 0.3450 - val_loss: 0.4607\n",
      "Epoch 30/100\n",
      "6091/6091 [==============================] - 1s 119us/sample - loss: 0.3420 - val_loss: 0.4609\n",
      "Predicting...\n",
      "(Took 20.695 sec)\n",
      "Combined confusion matrix:\n",
      "[[3846.  496.]\n",
      " [1037. 2234.]]\n",
      "(Overall, took 104.480 sec)\n",
      "Accuracy: 79.86% +/- 0.56%\n",
      "Precision for positive class: 78.76% +/- 1.35%\n",
      "Precision for negative class: 81.83% +/- 1.27%\n",
      "Recall for positive class: 88.57% +/- 0.97%\n",
      "Recall for negative class: 68.30% +/- 1.22%\n",
      "F for positive class: 83.37% +/- 0.83%\n",
      "F for negative class: 74.45% +/- 0.52%\n",
      "Mean F score: 78.91% +/- 0.38%\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/100\n",
      "6090/6090 [==============================] - 1s 164us/sample - loss: 0.6865 - val_loss: 0.6794\n",
      "Epoch 2/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.6745 - val_loss: 0.6699\n",
      "Epoch 3/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.6650 - val_loss: 0.6620\n",
      "Epoch 4/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.6566 - val_loss: 0.6551\n",
      "Epoch 5/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.6487 - val_loss: 0.6489\n",
      "Epoch 6/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.6413 - val_loss: 0.6429\n",
      "Epoch 7/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.6341 - val_loss: 0.6372\n",
      "Epoch 8/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.6271 - val_loss: 0.6317\n",
      "Epoch 9/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.6203 - val_loss: 0.6264\n",
      "Epoch 10/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.6137 - val_loss: 0.6212\n",
      "Epoch 11/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.6072 - val_loss: 0.6163\n",
      "Epoch 12/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.6010 - val_loss: 0.6115\n",
      "Epoch 13/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.5949 - val_loss: 0.6069\n",
      "Epoch 14/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.5890 - val_loss: 0.6025\n",
      "Epoch 15/100\n",
      "6090/6090 [==============================] - 1s 118us/sample - loss: 0.5833 - val_loss: 0.5981\n",
      "Epoch 16/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.5777 - val_loss: 0.5939\n",
      "Epoch 17/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.5723 - val_loss: 0.5899\n",
      "Epoch 18/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.5670 - val_loss: 0.5860\n",
      "Epoch 19/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.5618 - val_loss: 0.5822\n",
      "Epoch 20/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.5568 - val_loss: 0.5786\n",
      "Epoch 21/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.5519 - val_loss: 0.5750\n",
      "Epoch 22/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.5472 - val_loss: 0.5716\n",
      "Epoch 23/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.5425 - val_loss: 0.5683\n",
      "Epoch 24/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.5380 - val_loss: 0.5651\n",
      "Epoch 25/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.5336 - val_loss: 0.5620\n",
      "Epoch 26/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.5293 - val_loss: 0.5590\n",
      "Epoch 27/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.5251 - val_loss: 0.5560\n",
      "Epoch 28/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.5210 - val_loss: 0.5533\n",
      "Epoch 29/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.5170 - val_loss: 0.5505\n",
      "Epoch 30/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.5132 - val_loss: 0.5479\n",
      "Epoch 31/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.5094 - val_loss: 0.5454\n",
      "Epoch 32/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.5057 - val_loss: 0.5430\n",
      "Epoch 33/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.5021 - val_loss: 0.5406\n",
      "Epoch 34/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4985 - val_loss: 0.5383\n",
      "Epoch 35/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4951 - val_loss: 0.5361\n",
      "Epoch 36/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4918 - val_loss: 0.5340\n",
      "Epoch 37/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.4885 - val_loss: 0.5319\n",
      "Epoch 38/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4853 - val_loss: 0.5300\n",
      "Epoch 39/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.4822 - val_loss: 0.5281\n",
      "Epoch 40/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4791 - val_loss: 0.5263\n",
      "Epoch 41/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4762 - val_loss: 0.5244\n",
      "Epoch 42/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4733 - val_loss: 0.5227\n",
      "Epoch 43/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4704 - val_loss: 0.5210\n",
      "Epoch 44/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4676 - val_loss: 0.5194\n",
      "Epoch 45/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.4649 - val_loss: 0.5179\n",
      "Epoch 46/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.4623 - val_loss: 0.5164\n",
      "Epoch 47/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.4597 - val_loss: 0.5150\n",
      "Epoch 48/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.4572 - val_loss: 0.5136\n",
      "Epoch 49/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.4547 - val_loss: 0.5123\n",
      "Epoch 50/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.4523 - val_loss: 0.5110\n",
      "Epoch 51/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4499 - val_loss: 0.5097\n",
      "Epoch 52/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4476 - val_loss: 0.5086\n",
      "Epoch 53/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4453 - val_loss: 0.5074\n",
      "Epoch 54/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4431 - val_loss: 0.5063\n",
      "Epoch 55/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.4409 - val_loss: 0.5053\n",
      "Epoch 56/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.4387 - val_loss: 0.5042\n",
      "Epoch 57/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.4367 - val_loss: 0.5033\n",
      "Epoch 58/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.4346 - val_loss: 0.5023\n",
      "Epoch 59/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.4326 - val_loss: 0.5014\n",
      "Epoch 60/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4306 - val_loss: 0.5005\n",
      "Epoch 61/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4287 - val_loss: 0.4997\n",
      "Epoch 62/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4268 - val_loss: 0.4989\n",
      "Epoch 63/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.4249 - val_loss: 0.4981\n",
      "Epoch 64/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4231 - val_loss: 0.4973\n",
      "Epoch 65/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4214 - val_loss: 0.4966\n",
      "Epoch 66/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4196 - val_loss: 0.4959\n",
      "Epoch 67/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.4179 - val_loss: 0.4952\n",
      "Epoch 68/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.4162 - val_loss: 0.4946\n",
      "Epoch 69/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4145 - val_loss: 0.4940\n",
      "Epoch 70/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.4129 - val_loss: 0.4934\n",
      "Epoch 71/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.4113 - val_loss: 0.4929\n",
      "Epoch 72/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.4097 - val_loss: 0.4923\n",
      "Epoch 73/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.4082 - val_loss: 0.4918\n",
      "Epoch 74/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.4067 - val_loss: 0.4913\n",
      "Epoch 75/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.4052 - val_loss: 0.4908\n",
      "Epoch 76/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.4037 - val_loss: 0.4903\n",
      "Epoch 77/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.4023 - val_loss: 0.4900\n",
      "Epoch 78/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.4008 - val_loss: 0.4895\n",
      "Epoch 79/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.3994 - val_loss: 0.4891\n",
      "Epoch 80/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.3981 - val_loss: 0.4887\n",
      "Epoch 81/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.3967 - val_loss: 0.4884\n",
      "Epoch 82/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.3954 - val_loss: 0.4880\n",
      "Epoch 83/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.3941 - val_loss: 0.4877\n",
      "Epoch 84/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.3928 - val_loss: 0.4873\n",
      "Epoch 85/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.3915 - val_loss: 0.4871\n",
      "Epoch 86/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.3903 - val_loss: 0.4868\n",
      "Epoch 87/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.3891 - val_loss: 0.4866\n",
      "Epoch 88/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.3879 - val_loss: 0.4863\n",
      "Epoch 89/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.3867 - val_loss: 0.4861\n",
      "Epoch 90/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.3855 - val_loss: 0.4859\n",
      "Epoch 91/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.3843 - val_loss: 0.4857\n",
      "Epoch 92/100\n",
      "6090/6090 [==============================] - 1s 124us/sample - loss: 0.3832 - val_loss: 0.4855\n",
      "Epoch 93/100\n",
      "6090/6090 [==============================] - 1s 129us/sample - loss: 0.3821 - val_loss: 0.4853\n",
      "Epoch 94/100\n",
      "6090/6090 [==============================] - 1s 122us/sample - loss: 0.3810 - val_loss: 0.4852\n",
      "Epoch 95/100\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.3799 - val_loss: 0.4851\n",
      "Epoch 96/100\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.3788 - val_loss: 0.4849\n",
      "Epoch 97/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.3777 - val_loss: 0.4848\n",
      "Epoch 98/100\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.3767 - val_loss: 0.4846\n",
      "Epoch 99/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.3756 - val_loss: 0.4845\n",
      "Epoch 100/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.3746 - val_loss: 0.4845\n",
      "Predicting...\n",
      "(Took 67.868 sec)\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/100\n",
      "6090/6090 [==============================] - 1s 188us/sample - loss: 0.6869 - val_loss: 0.6785\n",
      "Epoch 2/100\n",
      "6090/6090 [==============================] - 1s 135us/sample - loss: 0.6753 - val_loss: 0.6682\n",
      "Epoch 3/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.6661 - val_loss: 0.6600\n",
      "Epoch 4/100\n",
      "6090/6090 [==============================] - 1s 130us/sample - loss: 0.6579 - val_loss: 0.6529\n",
      "Epoch 5/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.6502 - val_loss: 0.6463\n",
      "Epoch 6/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.6429 - val_loss: 0.6401\n",
      "Epoch 7/100\n",
      "6090/6090 [==============================] - 1s 135us/sample - loss: 0.6358 - val_loss: 0.6342\n",
      "Epoch 8/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.6288 - val_loss: 0.6284\n",
      "Epoch 9/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.6222 - val_loss: 0.6229\n",
      "Epoch 10/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.6156 - val_loss: 0.6176\n",
      "Epoch 11/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.6093 - val_loss: 0.6125\n",
      "Epoch 12/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.6031 - val_loss: 0.6075\n",
      "Epoch 13/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.5972 - val_loss: 0.6027\n",
      "Epoch 14/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.5913 - val_loss: 0.5981\n",
      "Epoch 15/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.5857 - val_loss: 0.5936\n",
      "Epoch 16/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.5802 - val_loss: 0.5893\n",
      "Epoch 17/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.5748 - val_loss: 0.5851\n",
      "Epoch 18/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.5695 - val_loss: 0.5809\n",
      "Epoch 19/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.5644 - val_loss: 0.5770\n",
      "Epoch 20/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.5595 - val_loss: 0.5732\n",
      "Epoch 21/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.5547 - val_loss: 0.5695\n",
      "Epoch 22/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.5500 - val_loss: 0.5660\n",
      "Epoch 23/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.5454 - val_loss: 0.5625\n",
      "Epoch 24/100\n",
      "6090/6090 [==============================] - 1s 117us/sample - loss: 0.5409 - val_loss: 0.5592\n",
      "Epoch 25/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.5365 - val_loss: 0.5558\n",
      "Epoch 26/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.5323 - val_loss: 0.5527\n",
      "Epoch 27/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.5282 - val_loss: 0.5497\n",
      "Epoch 28/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.5241 - val_loss: 0.5467\n",
      "Epoch 29/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.5202 - val_loss: 0.5439\n",
      "Epoch 30/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.5164 - val_loss: 0.5412\n",
      "Epoch 31/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.5126 - val_loss: 0.5384\n",
      "Epoch 32/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.5089 - val_loss: 0.5359\n",
      "Epoch 33/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.5054 - val_loss: 0.5334\n",
      "Epoch 34/100\n",
      "6090/6090 [==============================] - ETA: 0s - loss: 0.501 - 1s 109us/sample - loss: 0.5019 - val_loss: 0.5310\n",
      "Epoch 35/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4985 - val_loss: 0.5286\n",
      "Epoch 36/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4952 - val_loss: 0.5263\n",
      "Epoch 37/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.4920 - val_loss: 0.5242\n",
      "Epoch 38/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4888 - val_loss: 0.5222\n",
      "Epoch 39/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4857 - val_loss: 0.5200\n",
      "Epoch 40/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4827 - val_loss: 0.5181\n",
      "Epoch 41/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4797 - val_loss: 0.5162\n",
      "Epoch 42/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.4768 - val_loss: 0.5143\n",
      "Epoch 43/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.4740 - val_loss: 0.5125\n",
      "Epoch 44/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4713 - val_loss: 0.5108\n",
      "Epoch 45/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4686 - val_loss: 0.5091\n",
      "Epoch 46/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4660 - val_loss: 0.5075\n",
      "Epoch 47/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4634 - val_loss: 0.5060\n",
      "Epoch 48/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4609 - val_loss: 0.5045\n",
      "Epoch 49/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4584 - val_loss: 0.5030\n",
      "Epoch 50/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4560 - val_loss: 0.5016\n",
      "Epoch 51/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4536 - val_loss: 0.5003\n",
      "Epoch 52/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4513 - val_loss: 0.4990\n",
      "Epoch 53/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4491 - val_loss: 0.4977\n",
      "Epoch 54/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.4469 - val_loss: 0.4964\n",
      "Epoch 55/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.4447 - val_loss: 0.4953\n",
      "Epoch 56/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4426 - val_loss: 0.4942\n",
      "Epoch 57/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4405 - val_loss: 0.4931\n",
      "Epoch 58/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4385 - val_loss: 0.4920\n",
      "Epoch 59/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4365 - val_loss: 0.4910\n",
      "Epoch 60/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4346 - val_loss: 0.4900\n",
      "Epoch 61/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.4326 - val_loss: 0.4891\n",
      "Epoch 62/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4308 - val_loss: 0.4882\n",
      "Epoch 63/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.4289 - val_loss: 0.4873\n",
      "Epoch 64/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4271 - val_loss: 0.4864\n",
      "Epoch 65/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4253 - val_loss: 0.4856\n",
      "Epoch 66/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4236 - val_loss: 0.4848\n",
      "Epoch 67/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4219 - val_loss: 0.4840\n",
      "Epoch 68/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4202 - val_loss: 0.4833\n",
      "Epoch 69/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4186 - val_loss: 0.4826\n",
      "Epoch 70/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4170 - val_loss: 0.4820\n",
      "Epoch 71/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4154 - val_loss: 0.4814\n",
      "Epoch 72/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4138 - val_loss: 0.4807\n",
      "Epoch 73/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4123 - val_loss: 0.4801\n",
      "Epoch 74/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4108 - val_loss: 0.4795\n",
      "Epoch 75/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4093 - val_loss: 0.4790\n",
      "Epoch 76/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4078 - val_loss: 0.4785\n",
      "Epoch 77/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4064 - val_loss: 0.4780\n",
      "Epoch 78/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4050 - val_loss: 0.4775\n",
      "Epoch 79/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4036 - val_loss: 0.4770\n",
      "Epoch 80/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4022 - val_loss: 0.4766\n",
      "Epoch 81/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4009 - val_loss: 0.4761\n",
      "Epoch 82/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.3996 - val_loss: 0.4757\n",
      "Epoch 83/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.3983 - val_loss: 0.4753\n",
      "Epoch 84/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.3970 - val_loss: 0.4748\n",
      "Epoch 85/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.3957 - val_loss: 0.4746\n",
      "Epoch 86/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.3945 - val_loss: 0.4742\n",
      "Epoch 87/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.3933 - val_loss: 0.4738\n",
      "Epoch 88/100\n",
      "6090/6090 [==============================] - 1s 121us/sample - loss: 0.3921 - val_loss: 0.4736\n",
      "Epoch 89/100\n",
      "6090/6090 [==============================] - 1s 120us/sample - loss: 0.3909 - val_loss: 0.4732\n",
      "Epoch 90/100\n",
      "6090/6090 [==============================] - 1s 116us/sample - loss: 0.3897 - val_loss: 0.4730\n",
      "Epoch 91/100\n",
      "6090/6090 [==============================] - 1s 119us/sample - loss: 0.3886 - val_loss: 0.4727\n",
      "Epoch 92/100\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.3874 - val_loss: 0.4724\n",
      "Epoch 93/100\n",
      "6090/6090 [==============================] - 1s 116us/sample - loss: 0.3863 - val_loss: 0.4722\n",
      "Epoch 94/100\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.3852 - val_loss: 0.4719\n",
      "Epoch 95/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.3842 - val_loss: 0.4717\n",
      "Epoch 96/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.3831 - val_loss: 0.4715\n",
      "Epoch 97/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.3820 - val_loss: 0.4713\n",
      "Epoch 98/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.3810 - val_loss: 0.4711\n",
      "Epoch 99/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.3799 - val_loss: 0.4710\n",
      "Epoch 100/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.3789 - val_loss: 0.4709\n",
      "Predicting...\n",
      "(Took 68.269 sec)\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/100\n",
      "6090/6090 [==============================] - 1s 155us/sample - loss: 0.6857 - val_loss: 0.6819\n",
      "Epoch 2/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.6731 - val_loss: 0.6741\n",
      "Epoch 3/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.6634 - val_loss: 0.6677\n",
      "Epoch 4/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.6550 - val_loss: 0.6617\n",
      "Epoch 5/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.6471 - val_loss: 0.6560\n",
      "Epoch 6/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.6396 - val_loss: 0.6504\n",
      "Epoch 7/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.6323 - val_loss: 0.6450\n",
      "Epoch 8/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.6253 - val_loss: 0.6397\n",
      "Epoch 9/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.6185 - val_loss: 0.6346\n",
      "Epoch 10/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.6119 - val_loss: 0.6296\n",
      "Epoch 11/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.6055 - val_loss: 0.6248\n",
      "Epoch 12/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.5993 - val_loss: 0.6201\n",
      "Epoch 13/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.5932 - val_loss: 0.6156\n",
      "Epoch 14/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.5873 - val_loss: 0.6113\n",
      "Epoch 15/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.5815 - val_loss: 0.6070\n",
      "Epoch 16/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.5760 - val_loss: 0.6029\n",
      "Epoch 17/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.5705 - val_loss: 0.5990\n",
      "Epoch 18/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.5653 - val_loss: 0.5951\n",
      "Epoch 19/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.5601 - val_loss: 0.5914\n",
      "Epoch 20/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.5551 - val_loss: 0.5878\n",
      "Epoch 21/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.5503 - val_loss: 0.5842\n",
      "Epoch 22/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.5455 - val_loss: 0.5809\n",
      "Epoch 23/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.5409 - val_loss: 0.5776\n",
      "Epoch 24/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.5364 - val_loss: 0.5745\n",
      "Epoch 25/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.5320 - val_loss: 0.5714\n",
      "Epoch 26/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.5277 - val_loss: 0.5684\n",
      "Epoch 27/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.5236 - val_loss: 0.5656\n",
      "Epoch 28/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.5195 - val_loss: 0.5628\n",
      "Epoch 29/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.5156 - val_loss: 0.5601\n",
      "Epoch 30/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.5117 - val_loss: 0.5575\n",
      "Epoch 31/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.5080 - val_loss: 0.5550\n",
      "Epoch 32/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.5043 - val_loss: 0.5526\n",
      "Epoch 33/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.5007 - val_loss: 0.5502\n",
      "Epoch 34/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.4973 - val_loss: 0.5480\n",
      "Epoch 35/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.4939 - val_loss: 0.5457\n",
      "Epoch 36/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.4906 - val_loss: 0.5436\n",
      "Epoch 37/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.4873 - val_loss: 0.5415\n",
      "Epoch 38/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4841 - val_loss: 0.5395\n",
      "Epoch 39/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.4811 - val_loss: 0.5375\n",
      "Epoch 40/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.4781 - val_loss: 0.5357\n",
      "Epoch 41/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.4751 - val_loss: 0.5340\n",
      "Epoch 42/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.4722 - val_loss: 0.5322\n",
      "Epoch 43/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.4695 - val_loss: 0.5305\n",
      "Epoch 44/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.4667 - val_loss: 0.5289\n",
      "Epoch 45/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.4640 - val_loss: 0.5272\n",
      "Epoch 46/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4614 - val_loss: 0.5257\n",
      "Epoch 47/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4588 - val_loss: 0.5243\n",
      "Epoch 48/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4563 - val_loss: 0.5228\n",
      "Epoch 49/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4539 - val_loss: 0.5214\n",
      "Epoch 50/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4515 - val_loss: 0.5201\n",
      "Epoch 51/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.4492 - val_loss: 0.5188\n",
      "Epoch 52/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.4469 - val_loss: 0.5176\n",
      "Epoch 53/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4446 - val_loss: 0.5164\n",
      "Epoch 54/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4424 - val_loss: 0.5152\n",
      "Epoch 55/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.4402 - val_loss: 0.5140\n",
      "Epoch 56/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.4381 - val_loss: 0.5129\n",
      "Epoch 57/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4361 - val_loss: 0.5119\n",
      "Epoch 58/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.4340 - val_loss: 0.5109\n",
      "Epoch 59/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4321 - val_loss: 0.5099\n",
      "Epoch 60/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4301 - val_loss: 0.5090\n",
      "Epoch 61/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4282 - val_loss: 0.5081\n",
      "Epoch 62/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4263 - val_loss: 0.5071\n",
      "Epoch 63/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4245 - val_loss: 0.5063\n",
      "Epoch 64/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4227 - val_loss: 0.5055\n",
      "Epoch 65/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4210 - val_loss: 0.5047\n",
      "Epoch 66/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4192 - val_loss: 0.5040\n",
      "Epoch 67/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4175 - val_loss: 0.5033\n",
      "Epoch 68/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4159 - val_loss: 0.5025\n",
      "Epoch 69/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4142 - val_loss: 0.5018\n",
      "Epoch 70/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4126 - val_loss: 0.5011\n",
      "Epoch 71/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.4110 - val_loss: 0.5005\n",
      "Epoch 72/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4095 - val_loss: 0.4998\n",
      "Epoch 73/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.4080 - val_loss: 0.4993\n",
      "Epoch 74/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4065 - val_loss: 0.4987\n",
      "Epoch 75/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4050 - val_loss: 0.4981\n",
      "Epoch 76/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4035 - val_loss: 0.4976\n",
      "Epoch 77/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4021 - val_loss: 0.4971\n",
      "Epoch 78/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4007 - val_loss: 0.4966\n",
      "Epoch 79/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.3994 - val_loss: 0.4962\n",
      "Epoch 80/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.3980 - val_loss: 0.4957\n",
      "Epoch 81/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.3966 - val_loss: 0.4953\n",
      "Epoch 82/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.3953 - val_loss: 0.4949\n",
      "Epoch 83/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.3941 - val_loss: 0.4945\n",
      "Epoch 84/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.3928 - val_loss: 0.4941\n",
      "Epoch 85/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.3915 - val_loss: 0.4937\n",
      "Epoch 86/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.3903 - val_loss: 0.4933\n",
      "Epoch 87/100\n",
      "6090/6090 [==============================] - 1s 118us/sample - loss: 0.3891 - val_loss: 0.4930\n",
      "Epoch 88/100\n",
      "6090/6090 [==============================] - 1s 120us/sample - loss: 0.3879 - val_loss: 0.4927\n",
      "Epoch 89/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.3867 - val_loss: 0.4924\n",
      "Epoch 90/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.3855 - val_loss: 0.4921\n",
      "Epoch 91/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.3844 - val_loss: 0.4918\n",
      "Epoch 92/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.3833 - val_loss: 0.4915\n",
      "Epoch 93/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.3822 - val_loss: 0.4913\n",
      "Epoch 94/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.3811 - val_loss: 0.4911\n",
      "Epoch 95/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.3800 - val_loss: 0.4909\n",
      "Epoch 96/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.3789 - val_loss: 0.4906\n",
      "Epoch 97/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.3778 - val_loss: 0.4904\n",
      "Epoch 98/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.3769 - val_loss: 0.4903\n",
      "Epoch 99/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.3758 - val_loss: 0.4901\n",
      "Epoch 100/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.3748 - val_loss: 0.4899\n",
      "Predicting...\n",
      "(Took 66.810 sec)\n",
      "Train on 6091 samples, validate on 1522 samples\n",
      "Epoch 1/100\n",
      "6091/6091 [==============================] - 1s 151us/sample - loss: 0.6861 - val_loss: 0.6796\n",
      "Epoch 2/100\n",
      "6091/6091 [==============================] - 1s 115us/sample - loss: 0.6741 - val_loss: 0.6706\n",
      "Epoch 3/100\n",
      "6091/6091 [==============================] - 1s 117us/sample - loss: 0.6648 - val_loss: 0.6631\n",
      "Epoch 4/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.6565 - val_loss: 0.6564\n",
      "Epoch 5/100\n",
      "6091/6091 [==============================] - 1s 110us/sample - loss: 0.6488 - val_loss: 0.6501\n",
      "Epoch 6/100\n",
      "6091/6091 [==============================] - 1s 111us/sample - loss: 0.6414 - val_loss: 0.6441\n",
      "Epoch 7/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6091/6091 [==============================] - 1s 111us/sample - loss: 0.6343 - val_loss: 0.6383\n",
      "Epoch 8/100\n",
      "6091/6091 [==============================] - 1s 110us/sample - loss: 0.6274 - val_loss: 0.6328\n",
      "Epoch 9/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.6206 - val_loss: 0.6275\n",
      "Epoch 10/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.6141 - val_loss: 0.6223\n",
      "Epoch 11/100\n",
      "6091/6091 [==============================] - 1s 106us/sample - loss: 0.6078 - val_loss: 0.6172\n",
      "Epoch 12/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.6016 - val_loss: 0.6123\n",
      "Epoch 13/100\n",
      "6091/6091 [==============================] - 1s 106us/sample - loss: 0.5956 - val_loss: 0.6076\n",
      "Epoch 14/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.5898 - val_loss: 0.6030\n",
      "Epoch 15/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.5842 - val_loss: 0.5986\n",
      "Epoch 16/100\n",
      "6091/6091 [==============================] - 1s 110us/sample - loss: 0.5787 - val_loss: 0.5943\n",
      "Epoch 17/100\n",
      "6091/6091 [==============================] - 1s 132us/sample - loss: 0.5733 - val_loss: 0.5902\n",
      "Epoch 18/100\n",
      "6091/6091 [==============================] - 1s 132us/sample - loss: 0.5680 - val_loss: 0.5861\n",
      "Epoch 19/100\n",
      "6091/6091 [==============================] - 1s 133us/sample - loss: 0.5630 - val_loss: 0.5822\n",
      "Epoch 20/100\n",
      "6091/6091 [==============================] - 1s 136us/sample - loss: 0.5580 - val_loss: 0.5784\n",
      "Epoch 21/100\n",
      "6091/6091 [==============================] - 1s 133us/sample - loss: 0.5532 - val_loss: 0.5747\n",
      "Epoch 22/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.5485 - val_loss: 0.5711\n",
      "Epoch 23/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.5440 - val_loss: 0.5677\n",
      "Epoch 24/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.5395 - val_loss: 0.5643\n",
      "Epoch 25/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.5352 - val_loss: 0.5611\n",
      "Epoch 26/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.5310 - val_loss: 0.5580\n",
      "Epoch 27/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.5268 - val_loss: 0.5549\n",
      "Epoch 28/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.5228 - val_loss: 0.5520\n",
      "Epoch 29/100\n",
      "6091/6091 [==============================] - 1s 113us/sample - loss: 0.5189 - val_loss: 0.5492\n",
      "Epoch 30/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.5151 - val_loss: 0.5464\n",
      "Epoch 31/100\n",
      "6091/6091 [==============================] - 1s 110us/sample - loss: 0.5114 - val_loss: 0.5437\n",
      "Epoch 32/100\n",
      "6091/6091 [==============================] - 1s 116us/sample - loss: 0.5078 - val_loss: 0.5411\n",
      "Epoch 33/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.5042 - val_loss: 0.5386\n",
      "Epoch 34/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.5008 - val_loss: 0.5361\n",
      "Epoch 35/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.4974 - val_loss: 0.5338\n",
      "Epoch 36/100\n",
      "6091/6091 [==============================] - 1s 110us/sample - loss: 0.4942 - val_loss: 0.5315\n",
      "Epoch 37/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4909 - val_loss: 0.5293\n",
      "Epoch 38/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.4878 - val_loss: 0.5271\n",
      "Epoch 39/100\n",
      "6091/6091 [==============================] - 1s 110us/sample - loss: 0.4848 - val_loss: 0.5250\n",
      "Epoch 40/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4818 - val_loss: 0.5230\n",
      "Epoch 41/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4789 - val_loss: 0.5210\n",
      "Epoch 42/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4760 - val_loss: 0.5191\n",
      "Epoch 43/100\n",
      "6091/6091 [==============================] - 1s 106us/sample - loss: 0.4732 - val_loss: 0.5173\n",
      "Epoch 44/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4705 - val_loss: 0.5155\n",
      "Epoch 45/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.4679 - val_loss: 0.5138\n",
      "Epoch 46/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.4653 - val_loss: 0.5121\n",
      "Epoch 47/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.4627 - val_loss: 0.5104\n",
      "Epoch 48/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.4603 - val_loss: 0.5089\n",
      "Epoch 49/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.4578 - val_loss: 0.5074\n",
      "Epoch 50/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.4555 - val_loss: 0.5059\n",
      "Epoch 51/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4531 - val_loss: 0.5045\n",
      "Epoch 52/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.4509 - val_loss: 0.5031\n",
      "Epoch 53/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4486 - val_loss: 0.5018\n",
      "Epoch 54/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.4464 - val_loss: 0.5005\n",
      "Epoch 55/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.4443 - val_loss: 0.4992\n",
      "Epoch 56/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.4422 - val_loss: 0.4980\n",
      "Epoch 57/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.4402 - val_loss: 0.4968\n",
      "Epoch 58/100\n",
      "6091/6091 [==============================] - 1s 106us/sample - loss: 0.4382 - val_loss: 0.4957\n",
      "Epoch 59/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4362 - val_loss: 0.4946\n",
      "Epoch 60/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.4342 - val_loss: 0.4935\n",
      "Epoch 61/100\n",
      "6091/6091 [==============================] - 1s 106us/sample - loss: 0.4323 - val_loss: 0.4925\n",
      "Epoch 62/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.4305 - val_loss: 0.4915\n",
      "Epoch 63/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.4287 - val_loss: 0.4906\n",
      "Epoch 64/100\n",
      "6091/6091 [==============================] - 1s 106us/sample - loss: 0.4269 - val_loss: 0.4895\n",
      "Epoch 65/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4252 - val_loss: 0.4887\n",
      "Epoch 66/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.4234 - val_loss: 0.4878\n",
      "Epoch 67/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.4217 - val_loss: 0.4870\n",
      "Epoch 68/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.4201 - val_loss: 0.4861\n",
      "Epoch 69/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.4184 - val_loss: 0.4854\n",
      "Epoch 70/100\n",
      "6091/6091 [==============================] - 1s 106us/sample - loss: 0.4169 - val_loss: 0.4845\n",
      "Epoch 71/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.4153 - val_loss: 0.4838\n",
      "Epoch 72/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4137 - val_loss: 0.4831\n",
      "Epoch 73/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.4123 - val_loss: 0.4824\n",
      "Epoch 74/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.4107 - val_loss: 0.4817\n",
      "Epoch 75/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4093 - val_loss: 0.4811\n",
      "Epoch 76/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.4078 - val_loss: 0.4805\n",
      "Epoch 77/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.4064 - val_loss: 0.4798\n",
      "Epoch 78/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4050 - val_loss: 0.4792\n",
      "Epoch 79/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4037 - val_loss: 0.4787\n",
      "Epoch 80/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4023 - val_loss: 0.4781\n",
      "Epoch 81/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.4010 - val_loss: 0.4775\n",
      "Epoch 82/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.3997 - val_loss: 0.4771\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.3984 - val_loss: 0.4766\n",
      "Epoch 84/100\n",
      "6091/6091 [==============================] - 1s 120us/sample - loss: 0.3971 - val_loss: 0.4761\n",
      "Epoch 85/100\n",
      "6091/6091 [==============================] - 1s 119us/sample - loss: 0.3959 - val_loss: 0.4757\n",
      "Epoch 86/100\n",
      "6091/6091 [==============================] - 1s 116us/sample - loss: 0.3947 - val_loss: 0.4752\n",
      "Epoch 87/100\n",
      "6091/6091 [==============================] - 1s 113us/sample - loss: 0.3935 - val_loss: 0.4747\n",
      "Epoch 88/100\n",
      "6091/6091 [==============================] - 1s 114us/sample - loss: 0.3923 - val_loss: 0.4743\n",
      "Epoch 89/100\n",
      "6091/6091 [==============================] - 1s 114us/sample - loss: 0.3911 - val_loss: 0.4739\n",
      "Epoch 90/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.3899 - val_loss: 0.4735\n",
      "Epoch 91/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.3888 - val_loss: 0.4732\n",
      "Epoch 92/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.3877 - val_loss: 0.4728\n",
      "Epoch 93/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.3866 - val_loss: 0.4725\n",
      "Epoch 94/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.3855 - val_loss: 0.4721\n",
      "Epoch 95/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.3844 - val_loss: 0.4718\n",
      "Epoch 96/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.3833 - val_loss: 0.4715\n",
      "Epoch 97/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.3823 - val_loss: 0.4712\n",
      "Epoch 98/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.3813 - val_loss: 0.4710\n",
      "Epoch 99/100\n",
      "6091/6091 [==============================] - ETA: 0s - loss: 0.381 - 1s 107us/sample - loss: 0.3802 - val_loss: 0.4708\n",
      "Epoch 100/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.3792 - val_loss: 0.4704\n",
      "Predicting...\n",
      "(Took 67.763 sec)\n",
      "Train on 6091 samples, validate on 1522 samples\n",
      "Epoch 1/100\n",
      "6091/6091 [==============================] - 1s 153us/sample - loss: 0.6860 - val_loss: 0.6816\n",
      "Epoch 2/100\n",
      "6091/6091 [==============================] - 1s 111us/sample - loss: 0.6735 - val_loss: 0.6734\n",
      "Epoch 3/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.6637 - val_loss: 0.6666\n",
      "Epoch 4/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.6553 - val_loss: 0.6604\n",
      "Epoch 5/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.6475 - val_loss: 0.6545\n",
      "Epoch 6/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.6400 - val_loss: 0.6487\n",
      "Epoch 7/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.6328 - val_loss: 0.6431\n",
      "Epoch 8/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.6259 - val_loss: 0.6375\n",
      "Epoch 9/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.6192 - val_loss: 0.6322\n",
      "Epoch 10/100\n",
      "6091/6091 [==============================] - 1s 111us/sample - loss: 0.6126 - val_loss: 0.6271\n",
      "Epoch 11/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.6062 - val_loss: 0.6221\n",
      "Epoch 12/100\n",
      "6091/6091 [==============================] - 1s 110us/sample - loss: 0.6000 - val_loss: 0.6172\n",
      "Epoch 13/100\n",
      "6091/6091 [==============================] - 1s 110us/sample - loss: 0.5940 - val_loss: 0.6126\n",
      "Epoch 14/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.5881 - val_loss: 0.6081\n",
      "Epoch 15/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.5824 - val_loss: 0.6037\n",
      "Epoch 16/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.5769 - val_loss: 0.5995\n",
      "Epoch 17/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.5715 - val_loss: 0.5954\n",
      "Epoch 18/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.5662 - val_loss: 0.5915\n",
      "Epoch 19/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.5611 - val_loss: 0.5876\n",
      "Epoch 20/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.5561 - val_loss: 0.5840\n",
      "Epoch 21/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.5513 - val_loss: 0.5803\n",
      "Epoch 22/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.5466 - val_loss: 0.5768\n",
      "Epoch 23/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.5420 - val_loss: 0.5735\n",
      "Epoch 24/100\n",
      "6091/6091 [==============================] - 1s 111us/sample - loss: 0.5375 - val_loss: 0.5702\n",
      "Epoch 25/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.5331 - val_loss: 0.5671\n",
      "Epoch 26/100\n",
      "6091/6091 [==============================] - 1s 110us/sample - loss: 0.5289 - val_loss: 0.5640\n",
      "Epoch 27/100\n",
      "6091/6091 [==============================] - 1s 111us/sample - loss: 0.5247 - val_loss: 0.5610\n",
      "Epoch 28/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.5207 - val_loss: 0.5582\n",
      "Epoch 29/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.5168 - val_loss: 0.5554\n",
      "Epoch 30/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.5129 - val_loss: 0.5527\n",
      "Epoch 31/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.5091 - val_loss: 0.5501\n",
      "Epoch 32/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.5055 - val_loss: 0.5476\n",
      "Epoch 33/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.5019 - val_loss: 0.5452\n",
      "Epoch 34/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4984 - val_loss: 0.5428\n",
      "Epoch 35/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.4950 - val_loss: 0.5406\n",
      "Epoch 36/100\n",
      "6091/6091 [==============================] - 1s 110us/sample - loss: 0.4917 - val_loss: 0.5384\n",
      "Epoch 37/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4885 - val_loss: 0.5363\n",
      "Epoch 38/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4853 - val_loss: 0.5342\n",
      "Epoch 39/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4822 - val_loss: 0.5322\n",
      "Epoch 40/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.4792 - val_loss: 0.5303\n",
      "Epoch 41/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4763 - val_loss: 0.5284\n",
      "Epoch 42/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4734 - val_loss: 0.5266\n",
      "Epoch 43/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4706 - val_loss: 0.5248\n",
      "Epoch 44/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4679 - val_loss: 0.5232\n",
      "Epoch 45/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4652 - val_loss: 0.5215\n",
      "Epoch 46/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.4625 - val_loss: 0.5200\n",
      "Epoch 47/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.4600 - val_loss: 0.5185\n",
      "Epoch 48/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4575 - val_loss: 0.5171\n",
      "Epoch 49/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4550 - val_loss: 0.5156\n",
      "Epoch 50/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4526 - val_loss: 0.5142\n",
      "Epoch 51/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.4502 - val_loss: 0.5129\n",
      "Epoch 52/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4479 - val_loss: 0.5116\n",
      "Epoch 53/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4457 - val_loss: 0.5103\n",
      "Epoch 54/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.4435 - val_loss: 0.5092\n",
      "Epoch 55/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.4413 - val_loss: 0.5080\n",
      "Epoch 56/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4392 - val_loss: 0.5069\n",
      "Epoch 57/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.4371 - val_loss: 0.5058\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.4351 - val_loss: 0.5048\n",
      "Epoch 59/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4331 - val_loss: 0.5038\n",
      "Epoch 60/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.4311 - val_loss: 0.5028\n",
      "Epoch 61/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4292 - val_loss: 0.5019\n",
      "Epoch 62/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4274 - val_loss: 0.5010\n",
      "Epoch 63/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4255 - val_loss: 0.5001\n",
      "Epoch 64/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4237 - val_loss: 0.4993\n",
      "Epoch 65/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4219 - val_loss: 0.4985\n",
      "Epoch 66/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4202 - val_loss: 0.4977\n",
      "Epoch 67/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.4185 - val_loss: 0.4969\n",
      "Epoch 68/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4168 - val_loss: 0.4962\n",
      "Epoch 69/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4152 - val_loss: 0.4955\n",
      "Epoch 70/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4136 - val_loss: 0.4948\n",
      "Epoch 71/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4120 - val_loss: 0.4942\n",
      "Epoch 72/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.4104 - val_loss: 0.4936\n",
      "Epoch 73/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.4089 - val_loss: 0.4929\n",
      "Epoch 74/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.4074 - val_loss: 0.4924\n",
      "Epoch 75/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4059 - val_loss: 0.4918\n",
      "Epoch 76/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4044 - val_loss: 0.4913\n",
      "Epoch 77/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.4030 - val_loss: 0.4908\n",
      "Epoch 78/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.4016 - val_loss: 0.4903\n",
      "Epoch 79/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4002 - val_loss: 0.4897\n",
      "Epoch 80/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.3988 - val_loss: 0.4893\n",
      "Epoch 81/100\n",
      "6091/6091 [==============================] - 1s 115us/sample - loss: 0.3975 - val_loss: 0.4889\n",
      "Epoch 82/100\n",
      "6091/6091 [==============================] - 1s 126us/sample - loss: 0.3962 - val_loss: 0.4885\n",
      "Epoch 83/100\n",
      "6091/6091 [==============================] - 1s 128us/sample - loss: 0.3949 - val_loss: 0.4880\n",
      "Epoch 84/100\n",
      "6091/6091 [==============================] - 1s 117us/sample - loss: 0.3936 - val_loss: 0.4877\n",
      "Epoch 85/100\n",
      "6091/6091 [==============================] - 1s 116us/sample - loss: 0.3923 - val_loss: 0.4873\n",
      "Epoch 86/100\n",
      "6091/6091 [==============================] - 1s 126us/sample - loss: 0.3911 - val_loss: 0.4869\n",
      "Epoch 87/100\n",
      "6091/6091 [==============================] - 1s 119us/sample - loss: 0.3899 - val_loss: 0.4866\n",
      "Epoch 88/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.3887 - val_loss: 0.4862\n",
      "Epoch 89/100\n",
      "6091/6091 [==============================] - 1s 106us/sample - loss: 0.3875 - val_loss: 0.4859\n",
      "Epoch 90/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.3863 - val_loss: 0.4856\n",
      "Epoch 91/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.3852 - val_loss: 0.4853\n",
      "Epoch 92/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.3840 - val_loss: 0.4850\n",
      "Epoch 93/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.3829 - val_loss: 0.4848\n",
      "Epoch 94/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.3818 - val_loss: 0.4845\n",
      "Epoch 95/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.3807 - val_loss: 0.4843\n",
      "Epoch 96/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.3797 - val_loss: 0.4841\n",
      "Epoch 97/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.3786 - val_loss: 0.4839\n",
      "Epoch 98/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.3775 - val_loss: 0.4837\n",
      "Epoch 99/100\n",
      "6091/6091 [==============================] - 1s 111us/sample - loss: 0.3765 - val_loss: 0.4835\n",
      "Epoch 100/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.3755 - val_loss: 0.4833\n",
      "Predicting...\n",
      "(Took 67.372 sec)\n",
      "Combined confusion matrix:\n",
      "[[3738.  604.]\n",
      " [1016. 2255.]]\n",
      "(Overall, took 338.339 sec)\n",
      "Accuracy: 78.72% +/- 0.39%\n",
      "Precision for positive class: 78.62% +/- 1.19%\n",
      "Precision for negative class: 78.87% +/- 1.31%\n",
      "Recall for positive class: 86.08% +/- 1.11%\n",
      "Recall for negative class: 68.93% +/- 1.53%\n",
      "F for positive class: 82.18% +/- 0.74%\n",
      "F for negative class: 73.56% +/- 0.82%\n",
      "Mean F score: 77.87% +/- 0.30%\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/100\n",
      "6090/6090 [==============================] - 1s 152us/sample - loss: 0.6815 - val_loss: 0.6699\n",
      "Epoch 2/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.6591 - val_loss: 0.6527\n",
      "Epoch 3/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.6400 - val_loss: 0.6382\n",
      "Epoch 4/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.6228 - val_loss: 0.6253\n",
      "Epoch 5/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.6071 - val_loss: 0.6136\n",
      "Epoch 6/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.5924 - val_loss: 0.6029\n",
      "Epoch 7/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.5787 - val_loss: 0.5931\n",
      "Epoch 8/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.5659 - val_loss: 0.5841\n",
      "Epoch 9/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.5538 - val_loss: 0.5757\n",
      "Epoch 10/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.5425 - val_loss: 0.5680\n",
      "Epoch 11/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.5319 - val_loss: 0.5608\n",
      "Epoch 12/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.5218 - val_loss: 0.5542\n",
      "Epoch 13/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.5124 - val_loss: 0.5481\n",
      "Epoch 14/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.5035 - val_loss: 0.5425\n",
      "Epoch 15/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4950 - val_loss: 0.5372\n",
      "Epoch 16/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4870 - val_loss: 0.5324\n",
      "Epoch 17/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4795 - val_loss: 0.5280\n",
      "Epoch 18/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4723 - val_loss: 0.5239\n",
      "Epoch 19/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4655 - val_loss: 0.5200\n",
      "Epoch 20/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4590 - val_loss: 0.5165\n",
      "Epoch 21/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4529 - val_loss: 0.5134\n",
      "Epoch 22/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.4470 - val_loss: 0.5104\n",
      "Epoch 23/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4414 - val_loss: 0.5077\n",
      "Epoch 24/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4361 - val_loss: 0.5051\n",
      "Epoch 25/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4310 - val_loss: 0.5028\n",
      "Epoch 26/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4261 - val_loss: 0.5007\n",
      "Epoch 27/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4215 - val_loss: 0.4988\n",
      "Epoch 28/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4170 - val_loss: 0.4970\n",
      "Epoch 29/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4127 - val_loss: 0.4954\n",
      "Epoch 30/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4086 - val_loss: 0.4940\n",
      "Epoch 31/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4047 - val_loss: 0.4927\n",
      "Epoch 32/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4009 - val_loss: 0.4915\n",
      "Epoch 33/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.3973 - val_loss: 0.4904\n",
      "Epoch 34/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.3938 - val_loss: 0.4895\n",
      "Epoch 35/100\n",
      "6090/6090 [==============================] - 1s 132us/sample - loss: 0.3904 - val_loss: 0.4886\n",
      "Epoch 36/100\n",
      "6090/6090 [==============================] - 1s 125us/sample - loss: 0.3871 - val_loss: 0.4880\n",
      "Epoch 37/100\n",
      "6090/6090 [==============================] - 1s 124us/sample - loss: 0.3840 - val_loss: 0.4874\n",
      "Epoch 38/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.3810 - val_loss: 0.4868\n",
      "Epoch 39/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.3781 - val_loss: 0.4864\n",
      "Epoch 40/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.3752 - val_loss: 0.4860\n",
      "Epoch 41/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.3725 - val_loss: 0.4857\n",
      "Epoch 42/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.3699 - val_loss: 0.4855\n",
      "Epoch 43/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.3673 - val_loss: 0.4853\n",
      "Epoch 44/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.3648 - val_loss: 0.4852\n",
      "Epoch 45/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.3624 - val_loss: 0.4852\n",
      "Epoch 46/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.3601 - val_loss: 0.4853\n",
      "Predicting...\n",
      "(Took 31.368 sec)\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/100\n",
      "6090/6090 [==============================] - 1s 152us/sample - loss: 0.6824 - val_loss: 0.6689\n",
      "Epoch 2/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.6602 - val_loss: 0.6508\n",
      "Epoch 3/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.6415 - val_loss: 0.6355\n",
      "Epoch 4/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.6245 - val_loss: 0.6220\n",
      "Epoch 5/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.6090 - val_loss: 0.6100\n",
      "Epoch 6/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.5945 - val_loss: 0.5990\n",
      "Epoch 7/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.5809 - val_loss: 0.5888\n",
      "Epoch 8/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.5682 - val_loss: 0.5793\n",
      "Epoch 9/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.5563 - val_loss: 0.5707\n",
      "Epoch 10/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.5451 - val_loss: 0.5626\n",
      "Epoch 11/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.5346 - val_loss: 0.5552\n",
      "Epoch 12/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.5247 - val_loss: 0.5483\n",
      "Epoch 13/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.5153 - val_loss: 0.5420\n",
      "Epoch 14/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.5064 - val_loss: 0.5360\n",
      "Epoch 15/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4981 - val_loss: 0.5306\n",
      "Epoch 16/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4902 - val_loss: 0.5255\n",
      "Epoch 17/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4827 - val_loss: 0.5207\n",
      "Epoch 18/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4756 - val_loss: 0.5163\n",
      "Epoch 19/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4688 - val_loss: 0.5122\n",
      "Epoch 20/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4624 - val_loss: 0.5085\n",
      "Epoch 21/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4564 - val_loss: 0.5050\n",
      "Epoch 22/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4505 - val_loss: 0.5018\n",
      "Epoch 23/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4450 - val_loss: 0.4989\n",
      "Epoch 24/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4397 - val_loss: 0.4962\n",
      "Epoch 25/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.4347 - val_loss: 0.4936\n",
      "Epoch 26/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4299 - val_loss: 0.4914\n",
      "Epoch 27/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4253 - val_loss: 0.4892\n",
      "Epoch 28/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4209 - val_loss: 0.4872\n",
      "Epoch 29/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4167 - val_loss: 0.4854\n",
      "Epoch 30/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4126 - val_loss: 0.4837\n",
      "Epoch 31/100\n",
      "6090/6090 [==============================] - 1s 119us/sample - loss: 0.4087 - val_loss: 0.4823\n",
      "Epoch 32/100\n",
      "6090/6090 [==============================] - 1s 119us/sample - loss: 0.4049 - val_loss: 0.4808\n",
      "Epoch 33/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.4013 - val_loss: 0.4796\n",
      "Epoch 34/100\n",
      "6090/6090 [==============================] - 1s 122us/sample - loss: 0.3979 - val_loss: 0.4786\n",
      "Epoch 35/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.3945 - val_loss: 0.4776\n",
      "Epoch 36/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.3913 - val_loss: 0.4767\n",
      "Epoch 37/100\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.3882 - val_loss: 0.4759\n",
      "Epoch 38/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.3852 - val_loss: 0.4751\n",
      "Epoch 39/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.3823 - val_loss: 0.4746\n",
      "Epoch 40/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.3795 - val_loss: 0.4740\n",
      "Epoch 41/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.3768 - val_loss: 0.4736\n",
      "Epoch 42/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.3742 - val_loss: 0.4732\n",
      "Epoch 43/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.3716 - val_loss: 0.4729\n",
      "Epoch 44/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.3692 - val_loss: 0.4726\n",
      "Epoch 45/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.3668 - val_loss: 0.4724\n",
      "Epoch 46/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.3645 - val_loss: 0.4724\n",
      "Epoch 47/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.3622 - val_loss: 0.4723\n",
      "Epoch 48/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.3600 - val_loss: 0.4723\n",
      "Epoch 49/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.3579 - val_loss: 0.4723\n",
      "Predicting...\n",
      "(Took 33.340 sec)\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/100\n",
      "6090/6090 [==============================] - 1s 154us/sample - loss: 0.6817 - val_loss: 0.6736\n",
      "Epoch 2/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.6582 - val_loss: 0.6582\n",
      "Epoch 3/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.6387 - val_loss: 0.6450\n",
      "Epoch 4/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.6215 - val_loss: 0.6328\n",
      "Epoch 5/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.6058 - val_loss: 0.6216\n",
      "Epoch 6/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.5911 - val_loss: 0.6114\n",
      "Epoch 7/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.5775 - val_loss: 0.6017\n",
      "Epoch 8/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.5646 - val_loss: 0.5927\n",
      "Epoch 9/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.5526 - val_loss: 0.5845\n",
      "Epoch 10/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.5413 - val_loss: 0.5768\n",
      "Epoch 11/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.5307 - val_loss: 0.5696\n",
      "Epoch 12/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.5207 - val_loss: 0.5630\n",
      "Epoch 13/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.5113 - val_loss: 0.5569\n",
      "Epoch 14/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.5025 - val_loss: 0.5512\n",
      "Epoch 15/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4941 - val_loss: 0.5460\n",
      "Epoch 16/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4862 - val_loss: 0.5411\n",
      "Epoch 17/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4787 - val_loss: 0.5366\n",
      "Epoch 18/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4716 - val_loss: 0.5324\n",
      "Epoch 19/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.4649 - val_loss: 0.5284\n",
      "Epoch 20/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4585 - val_loss: 0.5249\n",
      "Epoch 21/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4524 - val_loss: 0.5216\n",
      "Epoch 22/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4467 - val_loss: 0.5186\n",
      "Epoch 23/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4411 - val_loss: 0.5157\n",
      "Epoch 24/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4359 - val_loss: 0.5131\n",
      "Epoch 25/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4308 - val_loss: 0.5105\n",
      "Epoch 26/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4260 - val_loss: 0.5083\n",
      "Epoch 27/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4215 - val_loss: 0.5062\n",
      "Epoch 28/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4170 - val_loss: 0.5043\n",
      "Epoch 29/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4128 - val_loss: 0.5025\n",
      "Epoch 30/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4088 - val_loss: 0.5009\n",
      "Epoch 31/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4049 - val_loss: 0.4995\n",
      "Epoch 32/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4012 - val_loss: 0.4981\n",
      "Epoch 33/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.3976 - val_loss: 0.4969\n",
      "Epoch 34/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.3941 - val_loss: 0.4958\n",
      "Epoch 35/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.3908 - val_loss: 0.4948\n",
      "Epoch 36/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.3876 - val_loss: 0.4939\n",
      "Epoch 37/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.3845 - val_loss: 0.4930\n",
      "Epoch 38/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.3815 - val_loss: 0.4923\n",
      "Epoch 39/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.3786 - val_loss: 0.4917\n",
      "Epoch 40/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.3758 - val_loss: 0.4911\n",
      "Epoch 41/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.3731 - val_loss: 0.4905\n",
      "Epoch 42/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.3705 - val_loss: 0.4902\n",
      "Epoch 43/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.3680 - val_loss: 0.4899\n",
      "Epoch 44/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.3655 - val_loss: 0.4896\n",
      "Epoch 45/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.3631 - val_loss: 0.4894\n",
      "Epoch 46/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.3608 - val_loss: 0.4891\n",
      "Epoch 47/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.3586 - val_loss: 0.4891\n",
      "Epoch 48/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.3564 - val_loss: 0.4890\n",
      "Epoch 49/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.3542 - val_loss: 0.4891\n",
      "Predicting...\n",
      "(Took 33.059 sec)\n",
      "Train on 6091 samples, validate on 1522 samples\n",
      "Epoch 1/100\n",
      "6091/6091 [==============================] - 1s 156us/sample - loss: 0.6826 - val_loss: 0.6717\n",
      "Epoch 2/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.6596 - val_loss: 0.6541\n",
      "Epoch 3/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.6404 - val_loss: 0.6395\n",
      "Epoch 4/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.6235 - val_loss: 0.6264\n",
      "Epoch 5/100\n",
      "6091/6091 [==============================] - 1s 106us/sample - loss: 0.6079 - val_loss: 0.6145\n",
      "Epoch 6/100\n",
      "6091/6091 [==============================] - 1s 106us/sample - loss: 0.5935 - val_loss: 0.6036\n",
      "Epoch 7/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.5799 - val_loss: 0.5934\n",
      "Epoch 8/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.5673 - val_loss: 0.5840\n",
      "Epoch 9/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.5553 - val_loss: 0.5753\n",
      "Epoch 10/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.5442 - val_loss: 0.5671\n",
      "Epoch 11/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.5337 - val_loss: 0.5596\n",
      "Epoch 12/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.5238 - val_loss: 0.5526\n",
      "Epoch 13/100\n",
      "6091/6091 [==============================] - 1s 119us/sample - loss: 0.5145 - val_loss: 0.5461\n",
      "Epoch 14/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.5057 - val_loss: 0.5401\n",
      "Epoch 15/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.4974 - val_loss: 0.5345\n",
      "Epoch 16/100\n",
      "6091/6091 [==============================] - 1s 114us/sample - loss: 0.4895 - val_loss: 0.5293\n",
      "Epoch 17/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.4821 - val_loss: 0.5245\n",
      "Epoch 18/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4751 - val_loss: 0.5200\n",
      "Epoch 19/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.4684 - val_loss: 0.5158\n",
      "Epoch 20/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4620 - val_loss: 0.5119\n",
      "Epoch 21/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.4560 - val_loss: 0.5082\n",
      "Epoch 22/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.4502 - val_loss: 0.5049\n",
      "Epoch 23/100\n",
      "6091/6091 [==============================] - 1s 106us/sample - loss: 0.4447 - val_loss: 0.5018\n",
      "Epoch 24/100\n",
      "6091/6091 [==============================] - 1s 106us/sample - loss: 0.4395 - val_loss: 0.4988\n",
      "Epoch 25/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.4345 - val_loss: 0.4961\n",
      "Epoch 26/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4298 - val_loss: 0.4937\n",
      "Epoch 27/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4252 - val_loss: 0.4914\n",
      "Epoch 28/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4208 - val_loss: 0.4892\n",
      "Epoch 29/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4166 - val_loss: 0.4872\n",
      "Epoch 30/100\n",
      "6091/6091 [==============================] - 1s 123us/sample - loss: 0.4126 - val_loss: 0.4854\n",
      "Epoch 31/100\n",
      "6091/6091 [==============================] - 1s 118us/sample - loss: 0.4087 - val_loss: 0.4836\n",
      "Epoch 32/100\n",
      "6091/6091 [==============================] - 1s 121us/sample - loss: 0.4050 - val_loss: 0.4820\n",
      "Epoch 33/100\n",
      "6091/6091 [==============================] - 1s 115us/sample - loss: 0.4014 - val_loss: 0.4806\n",
      "Epoch 34/100\n",
      "6091/6091 [==============================] - 1s 115us/sample - loss: 0.3980 - val_loss: 0.4792\n",
      "Epoch 35/100\n",
      "6091/6091 [==============================] - 1s 116us/sample - loss: 0.3947 - val_loss: 0.4780\n",
      "Epoch 36/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.3915 - val_loss: 0.4769\n",
      "Epoch 37/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.3884 - val_loss: 0.4758\n",
      "Epoch 38/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.3854 - val_loss: 0.4750\n",
      "Epoch 39/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.3826 - val_loss: 0.4741\n",
      "Epoch 40/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.3798 - val_loss: 0.4734\n",
      "Epoch 41/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.3771 - val_loss: 0.4726\n",
      "Epoch 42/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.3745 - val_loss: 0.4720\n",
      "Epoch 43/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.3720 - val_loss: 0.4715\n",
      "Epoch 44/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.3695 - val_loss: 0.4710\n",
      "Epoch 45/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.3671 - val_loss: 0.4706\n",
      "Epoch 46/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.3648 - val_loss: 0.4703\n",
      "Epoch 47/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.3626 - val_loss: 0.4701\n",
      "Epoch 48/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.3604 - val_loss: 0.4698\n",
      "Epoch 49/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.3583 - val_loss: 0.4695\n",
      "Epoch 50/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.3562 - val_loss: 0.4695\n",
      "Epoch 51/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.3542 - val_loss: 0.4695\n",
      "Epoch 52/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.3523 - val_loss: 0.4693\n",
      "Epoch 53/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.3504 - val_loss: 0.4694\n",
      "Predicting...\n",
      "(Took 35.983 sec)\n",
      "Train on 6091 samples, validate on 1522 samples\n",
      "Epoch 1/100\n",
      "6091/6091 [==============================] - 1s 158us/sample - loss: 0.6828 - val_loss: 0.6733\n",
      "Epoch 2/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.6595 - val_loss: 0.6572\n",
      "Epoch 3/100\n",
      "6091/6091 [==============================] - 1s 106us/sample - loss: 0.6402 - val_loss: 0.6436\n",
      "Epoch 4/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.6232 - val_loss: 0.6311\n",
      "Epoch 5/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.6074 - val_loss: 0.6195\n",
      "Epoch 6/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.5929 - val_loss: 0.6088\n",
      "Epoch 7/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.5792 - val_loss: 0.5989\n",
      "Epoch 8/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.5664 - val_loss: 0.5897\n",
      "Epoch 9/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.5544 - val_loss: 0.5811\n",
      "Epoch 10/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.5431 - val_loss: 0.5732\n",
      "Epoch 11/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.5325 - val_loss: 0.5659\n",
      "Epoch 12/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.5224 - val_loss: 0.5591\n",
      "Epoch 13/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.5130 - val_loss: 0.5528\n",
      "Epoch 14/100\n",
      "6091/6091 [==============================] - 1s 111us/sample - loss: 0.5041 - val_loss: 0.5468\n",
      "Epoch 15/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.4957 - val_loss: 0.5415\n",
      "Epoch 16/100\n",
      "6091/6091 [==============================] - 1s 111us/sample - loss: 0.4877 - val_loss: 0.5364\n",
      "Epoch 17/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.4802 - val_loss: 0.5317\n",
      "Epoch 18/100\n",
      "6091/6091 [==============================] - 1s 110us/sample - loss: 0.4730 - val_loss: 0.5276\n",
      "Epoch 19/100\n",
      "6091/6091 [==============================] - 1s 114us/sample - loss: 0.4663 - val_loss: 0.5236\n",
      "Epoch 20/100\n",
      "6091/6091 [==============================] - 1s 111us/sample - loss: 0.4598 - val_loss: 0.5199\n",
      "Epoch 21/100\n",
      "6091/6091 [==============================] - 1s 110us/sample - loss: 0.4537 - val_loss: 0.5165\n",
      "Epoch 22/100\n",
      "6091/6091 [==============================] - 1s 113us/sample - loss: 0.4478 - val_loss: 0.5134\n",
      "Epoch 23/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.4423 - val_loss: 0.5105\n",
      "Epoch 24/100\n",
      "6091/6091 [==============================] - 1s 111us/sample - loss: 0.4369 - val_loss: 0.5077\n",
      "Epoch 25/100\n",
      "6091/6091 [==============================] - 1s 110us/sample - loss: 0.4319 - val_loss: 0.5052\n",
      "Epoch 26/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.4270 - val_loss: 0.5030\n",
      "Epoch 27/100\n",
      "6091/6091 [==============================] - 1s 110us/sample - loss: 0.4224 - val_loss: 0.5008\n",
      "Epoch 28/100\n",
      "6091/6091 [==============================] - 1s 111us/sample - loss: 0.4180 - val_loss: 0.4990\n",
      "Epoch 29/100\n",
      "6091/6091 [==============================] - 1s 110us/sample - loss: 0.4137 - val_loss: 0.4972\n",
      "Epoch 30/100\n",
      "6091/6091 [==============================] - 1s 110us/sample - loss: 0.4096 - val_loss: 0.4955\n",
      "Epoch 31/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.4057 - val_loss: 0.4940\n",
      "Epoch 32/100\n",
      "6091/6091 [==============================] - 1s 111us/sample - loss: 0.4019 - val_loss: 0.4927\n",
      "Epoch 33/100\n",
      "6091/6091 [==============================] - 1s 110us/sample - loss: 0.3983 - val_loss: 0.4914\n",
      "Epoch 34/100\n",
      "6091/6091 [==============================] - 1s 110us/sample - loss: 0.3948 - val_loss: 0.4903\n",
      "Epoch 35/100\n",
      "6091/6091 [==============================] - 1s 111us/sample - loss: 0.3915 - val_loss: 0.4893\n",
      "Epoch 36/100\n",
      "6091/6091 [==============================] - 1s 110us/sample - loss: 0.3882 - val_loss: 0.4885\n",
      "Epoch 37/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.3851 - val_loss: 0.4876\n",
      "Epoch 38/100\n",
      "6091/6091 [==============================] - 1s 116us/sample - loss: 0.3821 - val_loss: 0.4869\n",
      "Epoch 39/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.3792 - val_loss: 0.4863\n",
      "Epoch 40/100\n",
      "6091/6091 [==============================] - 1s 111us/sample - loss: 0.3764 - val_loss: 0.4858\n",
      "Epoch 41/100\n",
      "6091/6091 [==============================] - 1s 111us/sample - loss: 0.3736 - val_loss: 0.4853\n",
      "Epoch 42/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.3710 - val_loss: 0.4849\n",
      "Epoch 43/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.3684 - val_loss: 0.4846\n",
      "Epoch 44/100\n",
      "6091/6091 [==============================] - 1s 111us/sample - loss: 0.3660 - val_loss: 0.4843\n",
      "Epoch 45/100\n",
      "6091/6091 [==============================] - 1s 111us/sample - loss: 0.3636 - val_loss: 0.4841\n",
      "Epoch 46/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.3613 - val_loss: 0.4839\n",
      "Epoch 47/100\n",
      "6091/6091 [==============================] - 1s 111us/sample - loss: 0.3590 - val_loss: 0.4838\n",
      "Epoch 48/100\n",
      "6091/6091 [==============================] - 1s 106us/sample - loss: 0.3568 - val_loss: 0.4837\n",
      "Epoch 49/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.3546 - val_loss: 0.4838\n",
      "Predicting...\n",
      "(Took 33.591 sec)\n",
      "Combined confusion matrix:\n",
      "[[3694.  648.]\n",
      " [1002. 2269.]]\n",
      "(Overall, took 167.622 sec)\n",
      "Accuracy: 78.33% +/- 0.52%\n",
      "Precision for positive class: 78.67% +/- 0.92%\n",
      "Precision for negative class: 77.79% +/- 2.41%\n",
      "Recall for positive class: 85.08% +/- 1.64%\n",
      "Recall for negative class: 69.35% +/- 1.89%\n",
      "F for positive class: 81.74% +/- 0.51%\n",
      "F for negative class: 73.31% +/- 1.43%\n",
      "Mean F score: 77.52% +/- 0.70%\n"
     ]
    }
   ],
   "source": [
    "# Cross-validated logistic models (all have max epochs of 100, but early stopping if validation loss starts to increase)\n",
    "# No prep, raw counts\n",
    "all_metrics(cross_validate_logistic(dftrain_minimal_noprep, 100))\n",
    "# No prep, normalized\n",
    "all_metrics(cross_validate_logistic(dftrain_min_norm_noprep, 100))\n",
    "# No prep, TF-IDF\n",
    "all_metrics(cross_validate_logistic(dftrain_min_tfidf_noprep, 100))\n",
    "# Preprocessed, raw counts\n",
    "all_metrics(cross_validate_logistic(dftrain_minimal, 100))\n",
    "# Preprocessed, normalized\n",
    "all_metrics(cross_validate_logistic(dftrain_min_norm, 100))\n",
    "# Preprocessed, TF-IDF\n",
    "all_metrics(cross_validate_logistic(dftrain_min_tfidf, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/100\n",
      "6090/6090 [==============================] - 4s 607us/sample - loss: 0.6505 - val_loss: 0.6104\n",
      "Epoch 2/100\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.5842 - val_loss: 0.5677\n",
      "Epoch 3/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.5442 - val_loss: 0.5404\n",
      "Epoch 4/100\n",
      "6090/6090 [==============================] - 1s 96us/sample - loss: 0.5156 - val_loss: 0.5216\n",
      "Epoch 5/100\n",
      "6090/6090 [==============================] - 1s 97us/sample - loss: 0.4932 - val_loss: 0.5073\n",
      "Epoch 6/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.4747 - val_loss: 0.4963\n",
      "Epoch 7/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.4591 - val_loss: 0.4875\n",
      "Epoch 8/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4455 - val_loss: 0.4800\n",
      "Epoch 9/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.4335 - val_loss: 0.4742\n",
      "Epoch 10/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4226 - val_loss: 0.4688\n",
      "Epoch 11/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4129 - val_loss: 0.4647\n",
      "Epoch 12/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4040 - val_loss: 0.4609\n",
      "Epoch 13/100\n",
      "6090/6090 [==============================] - 1s 124us/sample - loss: 0.3958 - val_loss: 0.4577\n",
      "Epoch 14/100\n",
      "6090/6090 [==============================] - 1s 121us/sample - loss: 0.3882 - val_loss: 0.4549\n",
      "Epoch 15/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.3812 - val_loss: 0.4521\n",
      "Epoch 16/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.3746 - val_loss: 0.4501\n",
      "Epoch 17/100\n",
      "6090/6090 [==============================] - 1s 101us/sample - loss: 0.3683 - val_loss: 0.4484\n",
      "Epoch 18/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.3626 - val_loss: 0.4468\n",
      "Epoch 19/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.3570 - val_loss: 0.4457\n",
      "Epoch 20/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.3518 - val_loss: 0.4445\n",
      "Epoch 21/100\n",
      "6090/6090 [==============================] - 1s 99us/sample - loss: 0.3469 - val_loss: 0.4436\n",
      "Epoch 22/100\n",
      "6090/6090 [==============================] - 1s 118us/sample - loss: 0.3422 - val_loss: 0.4426\n",
      "Epoch 23/100\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.3376 - val_loss: 0.4421\n",
      "Epoch 24/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.3333 - val_loss: 0.4416\n",
      "Epoch 25/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.3292 - val_loss: 0.4412\n",
      "Epoch 26/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.3253 - val_loss: 0.4407\n",
      "Epoch 27/100\n",
      "6090/6090 [==============================] - 1s 121us/sample - loss: 0.3214 - val_loss: 0.4404\n",
      "Epoch 28/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.3178 - val_loss: 0.4404\n",
      "Epoch 29/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.3143 - val_loss: 0.4405\n",
      "Predicting...\n",
      "(Took 35.273 sec)\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.6470 - val_loss: 0.6115\n",
      "Epoch 2/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.5824 - val_loss: 0.5737\n",
      "Epoch 3/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.5423 - val_loss: 0.5502\n",
      "Epoch 4/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.5133 - val_loss: 0.5336\n",
      "Epoch 5/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.4909 - val_loss: 0.5210\n",
      "Epoch 6/100\n",
      "6090/6090 [==============================] - 1s 125us/sample - loss: 0.4722 - val_loss: 0.5112\n",
      "Epoch 7/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.4565 - val_loss: 0.5035\n",
      "Epoch 8/100\n",
      "6090/6090 [==============================] - 1s 122us/sample - loss: 0.4429 - val_loss: 0.4969\n",
      "Epoch 9/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.4308 - val_loss: 0.4917\n",
      "Epoch 10/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4200 - val_loss: 0.4873\n",
      "Epoch 11/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.4104 - val_loss: 0.4833\n",
      "Epoch 12/100\n",
      "6090/6090 [==============================] - 1s 117us/sample - loss: 0.4013 - val_loss: 0.4797\n",
      "Epoch 13/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.3932 - val_loss: 0.4770\n",
      "Epoch 14/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.3857 - val_loss: 0.4750\n",
      "Epoch 15/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.3787 - val_loss: 0.4728\n",
      "Epoch 16/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.3721 - val_loss: 0.4712\n",
      "Epoch 17/100\n",
      "6090/6090 [==============================] - 1s 99us/sample - loss: 0.3659 - val_loss: 0.4697\n",
      "Epoch 18/100\n",
      "6090/6090 [==============================] - 1s 101us/sample - loss: 0.3603 - val_loss: 0.4687\n",
      "Epoch 19/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.3547 - val_loss: 0.4678\n",
      "Epoch 20/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.3495 - val_loss: 0.4672\n",
      "Epoch 21/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.3447 - val_loss: 0.4669\n",
      "Epoch 22/100\n",
      "6090/6090 [==============================] - 1s 101us/sample - loss: 0.3399 - val_loss: 0.4665\n",
      "Epoch 23/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.3355 - val_loss: 0.4664\n",
      "Epoch 24/100\n",
      "6090/6090 [==============================] - 1s 124us/sample - loss: 0.3311 - val_loss: 0.4661\n",
      "Epoch 25/100\n",
      "6090/6090 [==============================] - 1s 116us/sample - loss: 0.3271 - val_loss: 0.4665\n",
      "Predicting...\n",
      "(Took 17.318 sec)\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/100\n",
      "6090/6090 [==============================] - 1s 145us/sample - loss: 0.6459 - val_loss: 0.6172\n",
      "Epoch 2/100\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.5808 - val_loss: 0.5788\n",
      "Epoch 3/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.5409 - val_loss: 0.5545\n",
      "Epoch 4/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.5122 - val_loss: 0.5376\n",
      "Epoch 5/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.4897 - val_loss: 0.5252\n",
      "Epoch 6/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4713 - val_loss: 0.5150\n",
      "Epoch 7/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4557 - val_loss: 0.5072\n",
      "Epoch 8/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.4421 - val_loss: 0.5003\n",
      "Epoch 9/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4300 - val_loss: 0.4947\n",
      "Epoch 10/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4193 - val_loss: 0.4904\n",
      "Epoch 11/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.4094 - val_loss: 0.4865\n",
      "Epoch 12/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.4005 - val_loss: 0.4833\n",
      "Epoch 13/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.3924 - val_loss: 0.4806\n",
      "Epoch 14/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.3847 - val_loss: 0.4787\n",
      "Epoch 15/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.3776 - val_loss: 0.4766\n",
      "Epoch 16/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.3709 - val_loss: 0.4750\n",
      "Epoch 17/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.3647 - val_loss: 0.4738\n",
      "Epoch 18/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.3589 - val_loss: 0.4730\n",
      "Epoch 19/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.3532 - val_loss: 0.4724\n",
      "Epoch 20/100\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.3480 - val_loss: 0.4719\n",
      "Epoch 21/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6090/6090 [==============================] - 1s 99us/sample - loss: 0.3429 - val_loss: 0.4712\n",
      "Epoch 22/100\n",
      "6090/6090 [==============================] - 1s 97us/sample - loss: 0.3382 - val_loss: 0.4719\n",
      "Predicting...\n",
      "(Took 15.178 sec)\n",
      "Train on 6091 samples, validate on 1522 samples\n",
      "Epoch 1/100\n",
      "6091/6091 [==============================] - 1s 131us/sample - loss: 0.6465 - val_loss: 0.6133\n",
      "Epoch 2/100\n",
      "6091/6091 [==============================] - 1s 97us/sample - loss: 0.5828 - val_loss: 0.5726\n",
      "Epoch 3/100\n",
      "6091/6091 [==============================] - 1s 95us/sample - loss: 0.5434 - val_loss: 0.5462\n",
      "Epoch 4/100\n",
      "6091/6091 [==============================] - 1s 99us/sample - loss: 0.5151 - val_loss: 0.5276\n",
      "Epoch 5/100\n",
      "6091/6091 [==============================] - 1s 94us/sample - loss: 0.4929 - val_loss: 0.5134\n",
      "Epoch 6/100\n",
      "6091/6091 [==============================] - 1s 93us/sample - loss: 0.4745 - val_loss: 0.5022\n",
      "Epoch 7/100\n",
      "6091/6091 [==============================] - 1s 94us/sample - loss: 0.4590 - val_loss: 0.4931\n",
      "Epoch 8/100\n",
      "6091/6091 [==============================] - 1s 96us/sample - loss: 0.4454 - val_loss: 0.4855\n",
      "Epoch 9/100\n",
      "6091/6091 [==============================] - 1s 92us/sample - loss: 0.4334 - val_loss: 0.4791\n",
      "Epoch 10/100\n",
      "6091/6091 [==============================] - 1s 101us/sample - loss: 0.4228 - val_loss: 0.4737\n",
      "Epoch 11/100\n",
      "6091/6091 [==============================] - 1s 98us/sample - loss: 0.4130 - val_loss: 0.4697\n",
      "Epoch 12/100\n",
      "6091/6091 [==============================] - 1s 93us/sample - loss: 0.4041 - val_loss: 0.4656\n",
      "Epoch 13/100\n",
      "6091/6091 [==============================] - 1s 93us/sample - loss: 0.3959 - val_loss: 0.4624\n",
      "Epoch 14/100\n",
      "6091/6091 [==============================] - 1s 95us/sample - loss: 0.3884 - val_loss: 0.4597\n",
      "Epoch 15/100\n",
      "6091/6091 [==============================] - 1s 95us/sample - loss: 0.3813 - val_loss: 0.4573\n",
      "Epoch 16/100\n",
      "6091/6091 [==============================] - 1s 94us/sample - loss: 0.3748 - val_loss: 0.4552\n",
      "Epoch 17/100\n",
      "6091/6091 [==============================] - 1s 94us/sample - loss: 0.3685 - val_loss: 0.4534\n",
      "Epoch 18/100\n",
      "6091/6091 [==============================] - 1s 95us/sample - loss: 0.3628 - val_loss: 0.4521\n",
      "Epoch 19/100\n",
      "6091/6091 [==============================] - 1s 93us/sample - loss: 0.3573 - val_loss: 0.4505\n",
      "Epoch 20/100\n",
      "6091/6091 [==============================] - 1s 92us/sample - loss: 0.3520 - val_loss: 0.4498\n",
      "Epoch 21/100\n",
      "6091/6091 [==============================] - 1s 96us/sample - loss: 0.3470 - val_loss: 0.4488\n",
      "Epoch 22/100\n",
      "6091/6091 [==============================] - 1s 94us/sample - loss: 0.3424 - val_loss: 0.4483\n",
      "Epoch 23/100\n",
      "6091/6091 [==============================] - 1s 94us/sample - loss: 0.3379 - val_loss: 0.4480\n",
      "Epoch 24/100\n",
      "6091/6091 [==============================] - 1s 95us/sample - loss: 0.3336 - val_loss: 0.4474\n",
      "Epoch 25/100\n",
      "6091/6091 [==============================] - 1s 94us/sample - loss: 0.3294 - val_loss: 0.4473\n",
      "Epoch 26/100\n",
      "6091/6091 [==============================] - 1s 94us/sample - loss: 0.3255 - val_loss: 0.4469\n",
      "Epoch 27/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.3216 - val_loss: 0.4469\n",
      "Epoch 28/100\n",
      "6091/6091 [==============================] - 1s 115us/sample - loss: 0.3180 - val_loss: 0.4469\n",
      "Epoch 29/100\n",
      "6091/6091 [==============================] - 1s 110us/sample - loss: 0.3145 - val_loss: 0.4474\n",
      "Predicting...\n",
      "(Took 17.757 sec)\n",
      "Train on 6091 samples, validate on 1522 samples\n",
      "Epoch 1/100\n",
      "6091/6091 [==============================] - 1s 162us/sample - loss: 0.6499 - val_loss: 0.6167\n",
      "Epoch 2/100\n",
      "6091/6091 [==============================] - 1s 117us/sample - loss: 0.5838 - val_loss: 0.5762\n",
      "Epoch 3/100\n",
      "6091/6091 [==============================] - 1s 131us/sample - loss: 0.5435 - val_loss: 0.5502\n",
      "Epoch 4/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.5145 - val_loss: 0.5323\n",
      "Epoch 5/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4919 - val_loss: 0.5187\n",
      "Epoch 6/100\n",
      "6091/6091 [==============================] - 1s 111us/sample - loss: 0.4731 - val_loss: 0.5080\n",
      "Epoch 7/100\n",
      "6091/6091 [==============================] - 1s 110us/sample - loss: 0.4571 - val_loss: 0.4990\n",
      "Epoch 8/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.4433 - val_loss: 0.4920\n",
      "Epoch 9/100\n",
      "6091/6091 [==============================] - 1s 104us/sample - loss: 0.4311 - val_loss: 0.4862\n",
      "Epoch 10/100\n",
      "6091/6091 [==============================] - 1s 105us/sample - loss: 0.4200 - val_loss: 0.4812\n",
      "Epoch 11/100\n",
      "6091/6091 [==============================] - 1s 99us/sample - loss: 0.4102 - val_loss: 0.4770\n",
      "Epoch 12/100\n",
      "6091/6091 [==============================] - 1s 104us/sample - loss: 0.4012 - val_loss: 0.4735\n",
      "Epoch 13/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.3927 - val_loss: 0.4706\n",
      "Epoch 14/100\n",
      "6091/6091 [==============================] - 1s 106us/sample - loss: 0.3851 - val_loss: 0.4682\n",
      "Epoch 15/100\n",
      "6091/6091 [==============================] - 1s 114us/sample - loss: 0.3779 - val_loss: 0.4661\n",
      "Epoch 16/100\n",
      "6091/6091 [==============================] - 1s 105us/sample - loss: 0.3711 - val_loss: 0.4642\n",
      "Epoch 17/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.3649 - val_loss: 0.4627\n",
      "Epoch 18/100\n",
      "6091/6091 [==============================] - 1s 111us/sample - loss: 0.3590 - val_loss: 0.4616\n",
      "Epoch 19/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.3534 - val_loss: 0.4605\n",
      "Epoch 20/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.3481 - val_loss: 0.4597\n",
      "Epoch 21/100\n",
      "6091/6091 [==============================] - 1s 113us/sample - loss: 0.3430 - val_loss: 0.4591\n",
      "Epoch 22/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.3382 - val_loss: 0.4591\n",
      "Predicting...\n",
      "(Took 15.537 sec)\n",
      "Combined confusion matrix:\n",
      "[[3808.  534.]\n",
      " [ 994. 2277.]]\n",
      "(Overall, took 101.505 sec)\n",
      "Accuracy: 79.93% +/- 0.95%\n",
      "Precision for positive class: 79.31% +/- 2.17%\n",
      "Precision for negative class: 81.02% +/- 1.89%\n",
      "Recall for positive class: 87.71% +/- 1.25%\n",
      "Recall for negative class: 69.64% +/- 2.76%\n",
      "F for positive class: 83.29% +/- 0.89%\n",
      "F for negative class: 74.87% +/- 1.27%\n",
      "Mean F score: 79.08% +/- 0.98%\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/100\n",
      "6090/6090 [==============================] - 1s 146us/sample - loss: 0.6477 - val_loss: 0.6040\n",
      "Epoch 2/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.5817 - val_loss: 0.5614\n",
      "Epoch 3/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.5415 - val_loss: 0.5345\n",
      "Epoch 4/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.5125 - val_loss: 0.5152\n",
      "Epoch 5/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.4899 - val_loss: 0.5012\n",
      "Epoch 6/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4715 - val_loss: 0.4904\n",
      "Epoch 7/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.4557 - val_loss: 0.4815\n",
      "Epoch 8/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.4421 - val_loss: 0.4747\n",
      "Epoch 9/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4302 - val_loss: 0.4686\n",
      "Epoch 10/100\n",
      "6090/6090 [==============================] - 1s 119us/sample - loss: 0.4196 - val_loss: 0.4642\n",
      "Epoch 11/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4099 - val_loss: 0.4604\n",
      "Epoch 12/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.4014 - val_loss: 0.4570\n",
      "Epoch 13/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.3933 - val_loss: 0.4542\n",
      "Epoch 14/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.3859 - val_loss: 0.4518\n",
      "Epoch 15/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.3791 - val_loss: 0.4500\n",
      "Epoch 16/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.3728 - val_loss: 0.4482\n",
      "Epoch 17/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.3669 - val_loss: 0.4467\n",
      "Epoch 18/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6090/6090 [==============================] - 1s 99us/sample - loss: 0.3612 - val_loss: 0.4456\n",
      "Epoch 19/100\n",
      "6090/6090 [==============================] - 1s 101us/sample - loss: 0.3560 - val_loss: 0.4446\n",
      "Epoch 20/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.3510 - val_loss: 0.4438\n",
      "Epoch 21/100\n",
      "6090/6090 [==============================] - 1s 99us/sample - loss: 0.3464 - val_loss: 0.4436\n",
      "Epoch 22/100\n",
      "6090/6090 [==============================] - 1s 98us/sample - loss: 0.3419 - val_loss: 0.4429\n",
      "Epoch 23/100\n",
      "6090/6090 [==============================] - 1s 99us/sample - loss: 0.3377 - val_loss: 0.4428\n",
      "Epoch 24/100\n",
      "6090/6090 [==============================] - 1s 99us/sample - loss: 0.3337 - val_loss: 0.4426\n",
      "Epoch 25/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.3298 - val_loss: 0.4428\n",
      "Predicting...\n",
      "(Took 16.847 sec)\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/100\n",
      "6090/6090 [==============================] - 1s 152us/sample - loss: 0.6486 - val_loss: 0.6105\n",
      "Epoch 2/100\n",
      "6090/6090 [==============================] - 1s 97us/sample - loss: 0.5803 - val_loss: 0.5699\n",
      "Epoch 3/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.5383 - val_loss: 0.5453\n",
      "Epoch 4/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.5084 - val_loss: 0.5284\n",
      "Epoch 5/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.4855 - val_loss: 0.5159\n",
      "Epoch 6/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.4670 - val_loss: 0.5059\n",
      "Epoch 7/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.4514 - val_loss: 0.4980\n",
      "Epoch 8/100\n",
      "6090/6090 [==============================] - 1s 94us/sample - loss: 0.4379 - val_loss: 0.4914\n",
      "Epoch 9/100\n",
      "6090/6090 [==============================] - 1s 101us/sample - loss: 0.4261 - val_loss: 0.4864\n",
      "Epoch 10/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4157 - val_loss: 0.4816\n",
      "Epoch 11/100\n",
      "6090/6090 [==============================] - 1s 120us/sample - loss: 0.4063 - val_loss: 0.4778\n",
      "Epoch 12/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.3978 - val_loss: 0.4745\n",
      "Epoch 13/100\n",
      "6090/6090 [==============================] - 1s 97us/sample - loss: 0.3900 - val_loss: 0.4721\n",
      "Epoch 14/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.3828 - val_loss: 0.4696\n",
      "Epoch 15/100\n",
      "6090/6090 [==============================] - 1s 97us/sample - loss: 0.3761 - val_loss: 0.4679\n",
      "Epoch 16/100\n",
      "6090/6090 [==============================] - 1s 101us/sample - loss: 0.3699 - val_loss: 0.4664\n",
      "Epoch 17/100\n",
      "6090/6090 [==============================] - 1s 98us/sample - loss: 0.3642 - val_loss: 0.4653\n",
      "Epoch 18/100\n",
      "6090/6090 [==============================] - 1s 99us/sample - loss: 0.3587 - val_loss: 0.4642\n",
      "Epoch 19/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.3536 - val_loss: 0.4635\n",
      "Epoch 20/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.3487 - val_loss: 0.4632\n",
      "Epoch 21/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.3441 - val_loss: 0.4628\n",
      "Epoch 22/100\n",
      "6090/6090 [==============================] - 1s 97us/sample - loss: 0.3398 - val_loss: 0.4627\n",
      "Epoch 23/100\n",
      "6090/6090 [==============================] - 1s 96us/sample - loss: 0.3356 - val_loss: 0.4626\n",
      "Epoch 24/100\n",
      "6090/6090 [==============================] - 1s 119us/sample - loss: 0.3316 - val_loss: 0.4627\n",
      "Predicting...\n",
      "(Took 15.857 sec)\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/100\n",
      "6090/6090 [==============================] - 1s 153us/sample - loss: 0.6463 - val_loss: 0.6148\n",
      "Epoch 2/100\n",
      "6090/6090 [==============================] - 1s 116us/sample - loss: 0.5788 - val_loss: 0.5743\n",
      "Epoch 3/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.5376 - val_loss: 0.5476\n",
      "Epoch 4/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.5081 - val_loss: 0.5302\n",
      "Epoch 5/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4854 - val_loss: 0.5171\n",
      "Epoch 6/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.4671 - val_loss: 0.5069\n",
      "Epoch 7/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.4515 - val_loss: 0.4980\n",
      "Epoch 8/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4382 - val_loss: 0.4914\n",
      "Epoch 9/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.4265 - val_loss: 0.4858\n",
      "Epoch 10/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4159 - val_loss: 0.4815\n",
      "Epoch 11/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.4065 - val_loss: 0.4775\n",
      "Epoch 12/100\n",
      "6090/6090 [==============================] - ETA: 0s - loss: 0.397 - 1s 98us/sample - loss: 0.3979 - val_loss: 0.4742\n",
      "Epoch 13/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.3900 - val_loss: 0.4715\n",
      "Epoch 14/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.3827 - val_loss: 0.4693\n",
      "Epoch 15/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.3760 - val_loss: 0.4679\n",
      "Epoch 16/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.3697 - val_loss: 0.4667\n",
      "Epoch 17/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.3638 - val_loss: 0.4650\n",
      "Epoch 18/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.3583 - val_loss: 0.4643\n",
      "Epoch 19/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.3530 - val_loss: 0.4633\n",
      "Epoch 20/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.3481 - val_loss: 0.4628\n",
      "Epoch 21/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.3433 - val_loss: 0.4632\n",
      "Predicting...\n",
      "(Took 14.307 sec)\n",
      "Train on 6091 samples, validate on 1522 samples\n",
      "Epoch 1/100\n",
      "6091/6091 [==============================] - 1s 143us/sample - loss: 0.6495 - val_loss: 0.6133\n",
      "Epoch 2/100\n",
      "6091/6091 [==============================] - 1s 103us/sample - loss: 0.5810 - val_loss: 0.5704\n",
      "Epoch 3/100\n",
      "6091/6091 [==============================] - 1s 100us/sample - loss: 0.5399 - val_loss: 0.5432\n",
      "Epoch 4/100\n",
      "6091/6091 [==============================] - 1s 100us/sample - loss: 0.5108 - val_loss: 0.5243\n",
      "Epoch 5/100\n",
      "6091/6091 [==============================] - 1s 102us/sample - loss: 0.4883 - val_loss: 0.5099\n",
      "Epoch 6/100\n",
      "6091/6091 [==============================] - 1s 99us/sample - loss: 0.4700 - val_loss: 0.4987\n",
      "Epoch 7/100\n",
      "6091/6091 [==============================] - 1s 110us/sample - loss: 0.4546 - val_loss: 0.4895\n",
      "Epoch 8/100\n",
      "6091/6091 [==============================] - 1s 110us/sample - loss: 0.4412 - val_loss: 0.4820\n",
      "Epoch 9/100\n",
      "6091/6091 [==============================] - 1s 111us/sample - loss: 0.4295 - val_loss: 0.4758\n",
      "Epoch 10/100\n",
      "6091/6091 [==============================] - 1s 111us/sample - loss: 0.4190 - val_loss: 0.4707\n",
      "Epoch 11/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.4095 - val_loss: 0.4664\n",
      "Epoch 12/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4008 - val_loss: 0.4627\n",
      "Epoch 13/100\n",
      "6091/6091 [==============================] - 1s 110us/sample - loss: 0.3929 - val_loss: 0.4595\n",
      "Epoch 14/100\n",
      "6091/6091 [==============================] - 1s 106us/sample - loss: 0.3856 - val_loss: 0.4569\n",
      "Epoch 15/100\n",
      "6091/6091 [==============================] - 1s 103us/sample - loss: 0.3789 - val_loss: 0.4547\n",
      "Epoch 16/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.3726 - val_loss: 0.4528\n",
      "Epoch 17/100\n",
      "6091/6091 [==============================] - 1s 105us/sample - loss: 0.3666 - val_loss: 0.4516\n",
      "Epoch 18/100\n",
      "6091/6091 [==============================] - 1s 106us/sample - loss: 0.3611 - val_loss: 0.4504\n",
      "Epoch 19/100\n",
      "6091/6091 [==============================] - 1s 102us/sample - loss: 0.3558 - val_loss: 0.4493\n",
      "Epoch 20/100\n",
      "6091/6091 [==============================] - 1s 98us/sample - loss: 0.3509 - val_loss: 0.4485\n",
      "Epoch 21/100\n",
      "6091/6091 [==============================] - 1s 101us/sample - loss: 0.3462 - val_loss: 0.4480\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/100\n",
      "6091/6091 [==============================] - 1s 96us/sample - loss: 0.3418 - val_loss: 0.4476\n",
      "Epoch 23/100\n",
      "6091/6091 [==============================] - 1s 99us/sample - loss: 0.3375 - val_loss: 0.4472\n",
      "Epoch 24/100\n",
      "6091/6091 [==============================] - 1s 98us/sample - loss: 0.3334 - val_loss: 0.4473\n",
      "Predicting...\n",
      "(Took 15.948 sec)\n",
      "Train on 6091 samples, validate on 1522 samples\n",
      "Epoch 1/100\n",
      "6091/6091 [==============================] - 1s 145us/sample - loss: 0.6419 - val_loss: 0.6130\n",
      "Epoch 2/100\n",
      "6091/6091 [==============================] - 1s 104us/sample - loss: 0.5775 - val_loss: 0.5726\n",
      "Epoch 3/100\n",
      "6091/6091 [==============================] - 1s 98us/sample - loss: 0.5371 - val_loss: 0.5464\n",
      "Epoch 4/100\n",
      "6091/6091 [==============================] - 1s 102us/sample - loss: 0.5083 - val_loss: 0.5283\n",
      "Epoch 5/100\n",
      "6091/6091 [==============================] - 1s 104us/sample - loss: 0.4858 - val_loss: 0.5147\n",
      "Epoch 6/100\n",
      "6091/6091 [==============================] - 1s 101us/sample - loss: 0.4674 - val_loss: 0.5037\n",
      "Epoch 7/100\n",
      "6091/6091 [==============================] - 1s 100us/sample - loss: 0.4519 - val_loss: 0.4947\n",
      "Epoch 8/100\n",
      "6091/6091 [==============================] - 1s 99us/sample - loss: 0.4384 - val_loss: 0.4878\n",
      "Epoch 9/100\n",
      "6091/6091 [==============================] - 1s 99us/sample - loss: 0.4266 - val_loss: 0.4822\n",
      "Epoch 10/100\n",
      "6091/6091 [==============================] - 1s 104us/sample - loss: 0.4161 - val_loss: 0.4772\n",
      "Epoch 11/100\n",
      "6091/6091 [==============================] - 1s 105us/sample - loss: 0.4065 - val_loss: 0.4729\n",
      "Epoch 12/100\n",
      "6091/6091 [==============================] - 1s 99us/sample - loss: 0.3978 - val_loss: 0.4698\n",
      "Epoch 13/100\n",
      "6091/6091 [==============================] - 1s 102us/sample - loss: 0.3898 - val_loss: 0.4664\n",
      "Epoch 14/100\n",
      "6091/6091 [==============================] - 1s 106us/sample - loss: 0.3826 - val_loss: 0.4644\n",
      "Epoch 15/100\n",
      "6091/6091 [==============================] - 1s 99us/sample - loss: 0.3758 - val_loss: 0.4621\n",
      "Epoch 16/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.3694 - val_loss: 0.4607\n",
      "Epoch 17/100\n",
      "6091/6091 [==============================] - 1s 104us/sample - loss: 0.3636 - val_loss: 0.4589\n",
      "Epoch 18/100\n",
      "6091/6091 [==============================] - 1s 106us/sample - loss: 0.3581 - val_loss: 0.4584\n",
      "Epoch 19/100\n",
      "6091/6091 [==============================] - 1s 104us/sample - loss: 0.3528 - val_loss: 0.4575\n",
      "Epoch 20/100\n",
      "6091/6091 [==============================] - 1s 104us/sample - loss: 0.3479 - val_loss: 0.4567\n",
      "Epoch 21/100\n",
      "6091/6091 [==============================] - 1s 100us/sample - loss: 0.3432 - val_loss: 0.4561\n",
      "Epoch 22/100\n",
      "6091/6091 [==============================] - 1s 98us/sample - loss: 0.3388 - val_loss: 0.4559\n",
      "Epoch 23/100\n",
      "6091/6091 [==============================] - 1s 104us/sample - loss: 0.3345 - val_loss: 0.4558\n",
      "Epoch 24/100\n",
      "6091/6091 [==============================] - 1s 100us/sample - loss: 0.3304 - val_loss: 0.4556\n",
      "Epoch 25/100\n",
      "6091/6091 [==============================] - 1s 99us/sample - loss: 0.3266 - val_loss: 0.4558\n",
      "Predicting...\n",
      "(Took 16.299 sec)\n",
      "Combined confusion matrix:\n",
      "[[3820.  522.]\n",
      " [ 968. 2303.]]\n",
      "(Overall, took 79.507 sec)\n",
      "Accuracy: 80.43% +/- 0.90%\n",
      "Precision for positive class: 79.78% +/- 1.77%\n",
      "Precision for negative class: 81.52% +/- 1.70%\n",
      "Recall for positive class: 87.98% +/- 0.98%\n",
      "Recall for negative class: 70.42% +/- 2.02%\n",
      "F for positive class: 83.67% +/- 0.88%\n",
      "F for negative class: 75.55% +/- 1.18%\n",
      "Mean F score: 79.61% +/- 0.90%\n"
     ]
    }
   ],
   "source": [
    "# POS tagging\n",
    "# Raw counts\n",
    "all_metrics(cross_validate_logistic(dftrain_pos_minimal, 100))\n",
    "# Bigram\n",
    "# Raw counts\n",
    "all_metrics(cross_validate_logistic(dftrain_bi_minimal, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/100\n",
      "6090/6090 [==============================] - 1s 148us/sample - loss: 0.6864 - val_loss: 0.6787\n",
      "Epoch 2/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.6744 - val_loss: 0.6686\n",
      "Epoch 3/100\n",
      "6090/6090 [==============================] - 1s 101us/sample - loss: 0.6653 - val_loss: 0.6607\n",
      "Epoch 4/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.6576 - val_loss: 0.6539\n",
      "Epoch 5/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.6505 - val_loss: 0.6477\n",
      "Epoch 6/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.6437 - val_loss: 0.6417\n",
      "Epoch 7/100\n",
      "6090/6090 [==============================] - 1s 118us/sample - loss: 0.6372 - val_loss: 0.6362\n",
      "Epoch 8/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.6310 - val_loss: 0.6308\n",
      "Epoch 9/100\n",
      "6090/6090 [==============================] - 1s 101us/sample - loss: 0.6251 - val_loss: 0.6257\n",
      "Epoch 10/100\n",
      "6090/6090 [==============================] - 1s 122us/sample - loss: 0.6193 - val_loss: 0.6209\n",
      "Epoch 11/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.6138 - val_loss: 0.6162\n",
      "Epoch 12/100\n",
      "6090/6090 [==============================] - 1s 116us/sample - loss: 0.6085 - val_loss: 0.6117\n",
      "Epoch 13/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.6034 - val_loss: 0.6073\n",
      "Epoch 14/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.5984 - val_loss: 0.6032\n",
      "Epoch 15/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.5936 - val_loss: 0.5992\n",
      "Epoch 16/100\n",
      "6090/6090 [==============================] - 1s 118us/sample - loss: 0.5890 - val_loss: 0.5954\n",
      "Epoch 17/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.5845 - val_loss: 0.5917\n",
      "Epoch 18/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.5801 - val_loss: 0.5881\n",
      "Epoch 19/100\n",
      "6090/6090 [==============================] - 1s 116us/sample - loss: 0.5759 - val_loss: 0.5846\n",
      "Epoch 20/100\n",
      "6090/6090 [==============================] - 1s 133us/sample - loss: 0.5719 - val_loss: 0.5812\n",
      "Epoch 21/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.5679 - val_loss: 0.5780\n",
      "Epoch 22/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.5641 - val_loss: 0.5749\n",
      "Epoch 23/100\n",
      "6090/6090 [==============================] - 1s 118us/sample - loss: 0.5603 - val_loss: 0.5718\n",
      "Epoch 24/100\n",
      "6090/6090 [==============================] - 1s 125us/sample - loss: 0.5567 - val_loss: 0.5689\n",
      "Epoch 25/100\n",
      "6090/6090 [==============================] - 1s 122us/sample - loss: 0.5532 - val_loss: 0.5661\n",
      "Epoch 26/100\n",
      "6090/6090 [==============================] - 1s 119us/sample - loss: 0.5497 - val_loss: 0.5634\n",
      "Epoch 27/100\n",
      "6090/6090 [==============================] - 1s 124us/sample - loss: 0.5464 - val_loss: 0.5607\n",
      "Epoch 28/100\n",
      "6090/6090 [==============================] - 1s 117us/sample - loss: 0.5432 - val_loss: 0.5581\n",
      "Epoch 29/100\n",
      "6090/6090 [==============================] - 1s 101us/sample - loss: 0.5400 - val_loss: 0.5556\n",
      "Epoch 30/100\n",
      "6090/6090 [==============================] - 1s 101us/sample - loss: 0.5369 - val_loss: 0.5532\n",
      "Epoch 31/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.5339 - val_loss: 0.5508\n",
      "Epoch 32/100\n",
      "6090/6090 [==============================] - 1s 98us/sample - loss: 0.5309 - val_loss: 0.5485\n",
      "Epoch 33/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.5281 - val_loss: 0.5464\n",
      "Epoch 34/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.5253 - val_loss: 0.5442\n",
      "Epoch 35/100\n",
      "6090/6090 [==============================] - 1s 126us/sample - loss: 0.5225 - val_loss: 0.5421\n",
      "Epoch 36/100\n",
      "6090/6090 [==============================] - 1s 99us/sample - loss: 0.5199 - val_loss: 0.5401\n",
      "Epoch 37/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.5172 - val_loss: 0.5381\n",
      "Epoch 38/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.5147 - val_loss: 0.5362\n",
      "Epoch 39/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.5122 - val_loss: 0.5343\n",
      "Epoch 40/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.5097 - val_loss: 0.5325\n",
      "Epoch 41/100\n",
      "6090/6090 [==============================] - 1s 98us/sample - loss: 0.5073 - val_loss: 0.5307\n",
      "Epoch 42/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.5050 - val_loss: 0.5290\n",
      "Epoch 43/100\n",
      "6090/6090 [==============================] - 1s 121us/sample - loss: 0.5026 - val_loss: 0.5273\n",
      "Epoch 44/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.5004 - val_loss: 0.5257\n",
      "Epoch 45/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4982 - val_loss: 0.5241\n",
      "Epoch 46/100\n",
      "6090/6090 [==============================] - 1s 117us/sample - loss: 0.4960 - val_loss: 0.5226\n",
      "Epoch 47/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4939 - val_loss: 0.5211\n",
      "Epoch 48/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.4918 - val_loss: 0.5196\n",
      "Epoch 49/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4898 - val_loss: 0.5181\n",
      "Epoch 50/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.4877 - val_loss: 0.5167\n",
      "Epoch 51/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.4857 - val_loss: 0.5154\n",
      "Epoch 52/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.4838 - val_loss: 0.5140\n",
      "Epoch 53/100\n",
      "6090/6090 [==============================] - 1s 101us/sample - loss: 0.4818 - val_loss: 0.5127\n",
      "Epoch 54/100\n",
      "6090/6090 [==============================] - 1s 98us/sample - loss: 0.4800 - val_loss: 0.5114\n",
      "Epoch 55/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4781 - val_loss: 0.5102\n",
      "Epoch 56/100\n",
      "6090/6090 [==============================] - 1s 101us/sample - loss: 0.4763 - val_loss: 0.5090\n",
      "Epoch 57/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.4745 - val_loss: 0.5078\n",
      "Epoch 58/100\n",
      "6090/6090 [==============================] - 1s 97us/sample - loss: 0.4727 - val_loss: 0.5066\n",
      "Epoch 59/100\n",
      "6090/6090 [==============================] - 1s 94us/sample - loss: 0.4710 - val_loss: 0.5054\n",
      "Epoch 60/100\n",
      "6090/6090 [==============================] - 1s 95us/sample - loss: 0.4693 - val_loss: 0.5044\n",
      "Epoch 61/100\n",
      "6090/6090 [==============================] - 1s 97us/sample - loss: 0.4676 - val_loss: 0.5032\n",
      "Epoch 62/100\n",
      "6090/6090 [==============================] - 1s 96us/sample - loss: 0.4660 - val_loss: 0.5022\n",
      "Epoch 63/100\n",
      "6090/6090 [==============================] - 1s 98us/sample - loss: 0.4644 - val_loss: 0.5011\n",
      "Epoch 64/100\n",
      "6090/6090 [==============================] - 1s 99us/sample - loss: 0.4627 - val_loss: 0.5001\n",
      "Epoch 65/100\n",
      "6090/6090 [==============================] - 1s 98us/sample - loss: 0.4612 - val_loss: 0.4991\n",
      "Epoch 66/100\n",
      "6090/6090 [==============================] - 1s 122us/sample - loss: 0.4596 - val_loss: 0.4982\n",
      "Epoch 67/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4581 - val_loss: 0.4972\n",
      "Epoch 68/100\n",
      "6090/6090 [==============================] - 1s 116us/sample - loss: 0.4566 - val_loss: 0.4963\n",
      "Epoch 69/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.4551 - val_loss: 0.4954\n",
      "Epoch 70/100\n",
      "6090/6090 [==============================] - 1s 125us/sample - loss: 0.4536 - val_loss: 0.4945\n",
      "Epoch 71/100\n",
      "6090/6090 [==============================] - 1s 136us/sample - loss: 0.4521 - val_loss: 0.4936\n",
      "Epoch 72/100\n",
      "6090/6090 [==============================] - 1s 124us/sample - loss: 0.4507 - val_loss: 0.4928\n",
      "Epoch 73/100\n",
      "6090/6090 [==============================] - 1s 121us/sample - loss: 0.4493 - val_loss: 0.4919\n",
      "Epoch 74/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.4479 - val_loss: 0.4910\n",
      "Epoch 75/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.4465 - val_loss: 0.4903\n",
      "Epoch 76/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.4452 - val_loss: 0.4894\n",
      "Epoch 77/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4438 - val_loss: 0.4887\n",
      "Epoch 78/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.4425 - val_loss: 0.4879\n",
      "Epoch 79/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.4412 - val_loss: 0.4871\n",
      "Epoch 80/100\n",
      "6090/6090 [==============================] - 1s 99us/sample - loss: 0.4399 - val_loss: 0.4864\n",
      "Epoch 81/100\n",
      "6090/6090 [==============================] - 1s 99us/sample - loss: 0.4386 - val_loss: 0.4857\n",
      "Epoch 82/100\n",
      "6090/6090 [==============================] - 1s 98us/sample - loss: 0.4373 - val_loss: 0.4851\n",
      "Epoch 83/100\n",
      "6090/6090 [==============================] - 1s 95us/sample - loss: 0.4361 - val_loss: 0.4843\n",
      "Epoch 84/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.4349 - val_loss: 0.4837\n",
      "Epoch 85/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.4337 - val_loss: 0.4830\n",
      "Epoch 86/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.4325 - val_loss: 0.4824\n",
      "Epoch 87/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.4313 - val_loss: 0.4817\n",
      "Epoch 88/100\n",
      "6090/6090 [==============================] - 1s 120us/sample - loss: 0.4301 - val_loss: 0.4811\n",
      "Epoch 89/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4290 - val_loss: 0.4805\n",
      "Epoch 90/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4278 - val_loss: 0.4799\n",
      "Epoch 91/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.4267 - val_loss: 0.4793\n",
      "Epoch 92/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.4256 - val_loss: 0.4788\n",
      "Epoch 93/100\n",
      "6090/6090 [==============================] - 1s 99us/sample - loss: 0.4245 - val_loss: 0.4782\n",
      "Epoch 94/100\n",
      "6090/6090 [==============================] - 1s 101us/sample - loss: 0.4234 - val_loss: 0.4776\n",
      "Epoch 95/100\n",
      "6090/6090 [==============================] - 1s 99us/sample - loss: 0.4223 - val_loss: 0.4771\n",
      "Epoch 96/100\n",
      "6090/6090 [==============================] - 1s 98us/sample - loss: 0.4212 - val_loss: 0.4766\n",
      "Epoch 97/100\n",
      "6090/6090 [==============================] - 1s 95us/sample - loss: 0.4201 - val_loss: 0.4760\n",
      "Epoch 98/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.4191 - val_loss: 0.4755\n",
      "Epoch 99/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.4181 - val_loss: 0.4750\n",
      "Epoch 100/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.4170 - val_loss: 0.4745\n",
      "Predicting...\n",
      "(Took 66.295 sec)\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/100\n",
      "6090/6090 [==============================] - 1s 172us/sample - loss: 0.6864 - val_loss: 0.6780\n",
      "Epoch 2/100\n",
      "6090/6090 [==============================] - 1s 101us/sample - loss: 0.6747 - val_loss: 0.6675\n",
      "Epoch 3/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.6659 - val_loss: 0.6594\n",
      "Epoch 4/100\n",
      "6090/6090 [==============================] - 1s 96us/sample - loss: 0.6583 - val_loss: 0.6524\n",
      "Epoch 5/100\n",
      "6090/6090 [==============================] - 1s 96us/sample - loss: 0.6512 - val_loss: 0.6461\n",
      "Epoch 6/100\n",
      "6090/6090 [==============================] - 1s 99us/sample - loss: 0.6444 - val_loss: 0.6403\n",
      "Epoch 7/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.6379 - val_loss: 0.6347\n",
      "Epoch 8/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.6317 - val_loss: 0.6293\n",
      "Epoch 9/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.6257 - val_loss: 0.6243\n",
      "Epoch 10/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.6199 - val_loss: 0.6195\n",
      "Epoch 11/100\n",
      "6090/6090 [==============================] - 1s 96us/sample - loss: 0.6144 - val_loss: 0.6147\n",
      "Epoch 12/100\n",
      "6090/6090 [==============================] - 1s 95us/sample - loss: 0.6090 - val_loss: 0.6103\n",
      "Epoch 13/100\n",
      "6090/6090 [==============================] - 1s 98us/sample - loss: 0.6039 - val_loss: 0.6060\n",
      "Epoch 14/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.5989 - val_loss: 0.6019\n",
      "Epoch 15/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.5941 - val_loss: 0.5979\n",
      "Epoch 16/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.5894 - val_loss: 0.5941\n",
      "Epoch 17/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.5849 - val_loss: 0.5904\n",
      "Epoch 18/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.5805 - val_loss: 0.5869\n",
      "Epoch 19/100\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.5763 - val_loss: 0.5835\n",
      "Epoch 20/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.5722 - val_loss: 0.5801\n",
      "Epoch 21/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.5682 - val_loss: 0.5771\n",
      "Epoch 22/100\n",
      "6090/6090 [==============================] - 1s 97us/sample - loss: 0.5644 - val_loss: 0.5740\n",
      "Epoch 23/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.5606 - val_loss: 0.5709\n",
      "Epoch 24/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.5570 - val_loss: 0.5681\n",
      "Epoch 25/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.5534 - val_loss: 0.5653\n",
      "Epoch 26/100\n",
      "6090/6090 [==============================] - 1s 101us/sample - loss: 0.5500 - val_loss: 0.5627\n",
      "Epoch 27/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.5466 - val_loss: 0.5601\n",
      "Epoch 28/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.5434 - val_loss: 0.5575\n",
      "Epoch 29/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.5402 - val_loss: 0.5551\n",
      "Epoch 30/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.5371 - val_loss: 0.5528\n",
      "Epoch 31/100\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.5341 - val_loss: 0.5505\n",
      "Epoch 32/100\n",
      "6090/6090 [==============================] - 1s 99us/sample - loss: 0.5311 - val_loss: 0.5482\n",
      "Epoch 33/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.5282 - val_loss: 0.5461\n",
      "Epoch 34/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.5254 - val_loss: 0.5441\n",
      "Epoch 35/100\n",
      "6090/6090 [==============================] - 1s 98us/sample - loss: 0.5227 - val_loss: 0.5421\n",
      "Epoch 36/100\n",
      "6090/6090 [==============================] - 1s 97us/sample - loss: 0.5200 - val_loss: 0.5401\n",
      "Epoch 37/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.5173 - val_loss: 0.5382\n",
      "Epoch 38/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.5148 - val_loss: 0.5363\n",
      "Epoch 39/100\n",
      "6090/6090 [==============================] - 1s 99us/sample - loss: 0.5122 - val_loss: 0.5345\n",
      "Epoch 40/100\n",
      "6090/6090 [==============================] - 1s 101us/sample - loss: 0.5098 - val_loss: 0.5328\n",
      "Epoch 41/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.5074 - val_loss: 0.5311\n",
      "Epoch 42/100\n",
      "6090/6090 [==============================] - 1s 97us/sample - loss: 0.5050 - val_loss: 0.5293\n",
      "Epoch 43/100\n",
      "6090/6090 [==============================] - 1s 97us/sample - loss: 0.5027 - val_loss: 0.5277\n",
      "Epoch 44/100\n",
      "6090/6090 [==============================] - 1s 98us/sample - loss: 0.5004 - val_loss: 0.5262\n",
      "Epoch 45/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.4982 - val_loss: 0.5247\n",
      "Epoch 46/100\n",
      "6090/6090 [==============================] - 1s 121us/sample - loss: 0.4960 - val_loss: 0.5232\n",
      "Epoch 47/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4939 - val_loss: 0.5217\n",
      "Epoch 48/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.4917 - val_loss: 0.5203\n",
      "Epoch 49/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.4897 - val_loss: 0.5188\n",
      "Epoch 50/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4877 - val_loss: 0.5175\n",
      "Epoch 51/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6090/6090 [==============================] - 1s 123us/sample - loss: 0.4857 - val_loss: 0.5162\n",
      "Epoch 52/100\n",
      "6090/6090 [==============================] - 1s 132us/sample - loss: 0.4837 - val_loss: 0.5149\n",
      "Epoch 53/100\n",
      "6090/6090 [==============================] - 1s 127us/sample - loss: 0.4818 - val_loss: 0.5137\n",
      "Epoch 54/100\n",
      "6090/6090 [==============================] - 1s 126us/sample - loss: 0.4799 - val_loss: 0.5124\n",
      "Epoch 55/100\n",
      "6090/6090 [==============================] - 1s 98us/sample - loss: 0.4780 - val_loss: 0.5111\n",
      "Epoch 56/100\n",
      "6090/6090 [==============================] - 1s 121us/sample - loss: 0.4762 - val_loss: 0.5099\n",
      "Epoch 57/100\n",
      "6090/6090 [==============================] - 1s 120us/sample - loss: 0.4744 - val_loss: 0.5088\n",
      "Epoch 58/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.4726 - val_loss: 0.5077\n",
      "Epoch 59/100\n",
      "6090/6090 [==============================] - 1s 95us/sample - loss: 0.4708 - val_loss: 0.5065\n",
      "Epoch 60/100\n",
      "6090/6090 [==============================] - 1s 121us/sample - loss: 0.4691 - val_loss: 0.5055\n",
      "Epoch 61/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4675 - val_loss: 0.5045\n",
      "Epoch 62/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.4658 - val_loss: 0.5034\n",
      "Epoch 63/100\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.4641 - val_loss: 0.5025\n",
      "Epoch 64/100\n",
      "6090/6090 [==============================] - 1s 97us/sample - loss: 0.4625 - val_loss: 0.5015\n",
      "Epoch 65/100\n",
      "6090/6090 [==============================] - 1s 97us/sample - loss: 0.4609 - val_loss: 0.5006\n",
      "Epoch 66/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.4594 - val_loss: 0.4996\n",
      "Epoch 67/100\n",
      "6090/6090 [==============================] - 1s 122us/sample - loss: 0.4578 - val_loss: 0.4988\n",
      "Epoch 68/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.4563 - val_loss: 0.4978\n",
      "Epoch 69/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.4548 - val_loss: 0.4969\n",
      "Epoch 70/100\n",
      "6090/6090 [==============================] - 1s 118us/sample - loss: 0.4533 - val_loss: 0.4961\n",
      "Epoch 71/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.4519 - val_loss: 0.4952\n",
      "Epoch 72/100\n",
      "6090/6090 [==============================] - 1s 96us/sample - loss: 0.4504 - val_loss: 0.4944\n",
      "Epoch 73/100\n",
      "6090/6090 [==============================] - 1s 95us/sample - loss: 0.4490 - val_loss: 0.4936\n",
      "Epoch 74/100\n",
      "6090/6090 [==============================] - 1s 97us/sample - loss: 0.4476 - val_loss: 0.4928\n",
      "Epoch 75/100\n",
      "6090/6090 [==============================] - 1s 95us/sample - loss: 0.4462 - val_loss: 0.4921\n",
      "Epoch 76/100\n",
      "6090/6090 [==============================] - 1s 95us/sample - loss: 0.4448 - val_loss: 0.4913\n",
      "Epoch 77/100\n",
      "6090/6090 [==============================] - 1s 95us/sample - loss: 0.4435 - val_loss: 0.4906\n",
      "Epoch 78/100\n",
      "6090/6090 [==============================] - 1s 96us/sample - loss: 0.4422 - val_loss: 0.4900\n",
      "Epoch 79/100\n",
      "6090/6090 [==============================] - 1s 96us/sample - loss: 0.4409 - val_loss: 0.4892\n",
      "Epoch 80/100\n",
      "6090/6090 [==============================] - 1s 96us/sample - loss: 0.4396 - val_loss: 0.4885\n",
      "Epoch 81/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.4383 - val_loss: 0.4878\n",
      "Epoch 82/100\n",
      "6090/6090 [==============================] - 1s 96us/sample - loss: 0.4370 - val_loss: 0.4871\n",
      "Epoch 83/100\n",
      "6090/6090 [==============================] - 1s 95us/sample - loss: 0.4358 - val_loss: 0.4865\n",
      "Epoch 84/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4345 - val_loss: 0.4858\n",
      "Epoch 85/100\n",
      "6090/6090 [==============================] - 1s 94us/sample - loss: 0.4333 - val_loss: 0.4852\n",
      "Epoch 86/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.4321 - val_loss: 0.4846\n",
      "Epoch 87/100\n",
      "6090/6090 [==============================] - 1s 101us/sample - loss: 0.4309 - val_loss: 0.4839\n",
      "Epoch 88/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.4298 - val_loss: 0.4834\n",
      "Epoch 89/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.4286 - val_loss: 0.4829\n",
      "Epoch 90/100\n",
      "6090/6090 [==============================] - 1s 118us/sample - loss: 0.4274 - val_loss: 0.4823\n",
      "Epoch 91/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.4263 - val_loss: 0.4817\n",
      "Epoch 92/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.4252 - val_loss: 0.4812\n",
      "Epoch 93/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.4240 - val_loss: 0.4806\n",
      "Epoch 94/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.4230 - val_loss: 0.4801\n",
      "Epoch 95/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.4219 - val_loss: 0.4795\n",
      "Epoch 96/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.4208 - val_loss: 0.4791\n",
      "Epoch 97/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.4197 - val_loss: 0.4786\n",
      "Epoch 98/100\n",
      "6090/6090 [==============================] - 1s 99us/sample - loss: 0.4187 - val_loss: 0.4782\n",
      "Epoch 99/100\n",
      "6090/6090 [==============================] - 1s 97us/sample - loss: 0.4176 - val_loss: 0.4777\n",
      "Epoch 100/100\n",
      "6090/6090 [==============================] - 1s 99us/sample - loss: 0.4166 - val_loss: 0.4772\n",
      "Predicting...\n",
      "(Took 64.932 sec)\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/100\n",
      "6090/6090 [==============================] - 1s 151us/sample - loss: 0.6863 - val_loss: 0.6817\n",
      "Epoch 2/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.6739 - val_loss: 0.6735\n",
      "Epoch 3/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.6646 - val_loss: 0.6669\n",
      "Epoch 4/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.6567 - val_loss: 0.6608\n",
      "Epoch 5/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.6495 - val_loss: 0.6550\n",
      "Epoch 6/100\n",
      "6090/6090 [==============================] - 1s 101us/sample - loss: 0.6427 - val_loss: 0.6496\n",
      "Epoch 7/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.6363 - val_loss: 0.6444\n",
      "Epoch 8/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.6300 - val_loss: 0.6392\n",
      "Epoch 9/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.6241 - val_loss: 0.6342\n",
      "Epoch 10/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.6183 - val_loss: 0.6294\n",
      "Epoch 11/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.6127 - val_loss: 0.6249\n",
      "Epoch 12/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.6074 - val_loss: 0.6206\n",
      "Epoch 13/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.6022 - val_loss: 0.6163\n",
      "Epoch 14/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.5972 - val_loss: 0.6123\n",
      "Epoch 15/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.5924 - val_loss: 0.6085\n",
      "Epoch 16/100\n",
      "6090/6090 [==============================] - 1s 117us/sample - loss: 0.5877 - val_loss: 0.6048\n",
      "Epoch 17/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.5832 - val_loss: 0.6012\n",
      "Epoch 18/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.5789 - val_loss: 0.5977\n",
      "Epoch 19/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.5746 - val_loss: 0.5944\n",
      "Epoch 20/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.5705 - val_loss: 0.5913\n",
      "Epoch 21/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.5665 - val_loss: 0.5882\n",
      "Epoch 22/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.5627 - val_loss: 0.5852\n",
      "Epoch 23/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.5589 - val_loss: 0.5823\n",
      "Epoch 24/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.5553 - val_loss: 0.5796\n",
      "Epoch 25/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.5517 - val_loss: 0.5769\n",
      "Epoch 26/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6090/6090 [==============================] - 1s 96us/sample - loss: 0.5483 - val_loss: 0.5743\n",
      "Epoch 27/100\n",
      "6090/6090 [==============================] - 1s 96us/sample - loss: 0.5449 - val_loss: 0.5718\n",
      "Epoch 28/100\n",
      "6090/6090 [==============================] - 1s 95us/sample - loss: 0.5416 - val_loss: 0.5694\n",
      "Epoch 29/100\n",
      "6090/6090 [==============================] - 1s 99us/sample - loss: 0.5384 - val_loss: 0.5670\n",
      "Epoch 30/100\n",
      "6090/6090 [==============================] - 1s 99us/sample - loss: 0.5353 - val_loss: 0.5647\n",
      "Epoch 31/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.5322 - val_loss: 0.5625\n",
      "Epoch 32/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.5293 - val_loss: 0.5603\n",
      "Epoch 33/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.5264 - val_loss: 0.5582\n",
      "Epoch 34/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.5236 - val_loss: 0.5563\n",
      "Epoch 35/100\n",
      "6090/6090 [==============================] - 1s 99us/sample - loss: 0.5208 - val_loss: 0.5543\n",
      "Epoch 36/100\n",
      "6090/6090 [==============================] - 1s 97us/sample - loss: 0.5181 - val_loss: 0.5524\n",
      "Epoch 37/100\n",
      "6090/6090 [==============================] - 1s 97us/sample - loss: 0.5154 - val_loss: 0.5506\n",
      "Epoch 38/100\n",
      "6090/6090 [==============================] - 1s 97us/sample - loss: 0.5129 - val_loss: 0.5488\n",
      "Epoch 39/100\n",
      "6090/6090 [==============================] - 1s 95us/sample - loss: 0.5103 - val_loss: 0.5471\n",
      "Epoch 40/100\n",
      "6090/6090 [==============================] - 1s 96us/sample - loss: 0.5079 - val_loss: 0.5454\n",
      "Epoch 41/100\n",
      "6090/6090 [==============================] - 1s 95us/sample - loss: 0.5055 - val_loss: 0.5437\n",
      "Epoch 42/100\n",
      "6090/6090 [==============================] - 1s 95us/sample - loss: 0.5031 - val_loss: 0.5421\n",
      "Epoch 43/100\n",
      "6090/6090 [==============================] - 1s 96us/sample - loss: 0.5007 - val_loss: 0.5404\n",
      "Epoch 44/100\n",
      "6090/6090 [==============================] - 1s 96us/sample - loss: 0.4984 - val_loss: 0.5390\n",
      "Epoch 45/100\n",
      "6090/6090 [==============================] - 1s 95us/sample - loss: 0.4962 - val_loss: 0.5376\n",
      "Epoch 46/100\n",
      "6090/6090 [==============================] - ETA: 0s - loss: 0.494 - 1s 97us/sample - loss: 0.4941 - val_loss: 0.5361\n",
      "Epoch 47/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.4919 - val_loss: 0.5347\n",
      "Epoch 48/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.4898 - val_loss: 0.5333\n",
      "Epoch 49/100\n",
      "6090/6090 [==============================] - 1s 120us/sample - loss: 0.4877 - val_loss: 0.5318\n",
      "Epoch 50/100\n",
      "6090/6090 [==============================] - 1s 116us/sample - loss: 0.4856 - val_loss: 0.5305\n",
      "Epoch 51/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4836 - val_loss: 0.5293\n",
      "Epoch 52/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4817 - val_loss: 0.5281\n",
      "Epoch 53/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4798 - val_loss: 0.5268\n",
      "Epoch 54/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.4778 - val_loss: 0.5257\n",
      "Epoch 55/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4760 - val_loss: 0.5245\n",
      "Epoch 56/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.4741 - val_loss: 0.5234\n",
      "Epoch 57/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.4723 - val_loss: 0.5223\n",
      "Epoch 58/100\n",
      "6090/6090 [==============================] - 1s 101us/sample - loss: 0.4706 - val_loss: 0.5212\n",
      "Epoch 59/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4688 - val_loss: 0.5201\n",
      "Epoch 60/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.4671 - val_loss: 0.5192\n",
      "Epoch 61/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.4654 - val_loss: 0.5181\n",
      "Epoch 62/100\n",
      "6090/6090 [==============================] - 1s 101us/sample - loss: 0.4637 - val_loss: 0.5170\n",
      "Epoch 63/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.4621 - val_loss: 0.5161\n",
      "Epoch 64/100\n",
      "6090/6090 [==============================] - 1s 101us/sample - loss: 0.4605 - val_loss: 0.5151\n",
      "Epoch 65/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.4589 - val_loss: 0.5142\n",
      "Epoch 66/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.4573 - val_loss: 0.5133\n",
      "Epoch 67/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.4558 - val_loss: 0.5125\n",
      "Epoch 68/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4542 - val_loss: 0.5116\n",
      "Epoch 69/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.4528 - val_loss: 0.5107\n",
      "Epoch 70/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4513 - val_loss: 0.5100\n",
      "Epoch 71/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.4498 - val_loss: 0.5091\n",
      "Epoch 72/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.4484 - val_loss: 0.5083\n",
      "Epoch 73/100\n",
      "6090/6090 [==============================] - 1s 116us/sample - loss: 0.4469 - val_loss: 0.5075\n",
      "Epoch 74/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.4455 - val_loss: 0.5067\n",
      "Epoch 75/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.4442 - val_loss: 0.5060\n",
      "Epoch 76/100\n",
      "6090/6090 [==============================] - 1s 99us/sample - loss: 0.4428 - val_loss: 0.5053\n",
      "Epoch 77/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.4414 - val_loss: 0.5045\n",
      "Epoch 78/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.4401 - val_loss: 0.5038\n",
      "Epoch 79/100\n",
      "6090/6090 [==============================] - 1s 101us/sample - loss: 0.4388 - val_loss: 0.5031\n",
      "Epoch 80/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4375 - val_loss: 0.5024\n",
      "Epoch 81/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4362 - val_loss: 0.5018\n",
      "Epoch 82/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4349 - val_loss: 0.5011\n",
      "Epoch 83/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.4337 - val_loss: 0.5005\n",
      "Epoch 84/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.4325 - val_loss: 0.4998\n",
      "Epoch 85/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.4313 - val_loss: 0.4992\n",
      "Epoch 86/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.4300 - val_loss: 0.4986\n",
      "Epoch 87/100\n",
      "6090/6090 [==============================] - 1s 101us/sample - loss: 0.4288 - val_loss: 0.4980\n",
      "Epoch 88/100\n",
      "6090/6090 [==============================] - 1s 98us/sample - loss: 0.4277 - val_loss: 0.4975\n",
      "Epoch 89/100\n",
      "6090/6090 [==============================] - 1s 99us/sample - loss: 0.4265 - val_loss: 0.4969\n",
      "Epoch 90/100\n",
      "6090/6090 [==============================] - 1s 99us/sample - loss: 0.4253 - val_loss: 0.4964\n",
      "Epoch 91/100\n",
      "6090/6090 [==============================] - 1s 99us/sample - loss: 0.4242 - val_loss: 0.4959\n",
      "Epoch 92/100\n",
      "6090/6090 [==============================] - 1s 99us/sample - loss: 0.4231 - val_loss: 0.4953\n",
      "Epoch 93/100\n",
      "6090/6090 [==============================] - 1s 99us/sample - loss: 0.4219 - val_loss: 0.4947\n",
      "Epoch 94/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.4208 - val_loss: 0.4942\n",
      "Epoch 95/100\n",
      "6090/6090 [==============================] - 1s 99us/sample - loss: 0.4198 - val_loss: 0.4937\n",
      "Epoch 96/100\n",
      "6090/6090 [==============================] - 1s 98us/sample - loss: 0.4187 - val_loss: 0.4932\n",
      "Epoch 97/100\n",
      "6090/6090 [==============================] - 1s 99us/sample - loss: 0.4176 - val_loss: 0.4927\n",
      "Epoch 98/100\n",
      "6090/6090 [==============================] - 1s 98us/sample - loss: 0.4166 - val_loss: 0.4922\n",
      "Epoch 99/100\n",
      "6090/6090 [==============================] - 1s 99us/sample - loss: 0.4155 - val_loss: 0.4918\n",
      "Epoch 100/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.4145 - val_loss: 0.4913\n",
      "Predicting...\n",
      "(Took 63.935 sec)\n",
      "Train on 6091 samples, validate on 1522 samples\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6091/6091 [==============================] - ETA: 0s - loss: 0.686 - 1s 142us/sample - loss: 0.6855 - val_loss: 0.6789\n",
      "Epoch 2/100\n",
      "6091/6091 [==============================] - 1s 101us/sample - loss: 0.6737 - val_loss: 0.6695\n",
      "Epoch 3/100\n",
      "6091/6091 [==============================] - 1s 99us/sample - loss: 0.6648 - val_loss: 0.6621\n",
      "Epoch 4/100\n",
      "6091/6091 [==============================] - 1s 99us/sample - loss: 0.6571 - val_loss: 0.6555\n",
      "Epoch 5/100\n",
      "6091/6091 [==============================] - 1s 98us/sample - loss: 0.6501 - val_loss: 0.6494\n",
      "Epoch 6/100\n",
      "6091/6091 [==============================] - 1s 101us/sample - loss: 0.6433 - val_loss: 0.6435\n",
      "Epoch 7/100\n",
      "6091/6091 [==============================] - 1s 110us/sample - loss: 0.6369 - val_loss: 0.6380\n",
      "Epoch 8/100\n",
      "6091/6091 [==============================] - 1s 102us/sample - loss: 0.6307 - val_loss: 0.6327\n",
      "Epoch 9/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.6248 - val_loss: 0.6276\n",
      "Epoch 10/100\n",
      "6091/6091 [==============================] - 1s 105us/sample - loss: 0.6190 - val_loss: 0.6227\n",
      "Epoch 11/100\n",
      "6091/6091 [==============================] - 1s 97us/sample - loss: 0.6135 - val_loss: 0.6181\n",
      "Epoch 12/100\n",
      "6091/6091 [==============================] - 1s 98us/sample - loss: 0.6082 - val_loss: 0.6136\n",
      "Epoch 13/100\n",
      "6091/6091 [==============================] - 1s 97us/sample - loss: 0.6031 - val_loss: 0.6093\n",
      "Epoch 14/100\n",
      "6091/6091 [==============================] - 1s 96us/sample - loss: 0.5982 - val_loss: 0.6051\n",
      "Epoch 15/100\n",
      "6091/6091 [==============================] - 1s 98us/sample - loss: 0.5934 - val_loss: 0.6012\n",
      "Epoch 16/100\n",
      "6091/6091 [==============================] - 1s 97us/sample - loss: 0.5888 - val_loss: 0.5973\n",
      "Epoch 17/100\n",
      "6091/6091 [==============================] - 1s 102us/sample - loss: 0.5843 - val_loss: 0.5936\n",
      "Epoch 18/100\n",
      "6091/6091 [==============================] - 1s 97us/sample - loss: 0.5800 - val_loss: 0.5901\n",
      "Epoch 19/100\n",
      "6091/6091 [==============================] - 1s 98us/sample - loss: 0.5758 - val_loss: 0.5866\n",
      "Epoch 20/100\n",
      "6091/6091 [==============================] - 1s 98us/sample - loss: 0.5718 - val_loss: 0.5833\n",
      "Epoch 21/100\n",
      "6091/6091 [==============================] - 1s 111us/sample - loss: 0.5678 - val_loss: 0.5801\n",
      "Epoch 22/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.5640 - val_loss: 0.5769\n",
      "Epoch 23/100\n",
      "6091/6091 [==============================] - 1s 104us/sample - loss: 0.5603 - val_loss: 0.5740\n",
      "Epoch 24/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.5567 - val_loss: 0.5710\n",
      "Epoch 25/100\n",
      "6091/6091 [==============================] - 1s 105us/sample - loss: 0.5532 - val_loss: 0.5683\n",
      "Epoch 26/100\n",
      "6091/6091 [==============================] - 1s 102us/sample - loss: 0.5498 - val_loss: 0.5656\n",
      "Epoch 27/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.5465 - val_loss: 0.5629\n",
      "Epoch 28/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.5432 - val_loss: 0.5604\n",
      "Epoch 29/100\n",
      "6091/6091 [==============================] - 1s 97us/sample - loss: 0.5401 - val_loss: 0.5579\n",
      "Epoch 30/100\n",
      "6091/6091 [==============================] - 1s 97us/sample - loss: 0.5370 - val_loss: 0.5555\n",
      "Epoch 31/100\n",
      "6091/6091 [==============================] - 1s 99us/sample - loss: 0.5340 - val_loss: 0.5532\n",
      "Epoch 32/100\n",
      "6091/6091 [==============================] - 1s 98us/sample - loss: 0.5311 - val_loss: 0.5509\n",
      "Epoch 33/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.5282 - val_loss: 0.5487\n",
      "Epoch 34/100\n",
      "6091/6091 [==============================] - 1s 106us/sample - loss: 0.5255 - val_loss: 0.5466\n",
      "Epoch 35/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.5227 - val_loss: 0.5445\n",
      "Epoch 36/100\n",
      "6091/6091 [==============================] - 1s 103us/sample - loss: 0.5200 - val_loss: 0.5425\n",
      "Epoch 37/100\n",
      "6091/6091 [==============================] - 1s 103us/sample - loss: 0.5175 - val_loss: 0.5405\n",
      "Epoch 38/100\n",
      "6091/6091 [==============================] - 1s 105us/sample - loss: 0.5149 - val_loss: 0.5386\n",
      "Epoch 39/100\n",
      "6091/6091 [==============================] - 1s 103us/sample - loss: 0.5124 - val_loss: 0.5367\n",
      "Epoch 40/100\n",
      "6091/6091 [==============================] - 1s 119us/sample - loss: 0.5100 - val_loss: 0.5349\n",
      "Epoch 41/100\n",
      "6091/6091 [==============================] - 1s 116us/sample - loss: 0.5076 - val_loss: 0.5332\n",
      "Epoch 42/100\n",
      "6091/6091 [==============================] - 1s 111us/sample - loss: 0.5052 - val_loss: 0.5314\n",
      "Epoch 43/100\n",
      "6091/6091 [==============================] - 1s 116us/sample - loss: 0.5029 - val_loss: 0.5298\n",
      "Epoch 44/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.5007 - val_loss: 0.5281\n",
      "Epoch 45/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4985 - val_loss: 0.5265\n",
      "Epoch 46/100\n",
      "6091/6091 [==============================] - 1s 113us/sample - loss: 0.4963 - val_loss: 0.5250\n",
      "Epoch 47/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.4942 - val_loss: 0.5235\n",
      "Epoch 48/100\n",
      "6091/6091 [==============================] - 1s 117us/sample - loss: 0.4921 - val_loss: 0.5220\n",
      "Epoch 49/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.4901 - val_loss: 0.5205\n",
      "Epoch 50/100\n",
      "6091/6091 [==============================] - 1s 117us/sample - loss: 0.4881 - val_loss: 0.5191\n",
      "Epoch 51/100\n",
      "6091/6091 [==============================] - 1s 117us/sample - loss: 0.4861 - val_loss: 0.5178\n",
      "Epoch 52/100\n",
      "6091/6091 [==============================] - 1s 128us/sample - loss: 0.4842 - val_loss: 0.5164\n",
      "Epoch 53/100\n",
      "6091/6091 [==============================] - 1s 124us/sample - loss: 0.4823 - val_loss: 0.5151\n",
      "Epoch 54/100\n",
      "6091/6091 [==============================] - 1s 116us/sample - loss: 0.4804 - val_loss: 0.5138\n",
      "Epoch 55/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.4786 - val_loss: 0.5125\n",
      "Epoch 56/100\n",
      "6091/6091 [==============================] - 1s 110us/sample - loss: 0.4768 - val_loss: 0.5112\n",
      "Epoch 57/100\n",
      "6091/6091 [==============================] - 1s 145us/sample - loss: 0.4750 - val_loss: 0.5101\n",
      "Epoch 58/100\n",
      "6091/6091 [==============================] - 1s 140us/sample - loss: 0.4732 - val_loss: 0.5089\n",
      "Epoch 59/100\n",
      "6091/6091 [==============================] - 1s 146us/sample - loss: 0.4715 - val_loss: 0.5077\n",
      "Epoch 60/100\n",
      "6091/6091 [==============================] - 1s 140us/sample - loss: 0.4698 - val_loss: 0.5066\n",
      "Epoch 61/100\n",
      "6091/6091 [==============================] - 1s 127us/sample - loss: 0.4682 - val_loss: 0.5054\n",
      "Epoch 62/100\n",
      "6091/6091 [==============================] - 1s 139us/sample - loss: 0.4665 - val_loss: 0.5044\n",
      "Epoch 63/100\n",
      "6091/6091 [==============================] - 1s 132us/sample - loss: 0.4649 - val_loss: 0.5033\n",
      "Epoch 64/100\n",
      "6091/6091 [==============================] - 1s 124us/sample - loss: 0.4633 - val_loss: 0.5023\n",
      "Epoch 65/100\n",
      "6091/6091 [==============================] - 1s 132us/sample - loss: 0.4617 - val_loss: 0.5013\n",
      "Epoch 66/100\n",
      "6091/6091 [==============================] - 1s 125us/sample - loss: 0.4602 - val_loss: 0.5002\n",
      "Epoch 67/100\n",
      "6091/6091 [==============================] - 1s 122us/sample - loss: 0.4587 - val_loss: 0.4993\n",
      "Epoch 68/100\n",
      "6091/6091 [==============================] - 1s 124us/sample - loss: 0.4572 - val_loss: 0.4983\n",
      "Epoch 69/100\n",
      "6091/6091 [==============================] - 1s 105us/sample - loss: 0.4557 - val_loss: 0.4973\n",
      "Epoch 70/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.4542 - val_loss: 0.4965\n",
      "Epoch 71/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.4528 - val_loss: 0.4955\n",
      "Epoch 72/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.4513 - val_loss: 0.4946\n",
      "Epoch 73/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4499 - val_loss: 0.4938\n",
      "Epoch 74/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.4486 - val_loss: 0.4929\n",
      "Epoch 75/100\n",
      "6091/6091 [==============================] - 1s 105us/sample - loss: 0.4472 - val_loss: 0.4920\n",
      "Epoch 76/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.4458 - val_loss: 0.4913\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/100\n",
      "6091/6091 [==============================] - 1s 98us/sample - loss: 0.4445 - val_loss: 0.4904\n",
      "Epoch 78/100\n",
      "6091/6091 [==============================] - 1s 98us/sample - loss: 0.4432 - val_loss: 0.4896\n",
      "Epoch 79/100\n",
      "6091/6091 [==============================] - 1s 97us/sample - loss: 0.4419 - val_loss: 0.4889\n",
      "Epoch 80/100\n",
      "6091/6091 [==============================] - 1s 103us/sample - loss: 0.4406 - val_loss: 0.4881\n",
      "Epoch 81/100\n",
      "6091/6091 [==============================] - 1s 102us/sample - loss: 0.4394 - val_loss: 0.4873\n",
      "Epoch 82/100\n",
      "6091/6091 [==============================] - 1s 102us/sample - loss: 0.4381 - val_loss: 0.4865\n",
      "Epoch 83/100\n",
      "6091/6091 [==============================] - 1s 97us/sample - loss: 0.4369 - val_loss: 0.4858\n",
      "Epoch 84/100\n",
      "6091/6091 [==============================] - 1s 97us/sample - loss: 0.4357 - val_loss: 0.4851\n",
      "Epoch 85/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.4345 - val_loss: 0.4844\n",
      "Epoch 86/100\n",
      "6091/6091 [==============================] - 1s 99us/sample - loss: 0.4333 - val_loss: 0.4837\n",
      "Epoch 87/100\n",
      "6091/6091 [==============================] - 1s 97us/sample - loss: 0.4321 - val_loss: 0.4830\n",
      "Epoch 88/100\n",
      "6091/6091 [==============================] - 1s 98us/sample - loss: 0.4309 - val_loss: 0.4823\n",
      "Epoch 89/100\n",
      "6091/6091 [==============================] - 1s 98us/sample - loss: 0.4298 - val_loss: 0.4817\n",
      "Epoch 90/100\n",
      "6091/6091 [==============================] - 1s 99us/sample - loss: 0.4286 - val_loss: 0.4810\n",
      "Epoch 91/100\n",
      "6091/6091 [==============================] - 1s 99us/sample - loss: 0.4276 - val_loss: 0.4804\n",
      "Epoch 92/100\n",
      "6091/6091 [==============================] - 1s 98us/sample - loss: 0.4264 - val_loss: 0.4798\n",
      "Epoch 93/100\n",
      "6091/6091 [==============================] - 1s 98us/sample - loss: 0.4253 - val_loss: 0.4791\n",
      "Epoch 94/100\n",
      "6091/6091 [==============================] - 1s 97us/sample - loss: 0.4242 - val_loss: 0.4785\n",
      "Epoch 95/100\n",
      "6091/6091 [==============================] - ETA: 0s - loss: 0.424 - 1s 98us/sample - loss: 0.4232 - val_loss: 0.4779\n",
      "Epoch 96/100\n",
      "6091/6091 [==============================] - 1s 99us/sample - loss: 0.4221 - val_loss: 0.4774\n",
      "Epoch 97/100\n",
      "6091/6091 [==============================] - 1s 98us/sample - loss: 0.4210 - val_loss: 0.4768\n",
      "Epoch 98/100\n",
      "6091/6091 [==============================] - 1s 97us/sample - loss: 0.4200 - val_loss: 0.4762\n",
      "Epoch 99/100\n",
      "6091/6091 [==============================] - 1s 98us/sample - loss: 0.4190 - val_loss: 0.4756\n",
      "Epoch 100/100\n",
      "6091/6091 [==============================] - 1s 99us/sample - loss: 0.4179 - val_loss: 0.4751\n",
      "Predicting...\n",
      "(Took 66.374 sec)\n",
      "Train on 6091 samples, validate on 1522 samples\n",
      "Epoch 1/100\n",
      "6091/6091 [==============================] - 1s 140us/sample - loss: 0.6848 - val_loss: 0.6806\n",
      "Epoch 2/100\n",
      "6091/6091 [==============================] - 1s 99us/sample - loss: 0.6728 - val_loss: 0.6726\n",
      "Epoch 3/100\n",
      "6091/6091 [==============================] - 1s 99us/sample - loss: 0.6638 - val_loss: 0.6659\n",
      "Epoch 4/100\n",
      "6091/6091 [==============================] - 1s 98us/sample - loss: 0.6561 - val_loss: 0.6597\n",
      "Epoch 5/100\n",
      "6091/6091 [==============================] - 1s 99us/sample - loss: 0.6488 - val_loss: 0.6539\n",
      "Epoch 6/100\n",
      "6091/6091 [==============================] - 1s 98us/sample - loss: 0.6421 - val_loss: 0.6482\n",
      "Epoch 7/100\n",
      "6091/6091 [==============================] - 1s 99us/sample - loss: 0.6356 - val_loss: 0.6426\n",
      "Epoch 8/100\n",
      "6091/6091 [==============================] - 1s 100us/sample - loss: 0.6294 - val_loss: 0.6374\n",
      "Epoch 9/100\n",
      "6091/6091 [==============================] - 1s 99us/sample - loss: 0.6234 - val_loss: 0.6323\n",
      "Epoch 10/100\n",
      "6091/6091 [==============================] - 1s 102us/sample - loss: 0.6176 - val_loss: 0.6274\n",
      "Epoch 11/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.6121 - val_loss: 0.6229\n",
      "Epoch 12/100\n",
      "6091/6091 [==============================] - 1s 111us/sample - loss: 0.6068 - val_loss: 0.6184\n",
      "Epoch 13/100\n",
      "6091/6091 [==============================] - 1s 120us/sample - loss: 0.6016 - val_loss: 0.6142\n",
      "Epoch 14/100\n",
      "6091/6091 [==============================] - 1s 114us/sample - loss: 0.5967 - val_loss: 0.6100\n",
      "Epoch 15/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.5918 - val_loss: 0.6061\n",
      "Epoch 16/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.5872 - val_loss: 0.6023\n",
      "Epoch 17/100\n",
      "6091/6091 [==============================] - 1s 113us/sample - loss: 0.5827 - val_loss: 0.5987\n",
      "Epoch 18/100\n",
      "6091/6091 [==============================] - 1s 111us/sample - loss: 0.5784 - val_loss: 0.5951\n",
      "Epoch 19/100\n",
      "6091/6091 [==============================] - 1s 125us/sample - loss: 0.5741 - val_loss: 0.5917\n",
      "Epoch 20/100\n",
      "6091/6091 [==============================] - 1s 136us/sample - loss: 0.5701 - val_loss: 0.5886\n",
      "Epoch 21/100\n",
      "6091/6091 [==============================] - 1s 131us/sample - loss: 0.5661 - val_loss: 0.5853\n",
      "Epoch 22/100\n",
      "6091/6091 [==============================] - 1s 129us/sample - loss: 0.5623 - val_loss: 0.5823\n",
      "Epoch 23/100\n",
      "6091/6091 [==============================] - 1s 121us/sample - loss: 0.5585 - val_loss: 0.5794\n",
      "Epoch 24/100\n",
      "6091/6091 [==============================] - 1s 117us/sample - loss: 0.5549 - val_loss: 0.5765\n",
      "Epoch 25/100\n",
      "6091/6091 [==============================] - 1s 119us/sample - loss: 0.5514 - val_loss: 0.5737\n",
      "Epoch 26/100\n",
      "6091/6091 [==============================] - 1s 117us/sample - loss: 0.5480 - val_loss: 0.5711\n",
      "Epoch 27/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.5446 - val_loss: 0.5686\n",
      "Epoch 28/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.5413 - val_loss: 0.5661\n",
      "Epoch 29/100\n",
      "6091/6091 [==============================] - 1s 118us/sample - loss: 0.5382 - val_loss: 0.5637\n",
      "Epoch 30/100\n",
      "6091/6091 [==============================] - 1s 113us/sample - loss: 0.5351 - val_loss: 0.5613\n",
      "Epoch 31/100\n",
      "6091/6091 [==============================] - 1s 114us/sample - loss: 0.5321 - val_loss: 0.5590\n",
      "Epoch 32/100\n",
      "6091/6091 [==============================] - 1s 114us/sample - loss: 0.5291 - val_loss: 0.5568\n",
      "Epoch 33/100\n",
      "6091/6091 [==============================] - 1s 116us/sample - loss: 0.5262 - val_loss: 0.5547\n",
      "Epoch 34/100\n",
      "6091/6091 [==============================] - 1s 116us/sample - loss: 0.5235 - val_loss: 0.5526\n",
      "Epoch 35/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.5207 - val_loss: 0.5506\n",
      "Epoch 36/100\n",
      "6091/6091 [==============================] - 1s 113us/sample - loss: 0.5181 - val_loss: 0.5486\n",
      "Epoch 37/100\n",
      "6091/6091 [==============================] - 1s 113us/sample - loss: 0.5154 - val_loss: 0.5467\n",
      "Epoch 38/100\n",
      "6091/6091 [==============================] - 1s 115us/sample - loss: 0.5129 - val_loss: 0.5449\n",
      "Epoch 39/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.5104 - val_loss: 0.5431\n",
      "Epoch 40/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.5079 - val_loss: 0.5413\n",
      "Epoch 41/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.5055 - val_loss: 0.5396\n",
      "Epoch 42/100\n",
      "6091/6091 [==============================] - 1s 106us/sample - loss: 0.5032 - val_loss: 0.5379\n",
      "Epoch 43/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.5009 - val_loss: 0.5363\n",
      "Epoch 44/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.4986 - val_loss: 0.5347\n",
      "Epoch 45/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.4964 - val_loss: 0.5333\n",
      "Epoch 46/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4942 - val_loss: 0.5317\n",
      "Epoch 47/100\n",
      "6091/6091 [==============================] - 1s 111us/sample - loss: 0.4921 - val_loss: 0.5303\n",
      "Epoch 48/100\n",
      "6091/6091 [==============================] - 1s 117us/sample - loss: 0.4900 - val_loss: 0.5288\n",
      "Epoch 49/100\n",
      "6091/6091 [==============================] - 1s 164us/sample - loss: 0.4879 - val_loss: 0.5275\n",
      "Epoch 50/100\n",
      "6091/6091 [==============================] - 1s 129us/sample - loss: 0.4859 - val_loss: 0.5262\n",
      "Epoch 51/100\n",
      "6091/6091 [==============================] - 1s 127us/sample - loss: 0.4840 - val_loss: 0.5248\n",
      "Epoch 52/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.4820 - val_loss: 0.5235\n",
      "Epoch 53/100\n",
      "6091/6091 [==============================] - 1s 106us/sample - loss: 0.4801 - val_loss: 0.5222\n",
      "Epoch 54/100\n",
      "6091/6091 [==============================] - 1s 101us/sample - loss: 0.4782 - val_loss: 0.5210\n",
      "Epoch 55/100\n",
      "6091/6091 [==============================] - 1s 106us/sample - loss: 0.4764 - val_loss: 0.5198\n",
      "Epoch 56/100\n",
      "6091/6091 [==============================] - 1s 102us/sample - loss: 0.4746 - val_loss: 0.5186\n",
      "Epoch 57/100\n",
      "6091/6091 [==============================] - 1s 100us/sample - loss: 0.4728 - val_loss: 0.5175\n",
      "Epoch 58/100\n",
      "6091/6091 [==============================] - 1s 103us/sample - loss: 0.4710 - val_loss: 0.5163\n",
      "Epoch 59/100\n",
      "6091/6091 [==============================] - 1s 119us/sample - loss: 0.4693 - val_loss: 0.5153\n",
      "Epoch 60/100\n",
      "6091/6091 [==============================] - 1s 105us/sample - loss: 0.4676 - val_loss: 0.5142\n",
      "Epoch 61/100\n",
      "6091/6091 [==============================] - 1s 100us/sample - loss: 0.4659 - val_loss: 0.5132\n",
      "Epoch 62/100\n",
      "6091/6091 [==============================] - 1s 133us/sample - loss: 0.4643 - val_loss: 0.5121\n",
      "Epoch 63/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.4627 - val_loss: 0.5111\n",
      "Epoch 64/100\n",
      "6091/6091 [==============================] - 1s 105us/sample - loss: 0.4611 - val_loss: 0.5101\n",
      "Epoch 65/100\n",
      "6091/6091 [==============================] - 1s 96us/sample - loss: 0.4595 - val_loss: 0.5091\n",
      "Epoch 66/100\n",
      "6091/6091 [==============================] - 1s 99us/sample - loss: 0.4579 - val_loss: 0.5082\n",
      "Epoch 67/100\n",
      "6091/6091 [==============================] - 1s 94us/sample - loss: 0.4564 - val_loss: 0.5073\n",
      "Epoch 68/100\n",
      "6091/6091 [==============================] - 1s 92us/sample - loss: 0.4549 - val_loss: 0.5064\n",
      "Epoch 69/100\n",
      "6091/6091 [==============================] - 1s 92us/sample - loss: 0.4534 - val_loss: 0.5055\n",
      "Epoch 70/100\n",
      "6091/6091 [==============================] - 1s 98us/sample - loss: 0.4520 - val_loss: 0.5047\n",
      "Epoch 71/100\n",
      "6091/6091 [==============================] - 1s 93us/sample - loss: 0.4505 - val_loss: 0.5038\n",
      "Epoch 72/100\n",
      "6091/6091 [==============================] - 1s 92us/sample - loss: 0.4491 - val_loss: 0.5030\n",
      "Epoch 73/100\n",
      "6091/6091 [==============================] - 1s 94us/sample - loss: 0.4477 - val_loss: 0.5021\n",
      "Epoch 74/100\n",
      "6091/6091 [==============================] - 1s 94us/sample - loss: 0.4463 - val_loss: 0.5013\n",
      "Epoch 75/100\n",
      "6091/6091 [==============================] - 1s 103us/sample - loss: 0.4449 - val_loss: 0.5006\n",
      "Epoch 76/100\n",
      "6091/6091 [==============================] - 1s 93us/sample - loss: 0.4435 - val_loss: 0.4998\n",
      "Epoch 77/100\n",
      "6091/6091 [==============================] - 1s 93us/sample - loss: 0.4422 - val_loss: 0.4990\n",
      "Epoch 78/100\n",
      "6091/6091 [==============================] - 1s 92us/sample - loss: 0.4409 - val_loss: 0.4983\n",
      "Epoch 79/100\n",
      "6091/6091 [==============================] - 1s 93us/sample - loss: 0.4396 - val_loss: 0.4976\n",
      "Epoch 80/100\n",
      "6091/6091 [==============================] - 1s 93us/sample - loss: 0.4383 - val_loss: 0.4969\n",
      "Epoch 81/100\n",
      "6091/6091 [==============================] - 1s 93us/sample - loss: 0.4370 - val_loss: 0.4962\n",
      "Epoch 82/100\n",
      "6091/6091 [==============================] - 1s 93us/sample - loss: 0.4358 - val_loss: 0.4955\n",
      "Epoch 83/100\n",
      "6091/6091 [==============================] - 1s 93us/sample - loss: 0.4345 - val_loss: 0.4949\n",
      "Epoch 84/100\n",
      "6091/6091 [==============================] - 1s 93us/sample - loss: 0.4333 - val_loss: 0.4942\n",
      "Epoch 85/100\n",
      "6091/6091 [==============================] - 1s 93us/sample - loss: 0.4321 - val_loss: 0.4936\n",
      "Epoch 86/100\n",
      "6091/6091 [==============================] - 1s 93us/sample - loss: 0.4309 - val_loss: 0.4929\n",
      "Epoch 87/100\n",
      "6091/6091 [==============================] - 1s 94us/sample - loss: 0.4297 - val_loss: 0.4923\n",
      "Epoch 88/100\n",
      "6091/6091 [==============================] - 1s 132us/sample - loss: 0.4285 - val_loss: 0.4917\n",
      "Epoch 89/100\n",
      "6091/6091 [==============================] - 1s 96us/sample - loss: 0.4274 - val_loss: 0.4911\n",
      "Epoch 90/100\n",
      "6091/6091 [==============================] - 1s 114us/sample - loss: 0.4262 - val_loss: 0.4906\n",
      "Epoch 91/100\n",
      "6091/6091 [==============================] - 1s 121us/sample - loss: 0.4251 - val_loss: 0.4900\n",
      "Epoch 92/100\n",
      "6091/6091 [==============================] - 1s 117us/sample - loss: 0.4240 - val_loss: 0.4895\n",
      "Epoch 93/100\n",
      "6091/6091 [==============================] - 1s 133us/sample - loss: 0.4229 - val_loss: 0.4889\n",
      "Epoch 94/100\n",
      "6091/6091 [==============================] - 1s 113us/sample - loss: 0.4218 - val_loss: 0.4884\n",
      "Epoch 95/100\n",
      "6091/6091 [==============================] - 1s 98us/sample - loss: 0.4207 - val_loss: 0.4879\n",
      "Epoch 96/100\n",
      "6091/6091 [==============================] - 1s 94us/sample - loss: 0.4196 - val_loss: 0.4874\n",
      "Epoch 97/100\n",
      "6091/6091 [==============================] - 1s 122us/sample - loss: 0.4186 - val_loss: 0.4868\n",
      "Epoch 98/100\n",
      "6091/6091 [==============================] - 1s 124us/sample - loss: 0.4175 - val_loss: 0.4864\n",
      "Epoch 99/100\n",
      "6091/6091 [==============================] - 1s 125us/sample - loss: 0.4165 - val_loss: 0.4859\n",
      "Epoch 100/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4154 - val_loss: 0.4854\n",
      "Predicting...\n",
      "(Took 66.715 sec)\n",
      "Combined confusion matrix:\n",
      "[[3838.  504.]\n",
      " [1125. 2146.]]\n",
      "(Overall, took 328.608 sec)\n",
      "Accuracy: 78.60% +/- 0.78%\n",
      "Precision for positive class: 77.32% +/- 1.40%\n",
      "Precision for negative class: 80.98% +/- 0.80%\n",
      "Recall for positive class: 88.38% +/- 0.84%\n",
      "Recall for negative class: 65.61% +/- 0.73%\n",
      "F for positive class: 82.48% +/- 1.04%\n",
      "F for negative class: 72.49% +/- 0.38%\n",
      "Mean F score: 77.48% +/- 0.54%\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/100\n",
      "6090/6090 [==============================] - 1s 203us/sample - loss: 0.6833 - val_loss: 0.6729\n",
      "Epoch 2/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.6659 - val_loss: 0.6591\n",
      "Epoch 3/100\n",
      "6090/6090 [==============================] - 1s 117us/sample - loss: 0.6519 - val_loss: 0.6475\n",
      "Epoch 4/100\n",
      "6090/6090 [==============================] - 1s 116us/sample - loss: 0.6393 - val_loss: 0.6371\n",
      "Epoch 5/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.6275 - val_loss: 0.6276\n",
      "Epoch 6/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.6164 - val_loss: 0.6186\n",
      "Epoch 7/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.6058 - val_loss: 0.6102\n",
      "Epoch 8/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.5958 - val_loss: 0.6022\n",
      "Epoch 9/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.5862 - val_loss: 0.5947\n",
      "Epoch 10/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.5771 - val_loss: 0.5877\n",
      "Epoch 11/100\n",
      "6090/6090 [==============================] - 1s 131us/sample - loss: 0.5684 - val_loss: 0.5809\n",
      "Epoch 12/100\n",
      "6090/6090 [==============================] - 1s 123us/sample - loss: 0.5601 - val_loss: 0.5745\n",
      "Epoch 13/100\n",
      "6090/6090 [==============================] - 1s 127us/sample - loss: 0.5522 - val_loss: 0.5685\n",
      "Epoch 14/100\n",
      "6090/6090 [==============================] - 1s 126us/sample - loss: 0.5447 - val_loss: 0.5628\n",
      "Epoch 15/100\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.5374 - val_loss: 0.5574\n",
      "Epoch 16/100\n",
      "6090/6090 [==============================] - 1s 117us/sample - loss: 0.5305 - val_loss: 0.5522\n",
      "Epoch 17/100\n",
      "6090/6090 [==============================] - 1s 120us/sample - loss: 0.5239 - val_loss: 0.5474\n",
      "Epoch 18/100\n",
      "6090/6090 [==============================] - 1s 122us/sample - loss: 0.5175 - val_loss: 0.5427\n",
      "Epoch 19/100\n",
      "6090/6090 [==============================] - 1s 136us/sample - loss: 0.5115 - val_loss: 0.5383\n",
      "Epoch 20/100\n",
      "6090/6090 [==============================] - 1s 125us/sample - loss: 0.5056 - val_loss: 0.5341\n",
      "Epoch 21/100\n",
      "6090/6090 [==============================] - 1s 163us/sample - loss: 0.5000 - val_loss: 0.5302\n",
      "Epoch 22/100\n",
      "6090/6090 [==============================] - 1s 140us/sample - loss: 0.4946 - val_loss: 0.5264\n",
      "Epoch 23/100\n",
      "6090/6090 [==============================] - 1s 162us/sample - loss: 0.4894 - val_loss: 0.5227\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/100\n",
      "6090/6090 [==============================] - 1s 170us/sample - loss: 0.4844 - val_loss: 0.5193\n",
      "Epoch 25/100\n",
      "6090/6090 [==============================] - 1s 129us/sample - loss: 0.4796 - val_loss: 0.5161\n",
      "Epoch 26/100\n",
      "6090/6090 [==============================] - 1s 122us/sample - loss: 0.4749 - val_loss: 0.5129\n",
      "Epoch 27/100\n",
      "6090/6090 [==============================] - 1s 126us/sample - loss: 0.4704 - val_loss: 0.5100\n",
      "Epoch 28/100\n",
      "6090/6090 [==============================] - 1s 136us/sample - loss: 0.4661 - val_loss: 0.5072\n",
      "Epoch 29/100\n",
      "6090/6090 [==============================] - 1s 131us/sample - loss: 0.4619 - val_loss: 0.5045\n",
      "Epoch 30/100\n",
      "6090/6090 [==============================] - 1s 133us/sample - loss: 0.4579 - val_loss: 0.5019\n",
      "Epoch 31/100\n",
      "6090/6090 [==============================] - 1s 155us/sample - loss: 0.4540 - val_loss: 0.4994\n",
      "Epoch 32/100\n",
      "6090/6090 [==============================] - 1s 164us/sample - loss: 0.4501 - val_loss: 0.4971\n",
      "Epoch 33/100\n",
      "6090/6090 [==============================] - 1s 128us/sample - loss: 0.4465 - val_loss: 0.4949\n",
      "Epoch 34/100\n",
      "6090/6090 [==============================] - 1s 131us/sample - loss: 0.4429 - val_loss: 0.4927\n",
      "Epoch 35/100\n",
      "6090/6090 [==============================] - 1s 132us/sample - loss: 0.4395 - val_loss: 0.4907\n",
      "Epoch 36/100\n",
      "6090/6090 [==============================] - 1s 131us/sample - loss: 0.4361 - val_loss: 0.4887\n",
      "Epoch 37/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.4329 - val_loss: 0.4869\n",
      "Epoch 38/100\n",
      "6090/6090 [==============================] - 1s 122us/sample - loss: 0.4297 - val_loss: 0.4851\n",
      "Epoch 39/100\n",
      "6090/6090 [==============================] - 1s 119us/sample - loss: 0.4266 - val_loss: 0.4834\n",
      "Epoch 40/100\n",
      "6090/6090 [==============================] - 1s 123us/sample - loss: 0.4236 - val_loss: 0.4818\n",
      "Epoch 41/100\n",
      "6090/6090 [==============================] - 1s 128us/sample - loss: 0.4207 - val_loss: 0.4803\n",
      "Epoch 42/100\n",
      "6090/6090 [==============================] - 1s 134us/sample - loss: 0.4179 - val_loss: 0.4788\n",
      "Epoch 43/100\n",
      "6090/6090 [==============================] - 1s 130us/sample - loss: 0.4151 - val_loss: 0.4774\n",
      "Epoch 44/100\n",
      "6090/6090 [==============================] - 1s 141us/sample - loss: 0.4125 - val_loss: 0.4760\n",
      "Epoch 45/100\n",
      "6090/6090 [==============================] - 1s 172us/sample - loss: 0.4098 - val_loss: 0.4747\n",
      "Epoch 46/100\n",
      "6090/6090 [==============================] - 1s 152us/sample - loss: 0.4073 - val_loss: 0.4735\n",
      "Epoch 47/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.4048 - val_loss: 0.4723\n",
      "Epoch 48/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.4024 - val_loss: 0.4712\n",
      "Epoch 49/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.4000 - val_loss: 0.4701\n",
      "Epoch 50/100\n",
      "6090/6090 [==============================] - 1s 117us/sample - loss: 0.3977 - val_loss: 0.4690\n",
      "Epoch 51/100\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.3954 - val_loss: 0.4681\n",
      "Epoch 52/100\n",
      "6090/6090 [==============================] - 1s 117us/sample - loss: 0.3932 - val_loss: 0.4672\n",
      "Epoch 53/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.3910 - val_loss: 0.4662\n",
      "Epoch 54/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.3889 - val_loss: 0.4654\n",
      "Epoch 55/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.3868 - val_loss: 0.4646\n",
      "Epoch 56/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.3848 - val_loss: 0.4638\n",
      "Epoch 57/100\n",
      "6090/6090 [==============================] - 1s 101us/sample - loss: 0.3828 - val_loss: 0.4631\n",
      "Epoch 58/100\n",
      "6090/6090 [==============================] - 1s 99us/sample - loss: 0.3808 - val_loss: 0.4624\n",
      "Epoch 59/100\n",
      "6090/6090 [==============================] - 1s 101us/sample - loss: 0.3789 - val_loss: 0.4617\n",
      "Epoch 60/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.3771 - val_loss: 0.4610\n",
      "Epoch 61/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.3752 - val_loss: 0.4604\n",
      "Epoch 62/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.3734 - val_loss: 0.4598\n",
      "Epoch 63/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.3716 - val_loss: 0.4592\n",
      "Epoch 64/100\n",
      "6090/6090 [==============================] - 1s 101us/sample - loss: 0.3699 - val_loss: 0.4587\n",
      "Epoch 65/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.3682 - val_loss: 0.4582\n",
      "Epoch 66/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.3665 - val_loss: 0.4577\n",
      "Epoch 67/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.3649 - val_loss: 0.4573\n",
      "Epoch 68/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.3632 - val_loss: 0.4568\n",
      "Epoch 69/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.3616 - val_loss: 0.4564\n",
      "Epoch 70/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.3601 - val_loss: 0.4561\n",
      "Epoch 71/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.3585 - val_loss: 0.4557\n",
      "Epoch 72/100\n",
      "6090/6090 [==============================] - 1s 101us/sample - loss: 0.3570 - val_loss: 0.4553\n",
      "Epoch 73/100\n",
      "6090/6090 [==============================] - 1s 123us/sample - loss: 0.3555 - val_loss: 0.4550\n",
      "Epoch 74/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.3541 - val_loss: 0.4548\n",
      "Epoch 75/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.3526 - val_loss: 0.4545\n",
      "Epoch 76/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.3512 - val_loss: 0.4541\n",
      "Epoch 77/100\n",
      "6090/6090 [==============================] - 1s 101us/sample - loss: 0.3498 - val_loss: 0.4539\n",
      "Epoch 78/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.3484 - val_loss: 0.4536\n",
      "Epoch 79/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.3471 - val_loss: 0.4535\n",
      "Epoch 80/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.3457 - val_loss: 0.4532\n",
      "Epoch 81/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.3444 - val_loss: 0.4531\n",
      "Epoch 82/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.3431 - val_loss: 0.4529\n",
      "Epoch 83/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.3418 - val_loss: 0.4527\n",
      "Epoch 84/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.3406 - val_loss: 0.4526\n",
      "Epoch 85/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.3393 - val_loss: 0.4524\n",
      "Epoch 86/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.3381 - val_loss: 0.4523\n",
      "Epoch 87/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.3369 - val_loss: 0.4523\n",
      "Epoch 88/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.3357 - val_loss: 0.4522\n",
      "Epoch 89/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.3345 - val_loss: 0.4521\n",
      "Epoch 90/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.3334 - val_loss: 0.4522\n",
      "Predicting...\n",
      "(Took 66.020 sec)\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/100\n",
      "6090/6090 [==============================] - 1s 145us/sample - loss: 0.6837 - val_loss: 0.6726\n",
      "Epoch 2/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.6666 - val_loss: 0.6579\n",
      "Epoch 3/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.6526 - val_loss: 0.6461\n",
      "Epoch 4/100\n",
      "6090/6090 [==============================] - 1s 101us/sample - loss: 0.6400 - val_loss: 0.6356\n",
      "Epoch 5/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.6282 - val_loss: 0.6258\n",
      "Epoch 6/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.6172 - val_loss: 0.6169\n",
      "Epoch 7/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.6066 - val_loss: 0.6085\n",
      "Epoch 8/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.5966 - val_loss: 0.6005\n",
      "Epoch 9/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.5871 - val_loss: 0.5931\n",
      "Epoch 10/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.5779 - val_loss: 0.5860\n",
      "Epoch 11/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.5693 - val_loss: 0.5794\n",
      "Epoch 12/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.5610 - val_loss: 0.5730\n",
      "Epoch 13/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.5531 - val_loss: 0.5670\n",
      "Epoch 14/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.5455 - val_loss: 0.5615\n",
      "Epoch 15/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.5383 - val_loss: 0.5562\n",
      "Epoch 16/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.5313 - val_loss: 0.5511\n",
      "Epoch 17/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.5247 - val_loss: 0.5462\n",
      "Epoch 18/100\n",
      "6090/6090 [==============================] - 1s 125us/sample - loss: 0.5183 - val_loss: 0.5417\n",
      "Epoch 19/100\n",
      "6090/6090 [==============================] - 1s 128us/sample - loss: 0.5122 - val_loss: 0.5373\n",
      "Epoch 20/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.5064 - val_loss: 0.5333\n",
      "Epoch 21/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.5007 - val_loss: 0.5294\n",
      "Epoch 22/100\n",
      "6090/6090 [==============================] - 1s 119us/sample - loss: 0.4953 - val_loss: 0.5257\n",
      "Epoch 23/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.4901 - val_loss: 0.5223\n",
      "Epoch 24/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.4851 - val_loss: 0.5189\n",
      "Epoch 25/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.4802 - val_loss: 0.5158\n",
      "Epoch 26/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.4755 - val_loss: 0.5128\n",
      "Epoch 27/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.4710 - val_loss: 0.5099\n",
      "Epoch 28/100\n",
      "6090/6090 [==============================] - 1s 99us/sample - loss: 0.4667 - val_loss: 0.5073\n",
      "Epoch 29/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4625 - val_loss: 0.5047\n",
      "Epoch 30/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.4584 - val_loss: 0.5022\n",
      "Epoch 31/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.4545 - val_loss: 0.4999\n",
      "Epoch 32/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.4507 - val_loss: 0.4977\n",
      "Epoch 33/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.4470 - val_loss: 0.4956\n",
      "Epoch 34/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.4434 - val_loss: 0.4936\n",
      "Epoch 35/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.4400 - val_loss: 0.4916\n",
      "Epoch 36/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.4366 - val_loss: 0.4898\n",
      "Epoch 37/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.4333 - val_loss: 0.4881\n",
      "Epoch 38/100\n",
      "6090/6090 [==============================] - 1s 118us/sample - loss: 0.4301 - val_loss: 0.4865\n",
      "Epoch 39/100\n",
      "6090/6090 [==============================] - 1s 117us/sample - loss: 0.4270 - val_loss: 0.4850\n",
      "Epoch 40/100\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.4240 - val_loss: 0.4835\n",
      "Epoch 41/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.4211 - val_loss: 0.4821\n",
      "Epoch 42/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.4183 - val_loss: 0.4807\n",
      "Epoch 43/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.4155 - val_loss: 0.4794\n",
      "Epoch 44/100\n",
      "6090/6090 [==============================] - 1s 130us/sample - loss: 0.4128 - val_loss: 0.4783\n",
      "Epoch 45/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4102 - val_loss: 0.4771\n",
      "Epoch 46/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4076 - val_loss: 0.4760\n",
      "Epoch 47/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4051 - val_loss: 0.4749\n",
      "Epoch 48/100\n",
      "6090/6090 [==============================] - 1s 123us/sample - loss: 0.4027 - val_loss: 0.4740\n",
      "Epoch 49/100\n",
      "6090/6090 [==============================] - 1s 122us/sample - loss: 0.4003 - val_loss: 0.4730\n",
      "Epoch 50/100\n",
      "6090/6090 [==============================] - 1s 120us/sample - loss: 0.3979 - val_loss: 0.4721\n",
      "Epoch 51/100\n",
      "6090/6090 [==============================] - 1s 117us/sample - loss: 0.3956 - val_loss: 0.4713\n",
      "Epoch 52/100\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.3934 - val_loss: 0.4704\n",
      "Epoch 53/100\n",
      "6090/6090 [==============================] - 1s 126us/sample - loss: 0.3912 - val_loss: 0.4697\n",
      "Epoch 54/100\n",
      "6090/6090 [==============================] - 1s 118us/sample - loss: 0.3891 - val_loss: 0.4690\n",
      "Epoch 55/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.3870 - val_loss: 0.4684\n",
      "Epoch 56/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.3850 - val_loss: 0.4677\n",
      "Epoch 57/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.3830 - val_loss: 0.4671\n",
      "Epoch 58/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.3810 - val_loss: 0.4666\n",
      "Epoch 59/100\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.3791 - val_loss: 0.4660\n",
      "Epoch 60/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.3772 - val_loss: 0.4655\n",
      "Epoch 61/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.3753 - val_loss: 0.4650\n",
      "Epoch 62/100\n",
      "6090/6090 [==============================] - 1s 130us/sample - loss: 0.3735 - val_loss: 0.4645\n",
      "Epoch 63/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.3718 - val_loss: 0.4642\n",
      "Epoch 64/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.3700 - val_loss: 0.4638\n",
      "Epoch 65/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.3683 - val_loss: 0.4635\n",
      "Epoch 66/100\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.3666 - val_loss: 0.4631\n",
      "Epoch 67/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.3650 - val_loss: 0.4628\n",
      "Epoch 68/100\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.3634 - val_loss: 0.4625\n",
      "Epoch 69/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.3618 - val_loss: 0.4623\n",
      "Epoch 70/100\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.3602 - val_loss: 0.4620\n",
      "Epoch 71/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.3587 - val_loss: 0.4619\n",
      "Epoch 72/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.3572 - val_loss: 0.4617\n",
      "Epoch 73/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.3557 - val_loss: 0.4615\n",
      "Epoch 74/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.3542 - val_loss: 0.4614\n",
      "Epoch 75/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.3528 - val_loss: 0.4612\n",
      "Epoch 76/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.3514 - val_loss: 0.4611\n",
      "Epoch 77/100\n",
      "6090/6090 [==============================] - 1s 120us/sample - loss: 0.3500 - val_loss: 0.4610\n",
      "Epoch 78/100\n",
      "6090/6090 [==============================] - 1s 122us/sample - loss: 0.3486 - val_loss: 0.4609\n",
      "Epoch 79/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.3472 - val_loss: 0.4609\n",
      "Epoch 80/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.3459 - val_loss: 0.4608\n",
      "Epoch 81/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.3446 - val_loss: 0.4608\n",
      "Predicting...\n",
      "(Took 55.748 sec)\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/100\n",
      "6090/6090 [==============================] - 1s 149us/sample - loss: 0.6835 - val_loss: 0.6767\n",
      "Epoch 2/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.6655 - val_loss: 0.6644\n",
      "Epoch 3/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.6510 - val_loss: 0.6540\n",
      "Epoch 4/100\n",
      "6090/6090 [==============================] - 1s 99us/sample - loss: 0.6382 - val_loss: 0.6444\n",
      "Epoch 5/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.6264 - val_loss: 0.6354\n",
      "Epoch 6/100\n",
      "6090/6090 [==============================] - 1s 101us/sample - loss: 0.6152 - val_loss: 0.6268\n",
      "Epoch 7/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.6047 - val_loss: 0.6187\n",
      "Epoch 8/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.5947 - val_loss: 0.6110\n",
      "Epoch 9/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.5851 - val_loss: 0.6038\n",
      "Epoch 10/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.5761 - val_loss: 0.5970\n",
      "Epoch 11/100\n",
      "6090/6090 [==============================] - 1s 120us/sample - loss: 0.5674 - val_loss: 0.5904\n",
      "Epoch 12/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.5591 - val_loss: 0.5842\n",
      "Epoch 13/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.5512 - val_loss: 0.5785\n",
      "Epoch 14/100\n",
      "6090/6090 [==============================] - 1s 101us/sample - loss: 0.5436 - val_loss: 0.5730\n",
      "Epoch 15/100\n",
      "6090/6090 [==============================] - 1s 116us/sample - loss: 0.5364 - val_loss: 0.5678\n",
      "Epoch 16/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.5294 - val_loss: 0.5627\n",
      "Epoch 17/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.5228 - val_loss: 0.5581\n",
      "Epoch 18/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.5164 - val_loss: 0.5537\n",
      "Epoch 19/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.5103 - val_loss: 0.5494\n",
      "Epoch 20/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.5045 - val_loss: 0.5456\n",
      "Epoch 21/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.4989 - val_loss: 0.5418\n",
      "Epoch 22/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4934 - val_loss: 0.5381\n",
      "Epoch 23/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.4882 - val_loss: 0.5346\n",
      "Epoch 24/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.4832 - val_loss: 0.5314\n",
      "Epoch 25/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.4784 - val_loss: 0.5285\n",
      "Epoch 26/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4737 - val_loss: 0.5256\n",
      "Epoch 27/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.4691 - val_loss: 0.5227\n",
      "Epoch 28/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4648 - val_loss: 0.5201\n",
      "Epoch 29/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.4605 - val_loss: 0.5176\n",
      "Epoch 30/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.4565 - val_loss: 0.5152\n",
      "Epoch 31/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.4525 - val_loss: 0.5129\n",
      "Epoch 32/100\n",
      "6090/6090 [==============================] - 1s 99us/sample - loss: 0.4487 - val_loss: 0.5108\n",
      "Epoch 33/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.4450 - val_loss: 0.5086\n",
      "Epoch 34/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.4414 - val_loss: 0.5067\n",
      "Epoch 35/100\n",
      "6090/6090 [==============================] - 1s 116us/sample - loss: 0.4379 - val_loss: 0.5047\n",
      "Epoch 36/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.4345 - val_loss: 0.5031\n",
      "Epoch 37/100\n",
      "6090/6090 [==============================] - 1s 117us/sample - loss: 0.4312 - val_loss: 0.5014\n",
      "Epoch 38/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4280 - val_loss: 0.4997\n",
      "Epoch 39/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4249 - val_loss: 0.4982\n",
      "Epoch 40/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.4219 - val_loss: 0.4967\n",
      "Epoch 41/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.4190 - val_loss: 0.4954\n",
      "Epoch 42/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.4161 - val_loss: 0.4940\n",
      "Epoch 43/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.4133 - val_loss: 0.4927\n",
      "Epoch 44/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4106 - val_loss: 0.4916\n",
      "Epoch 45/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4079 - val_loss: 0.4903\n",
      "Epoch 46/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.4054 - val_loss: 0.4893\n",
      "Epoch 47/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.4029 - val_loss: 0.4882\n",
      "Epoch 48/100\n",
      "6090/6090 [==============================] - 1s 116us/sample - loss: 0.4004 - val_loss: 0.4872\n",
      "Epoch 49/100\n",
      "6090/6090 [==============================] - 1s 117us/sample - loss: 0.3980 - val_loss: 0.4863\n",
      "Epoch 50/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.3956 - val_loss: 0.4855\n",
      "Epoch 51/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.3934 - val_loss: 0.4847\n",
      "Epoch 52/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.3911 - val_loss: 0.4838\n",
      "Epoch 53/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.3889 - val_loss: 0.4831\n",
      "Epoch 54/100\n",
      "6090/6090 [==============================] - 1s 99us/sample - loss: 0.3868 - val_loss: 0.4824\n",
      "Epoch 55/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.3847 - val_loss: 0.4817\n",
      "Epoch 56/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.3826 - val_loss: 0.4810\n",
      "Epoch 57/100\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.3806 - val_loss: 0.4804\n",
      "Epoch 58/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.3786 - val_loss: 0.4798\n",
      "Epoch 59/100\n",
      "6090/6090 [==============================] - 1s 123us/sample - loss: 0.3767 - val_loss: 0.4791\n",
      "Epoch 60/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.3748 - val_loss: 0.4787\n",
      "Epoch 61/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.3729 - val_loss: 0.4782\n",
      "Epoch 62/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.3711 - val_loss: 0.4776\n",
      "Epoch 63/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.3693 - val_loss: 0.4773\n",
      "Epoch 64/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.3675 - val_loss: 0.4769\n",
      "Epoch 65/100\n",
      "6090/6090 [==============================] - 1s 122us/sample - loss: 0.3658 - val_loss: 0.4765\n",
      "Epoch 66/100\n",
      "6090/6090 [==============================] - 1s 122us/sample - loss: 0.3641 - val_loss: 0.4762\n",
      "Epoch 67/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.3624 - val_loss: 0.4758\n",
      "Epoch 68/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.3608 - val_loss: 0.4755\n",
      "Epoch 69/100\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.3592 - val_loss: 0.4753\n",
      "Epoch 70/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.3576 - val_loss: 0.4750\n",
      "Epoch 71/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.3560 - val_loss: 0.4748\n",
      "Epoch 72/100\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.3545 - val_loss: 0.4746\n",
      "Epoch 73/100\n",
      "6090/6090 [==============================] - 1s 117us/sample - loss: 0.3530 - val_loss: 0.4744\n",
      "Epoch 74/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.3515 - val_loss: 0.4741\n",
      "Epoch 75/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.3500 - val_loss: 0.4739\n",
      "Epoch 76/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.3486 - val_loss: 0.4739\n",
      "Epoch 77/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.3472 - val_loss: 0.4737\n",
      "Epoch 78/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.3458 - val_loss: 0.4736\n",
      "Epoch 79/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.3444 - val_loss: 0.4735\n",
      "Epoch 80/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.3431 - val_loss: 0.4734\n",
      "Epoch 81/100\n",
      "6090/6090 [==============================] - 1s 116us/sample - loss: 0.3417 - val_loss: 0.4734\n",
      "Epoch 82/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.3404 - val_loss: 0.4732\n",
      "Epoch 83/100\n",
      "6090/6090 [==============================] - 1s 101us/sample - loss: 0.3391 - val_loss: 0.4732\n",
      "Epoch 84/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.3378 - val_loss: 0.4732\n",
      "Epoch 85/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.3366 - val_loss: 0.4732\n",
      "Epoch 86/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.3353 - val_loss: 0.4732\n",
      "Predicting...\n",
      "(Took 57.685 sec)\n",
      "Train on 6091 samples, validate on 1522 samples\n",
      "Epoch 1/100\n",
      "6091/6091 [==============================] - 1s 149us/sample - loss: 0.6839 - val_loss: 0.6752\n",
      "Epoch 2/100\n",
      "6091/6091 [==============================] - 1s 113us/sample - loss: 0.6663 - val_loss: 0.6615\n",
      "Epoch 3/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.6519 - val_loss: 0.6502\n",
      "Epoch 4/100\n",
      "6091/6091 [==============================] - 1s 116us/sample - loss: 0.6392 - val_loss: 0.6400\n",
      "Epoch 5/100\n",
      "6091/6091 [==============================] - 1s 102us/sample - loss: 0.6274 - val_loss: 0.6306\n",
      "Epoch 6/100\n",
      "6091/6091 [==============================] - 1s 105us/sample - loss: 0.6163 - val_loss: 0.6218\n",
      "Epoch 7/100\n",
      "6091/6091 [==============================] - 1s 113us/sample - loss: 0.6058 - val_loss: 0.6134\n",
      "Epoch 8/100\n",
      "6091/6091 [==============================] - 1s 103us/sample - loss: 0.5958 - val_loss: 0.6056\n",
      "Epoch 9/100\n",
      "6091/6091 [==============================] - 1s 104us/sample - loss: 0.5863 - val_loss: 0.5982\n",
      "Epoch 10/100\n",
      "6091/6091 [==============================] - 1s 111us/sample - loss: 0.5773 - val_loss: 0.5912\n",
      "Epoch 11/100\n",
      "6091/6091 [==============================] - 1s 102us/sample - loss: 0.5686 - val_loss: 0.5844\n",
      "Epoch 12/100\n",
      "6091/6091 [==============================] - 1s 99us/sample - loss: 0.5604 - val_loss: 0.5780\n",
      "Epoch 13/100\n",
      "6091/6091 [==============================] - 1s 100us/sample - loss: 0.5526 - val_loss: 0.5720\n",
      "Epoch 14/100\n",
      "6091/6091 [==============================] - 1s 100us/sample - loss: 0.5451 - val_loss: 0.5664\n",
      "Epoch 15/100\n",
      "6091/6091 [==============================] - 1s 99us/sample - loss: 0.5379 - val_loss: 0.5609\n",
      "Epoch 16/100\n",
      "6091/6091 [==============================] - 1s 101us/sample - loss: 0.5310 - val_loss: 0.5558\n",
      "Epoch 17/100\n",
      "6091/6091 [==============================] - 1s 99us/sample - loss: 0.5244 - val_loss: 0.5509\n",
      "Epoch 18/100\n",
      "6091/6091 [==============================] - 1s 110us/sample - loss: 0.5182 - val_loss: 0.5463\n",
      "Epoch 19/100\n",
      "6091/6091 [==============================] - 1s 104us/sample - loss: 0.5121 - val_loss: 0.5418\n",
      "Epoch 20/100\n",
      "6091/6091 [==============================] - 1s 102us/sample - loss: 0.5063 - val_loss: 0.5376\n",
      "Epoch 21/100\n",
      "6091/6091 [==============================] - 1s 100us/sample - loss: 0.5008 - val_loss: 0.5336\n",
      "Epoch 22/100\n",
      "6091/6091 [==============================] - 1s 101us/sample - loss: 0.4954 - val_loss: 0.5297\n",
      "Epoch 23/100\n",
      "6091/6091 [==============================] - 1s 103us/sample - loss: 0.4903 - val_loss: 0.5260\n",
      "Epoch 24/100\n",
      "6091/6091 [==============================] - 1s 100us/sample - loss: 0.4853 - val_loss: 0.5226\n",
      "Epoch 25/100\n",
      "6091/6091 [==============================] - 1s 100us/sample - loss: 0.4805 - val_loss: 0.5192\n",
      "Epoch 26/100\n",
      "6091/6091 [==============================] - 1s 102us/sample - loss: 0.4759 - val_loss: 0.5160\n",
      "Epoch 27/100\n",
      "6091/6091 [==============================] - 1s 100us/sample - loss: 0.4715 - val_loss: 0.5130\n",
      "Epoch 28/100\n",
      "6091/6091 [==============================] - 1s 100us/sample - loss: 0.4671 - val_loss: 0.5101\n",
      "Epoch 29/100\n",
      "6091/6091 [==============================] - 1s 100us/sample - loss: 0.4630 - val_loss: 0.5073\n",
      "Epoch 30/100\n",
      "6091/6091 [==============================] - 1s 102us/sample - loss: 0.4590 - val_loss: 0.5046\n",
      "Epoch 31/100\n",
      "6091/6091 [==============================] - 1s 101us/sample - loss: 0.4551 - val_loss: 0.5021\n",
      "Epoch 32/100\n",
      "6091/6091 [==============================] - 1s 102us/sample - loss: 0.4513 - val_loss: 0.4997\n",
      "Epoch 33/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4477 - val_loss: 0.4974\n",
      "Epoch 34/100\n",
      "6091/6091 [==============================] - 1s 115us/sample - loss: 0.4442 - val_loss: 0.4951\n",
      "Epoch 35/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4407 - val_loss: 0.4930\n",
      "Epoch 36/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.4374 - val_loss: 0.4910\n",
      "Epoch 37/100\n",
      "6091/6091 [==============================] - 1s 106us/sample - loss: 0.4342 - val_loss: 0.4890\n",
      "Epoch 38/100\n",
      "6091/6091 [==============================] - 1s 105us/sample - loss: 0.4310 - val_loss: 0.4871\n",
      "Epoch 39/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.4280 - val_loss: 0.4853\n",
      "Epoch 40/100\n",
      "6091/6091 [==============================] - 1s 101us/sample - loss: 0.4250 - val_loss: 0.4836\n",
      "Epoch 41/100\n",
      "6091/6091 [==============================] - 1s 101us/sample - loss: 0.4221 - val_loss: 0.4820\n",
      "Epoch 42/100\n",
      "6091/6091 [==============================] - 1s 100us/sample - loss: 0.4194 - val_loss: 0.4803\n",
      "Epoch 43/100\n",
      "6091/6091 [==============================] - 1s 103us/sample - loss: 0.4166 - val_loss: 0.4788\n",
      "Epoch 44/100\n",
      "6091/6091 [==============================] - 1s 106us/sample - loss: 0.4140 - val_loss: 0.4774\n",
      "Epoch 45/100\n",
      "6091/6091 [==============================] - 1s 106us/sample - loss: 0.4113 - val_loss: 0.4759\n",
      "Epoch 46/100\n",
      "6091/6091 [==============================] - 1s 118us/sample - loss: 0.4088 - val_loss: 0.4746\n",
      "Epoch 47/100\n",
      "6091/6091 [==============================] - 1s 118us/sample - loss: 0.4063 - val_loss: 0.4733\n",
      "Epoch 48/100\n",
      "6091/6091 [==============================] - 1s 123us/sample - loss: 0.4039 - val_loss: 0.4720\n",
      "Epoch 49/100\n",
      "6091/6091 [==============================] - 1s 116us/sample - loss: 0.4016 - val_loss: 0.4709\n",
      "Epoch 50/100\n",
      "6091/6091 [==============================] - 1s 111us/sample - loss: 0.3993 - val_loss: 0.4697\n",
      "Epoch 51/100\n",
      "6091/6091 [==============================] - 1s 119us/sample - loss: 0.3970 - val_loss: 0.4686\n",
      "Epoch 52/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.3948 - val_loss: 0.4675\n",
      "Epoch 53/100\n",
      "6091/6091 [==============================] - 1s 111us/sample - loss: 0.3927 - val_loss: 0.4665\n",
      "Epoch 54/100\n",
      "6091/6091 [==============================] - 1s 105us/sample - loss: 0.3905 - val_loss: 0.4655\n",
      "Epoch 55/100\n",
      "6091/6091 [==============================] - 1s 105us/sample - loss: 0.3885 - val_loss: 0.4645\n",
      "Epoch 56/100\n",
      "6091/6091 [==============================] - 1s 118us/sample - loss: 0.3864 - val_loss: 0.4637\n",
      "Epoch 57/100\n",
      "6091/6091 [==============================] - 1s 121us/sample - loss: 0.3845 - val_loss: 0.4628\n",
      "Epoch 58/100\n",
      "6091/6091 [==============================] - 1s 113us/sample - loss: 0.3825 - val_loss: 0.4619\n",
      "Epoch 59/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.3806 - val_loss: 0.4612\n",
      "Epoch 60/100\n",
      "6091/6091 [==============================] - 1s 105us/sample - loss: 0.3788 - val_loss: 0.4604\n",
      "Epoch 61/100\n",
      "6091/6091 [==============================] - 1s 106us/sample - loss: 0.3769 - val_loss: 0.4596\n",
      "Epoch 62/100\n",
      "6091/6091 [==============================] - 1s 106us/sample - loss: 0.3751 - val_loss: 0.4589\n",
      "Epoch 63/100\n",
      "6091/6091 [==============================] - 1s 114us/sample - loss: 0.3734 - val_loss: 0.4582\n",
      "Epoch 64/100\n",
      "6091/6091 [==============================] - 1s 103us/sample - loss: 0.3716 - val_loss: 0.4576\n",
      "Epoch 65/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.3699 - val_loss: 0.4569\n",
      "Epoch 66/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.3683 - val_loss: 0.4564\n",
      "Epoch 67/100\n",
      "6091/6091 [==============================] - 1s 106us/sample - loss: 0.3666 - val_loss: 0.4558\n",
      "Epoch 68/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.3650 - val_loss: 0.4552\n",
      "Epoch 69/100\n",
      "6091/6091 [==============================] - 1s 103us/sample - loss: 0.3634 - val_loss: 0.4547\n",
      "Epoch 70/100\n",
      "6091/6091 [==============================] - 1s 116us/sample - loss: 0.3619 - val_loss: 0.4542\n",
      "Epoch 71/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.3603 - val_loss: 0.4537\n",
      "Epoch 72/100\n",
      "6091/6091 [==============================] - 1s 103us/sample - loss: 0.3588 - val_loss: 0.4532\n",
      "Epoch 73/100\n",
      "6091/6091 [==============================] - 1s 100us/sample - loss: 0.3573 - val_loss: 0.4527\n",
      "Epoch 74/100\n",
      "6091/6091 [==============================] - 1s 100us/sample - loss: 0.3559 - val_loss: 0.4523\n",
      "Epoch 75/100\n",
      "6091/6091 [==============================] - 1s 102us/sample - loss: 0.3544 - val_loss: 0.4519\n",
      "Epoch 76/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.3530 - val_loss: 0.4515\n",
      "Epoch 77/100\n",
      "6091/6091 [==============================] - 1s 113us/sample - loss: 0.3516 - val_loss: 0.4512\n",
      "Epoch 78/100\n",
      "6091/6091 [==============================] - 1s 122us/sample - loss: 0.3503 - val_loss: 0.4509\n",
      "Epoch 79/100\n",
      "6091/6091 [==============================] - 1s 117us/sample - loss: 0.3489 - val_loss: 0.4505\n",
      "Epoch 80/100\n",
      "6091/6091 [==============================] - 1s 116us/sample - loss: 0.3476 - val_loss: 0.4502\n",
      "Epoch 81/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.3463 - val_loss: 0.4499\n",
      "Epoch 82/100\n",
      "6091/6091 [==============================] - 1s 115us/sample - loss: 0.3450 - val_loss: 0.4496\n",
      "Epoch 83/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.3437 - val_loss: 0.4493\n",
      "Epoch 84/100\n",
      "6091/6091 [==============================] - 1s 105us/sample - loss: 0.3424 - val_loss: 0.4490\n",
      "Epoch 85/100\n",
      "6091/6091 [==============================] - 1s 106us/sample - loss: 0.3412 - val_loss: 0.4488\n",
      "Epoch 86/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.3400 - val_loss: 0.4485\n",
      "Epoch 87/100\n",
      "6091/6091 [==============================] - 1s 103us/sample - loss: 0.3388 - val_loss: 0.4484\n",
      "Epoch 88/100\n",
      "6091/6091 [==============================] - 1s 126us/sample - loss: 0.3376 - val_loss: 0.4482\n",
      "Epoch 89/100\n",
      "6091/6091 [==============================] - 1s 111us/sample - loss: 0.3364 - val_loss: 0.4480\n",
      "Epoch 90/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.3352 - val_loss: 0.4478\n",
      "Epoch 91/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.3341 - val_loss: 0.4477\n",
      "Epoch 92/100\n",
      "6091/6091 [==============================] - 1s 120us/sample - loss: 0.3330 - val_loss: 0.4475\n",
      "Epoch 93/100\n",
      "6091/6091 [==============================] - 1s 120us/sample - loss: 0.3318 - val_loss: 0.4474\n",
      "Epoch 94/100\n",
      "6091/6091 [==============================] - 1s 114us/sample - loss: 0.3307 - val_loss: 0.4473\n",
      "Epoch 95/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.3297 - val_loss: 0.4472\n",
      "Epoch 96/100\n",
      "6091/6091 [==============================] - 1s 115us/sample - loss: 0.3286 - val_loss: 0.4471\n",
      "Epoch 97/100\n",
      "6091/6091 [==============================] - 1s 111us/sample - loss: 0.3275 - val_loss: 0.4470\n",
      "Epoch 98/100\n",
      "6091/6091 [==============================] - 1s 110us/sample - loss: 0.3265 - val_loss: 0.4469\n",
      "Epoch 99/100\n",
      "6091/6091 [==============================] - 1s 122us/sample - loss: 0.3254 - val_loss: 0.4469\n",
      "Predicting...\n",
      "(Took 66.018 sec)\n",
      "Train on 6091 samples, validate on 1522 samples\n",
      "Epoch 1/100\n",
      "6091/6091 [==============================] - 1s 159us/sample - loss: 0.6834 - val_loss: 0.6764\n",
      "Epoch 2/100\n",
      "6091/6091 [==============================] - 1s 106us/sample - loss: 0.6653 - val_loss: 0.6641\n",
      "Epoch 3/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.6508 - val_loss: 0.6536\n",
      "Epoch 4/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.6380 - val_loss: 0.6438\n",
      "Epoch 5/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.6262 - val_loss: 0.6347\n",
      "Epoch 6/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.6152 - val_loss: 0.6259\n",
      "Epoch 7/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.6046 - val_loss: 0.6177\n",
      "Epoch 8/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.5946 - val_loss: 0.6099\n",
      "Epoch 9/100\n",
      "6091/6091 [==============================] - 1s 106us/sample - loss: 0.5850 - val_loss: 0.6025\n",
      "Epoch 10/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.5760 - val_loss: 0.5954\n",
      "Epoch 11/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.5673 - val_loss: 0.5888\n",
      "Epoch 12/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.5591 - val_loss: 0.5825\n",
      "Epoch 13/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.5512 - val_loss: 0.5767\n",
      "Epoch 14/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.5436 - val_loss: 0.5708\n",
      "Epoch 15/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.5364 - val_loss: 0.5655\n",
      "Epoch 16/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.5295 - val_loss: 0.5605\n",
      "Epoch 17/100\n",
      "6091/6091 [==============================] - 1s 122us/sample - loss: 0.5230 - val_loss: 0.5556\n",
      "Epoch 18/100\n",
      "6091/6091 [==============================] - 1s 118us/sample - loss: 0.5166 - val_loss: 0.5510\n",
      "Epoch 19/100\n",
      "6091/6091 [==============================] - 1s 114us/sample - loss: 0.5105 - val_loss: 0.5466\n",
      "Epoch 20/100\n",
      "6091/6091 [==============================] - 1s 116us/sample - loss: 0.5047 - val_loss: 0.5425\n",
      "Epoch 21/100\n",
      "6091/6091 [==============================] - 1s 111us/sample - loss: 0.4991 - val_loss: 0.5386\n",
      "Epoch 22/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.4937 - val_loss: 0.5350\n",
      "Epoch 23/100\n",
      "6091/6091 [==============================] - 1s 111us/sample - loss: 0.4885 - val_loss: 0.5314\n",
      "Epoch 24/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.4835 - val_loss: 0.5281\n",
      "Epoch 25/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4787 - val_loss: 0.5248\n",
      "Epoch 26/100\n",
      "6091/6091 [==============================] - 1s 121us/sample - loss: 0.4741 - val_loss: 0.5217\n",
      "Epoch 27/100\n",
      "6091/6091 [==============================] - 1s 118us/sample - loss: 0.4696 - val_loss: 0.5189\n",
      "Epoch 28/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.4653 - val_loss: 0.5161\n",
      "Epoch 29/100\n",
      "6091/6091 [==============================] - 1s 110us/sample - loss: 0.4610 - val_loss: 0.5134\n",
      "Epoch 30/100\n",
      "6091/6091 [==============================] - 1s 115us/sample - loss: 0.4570 - val_loss: 0.5110\n",
      "Epoch 31/100\n",
      "6091/6091 [==============================] - 1s 111us/sample - loss: 0.4531 - val_loss: 0.5085\n",
      "Epoch 32/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.4493 - val_loss: 0.5063\n",
      "Epoch 33/100\n",
      "6091/6091 [==============================] - 1s 110us/sample - loss: 0.4456 - val_loss: 0.5041\n",
      "Epoch 34/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.4420 - val_loss: 0.5021\n",
      "Epoch 35/100\n",
      "6091/6091 [==============================] - 1s 110us/sample - loss: 0.4385 - val_loss: 0.5001\n",
      "Epoch 36/100\n",
      "6091/6091 [==============================] - 1s 114us/sample - loss: 0.4352 - val_loss: 0.4982\n",
      "Epoch 37/100\n",
      "6091/6091 [==============================] - 1s 116us/sample - loss: 0.4319 - val_loss: 0.4965\n",
      "Epoch 38/100\n",
      "6091/6091 [==============================] - 1s 118us/sample - loss: 0.4287 - val_loss: 0.4947\n",
      "Epoch 39/100\n",
      "6091/6091 [==============================] - 1s 113us/sample - loss: 0.4256 - val_loss: 0.4930\n",
      "Epoch 40/100\n",
      "6091/6091 [==============================] - 1s 132us/sample - loss: 0.4226 - val_loss: 0.4916\n",
      "Epoch 41/100\n",
      "6091/6091 [==============================] - 1s 113us/sample - loss: 0.4197 - val_loss: 0.4901\n",
      "Epoch 42/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.4169 - val_loss: 0.4887\n",
      "Epoch 43/100\n",
      "6091/6091 [==============================] - 1s 131us/sample - loss: 0.4141 - val_loss: 0.4874\n",
      "Epoch 44/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6091/6091 [==============================] - 1s 152us/sample - loss: 0.4114 - val_loss: 0.4861\n",
      "Epoch 45/100\n",
      "6091/6091 [==============================] - 1s 122us/sample - loss: 0.4087 - val_loss: 0.4848\n",
      "Epoch 46/100\n",
      "6091/6091 [==============================] - 1s 123us/sample - loss: 0.4062 - val_loss: 0.4837\n",
      "Epoch 47/100\n",
      "6091/6091 [==============================] - 1s 115us/sample - loss: 0.4037 - val_loss: 0.4827\n",
      "Epoch 48/100\n",
      "6091/6091 [==============================] - 1s 117us/sample - loss: 0.4012 - val_loss: 0.4816\n",
      "Epoch 49/100\n",
      "6091/6091 [==============================] - 1s 111us/sample - loss: 0.3988 - val_loss: 0.4807\n",
      "Epoch 50/100\n",
      "6091/6091 [==============================] - 1s 105us/sample - loss: 0.3965 - val_loss: 0.4797\n",
      "Epoch 51/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.3942 - val_loss: 0.4788\n",
      "Epoch 52/100\n",
      "6091/6091 [==============================] - 1s 106us/sample - loss: 0.3919 - val_loss: 0.4779\n",
      "Epoch 53/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.3898 - val_loss: 0.4772\n",
      "Epoch 54/100\n",
      "6091/6091 [==============================] - 1s 106us/sample - loss: 0.3876 - val_loss: 0.4764\n",
      "Epoch 55/100\n",
      "6091/6091 [==============================] - 1s 110us/sample - loss: 0.3855 - val_loss: 0.4757\n",
      "Epoch 56/100\n",
      "6091/6091 [==============================] - 1s 106us/sample - loss: 0.3835 - val_loss: 0.4750\n",
      "Epoch 57/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.3815 - val_loss: 0.4744\n",
      "Epoch 58/100\n",
      "6091/6091 [==============================] - 1s 122us/sample - loss: 0.3795 - val_loss: 0.4737\n",
      "Epoch 59/100\n",
      "6091/6091 [==============================] - 1s 106us/sample - loss: 0.3775 - val_loss: 0.4732\n",
      "Epoch 60/100\n",
      "6091/6091 [==============================] - 1s 105us/sample - loss: 0.3757 - val_loss: 0.4727\n",
      "Epoch 61/100\n",
      "6091/6091 [==============================] - 1s 106us/sample - loss: 0.3738 - val_loss: 0.4722\n",
      "Epoch 62/100\n",
      "6091/6091 [==============================] - 1s 104us/sample - loss: 0.3720 - val_loss: 0.4717\n",
      "Epoch 63/100\n",
      "6091/6091 [==============================] - 1s 105us/sample - loss: 0.3702 - val_loss: 0.4712\n",
      "Epoch 64/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.3684 - val_loss: 0.4708\n",
      "Epoch 65/100\n",
      "6091/6091 [==============================] - 1s 104us/sample - loss: 0.3667 - val_loss: 0.4704\n",
      "Epoch 66/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.3650 - val_loss: 0.4701\n",
      "Epoch 67/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.3633 - val_loss: 0.4697\n",
      "Epoch 68/100\n",
      "6091/6091 [==============================] - 1s 102us/sample - loss: 0.3617 - val_loss: 0.4695\n",
      "Epoch 69/100\n",
      "6091/6091 [==============================] - 1s 105us/sample - loss: 0.3601 - val_loss: 0.4692\n",
      "Epoch 70/100\n",
      "6091/6091 [==============================] - 1s 104us/sample - loss: 0.3585 - val_loss: 0.4689\n",
      "Epoch 71/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.3569 - val_loss: 0.4687\n",
      "Epoch 72/100\n",
      "6091/6091 [==============================] - 1s 105us/sample - loss: 0.3554 - val_loss: 0.4684\n",
      "Epoch 73/100\n",
      "6091/6091 [==============================] - 1s 105us/sample - loss: 0.3539 - val_loss: 0.4682\n",
      "Epoch 74/100\n",
      "6091/6091 [==============================] - 1s 116us/sample - loss: 0.3524 - val_loss: 0.4680\n",
      "Epoch 75/100\n",
      "6091/6091 [==============================] - 1s 114us/sample - loss: 0.3509 - val_loss: 0.4679\n",
      "Epoch 76/100\n",
      "6091/6091 [==============================] - 1s 119us/sample - loss: 0.3495 - val_loss: 0.4677\n",
      "Epoch 77/100\n",
      "6091/6091 [==============================] - 1s 115us/sample - loss: 0.3481 - val_loss: 0.4677\n",
      "Epoch 78/100\n",
      "6091/6091 [==============================] - 1s 126us/sample - loss: 0.3467 - val_loss: 0.4675\n",
      "Epoch 79/100\n",
      "6091/6091 [==============================] - 1s 119us/sample - loss: 0.3453 - val_loss: 0.4674\n",
      "Epoch 80/100\n",
      "6091/6091 [==============================] - 1s 126us/sample - loss: 0.3439 - val_loss: 0.4674\n",
      "Epoch 81/100\n",
      "6091/6091 [==============================] - 1s 124us/sample - loss: 0.3426 - val_loss: 0.4673\n",
      "Epoch 82/100\n",
      "6091/6091 [==============================] - 1s 124us/sample - loss: 0.3413 - val_loss: 0.4672\n",
      "Epoch 83/100\n",
      "6091/6091 [==============================] - 1s 128us/sample - loss: 0.3400 - val_loss: 0.4672\n",
      "Predicting...\n",
      "(Took 57.757 sec)\n",
      "Combined confusion matrix:\n",
      "[[3805.  537.]\n",
      " [1058. 2213.]]\n",
      "(Overall, took 303.652 sec)\n",
      "Accuracy: 79.05% +/- 1.01%\n",
      "Precision for positive class: 78.25% +/- 2.02%\n",
      "Precision for negative class: 80.49% +/- 1.21%\n",
      "Recall for positive class: 87.63% +/- 1.18%\n",
      "Recall for negative class: 67.68% +/- 2.44%\n",
      "F for positive class: 82.66% +/- 1.12%\n",
      "F for negative class: 73.51% +/- 1.08%\n",
      "Mean F score: 78.08% +/- 0.94%\n"
     ]
    }
   ],
   "source": [
    "# POS tagging, transformed\n",
    "# Normalized\n",
    "all_metrics(cross_validate_logistic(dftrain_pos_min_norm, 100))\n",
    "# TF-IDF\n",
    "all_metrics(cross_validate_logistic(dftrain_pos_min_tfidf, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/100\n",
      "6090/6090 [==============================] - 1s 154us/sample - loss: 0.6857 - val_loss: 0.6779\n",
      "Epoch 2/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.6737 - val_loss: 0.6679\n",
      "Epoch 3/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.6646 - val_loss: 0.6598\n",
      "Epoch 4/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.6568 - val_loss: 0.6527\n",
      "Epoch 5/100\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.6494 - val_loss: 0.6462\n",
      "Epoch 6/100\n",
      "6090/6090 [==============================] - 1s 116us/sample - loss: 0.6425 - val_loss: 0.6401\n",
      "Epoch 7/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.6358 - val_loss: 0.6342\n",
      "Epoch 8/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.6294 - val_loss: 0.6286\n",
      "Epoch 9/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.6232 - val_loss: 0.6232\n",
      "Epoch 10/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.6172 - val_loss: 0.6180\n",
      "Epoch 11/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.6115 - val_loss: 0.6131\n",
      "Epoch 12/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.6059 - val_loss: 0.6083\n",
      "Epoch 13/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.6005 - val_loss: 0.6037\n",
      "Epoch 14/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.5953 - val_loss: 0.5993\n",
      "Epoch 15/100\n",
      "6090/6090 [==============================] - 1s 118us/sample - loss: 0.5903 - val_loss: 0.5950\n",
      "Epoch 16/100\n",
      "6090/6090 [==============================] - 1s 133us/sample - loss: 0.5854 - val_loss: 0.5909\n",
      "Epoch 17/100\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.5806 - val_loss: 0.5869\n",
      "Epoch 18/100\n",
      "6090/6090 [==============================] - 1s 125us/sample - loss: 0.5761 - val_loss: 0.5830\n",
      "Epoch 19/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.5717 - val_loss: 0.5793\n",
      "Epoch 20/100\n",
      "6090/6090 [==============================] - 1s 133us/sample - loss: 0.5674 - val_loss: 0.5757\n",
      "Epoch 21/100\n",
      "6090/6090 [==============================] - 1s 132us/sample - loss: 0.5632 - val_loss: 0.5723\n",
      "Epoch 22/100\n",
      "6090/6090 [==============================] - 1s 130us/sample - loss: 0.5592 - val_loss: 0.5690\n",
      "Epoch 23/100\n",
      "6090/6090 [==============================] - 1s 136us/sample - loss: 0.5553 - val_loss: 0.5657\n",
      "Epoch 24/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.5515 - val_loss: 0.5626\n",
      "Epoch 25/100\n",
      "6090/6090 [==============================] - 1s 124us/sample - loss: 0.5478 - val_loss: 0.5596\n",
      "Epoch 26/100\n",
      "6090/6090 [==============================] - 1s 133us/sample - loss: 0.5442 - val_loss: 0.5567\n",
      "Epoch 27/100\n",
      "6090/6090 [==============================] - 1s 124us/sample - loss: 0.5407 - val_loss: 0.5539\n",
      "Epoch 28/100\n",
      "6090/6090 [==============================] - 1s 116us/sample - loss: 0.5373 - val_loss: 0.5512\n",
      "Epoch 29/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.5340 - val_loss: 0.5485\n",
      "Epoch 30/100\n",
      "6090/6090 [==============================] - 1s 120us/sample - loss: 0.5308 - val_loss: 0.5460\n",
      "Epoch 31/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.5276 - val_loss: 0.5435\n",
      "Epoch 32/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.5246 - val_loss: 0.5412\n",
      "Epoch 33/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.5216 - val_loss: 0.5389\n",
      "Epoch 34/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.5187 - val_loss: 0.5366\n",
      "Epoch 35/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.5159 - val_loss: 0.5345\n",
      "Epoch 36/100\n",
      "6090/6090 [==============================] - 1s 126us/sample - loss: 0.5131 - val_loss: 0.5324\n",
      "Epoch 37/100\n",
      "6090/6090 [==============================] - 1s 123us/sample - loss: 0.5104 - val_loss: 0.5303\n",
      "Epoch 38/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.5078 - val_loss: 0.5283\n",
      "Epoch 39/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.5052 - val_loss: 0.5263\n",
      "Epoch 40/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.5027 - val_loss: 0.5245\n",
      "Epoch 41/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.5002 - val_loss: 0.5227\n",
      "Epoch 42/100\n",
      "6090/6090 [==============================] - 1s 117us/sample - loss: 0.4978 - val_loss: 0.5209\n",
      "Epoch 43/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.4955 - val_loss: 0.5192\n",
      "Epoch 44/100\n",
      "6090/6090 [==============================] - 1s 117us/sample - loss: 0.4932 - val_loss: 0.5175\n",
      "Epoch 45/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.4909 - val_loss: 0.5159\n",
      "Epoch 46/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.4887 - val_loss: 0.5144\n",
      "Epoch 47/100\n",
      "6090/6090 [==============================] - 1s 116us/sample - loss: 0.4866 - val_loss: 0.5129\n",
      "Epoch 48/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4844 - val_loss: 0.5114\n",
      "Epoch 49/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4824 - val_loss: 0.5099\n",
      "Epoch 50/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4803 - val_loss: 0.5085\n",
      "Epoch 51/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4784 - val_loss: 0.5072\n",
      "Epoch 52/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.4764 - val_loss: 0.5058\n",
      "Epoch 53/100\n",
      "6090/6090 [==============================] - 1s 122us/sample - loss: 0.4745 - val_loss: 0.5045\n",
      "Epoch 54/100\n",
      "6090/6090 [==============================] - 1s 125us/sample - loss: 0.4726 - val_loss: 0.5033\n",
      "Epoch 55/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.4708 - val_loss: 0.5021\n",
      "Epoch 56/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.4689 - val_loss: 0.5008\n",
      "Epoch 57/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.4672 - val_loss: 0.4997\n",
      "Epoch 58/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.4654 - val_loss: 0.4986\n",
      "Epoch 59/100\n",
      "6090/6090 [==============================] - 1s 120us/sample - loss: 0.4637 - val_loss: 0.4975\n",
      "Epoch 60/100\n",
      "6090/6090 [==============================] - 1s 123us/sample - loss: 0.4620 - val_loss: 0.4964\n",
      "Epoch 61/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4603 - val_loss: 0.4953\n",
      "Epoch 62/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.4587 - val_loss: 0.4943\n",
      "Epoch 63/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4571 - val_loss: 0.4933\n",
      "Epoch 64/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4555 - val_loss: 0.4923\n",
      "Epoch 65/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.4539 - val_loss: 0.4914\n",
      "Epoch 66/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.4524 - val_loss: 0.4905\n",
      "Epoch 67/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4509 - val_loss: 0.4896\n",
      "Epoch 68/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.4494 - val_loss: 0.4886\n",
      "Epoch 69/100\n",
      "6090/6090 [==============================] - 1s 116us/sample - loss: 0.4479 - val_loss: 0.4878\n",
      "Epoch 70/100\n",
      "6090/6090 [==============================] - 1s 117us/sample - loss: 0.4465 - val_loss: 0.4870\n",
      "Epoch 71/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.4450 - val_loss: 0.4862\n",
      "Epoch 72/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4436 - val_loss: 0.4853\n",
      "Epoch 73/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.4423 - val_loss: 0.4846\n",
      "Epoch 74/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.4409 - val_loss: 0.4838\n",
      "Epoch 75/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.4396 - val_loss: 0.4831\n",
      "Epoch 76/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.4383 - val_loss: 0.4824\n",
      "Epoch 77/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4369 - val_loss: 0.4816\n",
      "Epoch 78/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4356 - val_loss: 0.4809\n",
      "Epoch 79/100\n",
      "6090/6090 [==============================] - 1s 101us/sample - loss: 0.4344 - val_loss: 0.4802\n",
      "Epoch 80/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.4331 - val_loss: 0.4795\n",
      "Epoch 81/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4319 - val_loss: 0.4789\n",
      "Epoch 82/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.4306 - val_loss: 0.4782\n",
      "Epoch 83/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.4294 - val_loss: 0.4776\n",
      "Epoch 84/100\n",
      "6090/6090 [==============================] - 1s 116us/sample - loss: 0.4282 - val_loss: 0.4770\n",
      "Epoch 85/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.4270 - val_loss: 0.4764\n",
      "Epoch 86/100\n",
      "6090/6090 [==============================] - 1s 121us/sample - loss: 0.4259 - val_loss: 0.4758\n",
      "Epoch 87/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.4247 - val_loss: 0.4753\n",
      "Epoch 88/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.4236 - val_loss: 0.4747\n",
      "Epoch 89/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.4225 - val_loss: 0.4742\n",
      "Epoch 90/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.4214 - val_loss: 0.4736\n",
      "Epoch 91/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.4203 - val_loss: 0.4731\n",
      "Epoch 92/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.4192 - val_loss: 0.4726\n",
      "Epoch 93/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.4181 - val_loss: 0.4721\n",
      "Epoch 94/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.4170 - val_loss: 0.4716\n",
      "Epoch 95/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.4160 - val_loss: 0.4711\n",
      "Epoch 96/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.4150 - val_loss: 0.4707\n",
      "Epoch 97/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.4139 - val_loss: 0.4702\n",
      "Epoch 98/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4129 - val_loss: 0.4697\n",
      "Epoch 99/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.4119 - val_loss: 0.4693\n",
      "Epoch 100/100\n",
      "6090/6090 [==============================] - 1s 99us/sample - loss: 0.4109 - val_loss: 0.4689\n",
      "Predicting...\n",
      "(Took 69.355 sec)\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/100\n",
      "6090/6090 [==============================] - 1s 141us/sample - loss: 0.6865 - val_loss: 0.6775\n",
      "Epoch 2/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.6745 - val_loss: 0.6669\n",
      "Epoch 3/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.6654 - val_loss: 0.6586\n",
      "Epoch 4/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.6575 - val_loss: 0.6515\n",
      "Epoch 5/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.6502 - val_loss: 0.6451\n",
      "Epoch 6/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.6432 - val_loss: 0.6389\n",
      "Epoch 7/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.6365 - val_loss: 0.6332\n",
      "Epoch 8/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.6300 - val_loss: 0.6275\n",
      "Epoch 9/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.6238 - val_loss: 0.6222\n",
      "Epoch 10/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.6178 - val_loss: 0.6172\n",
      "Epoch 11/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.6119 - val_loss: 0.6122\n",
      "Epoch 12/100\n",
      "6090/6090 [==============================] - 1s 118us/sample - loss: 0.6064 - val_loss: 0.6075\n",
      "Epoch 13/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.6009 - val_loss: 0.6029\n",
      "Epoch 14/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.5957 - val_loss: 0.5986\n",
      "Epoch 15/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.5906 - val_loss: 0.5943\n",
      "Epoch 16/100\n",
      "6090/6090 [==============================] - 1s 121us/sample - loss: 0.5857 - val_loss: 0.5903\n",
      "Epoch 17/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.5810 - val_loss: 0.5865\n",
      "Epoch 18/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.5764 - val_loss: 0.5826\n",
      "Epoch 19/100\n",
      "6090/6090 [==============================] - 1s 117us/sample - loss: 0.5719 - val_loss: 0.5790\n",
      "Epoch 20/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.5676 - val_loss: 0.5754\n",
      "Epoch 21/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.5634 - val_loss: 0.5721\n",
      "Epoch 22/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.5594 - val_loss: 0.5687\n",
      "Epoch 23/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.5555 - val_loss: 0.5657\n",
      "Epoch 24/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.5516 - val_loss: 0.5627\n",
      "Epoch 25/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.5479 - val_loss: 0.5597\n",
      "Epoch 26/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.5443 - val_loss: 0.5569\n",
      "Epoch 27/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.5408 - val_loss: 0.5541\n",
      "Epoch 28/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.5374 - val_loss: 0.5514\n",
      "Epoch 29/100\n",
      "6090/6090 [==============================] - 1s 123us/sample - loss: 0.5341 - val_loss: 0.5489\n",
      "Epoch 30/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.5308 - val_loss: 0.5464\n",
      "Epoch 31/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.5277 - val_loss: 0.5440\n",
      "Epoch 32/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.5246 - val_loss: 0.5417\n",
      "Epoch 33/100\n",
      "6090/6090 [==============================] - 1s 117us/sample - loss: 0.5216 - val_loss: 0.5395\n",
      "Epoch 34/100\n",
      "6090/6090 [==============================] - 1s 120us/sample - loss: 0.5187 - val_loss: 0.5373\n",
      "Epoch 35/100\n",
      "6090/6090 [==============================] - 1s 129us/sample - loss: 0.5159 - val_loss: 0.5352\n",
      "Epoch 36/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.5131 - val_loss: 0.5331\n",
      "Epoch 37/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.5104 - val_loss: 0.5311\n",
      "Epoch 38/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.5078 - val_loss: 0.5292\n",
      "Epoch 39/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.5052 - val_loss: 0.5273\n",
      "Epoch 40/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.5027 - val_loss: 0.5255\n",
      "Epoch 41/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.5002 - val_loss: 0.5237\n",
      "Epoch 42/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.4978 - val_loss: 0.5220\n",
      "Epoch 43/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4955 - val_loss: 0.5205\n",
      "Epoch 44/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.4931 - val_loss: 0.5188\n",
      "Epoch 45/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.4909 - val_loss: 0.5173\n",
      "Epoch 46/100\n",
      "6090/6090 [==============================] - 1s 117us/sample - loss: 0.4887 - val_loss: 0.5157\n",
      "Epoch 47/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.4865 - val_loss: 0.5141\n",
      "Epoch 48/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.4844 - val_loss: 0.5127\n",
      "Epoch 49/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.4823 - val_loss: 0.5113\n",
      "Epoch 50/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.4803 - val_loss: 0.5099\n",
      "Epoch 51/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.4783 - val_loss: 0.5086\n",
      "Epoch 52/100\n",
      "6090/6090 [==============================] - 1s 117us/sample - loss: 0.4763 - val_loss: 0.5073\n",
      "Epoch 53/100\n",
      "6090/6090 [==============================] - 1s 117us/sample - loss: 0.4744 - val_loss: 0.5061\n",
      "Epoch 54/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4725 - val_loss: 0.5048\n",
      "Epoch 55/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.4707 - val_loss: 0.5036\n",
      "Epoch 56/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.4689 - val_loss: 0.5025\n",
      "Epoch 57/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.4671 - val_loss: 0.5014\n",
      "Epoch 58/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.4653 - val_loss: 0.5004\n",
      "Epoch 59/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.4636 - val_loss: 0.4992\n",
      "Epoch 60/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.4619 - val_loss: 0.4981\n",
      "Epoch 61/100\n",
      "6090/6090 [==============================] - 1s 99us/sample - loss: 0.4602 - val_loss: 0.4970\n",
      "Epoch 62/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4586 - val_loss: 0.4960\n",
      "Epoch 63/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4570 - val_loss: 0.4951\n",
      "Epoch 64/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.4554 - val_loss: 0.4941\n",
      "Epoch 65/100\n",
      "6090/6090 [==============================] - 1s 124us/sample - loss: 0.4538 - val_loss: 0.4932\n",
      "Epoch 66/100\n",
      "6090/6090 [==============================] - 1s 122us/sample - loss: 0.4523 - val_loss: 0.4923\n",
      "Epoch 67/100\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.4508 - val_loss: 0.4914\n",
      "Epoch 68/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4493 - val_loss: 0.4905\n",
      "Epoch 69/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4478 - val_loss: 0.4897\n",
      "Epoch 70/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4464 - val_loss: 0.4888\n",
      "Epoch 71/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4450 - val_loss: 0.4881\n",
      "Epoch 72/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.4435 - val_loss: 0.4873\n",
      "Epoch 73/100\n",
      "6090/6090 [==============================] - 1s 101us/sample - loss: 0.4422 - val_loss: 0.4865\n",
      "Epoch 74/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.4408 - val_loss: 0.4857\n",
      "Epoch 75/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.4395 - val_loss: 0.4849\n",
      "Epoch 76/100\n",
      "6090/6090 [==============================] - 1s 121us/sample - loss: 0.4381 - val_loss: 0.4842\n",
      "Epoch 77/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.4368 - val_loss: 0.4835\n",
      "Epoch 78/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.4355 - val_loss: 0.4828\n",
      "Epoch 79/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.4343 - val_loss: 0.4821\n",
      "Epoch 80/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.4330 - val_loss: 0.4815\n",
      "Epoch 81/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.4318 - val_loss: 0.4807\n",
      "Epoch 82/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.4305 - val_loss: 0.4801\n",
      "Epoch 83/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.4293 - val_loss: 0.4795\n",
      "Epoch 84/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.4281 - val_loss: 0.4789\n",
      "Epoch 85/100\n",
      "6090/6090 [==============================] - 1s 147us/sample - loss: 0.4269 - val_loss: 0.4783\n",
      "Epoch 86/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4258 - val_loss: 0.4777\n",
      "Epoch 87/100\n",
      "6090/6090 [==============================] - 1s 119us/sample - loss: 0.4247 - val_loss: 0.4771\n",
      "Epoch 88/100\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.4235 - val_loss: 0.4766\n",
      "Epoch 89/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.4224 - val_loss: 0.4761\n",
      "Epoch 90/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.4213 - val_loss: 0.4755\n",
      "Epoch 91/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.4202 - val_loss: 0.4750\n",
      "Epoch 92/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.4191 - val_loss: 0.4745\n",
      "Epoch 93/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.4180 - val_loss: 0.4740\n",
      "Epoch 94/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.4170 - val_loss: 0.4735\n",
      "Epoch 95/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.4159 - val_loss: 0.4730\n",
      "Epoch 96/100\n",
      "6090/6090 [==============================] - 1s 121us/sample - loss: 0.4149 - val_loss: 0.4725\n",
      "Epoch 97/100\n",
      "6090/6090 [==============================] - 1s 119us/sample - loss: 0.4139 - val_loss: 0.4721\n",
      "Epoch 98/100\n",
      "6090/6090 [==============================] - 1s 118us/sample - loss: 0.4129 - val_loss: 0.4716\n",
      "Epoch 99/100\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.4118 - val_loss: 0.4712\n",
      "Epoch 100/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.4109 - val_loss: 0.4708\n",
      "Predicting...\n",
      "(Took 67.814 sec)\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/100\n",
      "6090/6090 [==============================] - 1s 144us/sample - loss: 0.6853 - val_loss: 0.6805\n",
      "Epoch 2/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.6727 - val_loss: 0.6721\n",
      "Epoch 3/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.6633 - val_loss: 0.6653\n",
      "Epoch 4/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.6553 - val_loss: 0.6589\n",
      "Epoch 5/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.6479 - val_loss: 0.6529\n",
      "Epoch 6/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.6410 - val_loss: 0.6471\n",
      "Epoch 7/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.6342 - val_loss: 0.6415\n",
      "Epoch 8/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.6278 - val_loss: 0.6359\n",
      "Epoch 9/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.6217 - val_loss: 0.6308\n",
      "Epoch 10/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.6157 - val_loss: 0.6257\n",
      "Epoch 11/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.6099 - val_loss: 0.6208\n",
      "Epoch 12/100\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.6043 - val_loss: 0.6162\n",
      "Epoch 13/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.5990 - val_loss: 0.6116\n",
      "Epoch 14/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.5937 - val_loss: 0.6074\n",
      "Epoch 15/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.5887 - val_loss: 0.6032\n",
      "Epoch 16/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.5838 - val_loss: 0.5992\n",
      "Epoch 17/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.5791 - val_loss: 0.5954\n",
      "Epoch 18/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.5745 - val_loss: 0.5917\n",
      "Epoch 19/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.5701 - val_loss: 0.5882\n",
      "Epoch 20/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.5658 - val_loss: 0.5848\n",
      "Epoch 21/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.5616 - val_loss: 0.5815\n",
      "Epoch 22/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.5575 - val_loss: 0.5782\n",
      "Epoch 23/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.5536 - val_loss: 0.5751\n",
      "Epoch 24/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.5498 - val_loss: 0.5721\n",
      "Epoch 25/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.5460 - val_loss: 0.5692\n",
      "Epoch 26/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.5424 - val_loss: 0.5664\n",
      "Epoch 27/100\n",
      "6090/6090 [==============================] - 1s 98us/sample - loss: 0.5390 - val_loss: 0.5637\n",
      "Epoch 28/100\n",
      "6090/6090 [==============================] - 1s 99us/sample - loss: 0.5355 - val_loss: 0.5612\n",
      "Epoch 29/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.5322 - val_loss: 0.5586\n",
      "Epoch 30/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.5290 - val_loss: 0.5562\n",
      "Epoch 31/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.5258 - val_loss: 0.5539\n",
      "Epoch 32/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.5227 - val_loss: 0.5516\n",
      "Epoch 33/100\n",
      "6090/6090 [==============================] - 1s 98us/sample - loss: 0.5197 - val_loss: 0.5494\n",
      "Epoch 34/100\n",
      "6090/6090 [==============================] - 1s 99us/sample - loss: 0.5168 - val_loss: 0.5472\n",
      "Epoch 35/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.5140 - val_loss: 0.5452\n",
      "Epoch 36/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.5112 - val_loss: 0.5431\n",
      "Epoch 37/100\n",
      "6090/6090 [==============================] - 1s 98us/sample - loss: 0.5085 - val_loss: 0.5412\n",
      "Epoch 38/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.5059 - val_loss: 0.5393\n",
      "Epoch 39/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.5033 - val_loss: 0.5375\n",
      "Epoch 40/100\n",
      "6090/6090 [==============================] - 1s 98us/sample - loss: 0.5008 - val_loss: 0.5358\n",
      "Epoch 41/100\n",
      "6090/6090 [==============================] - 1s 99us/sample - loss: 0.4983 - val_loss: 0.5341\n",
      "Epoch 42/100\n",
      "6090/6090 [==============================] - 1s 98us/sample - loss: 0.4959 - val_loss: 0.5324\n",
      "Epoch 43/100\n",
      "6090/6090 [==============================] - 1s 98us/sample - loss: 0.4936 - val_loss: 0.5308\n",
      "Epoch 44/100\n",
      "6090/6090 [==============================] - 1s 99us/sample - loss: 0.4913 - val_loss: 0.5292\n",
      "Epoch 45/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.4890 - val_loss: 0.5276\n",
      "Epoch 46/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.4868 - val_loss: 0.5261\n",
      "Epoch 47/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.4847 - val_loss: 0.5247\n",
      "Epoch 48/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.4825 - val_loss: 0.5233\n",
      "Epoch 49/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.4804 - val_loss: 0.5219\n",
      "Epoch 50/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.4784 - val_loss: 0.5207\n",
      "Epoch 51/100\n",
      "6090/6090 [==============================] - 1s 120us/sample - loss: 0.4764 - val_loss: 0.5194\n",
      "Epoch 52/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.4744 - val_loss: 0.5181\n",
      "Epoch 53/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4725 - val_loss: 0.5169\n",
      "Epoch 54/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4706 - val_loss: 0.5158\n",
      "Epoch 55/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.4688 - val_loss: 0.5146\n",
      "Epoch 56/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.4669 - val_loss: 0.5134\n",
      "Epoch 57/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.4651 - val_loss: 0.5123\n",
      "Epoch 58/100\n",
      "6090/6090 [==============================] - 1s 99us/sample - loss: 0.4634 - val_loss: 0.5112\n",
      "Epoch 59/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.4616 - val_loss: 0.5102\n",
      "Epoch 60/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.4599 - val_loss: 0.5092\n",
      "Epoch 61/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.4583 - val_loss: 0.5081\n",
      "Epoch 62/100\n",
      "6090/6090 [==============================] - 1s 99us/sample - loss: 0.4566 - val_loss: 0.5072\n",
      "Epoch 63/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.4550 - val_loss: 0.5062\n",
      "Epoch 64/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4534 - val_loss: 0.5053\n",
      "Epoch 65/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.4519 - val_loss: 0.5044\n",
      "Epoch 66/100\n",
      "6090/6090 [==============================] - 1s 99us/sample - loss: 0.4503 - val_loss: 0.5036\n",
      "Epoch 67/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.4488 - val_loss: 0.5027\n",
      "Epoch 68/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.4473 - val_loss: 0.5018\n",
      "Epoch 69/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.4458 - val_loss: 0.5010\n",
      "Epoch 70/100\n",
      "6090/6090 [==============================] - 1s 98us/sample - loss: 0.4444 - val_loss: 0.5001\n",
      "Epoch 71/100\n",
      "6090/6090 [==============================] - 1s 101us/sample - loss: 0.4430 - val_loss: 0.4993\n",
      "Epoch 72/100\n",
      "6090/6090 [==============================] - 1s 99us/sample - loss: 0.4415 - val_loss: 0.4986\n",
      "Epoch 73/100\n",
      "6090/6090 [==============================] - 1s 99us/sample - loss: 0.4402 - val_loss: 0.4978\n",
      "Epoch 74/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.4388 - val_loss: 0.4970\n",
      "Epoch 75/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4374 - val_loss: 0.4963\n",
      "Epoch 76/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.4361 - val_loss: 0.4956\n",
      "Epoch 77/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.4348 - val_loss: 0.4949\n",
      "Epoch 78/100\n",
      "6090/6090 [==============================] - 1s 101us/sample - loss: 0.4335 - val_loss: 0.4943\n",
      "Epoch 79/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.4322 - val_loss: 0.4937\n",
      "Epoch 80/100\n",
      "6090/6090 [==============================] - 1s 99us/sample - loss: 0.4309 - val_loss: 0.4929\n",
      "Epoch 81/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.4297 - val_loss: 0.4923\n",
      "Epoch 82/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.4285 - val_loss: 0.4917\n",
      "Epoch 83/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4273 - val_loss: 0.4910\n",
      "Epoch 84/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4261 - val_loss: 0.4904\n",
      "Epoch 85/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4249 - val_loss: 0.4898\n",
      "Epoch 86/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.4237 - val_loss: 0.4893\n",
      "Epoch 87/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.4226 - val_loss: 0.4887\n",
      "Epoch 88/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.4214 - val_loss: 0.4882\n",
      "Epoch 89/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.4203 - val_loss: 0.4877\n",
      "Epoch 90/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4192 - val_loss: 0.4871\n",
      "Epoch 91/100\n",
      "6090/6090 [==============================] - 1s 99us/sample - loss: 0.4181 - val_loss: 0.4866\n",
      "Epoch 92/100\n",
      "6090/6090 [==============================] - 1s 101us/sample - loss: 0.4170 - val_loss: 0.4861\n",
      "Epoch 93/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.4160 - val_loss: 0.4856\n",
      "Epoch 94/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.4149 - val_loss: 0.4851\n",
      "Epoch 95/100\n",
      "6090/6090 [==============================] - 1s 101us/sample - loss: 0.4138 - val_loss: 0.4846\n",
      "Epoch 96/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.4128 - val_loss: 0.4841\n",
      "Epoch 97/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.4118 - val_loss: 0.4837\n",
      "Epoch 98/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.4108 - val_loss: 0.4832\n",
      "Epoch 99/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.4098 - val_loss: 0.4828\n",
      "Epoch 100/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4088 - val_loss: 0.4824\n",
      "Predicting...\n",
      "(Took 63.991 sec)\n",
      "Train on 6091 samples, validate on 1522 samples\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6091/6091 [==============================] - 1s 146us/sample - loss: 0.6856 - val_loss: 0.6786\n",
      "Epoch 2/100\n",
      "6091/6091 [==============================] - 1s 99us/sample - loss: 0.6732 - val_loss: 0.6690\n",
      "Epoch 3/100\n",
      "6091/6091 [==============================] - 1s 104us/sample - loss: 0.6640 - val_loss: 0.6614\n",
      "Epoch 4/100\n",
      "6091/6091 [==============================] - 1s 104us/sample - loss: 0.6561 - val_loss: 0.6547\n",
      "Epoch 5/100\n",
      "6091/6091 [==============================] - 1s 101us/sample - loss: 0.6488 - val_loss: 0.6485\n",
      "Epoch 6/100\n",
      "6091/6091 [==============================] - 1s 98us/sample - loss: 0.6419 - val_loss: 0.6425\n",
      "Epoch 7/100\n",
      "6091/6091 [==============================] - 1s 99us/sample - loss: 0.6352 - val_loss: 0.6368\n",
      "Epoch 8/100\n",
      "6091/6091 [==============================] - 1s 99us/sample - loss: 0.6288 - val_loss: 0.6313\n",
      "Epoch 9/100\n",
      "6091/6091 [==============================] - 1s 99us/sample - loss: 0.6226 - val_loss: 0.6261\n",
      "Epoch 10/100\n",
      "6091/6091 [==============================] - 1s 99us/sample - loss: 0.6166 - val_loss: 0.6210\n",
      "Epoch 11/100\n",
      "6091/6091 [==============================] - 1s 98us/sample - loss: 0.6109 - val_loss: 0.6161\n",
      "Epoch 12/100\n",
      "6091/6091 [==============================] - 1s 101us/sample - loss: 0.6053 - val_loss: 0.6114\n",
      "Epoch 13/100\n",
      "6091/6091 [==============================] - 1s 98us/sample - loss: 0.5999 - val_loss: 0.6069\n",
      "Epoch 14/100\n",
      "6091/6091 [==============================] - 1s 99us/sample - loss: 0.5947 - val_loss: 0.6026\n",
      "Epoch 15/100\n",
      "6091/6091 [==============================] - 1s 100us/sample - loss: 0.5897 - val_loss: 0.5984\n",
      "Epoch 16/100\n",
      "6091/6091 [==============================] - 1s 100us/sample - loss: 0.5849 - val_loss: 0.5943\n",
      "Epoch 17/100\n",
      "6091/6091 [==============================] - 1s 103us/sample - loss: 0.5801 - val_loss: 0.5904\n",
      "Epoch 18/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.5756 - val_loss: 0.5866\n",
      "Epoch 19/100\n",
      "6091/6091 [==============================] - 1s 106us/sample - loss: 0.5711 - val_loss: 0.5830\n",
      "Epoch 20/100\n",
      "6091/6091 [==============================] - 1s 99us/sample - loss: 0.5669 - val_loss: 0.5795\n",
      "Epoch 21/100\n",
      "6091/6091 [==============================] - 1s 103us/sample - loss: 0.5628 - val_loss: 0.5761\n",
      "Epoch 22/100\n",
      "6091/6091 [==============================] - 1s 100us/sample - loss: 0.5587 - val_loss: 0.5729\n",
      "Epoch 23/100\n",
      "6091/6091 [==============================] - 1s 104us/sample - loss: 0.5548 - val_loss: 0.5697\n",
      "Epoch 24/100\n",
      "6091/6091 [==============================] - 1s 103us/sample - loss: 0.5510 - val_loss: 0.5667\n",
      "Epoch 25/100\n",
      "6091/6091 [==============================] - 1s 100us/sample - loss: 0.5474 - val_loss: 0.5637\n",
      "Epoch 26/100\n",
      "6091/6091 [==============================] - 1s 100us/sample - loss: 0.5438 - val_loss: 0.5609\n",
      "Epoch 27/100\n",
      "6091/6091 [==============================] - 1s 100us/sample - loss: 0.5404 - val_loss: 0.5581\n",
      "Epoch 28/100\n",
      "6091/6091 [==============================] - 1s 100us/sample - loss: 0.5370 - val_loss: 0.5554\n",
      "Epoch 29/100\n",
      "6091/6091 [==============================] - 1s 102us/sample - loss: 0.5337 - val_loss: 0.5528\n",
      "Epoch 30/100\n",
      "6091/6091 [==============================] - 1s 99us/sample - loss: 0.5305 - val_loss: 0.5503\n",
      "Epoch 31/100\n",
      "6091/6091 [==============================] - 1s 101us/sample - loss: 0.5274 - val_loss: 0.5479\n",
      "Epoch 32/100\n",
      "6091/6091 [==============================] - 1s 99us/sample - loss: 0.5244 - val_loss: 0.5455\n",
      "Epoch 33/100\n",
      "6091/6091 [==============================] - 1s 102us/sample - loss: 0.5214 - val_loss: 0.5433\n",
      "Epoch 34/100\n",
      "6091/6091 [==============================] - 1s 100us/sample - loss: 0.5186 - val_loss: 0.5411\n",
      "Epoch 35/100\n",
      "6091/6091 [==============================] - 1s 100us/sample - loss: 0.5158 - val_loss: 0.5389\n",
      "Epoch 36/100\n",
      "6091/6091 [==============================] - 1s 101us/sample - loss: 0.5130 - val_loss: 0.5368\n",
      "Epoch 37/100\n",
      "6091/6091 [==============================] - 1s 100us/sample - loss: 0.5103 - val_loss: 0.5348\n",
      "Epoch 38/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.5078 - val_loss: 0.5328\n",
      "Epoch 39/100\n",
      "6091/6091 [==============================] - 1s 100us/sample - loss: 0.5052 - val_loss: 0.5309\n",
      "Epoch 40/100\n",
      "6091/6091 [==============================] - 1s 105us/sample - loss: 0.5027 - val_loss: 0.5291\n",
      "Epoch 41/100\n",
      "6091/6091 [==============================] - 1s 100us/sample - loss: 0.5003 - val_loss: 0.5273\n",
      "Epoch 42/100\n",
      "6091/6091 [==============================] - 1s 103us/sample - loss: 0.4979 - val_loss: 0.5255\n",
      "Epoch 43/100\n",
      "6091/6091 [==============================] - 1s 103us/sample - loss: 0.4956 - val_loss: 0.5237\n",
      "Epoch 44/100\n",
      "6091/6091 [==============================] - 1s 104us/sample - loss: 0.4933 - val_loss: 0.5221\n",
      "Epoch 45/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.4911 - val_loss: 0.5204\n",
      "Epoch 46/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4890 - val_loss: 0.5189\n",
      "Epoch 47/100\n",
      "6091/6091 [==============================] - 1s 117us/sample - loss: 0.4868 - val_loss: 0.5174\n",
      "Epoch 48/100\n",
      "6091/6091 [==============================] - 1s 110us/sample - loss: 0.4847 - val_loss: 0.5158\n",
      "Epoch 49/100\n",
      "6091/6091 [==============================] - 1s 117us/sample - loss: 0.4827 - val_loss: 0.5144\n",
      "Epoch 50/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4807 - val_loss: 0.5129\n",
      "Epoch 51/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.4787 - val_loss: 0.5115\n",
      "Epoch 52/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.4768 - val_loss: 0.5102\n",
      "Epoch 53/100\n",
      "6091/6091 [==============================] - 1s 110us/sample - loss: 0.4749 - val_loss: 0.5088\n",
      "Epoch 54/100\n",
      "6091/6091 [==============================] - 1s 125us/sample - loss: 0.4730 - val_loss: 0.5076\n",
      "Epoch 55/100\n",
      "6091/6091 [==============================] - 1s 125us/sample - loss: 0.4712 - val_loss: 0.5063\n",
      "Epoch 56/100\n",
      "6091/6091 [==============================] - 1s 125us/sample - loss: 0.4694 - val_loss: 0.5050\n",
      "Epoch 57/100\n",
      "6091/6091 [==============================] - 1s 118us/sample - loss: 0.4676 - val_loss: 0.5038\n",
      "Epoch 58/100\n",
      "6091/6091 [==============================] - 1s 115us/sample - loss: 0.4659 - val_loss: 0.5026\n",
      "Epoch 59/100\n",
      "6091/6091 [==============================] - 1s 115us/sample - loss: 0.4642 - val_loss: 0.5015\n",
      "Epoch 60/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.4625 - val_loss: 0.5003\n",
      "Epoch 61/100\n",
      "6091/6091 [==============================] - 1s 114us/sample - loss: 0.4609 - val_loss: 0.4992\n",
      "Epoch 62/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.4592 - val_loss: 0.4982\n",
      "Epoch 63/100\n",
      "6091/6091 [==============================] - 1s 106us/sample - loss: 0.4576 - val_loss: 0.4971\n",
      "Epoch 64/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.4561 - val_loss: 0.4961\n",
      "Epoch 65/100\n",
      "6091/6091 [==============================] - 1s 113us/sample - loss: 0.4545 - val_loss: 0.4950\n",
      "Epoch 66/100\n",
      "6091/6091 [==============================] - 1s 148us/sample - loss: 0.4530 - val_loss: 0.4940\n",
      "Epoch 67/100\n",
      "6091/6091 [==============================] - 1s 129us/sample - loss: 0.4515 - val_loss: 0.4931\n",
      "Epoch 68/100\n",
      "6091/6091 [==============================] - 1s 148us/sample - loss: 0.4501 - val_loss: 0.4921\n",
      "Epoch 69/100\n",
      "6091/6091 [==============================] - 1s 134us/sample - loss: 0.4486 - val_loss: 0.4912\n",
      "Epoch 70/100\n",
      "6091/6091 [==============================] - 1s 152us/sample - loss: 0.4472 - val_loss: 0.4903\n",
      "Epoch 71/100\n",
      "6091/6091 [==============================] - 1s 137us/sample - loss: 0.4458 - val_loss: 0.4894\n",
      "Epoch 72/100\n",
      "6091/6091 [==============================] - 1s 130us/sample - loss: 0.4444 - val_loss: 0.4885\n",
      "Epoch 73/100\n",
      "6091/6091 [==============================] - 1s 105us/sample - loss: 0.4430 - val_loss: 0.4877\n",
      "Epoch 74/100\n",
      "6091/6091 [==============================] - 1s 104us/sample - loss: 0.4417 - val_loss: 0.4869\n",
      "Epoch 75/100\n",
      "6091/6091 [==============================] - 1s 111us/sample - loss: 0.4403 - val_loss: 0.4861\n",
      "Epoch 76/100\n",
      "6091/6091 [==============================] - 1s 127us/sample - loss: 0.4390 - val_loss: 0.4852\n",
      "Epoch 77/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6091/6091 [==============================] - 1s 133us/sample - loss: 0.4377 - val_loss: 0.4844\n",
      "Epoch 78/100\n",
      "6091/6091 [==============================] - 1s 134us/sample - loss: 0.4365 - val_loss: 0.4836\n",
      "Epoch 79/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.4352 - val_loss: 0.4829\n",
      "Epoch 80/100\n",
      "6091/6091 [==============================] - 1s 142us/sample - loss: 0.4339 - val_loss: 0.4822\n",
      "Epoch 81/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.4327 - val_loss: 0.4814\n",
      "Epoch 82/100\n",
      "6091/6091 [==============================] - 1s 119us/sample - loss: 0.4315 - val_loss: 0.4807\n",
      "Epoch 83/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.4303 - val_loss: 0.4799\n",
      "Epoch 84/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.4291 - val_loss: 0.4792\n",
      "Epoch 85/100\n",
      "6091/6091 [==============================] - 1s 131us/sample - loss: 0.4280 - val_loss: 0.4786\n",
      "Epoch 86/100\n",
      "6091/6091 [==============================] - 1s 120us/sample - loss: 0.4268 - val_loss: 0.4779\n",
      "Epoch 87/100\n",
      "6091/6091 [==============================] - 1s 105us/sample - loss: 0.4257 - val_loss: 0.4772\n",
      "Epoch 88/100\n",
      "6091/6091 [==============================] - 1s 135us/sample - loss: 0.4246 - val_loss: 0.4766\n",
      "Epoch 89/100\n",
      "6091/6091 [==============================] - 1s 122us/sample - loss: 0.4234 - val_loss: 0.4760\n",
      "Epoch 90/100\n",
      "6091/6091 [==============================] - 1s 114us/sample - loss: 0.4224 - val_loss: 0.4754\n",
      "Epoch 91/100\n",
      "6091/6091 [==============================] - 1s 100us/sample - loss: 0.4213 - val_loss: 0.4748\n",
      "Epoch 92/100\n",
      "6091/6091 [==============================] - 1s 106us/sample - loss: 0.4202 - val_loss: 0.4742\n",
      "Epoch 93/100\n",
      "6091/6091 [==============================] - 1s 103us/sample - loss: 0.4191 - val_loss: 0.4736\n",
      "Epoch 94/100\n",
      "6091/6091 [==============================] - 1s 110us/sample - loss: 0.4181 - val_loss: 0.4730\n",
      "Epoch 95/100\n",
      "6091/6091 [==============================] - 1s 128us/sample - loss: 0.4171 - val_loss: 0.4725\n",
      "Epoch 96/100\n",
      "6091/6091 [==============================] - 1s 113us/sample - loss: 0.4160 - val_loss: 0.4719\n",
      "Epoch 97/100\n",
      "6091/6091 [==============================] - 1s 100us/sample - loss: 0.4150 - val_loss: 0.4714\n",
      "Epoch 98/100\n",
      "6091/6091 [==============================] - 1s 102us/sample - loss: 0.4140 - val_loss: 0.4708\n",
      "Epoch 99/100\n",
      "6091/6091 [==============================] - 1s 125us/sample - loss: 0.4130 - val_loss: 0.4703\n",
      "Epoch 100/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.4120 - val_loss: 0.4697\n",
      "Predicting...\n",
      "(Took 68.169 sec)\n",
      "Train on 6091 samples, validate on 1522 samples\n",
      "Epoch 1/100\n",
      "6091/6091 [==============================] - 1s 150us/sample - loss: 0.6855 - val_loss: 0.6805\n",
      "Epoch 2/100\n",
      "6091/6091 [==============================] - 1s 127us/sample - loss: 0.6727 - val_loss: 0.6722\n",
      "Epoch 3/100\n",
      "6091/6091 [==============================] - 1s 139us/sample - loss: 0.6631 - val_loss: 0.6653\n",
      "Epoch 4/100\n",
      "6091/6091 [==============================] - 1s 105us/sample - loss: 0.6550 - val_loss: 0.6590\n",
      "Epoch 5/100\n",
      "6091/6091 [==============================] - 1s 117us/sample - loss: 0.6476 - val_loss: 0.6530\n",
      "Epoch 6/100\n",
      "6091/6091 [==============================] - 1s 116us/sample - loss: 0.6406 - val_loss: 0.6471\n",
      "Epoch 7/100\n",
      "6091/6091 [==============================] - 1s 105us/sample - loss: 0.6338 - val_loss: 0.6415\n",
      "Epoch 8/100\n",
      "6091/6091 [==============================] - 1s 100us/sample - loss: 0.6273 - val_loss: 0.6360\n",
      "Epoch 9/100\n",
      "6091/6091 [==============================] - 1s 110us/sample - loss: 0.6210 - val_loss: 0.6307\n",
      "Epoch 10/100\n",
      "6091/6091 [==============================] - 1s 120us/sample - loss: 0.6150 - val_loss: 0.6257\n",
      "Epoch 11/100\n",
      "6091/6091 [==============================] - 1s 119us/sample - loss: 0.6092 - val_loss: 0.6209\n",
      "Epoch 12/100\n",
      "6091/6091 [==============================] - 1s 100us/sample - loss: 0.6036 - val_loss: 0.6162\n",
      "Epoch 13/100\n",
      "6091/6091 [==============================] - 1s 104us/sample - loss: 0.5981 - val_loss: 0.6117\n",
      "Epoch 14/100\n",
      "6091/6091 [==============================] - 1s 102us/sample - loss: 0.5929 - val_loss: 0.6074\n",
      "Epoch 15/100\n",
      "6091/6091 [==============================] - 1s 103us/sample - loss: 0.5878 - val_loss: 0.6032\n",
      "Epoch 16/100\n",
      "6091/6091 [==============================] - 1s 101us/sample - loss: 0.5829 - val_loss: 0.5992\n",
      "Epoch 17/100\n",
      "6091/6091 [==============================] - 1s 103us/sample - loss: 0.5782 - val_loss: 0.5954\n",
      "Epoch 18/100\n",
      "6091/6091 [==============================] - 1s 115us/sample - loss: 0.5736 - val_loss: 0.5917\n",
      "Epoch 19/100\n",
      "6091/6091 [==============================] - 1s 131us/sample - loss: 0.5691 - val_loss: 0.5881\n",
      "Epoch 20/100\n",
      "6091/6091 [==============================] - 1s 138us/sample - loss: 0.5649 - val_loss: 0.5846\n",
      "Epoch 21/100\n",
      "6091/6091 [==============================] - 1s 100us/sample - loss: 0.5607 - val_loss: 0.5813\n",
      "Epoch 22/100\n",
      "6091/6091 [==============================] - 1s 138us/sample - loss: 0.5566 - val_loss: 0.5780\n",
      "Epoch 23/100\n",
      "6091/6091 [==============================] - 1s 100us/sample - loss: 0.5527 - val_loss: 0.5749\n",
      "Epoch 24/100\n",
      "6091/6091 [==============================] - 1s 104us/sample - loss: 0.5489 - val_loss: 0.5720\n",
      "Epoch 25/100\n",
      "6091/6091 [==============================] - 1s 102us/sample - loss: 0.5452 - val_loss: 0.5691\n",
      "Epoch 26/100\n",
      "6091/6091 [==============================] - 1s 113us/sample - loss: 0.5416 - val_loss: 0.5663\n",
      "Epoch 27/100\n",
      "6091/6091 [==============================] - 1s 124us/sample - loss: 0.5381 - val_loss: 0.5636\n",
      "Epoch 28/100\n",
      "6091/6091 [==============================] - 1s 118us/sample - loss: 0.5347 - val_loss: 0.5609\n",
      "Epoch 29/100\n",
      "6091/6091 [==============================] - 1s 136us/sample - loss: 0.5314 - val_loss: 0.5585\n",
      "Epoch 30/100\n",
      "6091/6091 [==============================] - 1s 141us/sample - loss: 0.5282 - val_loss: 0.5560\n",
      "Epoch 31/100\n",
      "6091/6091 [==============================] - 1s 123us/sample - loss: 0.5250 - val_loss: 0.5536\n",
      "Epoch 32/100\n",
      "6091/6091 [==============================] - 1s 104us/sample - loss: 0.5219 - val_loss: 0.5513\n",
      "Epoch 33/100\n",
      "6091/6091 [==============================] - 1s 102us/sample - loss: 0.5190 - val_loss: 0.5491\n",
      "Epoch 34/100\n",
      "6091/6091 [==============================] - 1s 104us/sample - loss: 0.5161 - val_loss: 0.5470\n",
      "Epoch 35/100\n",
      "6091/6091 [==============================] - 1s 99us/sample - loss: 0.5133 - val_loss: 0.5449\n",
      "Epoch 36/100\n",
      "6091/6091 [==============================] - 1s 106us/sample - loss: 0.5105 - val_loss: 0.5429\n",
      "Epoch 37/100\n",
      "6091/6091 [==============================] - 1s 99us/sample - loss: 0.5078 - val_loss: 0.5410\n",
      "Epoch 38/100\n",
      "6091/6091 [==============================] - 1s 132us/sample - loss: 0.5052 - val_loss: 0.5390\n",
      "Epoch 39/100\n",
      "6091/6091 [==============================] - 1s 105us/sample - loss: 0.5026 - val_loss: 0.5371\n",
      "Epoch 40/100\n",
      "6091/6091 [==============================] - 1s 101us/sample - loss: 0.5001 - val_loss: 0.5353\n",
      "Epoch 41/100\n",
      "6091/6091 [==============================] - 1s 127us/sample - loss: 0.4977 - val_loss: 0.5336\n",
      "Epoch 42/100\n",
      "6091/6091 [==============================] - 1s 105us/sample - loss: 0.4953 - val_loss: 0.5319\n",
      "Epoch 43/100\n",
      "6091/6091 [==============================] - 1s 99us/sample - loss: 0.4929 - val_loss: 0.5303\n",
      "Epoch 44/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.4906 - val_loss: 0.5287\n",
      "Epoch 45/100\n",
      "6091/6091 [==============================] - 1s 101us/sample - loss: 0.4884 - val_loss: 0.5271\n",
      "Epoch 46/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.4862 - val_loss: 0.5256\n",
      "Epoch 47/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4841 - val_loss: 0.5241\n",
      "Epoch 48/100\n",
      "6091/6091 [==============================] - 1s 142us/sample - loss: 0.4819 - val_loss: 0.5227\n",
      "Epoch 49/100\n",
      "6091/6091 [==============================] - 1s 136us/sample - loss: 0.4799 - val_loss: 0.5213\n",
      "Epoch 50/100\n",
      "6091/6091 [==============================] - 1s 160us/sample - loss: 0.4779 - val_loss: 0.5200\n",
      "Epoch 51/100\n",
      "6091/6091 [==============================] - 1s 155us/sample - loss: 0.4759 - val_loss: 0.5187\n",
      "Epoch 52/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6091/6091 [==============================] - 1s 143us/sample - loss: 0.4740 - val_loss: 0.5174\n",
      "Epoch 53/100\n",
      "6091/6091 [==============================] - 1s 146us/sample - loss: 0.4720 - val_loss: 0.5161\n",
      "Epoch 54/100\n",
      "6091/6091 [==============================] - 1s 141us/sample - loss: 0.4701 - val_loss: 0.5149\n",
      "Epoch 55/100\n",
      "6091/6091 [==============================] - 1s 118us/sample - loss: 0.4683 - val_loss: 0.5137\n",
      "Epoch 56/100\n",
      "6091/6091 [==============================] - 1s 118us/sample - loss: 0.4665 - val_loss: 0.5126\n",
      "Epoch 57/100\n",
      "6091/6091 [==============================] - 1s 116us/sample - loss: 0.4648 - val_loss: 0.5115\n",
      "Epoch 58/100\n",
      "6091/6091 [==============================] - 1s 121us/sample - loss: 0.4630 - val_loss: 0.5104\n",
      "Epoch 59/100\n",
      "6091/6091 [==============================] - 1s 120us/sample - loss: 0.4613 - val_loss: 0.5093\n",
      "Epoch 60/100\n",
      "6091/6091 [==============================] - 1s 122us/sample - loss: 0.4596 - val_loss: 0.5083\n",
      "Epoch 61/100\n",
      "6091/6091 [==============================] - 1s 124us/sample - loss: 0.4580 - val_loss: 0.5072\n",
      "Epoch 62/100\n",
      "6091/6091 [==============================] - 1s 114us/sample - loss: 0.4563 - val_loss: 0.5062\n",
      "Epoch 63/100\n",
      "6091/6091 [==============================] - 1s 115us/sample - loss: 0.4547 - val_loss: 0.5053\n",
      "Epoch 64/100\n",
      "6091/6091 [==============================] - 1s 115us/sample - loss: 0.4531 - val_loss: 0.5043\n",
      "Epoch 65/100\n",
      "6091/6091 [==============================] - 1s 116us/sample - loss: 0.4516 - val_loss: 0.5034\n",
      "Epoch 66/100\n",
      "6091/6091 [==============================] - 1s 118us/sample - loss: 0.4501 - val_loss: 0.5024\n",
      "Epoch 67/100\n",
      "6091/6091 [==============================] - 1s 123us/sample - loss: 0.4485 - val_loss: 0.5015\n",
      "Epoch 68/100\n",
      "6091/6091 [==============================] - 1s 137us/sample - loss: 0.4471 - val_loss: 0.5007\n",
      "Epoch 69/100\n",
      "6091/6091 [==============================] - 1s 132us/sample - loss: 0.4456 - val_loss: 0.4998\n",
      "Epoch 70/100\n",
      "6091/6091 [==============================] - 1s 129us/sample - loss: 0.4442 - val_loss: 0.4990\n",
      "Epoch 71/100\n",
      "6091/6091 [==============================] - 1s 114us/sample - loss: 0.4428 - val_loss: 0.4982\n",
      "Epoch 72/100\n",
      "6091/6091 [==============================] - 1s 121us/sample - loss: 0.4414 - val_loss: 0.4974\n",
      "Epoch 73/100\n",
      "6091/6091 [==============================] - 1s 122us/sample - loss: 0.4400 - val_loss: 0.4966\n",
      "Epoch 74/100\n",
      "6091/6091 [==============================] - 1s 128us/sample - loss: 0.4386 - val_loss: 0.4959\n",
      "Epoch 75/100\n",
      "6091/6091 [==============================] - 1s 128us/sample - loss: 0.4373 - val_loss: 0.4951\n",
      "Epoch 76/100\n",
      "6091/6091 [==============================] - 1s 124us/sample - loss: 0.4360 - val_loss: 0.4944\n",
      "Epoch 77/100\n",
      "6091/6091 [==============================] - 1s 126us/sample - loss: 0.4347 - val_loss: 0.4937\n",
      "Epoch 78/100\n",
      "6091/6091 [==============================] - 1s 121us/sample - loss: 0.4334 - val_loss: 0.4930\n",
      "Epoch 79/100\n",
      "6091/6091 [==============================] - 1s 126us/sample - loss: 0.4321 - val_loss: 0.4923\n",
      "Epoch 80/100\n",
      "6091/6091 [==============================] - 1s 119us/sample - loss: 0.4309 - val_loss: 0.4917\n",
      "Epoch 81/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.4296 - val_loss: 0.4910\n",
      "Epoch 82/100\n",
      "6091/6091 [==============================] - 1s 116us/sample - loss: 0.4284 - val_loss: 0.4904\n",
      "Epoch 83/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.4272 - val_loss: 0.4898\n",
      "Epoch 84/100\n",
      "6091/6091 [==============================] - 1s 104us/sample - loss: 0.4260 - val_loss: 0.4892\n",
      "Epoch 85/100\n",
      "6091/6091 [==============================] - 1s 106us/sample - loss: 0.4248 - val_loss: 0.4886\n",
      "Epoch 86/100\n",
      "6091/6091 [==============================] - 1s 106us/sample - loss: 0.4237 - val_loss: 0.4880\n",
      "Epoch 87/100\n",
      "6091/6091 [==============================] - 1s 117us/sample - loss: 0.4225 - val_loss: 0.4875\n",
      "Epoch 88/100\n",
      "6091/6091 [==============================] - 1s 105us/sample - loss: 0.4214 - val_loss: 0.4869\n",
      "Epoch 89/100\n",
      "6091/6091 [==============================] - 1s 111us/sample - loss: 0.4203 - val_loss: 0.4864\n",
      "Epoch 90/100\n",
      "6091/6091 [==============================] - 1s 114us/sample - loss: 0.4192 - val_loss: 0.4858\n",
      "Epoch 91/100\n",
      "6091/6091 [==============================] - 1s 106us/sample - loss: 0.4181 - val_loss: 0.4853\n",
      "Epoch 92/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4170 - val_loss: 0.4848\n",
      "Epoch 93/100\n",
      "6091/6091 [==============================] - 1s 105us/sample - loss: 0.4159 - val_loss: 0.4843\n",
      "Epoch 94/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.4149 - val_loss: 0.4838\n",
      "Epoch 95/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4138 - val_loss: 0.4833\n",
      "Epoch 96/100\n",
      "6091/6091 [==============================] - 1s 105us/sample - loss: 0.4128 - val_loss: 0.4829\n",
      "Epoch 97/100\n",
      "6091/6091 [==============================] - 1s 105us/sample - loss: 0.4118 - val_loss: 0.4824\n",
      "Epoch 98/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.4108 - val_loss: 0.4820\n",
      "Epoch 99/100\n",
      "6091/6091 [==============================] - 1s 104us/sample - loss: 0.4098 - val_loss: 0.4815\n",
      "Epoch 100/100\n",
      "6091/6091 [==============================] - 1s 105us/sample - loss: 0.4088 - val_loss: 0.4811\n",
      "Predicting...\n",
      "(Took 71.664 sec)\n",
      "Combined confusion matrix:\n",
      "[[3853.  489.]\n",
      " [1123. 2148.]]\n",
      "(Overall, took 341.345 sec)\n",
      "Accuracy: 78.83% +/- 0.95%\n",
      "Precision for positive class: 77.43% +/- 1.73%\n",
      "Precision for negative class: 81.47% +/- 1.82%\n",
      "Recall for positive class: 88.74% +/- 1.25%\n",
      "Recall for negative class: 65.69% +/- 1.12%\n",
      "F for positive class: 82.69% +/- 1.10%\n",
      "F for negative class: 72.72% +/- 0.47%\n",
      "Mean F score: 77.70% +/- 0.77%\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/100\n",
      "6090/6090 [==============================] - 1s 146us/sample - loss: 0.6833 - val_loss: 0.6727\n",
      "Epoch 2/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.6650 - val_loss: 0.6578\n",
      "Epoch 3/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.6502 - val_loss: 0.6456\n",
      "Epoch 4/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.6369 - val_loss: 0.6346\n",
      "Epoch 5/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.6245 - val_loss: 0.6245\n",
      "Epoch 6/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.6129 - val_loss: 0.6150\n",
      "Epoch 7/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.6019 - val_loss: 0.6062\n",
      "Epoch 8/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.5915 - val_loss: 0.5979\n",
      "Epoch 9/100\n",
      "6090/6090 [==============================] - 1s 116us/sample - loss: 0.5816 - val_loss: 0.5899\n",
      "Epoch 10/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.5722 - val_loss: 0.5826\n",
      "Epoch 11/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.5633 - val_loss: 0.5756\n",
      "Epoch 12/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.5548 - val_loss: 0.5690\n",
      "Epoch 13/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.5468 - val_loss: 0.5628\n",
      "Epoch 14/100\n",
      "6090/6090 [==============================] - 1s 132us/sample - loss: 0.5390 - val_loss: 0.5570\n",
      "Epoch 15/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.5317 - val_loss: 0.5514\n",
      "Epoch 16/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.5246 - val_loss: 0.5461\n",
      "Epoch 17/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.5180 - val_loss: 0.5412\n",
      "Epoch 18/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.5115 - val_loss: 0.5365\n",
      "Epoch 19/100\n",
      "6090/6090 [==============================] - 1s 130us/sample - loss: 0.5054 - val_loss: 0.5320\n",
      "Epoch 20/100\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.4995 - val_loss: 0.5278\n",
      "Epoch 21/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.4939 - val_loss: 0.5238\n",
      "Epoch 22/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4885 - val_loss: 0.5200\n",
      "Epoch 23/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.4833 - val_loss: 0.5164\n",
      "Epoch 24/100\n",
      "6090/6090 [==============================] - 1s 101us/sample - loss: 0.4783 - val_loss: 0.5130\n",
      "Epoch 25/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.4735 - val_loss: 0.5098\n",
      "Epoch 26/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.4689 - val_loss: 0.5068\n",
      "Epoch 27/100\n",
      "6090/6090 [==============================] - 1s 99us/sample - loss: 0.4645 - val_loss: 0.5039\n",
      "Epoch 28/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.4602 - val_loss: 0.5012\n",
      "Epoch 29/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4561 - val_loss: 0.4986\n",
      "Epoch 30/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.4521 - val_loss: 0.4961\n",
      "Epoch 31/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.4482 - val_loss: 0.4937\n",
      "Epoch 32/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4445 - val_loss: 0.4915\n",
      "Epoch 33/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.4409 - val_loss: 0.4894\n",
      "Epoch 34/100\n",
      "6090/6090 [==============================] - 1s 101us/sample - loss: 0.4375 - val_loss: 0.4873\n",
      "Epoch 35/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.4341 - val_loss: 0.4855\n",
      "Epoch 36/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.4308 - val_loss: 0.4837\n",
      "Epoch 37/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4277 - val_loss: 0.4819\n",
      "Epoch 38/100\n",
      "6090/6090 [==============================] - 1s 101us/sample - loss: 0.4246 - val_loss: 0.4804\n",
      "Epoch 39/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.4216 - val_loss: 0.4788\n",
      "Epoch 40/100\n",
      "6090/6090 [==============================] - 1s 116us/sample - loss: 0.4187 - val_loss: 0.4773\n",
      "Epoch 41/100\n",
      "6090/6090 [==============================] - 1s 124us/sample - loss: 0.4159 - val_loss: 0.4759\n",
      "Epoch 42/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.4132 - val_loss: 0.4746\n",
      "Epoch 43/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.4105 - val_loss: 0.4733\n",
      "Epoch 44/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4079 - val_loss: 0.4722\n",
      "Epoch 45/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.4054 - val_loss: 0.4711\n",
      "Epoch 46/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4030 - val_loss: 0.4700\n",
      "Epoch 47/100\n",
      "6090/6090 [==============================] - 1s 101us/sample - loss: 0.4006 - val_loss: 0.4690\n",
      "Epoch 48/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.3983 - val_loss: 0.4680\n",
      "Epoch 49/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.3960 - val_loss: 0.4671\n",
      "Epoch 50/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.3938 - val_loss: 0.4662\n",
      "Epoch 51/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.3916 - val_loss: 0.4654\n",
      "Epoch 52/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.3895 - val_loss: 0.4647\n",
      "Epoch 53/100\n",
      "6090/6090 [==============================] - 1s 101us/sample - loss: 0.3874 - val_loss: 0.4639\n",
      "Epoch 54/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.3854 - val_loss: 0.4632\n",
      "Epoch 55/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.3834 - val_loss: 0.4626\n",
      "Epoch 56/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.3815 - val_loss: 0.4620\n",
      "Epoch 57/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.3796 - val_loss: 0.4614\n",
      "Epoch 58/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.3778 - val_loss: 0.4609\n",
      "Epoch 59/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.3760 - val_loss: 0.4604\n",
      "Epoch 60/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.3742 - val_loss: 0.4599\n",
      "Epoch 61/100\n",
      "6090/6090 [==============================] - 1s 119us/sample - loss: 0.3724 - val_loss: 0.4595\n",
      "Epoch 62/100\n",
      "6090/6090 [==============================] - 1s 144us/sample - loss: 0.3707 - val_loss: 0.4590\n",
      "Epoch 63/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.3690 - val_loss: 0.4586\n",
      "Epoch 64/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.3674 - val_loss: 0.4582\n",
      "Epoch 65/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.3658 - val_loss: 0.4579\n",
      "Epoch 66/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.3642 - val_loss: 0.4576\n",
      "Epoch 67/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.3626 - val_loss: 0.4573\n",
      "Epoch 68/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.3611 - val_loss: 0.4570\n",
      "Epoch 69/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.3596 - val_loss: 0.4568\n",
      "Epoch 70/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.3581 - val_loss: 0.4566\n",
      "Epoch 71/100\n",
      "6090/6090 [==============================] - 1s 116us/sample - loss: 0.3567 - val_loss: 0.4564\n",
      "Epoch 72/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.3553 - val_loss: 0.4562\n",
      "Epoch 73/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.3539 - val_loss: 0.4560\n",
      "Epoch 74/100\n",
      "6090/6090 [==============================] - 1s 117us/sample - loss: 0.3525 - val_loss: 0.4559\n",
      "Epoch 75/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.3512 - val_loss: 0.4557\n",
      "Epoch 76/100\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.3498 - val_loss: 0.4556\n",
      "Epoch 77/100\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.3485 - val_loss: 0.4556\n",
      "Epoch 78/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.3472 - val_loss: 0.4555\n",
      "Epoch 79/100\n",
      "6090/6090 [==============================] - 1s 123us/sample - loss: 0.3460 - val_loss: 0.4554\n",
      "Epoch 80/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.3447 - val_loss: 0.4554\n",
      "Epoch 81/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.3435 - val_loss: 0.4554\n",
      "Predicting...\n",
      "(Took 54.274 sec)\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/100\n",
      "6090/6090 [==============================] - 1s 146us/sample - loss: 0.6829 - val_loss: 0.6710\n",
      "Epoch 2/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.6649 - val_loss: 0.6557\n",
      "Epoch 3/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.6503 - val_loss: 0.6433\n",
      "Epoch 4/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.6371 - val_loss: 0.6323\n",
      "Epoch 5/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.6248 - val_loss: 0.6220\n",
      "Epoch 6/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.6132 - val_loss: 0.6126\n",
      "Epoch 7/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.6023 - val_loss: 0.6038\n",
      "Epoch 8/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.5919 - val_loss: 0.5956\n",
      "Epoch 9/100\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.5820 - val_loss: 0.5877\n",
      "Epoch 10/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.5726 - val_loss: 0.5805\n",
      "Epoch 11/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.5638 - val_loss: 0.5735\n",
      "Epoch 12/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.5553 - val_loss: 0.5670\n",
      "Epoch 13/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.5472 - val_loss: 0.5608\n",
      "Epoch 14/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.5395 - val_loss: 0.5551\n",
      "Epoch 15/100\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.5322 - val_loss: 0.5496\n",
      "Epoch 16/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.5252 - val_loss: 0.5443\n",
      "Epoch 17/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6090/6090 [==============================] - 1s 122us/sample - loss: 0.5185 - val_loss: 0.5396\n",
      "Epoch 18/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.5121 - val_loss: 0.5350\n",
      "Epoch 19/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.5059 - val_loss: 0.5306\n",
      "Epoch 20/100\n",
      "6090/6090 [==============================] - 1s 134us/sample - loss: 0.5000 - val_loss: 0.5265\n",
      "Epoch 21/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.4944 - val_loss: 0.5226\n",
      "Epoch 22/100\n",
      "6090/6090 [==============================] - 1s 130us/sample - loss: 0.4890 - val_loss: 0.5188\n",
      "Epoch 23/100\n",
      "6090/6090 [==============================] - 1s 118us/sample - loss: 0.4838 - val_loss: 0.5154\n",
      "Epoch 24/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.4788 - val_loss: 0.5121\n",
      "Epoch 25/100\n",
      "6090/6090 [==============================] - 1s 123us/sample - loss: 0.4740 - val_loss: 0.5089\n",
      "Epoch 26/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.4695 - val_loss: 0.5059\n",
      "Epoch 27/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.4650 - val_loss: 0.5031\n",
      "Epoch 28/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.4607 - val_loss: 0.5004\n",
      "Epoch 29/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.4566 - val_loss: 0.4978\n",
      "Epoch 30/100\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.4526 - val_loss: 0.4954\n",
      "Epoch 31/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.4487 - val_loss: 0.4931\n",
      "Epoch 32/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4450 - val_loss: 0.4909\n",
      "Epoch 33/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.4414 - val_loss: 0.4889\n",
      "Epoch 34/100\n",
      "6090/6090 [==============================] - 1s 119us/sample - loss: 0.4379 - val_loss: 0.4870\n",
      "Epoch 35/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.4345 - val_loss: 0.4851\n",
      "Epoch 36/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.4313 - val_loss: 0.4834\n",
      "Epoch 37/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.4281 - val_loss: 0.4817\n",
      "Epoch 38/100\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.4250 - val_loss: 0.4802\n",
      "Epoch 39/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.4220 - val_loss: 0.4787\n",
      "Epoch 40/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.4191 - val_loss: 0.4772\n",
      "Epoch 41/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4163 - val_loss: 0.4758\n",
      "Epoch 42/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.4136 - val_loss: 0.4745\n",
      "Epoch 43/100\n",
      "6090/6090 [==============================] - 1s 116us/sample - loss: 0.4109 - val_loss: 0.4733\n",
      "Epoch 44/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4083 - val_loss: 0.4721\n",
      "Epoch 45/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.4058 - val_loss: 0.4710\n",
      "Epoch 46/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4033 - val_loss: 0.4700\n",
      "Epoch 47/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4009 - val_loss: 0.4690\n",
      "Epoch 48/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.3986 - val_loss: 0.4680\n",
      "Epoch 49/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.3963 - val_loss: 0.4671\n",
      "Epoch 50/100\n",
      "6090/6090 [==============================] - 1s 101us/sample - loss: 0.3941 - val_loss: 0.4662\n",
      "Epoch 51/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.3919 - val_loss: 0.4655\n",
      "Epoch 52/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.3898 - val_loss: 0.4648\n",
      "Epoch 53/100\n",
      "6090/6090 [==============================] - 1s 119us/sample - loss: 0.3877 - val_loss: 0.4641\n",
      "Epoch 54/100\n",
      "6090/6090 [==============================] - 1s 116us/sample - loss: 0.3857 - val_loss: 0.4634\n",
      "Epoch 55/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.3837 - val_loss: 0.4628\n",
      "Epoch 56/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.3818 - val_loss: 0.4621\n",
      "Epoch 57/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.3799 - val_loss: 0.4616\n",
      "Epoch 58/100\n",
      "6090/6090 [==============================] - 1s 129us/sample - loss: 0.3780 - val_loss: 0.4611\n",
      "Epoch 59/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.3762 - val_loss: 0.4606\n",
      "Epoch 60/100\n",
      "6090/6090 [==============================] - 1s 125us/sample - loss: 0.3744 - val_loss: 0.4602\n",
      "Epoch 61/100\n",
      "6090/6090 [==============================] - 1s 123us/sample - loss: 0.3727 - val_loss: 0.4597\n",
      "Epoch 62/100\n",
      "6090/6090 [==============================] - 1s 134us/sample - loss: 0.3710 - val_loss: 0.4593\n",
      "Epoch 63/100\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.3693 - val_loss: 0.4589\n",
      "Epoch 64/100\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.3676 - val_loss: 0.4585\n",
      "Epoch 65/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.3660 - val_loss: 0.4583\n",
      "Epoch 66/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.3644 - val_loss: 0.4579\n",
      "Epoch 67/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.3629 - val_loss: 0.4577\n",
      "Epoch 68/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.3614 - val_loss: 0.4575\n",
      "Epoch 69/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.3599 - val_loss: 0.4573\n",
      "Epoch 70/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.3584 - val_loss: 0.4571\n",
      "Epoch 71/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.3569 - val_loss: 0.4569\n",
      "Epoch 72/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.3555 - val_loss: 0.4568\n",
      "Epoch 73/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.3541 - val_loss: 0.4565\n",
      "Epoch 74/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.3527 - val_loss: 0.4564\n",
      "Epoch 75/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.3514 - val_loss: 0.4563\n",
      "Epoch 76/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.3501 - val_loss: 0.4562\n",
      "Epoch 77/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.3488 - val_loss: 0.4561\n",
      "Epoch 78/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.3475 - val_loss: 0.4561\n",
      "Epoch 79/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.3462 - val_loss: 0.4560\n",
      "Epoch 80/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.3450 - val_loss: 0.4560\n",
      "Epoch 81/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.3437 - val_loss: 0.4560\n",
      "Predicting...\n",
      "(Took 54.853 sec)\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/100\n",
      "6090/6090 [==============================] - 1s 144us/sample - loss: 0.6831 - val_loss: 0.6751\n",
      "Epoch 2/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.6639 - val_loss: 0.6620\n",
      "Epoch 3/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.6489 - val_loss: 0.6511\n",
      "Epoch 4/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.6357 - val_loss: 0.6408\n",
      "Epoch 5/100\n",
      "6090/6090 [==============================] - 1s 120us/sample - loss: 0.6234 - val_loss: 0.6312\n",
      "Epoch 6/100\n",
      "6090/6090 [==============================] - 1s 140us/sample - loss: 0.6118 - val_loss: 0.6222\n",
      "Epoch 7/100\n",
      "6090/6090 [==============================] - 1s 119us/sample - loss: 0.6009 - val_loss: 0.6135\n",
      "Epoch 8/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.5905 - val_loss: 0.6055\n",
      "Epoch 9/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.5807 - val_loss: 0.5978\n",
      "Epoch 10/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.5713 - val_loss: 0.5905\n",
      "Epoch 11/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.5624 - val_loss: 0.5836\n",
      "Epoch 12/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.5540 - val_loss: 0.5772\n",
      "Epoch 13/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.5459 - val_loss: 0.5711\n",
      "Epoch 14/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.5382 - val_loss: 0.5655\n",
      "Epoch 15/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.5309 - val_loss: 0.5599\n",
      "Epoch 16/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.5239 - val_loss: 0.5547\n",
      "Epoch 17/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.5171 - val_loss: 0.5499\n",
      "Epoch 18/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.5108 - val_loss: 0.5453\n",
      "Epoch 19/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.5046 - val_loss: 0.5410\n",
      "Epoch 20/100\n",
      "6090/6090 [==============================] - 1s 116us/sample - loss: 0.4988 - val_loss: 0.5368\n",
      "Epoch 21/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.4931 - val_loss: 0.5329\n",
      "Epoch 22/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.4877 - val_loss: 0.5293\n",
      "Epoch 23/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4825 - val_loss: 0.5258\n",
      "Epoch 24/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.4775 - val_loss: 0.5224\n",
      "Epoch 25/100\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.4727 - val_loss: 0.5193\n",
      "Epoch 26/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.4681 - val_loss: 0.5163\n",
      "Epoch 27/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4636 - val_loss: 0.5135\n",
      "Epoch 28/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.4593 - val_loss: 0.5107\n",
      "Epoch 29/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.4551 - val_loss: 0.5082\n",
      "Epoch 30/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.4511 - val_loss: 0.5058\n",
      "Epoch 31/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.4473 - val_loss: 0.5035\n",
      "Epoch 32/100\n",
      "6090/6090 [==============================] - 1s 125us/sample - loss: 0.4435 - val_loss: 0.5013\n",
      "Epoch 33/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.4399 - val_loss: 0.4992\n",
      "Epoch 34/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.4364 - val_loss: 0.4972\n",
      "Epoch 35/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.4330 - val_loss: 0.4953\n",
      "Epoch 36/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.4297 - val_loss: 0.4936\n",
      "Epoch 37/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.4266 - val_loss: 0.4919\n",
      "Epoch 38/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.4235 - val_loss: 0.4902\n",
      "Epoch 39/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.4205 - val_loss: 0.4888\n",
      "Epoch 40/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.4176 - val_loss: 0.4873\n",
      "Epoch 41/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.4147 - val_loss: 0.4859\n",
      "Epoch 42/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.4120 - val_loss: 0.4846\n",
      "Epoch 43/100\n",
      "6090/6090 [==============================] - 1s 101us/sample - loss: 0.4093 - val_loss: 0.4834\n",
      "Epoch 44/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.4067 - val_loss: 0.4822\n",
      "Epoch 45/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.4041 - val_loss: 0.4810\n",
      "Epoch 46/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.4016 - val_loss: 0.4800\n",
      "Epoch 47/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.3992 - val_loss: 0.4789\n",
      "Epoch 48/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.3969 - val_loss: 0.4780\n",
      "Epoch 49/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.3946 - val_loss: 0.4771\n",
      "Epoch 50/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.3923 - val_loss: 0.4763\n",
      "Epoch 51/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.3902 - val_loss: 0.4755\n",
      "Epoch 52/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.3880 - val_loss: 0.4747\n",
      "Epoch 53/100\n",
      "6090/6090 [==============================] - 1s 101us/sample - loss: 0.3859 - val_loss: 0.4739\n",
      "Epoch 54/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.3839 - val_loss: 0.4732\n",
      "Epoch 55/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.3819 - val_loss: 0.4725\n",
      "Epoch 56/100\n",
      "6090/6090 [==============================] - 1s 117us/sample - loss: 0.3799 - val_loss: 0.4719\n",
      "Epoch 57/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.3780 - val_loss: 0.4714\n",
      "Epoch 58/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.3762 - val_loss: 0.4708\n",
      "Epoch 59/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.3743 - val_loss: 0.4703\n",
      "Epoch 60/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.3725 - val_loss: 0.4698\n",
      "Epoch 61/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.3708 - val_loss: 0.4693\n",
      "Epoch 62/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.3690 - val_loss: 0.4688\n",
      "Epoch 63/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.3674 - val_loss: 0.4685\n",
      "Epoch 64/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.3657 - val_loss: 0.4682\n",
      "Epoch 65/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.3641 - val_loss: 0.4678\n",
      "Epoch 66/100\n",
      "6090/6090 [==============================] - 1s 101us/sample - loss: 0.3625 - val_loss: 0.4675\n",
      "Epoch 67/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.3609 - val_loss: 0.4672\n",
      "Epoch 68/100\n",
      "6090/6090 [==============================] - 1s 102us/sample - loss: 0.3593 - val_loss: 0.4668\n",
      "Epoch 69/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.3578 - val_loss: 0.4666\n",
      "Epoch 70/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.3563 - val_loss: 0.4663\n",
      "Epoch 71/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.3549 - val_loss: 0.4660\n",
      "Epoch 72/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.3534 - val_loss: 0.4658\n",
      "Epoch 73/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.3520 - val_loss: 0.4658\n",
      "Epoch 74/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.3506 - val_loss: 0.4655\n",
      "Epoch 75/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.3492 - val_loss: 0.4654\n",
      "Epoch 76/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.3479 - val_loss: 0.4652\n",
      "Epoch 77/100\n",
      "6090/6090 [==============================] - 1s 104us/sample - loss: 0.3466 - val_loss: 0.4652\n",
      "Epoch 78/100\n",
      "6090/6090 [==============================] - 1s 101us/sample - loss: 0.3453 - val_loss: 0.4651\n",
      "Epoch 79/100\n",
      "6090/6090 [==============================] - 1s 103us/sample - loss: 0.3440 - val_loss: 0.4651\n",
      "Epoch 80/100\n",
      "6090/6090 [==============================] - 1s 122us/sample - loss: 0.3427 - val_loss: 0.4650\n",
      "Epoch 81/100\n",
      "6090/6090 [==============================] - 1s 100us/sample - loss: 0.3415 - val_loss: 0.4649\n",
      "Epoch 82/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.3402 - val_loss: 0.4649\n",
      "Epoch 83/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.3390 - val_loss: 0.4649\n",
      "Epoch 84/100\n",
      "6090/6090 [==============================] - 1s 116us/sample - loss: 0.3378 - val_loss: 0.4649\n",
      "Epoch 85/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.3366 - val_loss: 0.4649\n",
      "Predicting...\n",
      "(Took 55.818 sec)\n",
      "Train on 6091 samples, validate on 1522 samples\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6091/6091 [==============================] - 1s 166us/sample - loss: 0.6832 - val_loss: 0.6741\n",
      "Epoch 2/100\n",
      "6091/6091 [==============================] - 1s 117us/sample - loss: 0.6648 - val_loss: 0.6600\n",
      "Epoch 3/100\n",
      "6091/6091 [==============================] - 1s 106us/sample - loss: 0.6497 - val_loss: 0.6480\n",
      "Epoch 4/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.6363 - val_loss: 0.6373\n",
      "Epoch 5/100\n",
      "6091/6091 [==============================] - 1s 101us/sample - loss: 0.6239 - val_loss: 0.6273\n",
      "Epoch 6/100\n",
      "6091/6091 [==============================] - 1s 101us/sample - loss: 0.6123 - val_loss: 0.6181\n",
      "Epoch 7/100\n",
      "6091/6091 [==============================] - 1s 104us/sample - loss: 0.6013 - val_loss: 0.6094\n",
      "Epoch 8/100\n",
      "6091/6091 [==============================] - 1s 103us/sample - loss: 0.5910 - val_loss: 0.6012\n",
      "Epoch 9/100\n",
      "6091/6091 [==============================] - 1s 101us/sample - loss: 0.5811 - val_loss: 0.5935\n",
      "Epoch 10/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.5718 - val_loss: 0.5862\n",
      "Epoch 11/100\n",
      "6091/6091 [==============================] - 1s 101us/sample - loss: 0.5629 - val_loss: 0.5793\n",
      "Epoch 12/100\n",
      "6091/6091 [==============================] - 1s 100us/sample - loss: 0.5545 - val_loss: 0.5728\n",
      "Epoch 13/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.5464 - val_loss: 0.5666\n",
      "Epoch 14/100\n",
      "6091/6091 [==============================] - 1s 114us/sample - loss: 0.5388 - val_loss: 0.5608\n",
      "Epoch 15/100\n",
      "6091/6091 [==============================] - 1s 106us/sample - loss: 0.5315 - val_loss: 0.5553\n",
      "Epoch 16/100\n",
      "6091/6091 [==============================] - 1s 101us/sample - loss: 0.5245 - val_loss: 0.5501\n",
      "Epoch 17/100\n",
      "6091/6091 [==============================] - 1s 100us/sample - loss: 0.5179 - val_loss: 0.5451\n",
      "Epoch 18/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.5115 - val_loss: 0.5404\n",
      "Epoch 19/100\n",
      "6091/6091 [==============================] - 1s 103us/sample - loss: 0.5054 - val_loss: 0.5360\n",
      "Epoch 20/100\n",
      "6091/6091 [==============================] - 1s 124us/sample - loss: 0.4997 - val_loss: 0.5318\n",
      "Epoch 21/100\n",
      "6091/6091 [==============================] - 1s 105us/sample - loss: 0.4941 - val_loss: 0.5277\n",
      "Epoch 22/100\n",
      "6091/6091 [==============================] - 1s 114us/sample - loss: 0.4887 - val_loss: 0.5239\n",
      "Epoch 23/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.4836 - val_loss: 0.5203\n",
      "Epoch 24/100\n",
      "6091/6091 [==============================] - 1s 118us/sample - loss: 0.4786 - val_loss: 0.5169\n",
      "Epoch 25/100\n",
      "6091/6091 [==============================] - 1s 110us/sample - loss: 0.4739 - val_loss: 0.5136\n",
      "Epoch 26/100\n",
      "6091/6091 [==============================] - 1s 117us/sample - loss: 0.4693 - val_loss: 0.5104\n",
      "Epoch 27/100\n",
      "6091/6091 [==============================] - 1s 120us/sample - loss: 0.4649 - val_loss: 0.5075\n",
      "Epoch 28/100\n",
      "6091/6091 [==============================] - 1s 134us/sample - loss: 0.4607 - val_loss: 0.5047\n",
      "Epoch 29/100\n",
      "6091/6091 [==============================] - 1s 129us/sample - loss: 0.4566 - val_loss: 0.5019\n",
      "Epoch 30/100\n",
      "6091/6091 [==============================] - 1s 115us/sample - loss: 0.4527 - val_loss: 0.4993\n",
      "Epoch 31/100\n",
      "6091/6091 [==============================] - 1s 115us/sample - loss: 0.4489 - val_loss: 0.4969\n",
      "Epoch 32/100\n",
      "6091/6091 [==============================] - 1s 121us/sample - loss: 0.4452 - val_loss: 0.4946\n",
      "Epoch 33/100\n",
      "6091/6091 [==============================] - 1s 119us/sample - loss: 0.4416 - val_loss: 0.4923\n",
      "Epoch 34/100\n",
      "6091/6091 [==============================] - 1s 102us/sample - loss: 0.4382 - val_loss: 0.4902\n",
      "Epoch 35/100\n",
      "6091/6091 [==============================] - 1s 102us/sample - loss: 0.4349 - val_loss: 0.4882\n",
      "Epoch 36/100\n",
      "6091/6091 [==============================] - 1s 127us/sample - loss: 0.4317 - val_loss: 0.4863\n",
      "Epoch 37/100\n",
      "6091/6091 [==============================] - 1s 124us/sample - loss: 0.4285 - val_loss: 0.4844\n",
      "Epoch 38/100\n",
      "6091/6091 [==============================] - 1s 103us/sample - loss: 0.4255 - val_loss: 0.4826\n",
      "Epoch 39/100\n",
      "6091/6091 [==============================] - 1s 122us/sample - loss: 0.4225 - val_loss: 0.4809\n",
      "Epoch 40/100\n",
      "6091/6091 [==============================] - 1s 123us/sample - loss: 0.4197 - val_loss: 0.4792\n",
      "Epoch 41/100\n",
      "6091/6091 [==============================] - 1s 104us/sample - loss: 0.4169 - val_loss: 0.4777\n",
      "Epoch 42/100\n",
      "6091/6091 [==============================] - 1s 119us/sample - loss: 0.4142 - val_loss: 0.4762\n",
      "Epoch 43/100\n",
      "6091/6091 [==============================] - 1s 106us/sample - loss: 0.4116 - val_loss: 0.4747\n",
      "Epoch 44/100\n",
      "6091/6091 [==============================] - 1s 120us/sample - loss: 0.4090 - val_loss: 0.4734\n",
      "Epoch 45/100\n",
      "6091/6091 [==============================] - 1s 104us/sample - loss: 0.4065 - val_loss: 0.4721\n",
      "Epoch 46/100\n",
      "6091/6091 [==============================] - 1s 114us/sample - loss: 0.4041 - val_loss: 0.4708\n",
      "Epoch 47/100\n",
      "6091/6091 [==============================] - 1s 117us/sample - loss: 0.4018 - val_loss: 0.4696\n",
      "Epoch 48/100\n",
      "6091/6091 [==============================] - 1s 113us/sample - loss: 0.3995 - val_loss: 0.4685\n",
      "Epoch 49/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.3972 - val_loss: 0.4674\n",
      "Epoch 50/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.3950 - val_loss: 0.4664\n",
      "Epoch 51/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.3929 - val_loss: 0.4653\n",
      "Epoch 52/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.3908 - val_loss: 0.4644\n",
      "Epoch 53/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.3887 - val_loss: 0.4635\n",
      "Epoch 54/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.3867 - val_loss: 0.4627\n",
      "Epoch 55/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.3848 - val_loss: 0.4618\n",
      "Epoch 56/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.3828 - val_loss: 0.4610\n",
      "Epoch 57/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.3810 - val_loss: 0.4603\n",
      "Epoch 58/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.3791 - val_loss: 0.4596\n",
      "Epoch 59/100\n",
      "6091/6091 [==============================] - 1s 110us/sample - loss: 0.3773 - val_loss: 0.4588\n",
      "Epoch 60/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.3756 - val_loss: 0.4582\n",
      "Epoch 61/100\n",
      "6091/6091 [==============================] - 1s 119us/sample - loss: 0.3739 - val_loss: 0.4575\n",
      "Epoch 62/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.3722 - val_loss: 0.4569\n",
      "Epoch 63/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.3705 - val_loss: 0.4563\n",
      "Epoch 64/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.3689 - val_loss: 0.4558\n",
      "Epoch 65/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.3673 - val_loss: 0.4552\n",
      "Epoch 66/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.3657 - val_loss: 0.4547\n",
      "Epoch 67/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.3642 - val_loss: 0.4543\n",
      "Epoch 68/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.3627 - val_loss: 0.4538\n",
      "Epoch 69/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.3612 - val_loss: 0.4535\n",
      "Epoch 70/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.3597 - val_loss: 0.4529\n",
      "Epoch 71/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.3583 - val_loss: 0.4526\n",
      "Epoch 72/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.3569 - val_loss: 0.4522\n",
      "Epoch 73/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.3555 - val_loss: 0.4519\n",
      "Epoch 74/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.3542 - val_loss: 0.4515\n",
      "Epoch 75/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.3528 - val_loss: 0.4512\n",
      "Epoch 76/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.3515 - val_loss: 0.4509\n",
      "Epoch 77/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6091/6091 [==============================] - 1s 102us/sample - loss: 0.3502 - val_loss: 0.4506\n",
      "Epoch 78/100\n",
      "6091/6091 [==============================] - 1s 102us/sample - loss: 0.3489 - val_loss: 0.4504\n",
      "Epoch 79/100\n",
      "6091/6091 [==============================] - 1s 99us/sample - loss: 0.3477 - val_loss: 0.4502\n",
      "Epoch 80/100\n",
      "6091/6091 [==============================] - 1s 103us/sample - loss: 0.3464 - val_loss: 0.4500\n",
      "Epoch 81/100\n",
      "6091/6091 [==============================] - 1s 101us/sample - loss: 0.3452 - val_loss: 0.4498\n",
      "Epoch 82/100\n",
      "6091/6091 [==============================] - 1s 106us/sample - loss: 0.3440 - val_loss: 0.4496\n",
      "Epoch 83/100\n",
      "6091/6091 [==============================] - 1s 100us/sample - loss: 0.3428 - val_loss: 0.4495\n",
      "Epoch 84/100\n",
      "6091/6091 [==============================] - 1s 102us/sample - loss: 0.3416 - val_loss: 0.4493\n",
      "Epoch 85/100\n",
      "6091/6091 [==============================] - 1s 101us/sample - loss: 0.3405 - val_loss: 0.4491\n",
      "Epoch 86/100\n",
      "6091/6091 [==============================] - 1s 111us/sample - loss: 0.3394 - val_loss: 0.4490\n",
      "Epoch 87/100\n",
      "6091/6091 [==============================] - 1s 101us/sample - loss: 0.3382 - val_loss: 0.4489\n",
      "Epoch 88/100\n",
      "6091/6091 [==============================] - 1s 101us/sample - loss: 0.3371 - val_loss: 0.4488\n",
      "Epoch 89/100\n",
      "6091/6091 [==============================] - 1s 100us/sample - loss: 0.3361 - val_loss: 0.4488\n",
      "Epoch 90/100\n",
      "6091/6091 [==============================] - 1s 100us/sample - loss: 0.3350 - val_loss: 0.4487\n",
      "Epoch 91/100\n",
      "6091/6091 [==============================] - 1s 103us/sample - loss: 0.3339 - val_loss: 0.4486\n",
      "Epoch 92/100\n",
      "6091/6091 [==============================] - 1s 101us/sample - loss: 0.3329 - val_loss: 0.4485\n",
      "Epoch 93/100\n",
      "6091/6091 [==============================] - 1s 100us/sample - loss: 0.3318 - val_loss: 0.4484\n",
      "Epoch 94/100\n",
      "6091/6091 [==============================] - 1s 110us/sample - loss: 0.3309 - val_loss: 0.4485\n",
      "Predicting...\n",
      "(Took 64.642 sec)\n",
      "Train on 6091 samples, validate on 1522 samples\n",
      "Epoch 1/100\n",
      "6091/6091 [==============================] - 1s 178us/sample - loss: 0.6826 - val_loss: 0.6755\n",
      "Epoch 2/100\n",
      "6091/6091 [==============================] - 1s 114us/sample - loss: 0.6636 - val_loss: 0.6623\n",
      "Epoch 3/100\n",
      "6091/6091 [==============================] - 1s 117us/sample - loss: 0.6484 - val_loss: 0.6510\n",
      "Epoch 4/100\n",
      "6091/6091 [==============================] - 1s 115us/sample - loss: 0.6350 - val_loss: 0.6407\n",
      "Epoch 5/100\n",
      "6091/6091 [==============================] - 1s 110us/sample - loss: 0.6227 - val_loss: 0.6310\n",
      "Epoch 6/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.6110 - val_loss: 0.6217\n",
      "Epoch 7/100\n",
      "6091/6091 [==============================] - 1s 102us/sample - loss: 0.6000 - val_loss: 0.6130\n",
      "Epoch 8/100\n",
      "6091/6091 [==============================] - 1s 102us/sample - loss: 0.5896 - val_loss: 0.6047\n",
      "Epoch 9/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.5797 - val_loss: 0.5970\n",
      "Epoch 10/100\n",
      "6091/6091 [==============================] - 1s 126us/sample - loss: 0.5703 - val_loss: 0.5896\n",
      "Epoch 11/100\n",
      "6091/6091 [==============================] - 1s 104us/sample - loss: 0.5614 - val_loss: 0.5828\n",
      "Epoch 12/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.5530 - val_loss: 0.5762\n",
      "Epoch 13/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.5449 - val_loss: 0.5701\n",
      "Epoch 14/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.5372 - val_loss: 0.5644\n",
      "Epoch 15/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.5298 - val_loss: 0.5589\n",
      "Epoch 16/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.5228 - val_loss: 0.5536\n",
      "Epoch 17/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.5161 - val_loss: 0.5487\n",
      "Epoch 18/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.5097 - val_loss: 0.5441\n",
      "Epoch 19/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.5036 - val_loss: 0.5398\n",
      "Epoch 20/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4978 - val_loss: 0.5356\n",
      "Epoch 21/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4922 - val_loss: 0.5317\n",
      "Epoch 22/100\n",
      "6091/6091 [==============================] - 1s 107us/sample - loss: 0.4868 - val_loss: 0.5280\n",
      "Epoch 23/100\n",
      "6091/6091 [==============================] - ETA: 0s - loss: 0.482 - 1s 116us/sample - loss: 0.4816 - val_loss: 0.5244\n",
      "Epoch 24/100\n",
      "6091/6091 [==============================] - 1s 117us/sample - loss: 0.4766 - val_loss: 0.5211\n",
      "Epoch 25/100\n",
      "6091/6091 [==============================] - 1s 116us/sample - loss: 0.4718 - val_loss: 0.5180\n",
      "Epoch 26/100\n",
      "6091/6091 [==============================] - 1s 114us/sample - loss: 0.4672 - val_loss: 0.5151\n",
      "Epoch 27/100\n",
      "6091/6091 [==============================] - 1s 115us/sample - loss: 0.4628 - val_loss: 0.5122\n",
      "Epoch 28/100\n",
      "6091/6091 [==============================] - 1s 116us/sample - loss: 0.4585 - val_loss: 0.5096\n",
      "Epoch 29/100\n",
      "6091/6091 [==============================] - 1s 113us/sample - loss: 0.4544 - val_loss: 0.5070\n",
      "Epoch 30/100\n",
      "6091/6091 [==============================] - 1s 113us/sample - loss: 0.4504 - val_loss: 0.5046\n",
      "Epoch 31/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.4466 - val_loss: 0.5023\n",
      "Epoch 32/100\n",
      "6091/6091 [==============================] - 1s 114us/sample - loss: 0.4429 - val_loss: 0.5001\n",
      "Epoch 33/100\n",
      "6091/6091 [==============================] - 1s 115us/sample - loss: 0.4393 - val_loss: 0.4980\n",
      "Epoch 34/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.4358 - val_loss: 0.4961\n",
      "Epoch 35/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.4324 - val_loss: 0.4943\n",
      "Epoch 36/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4291 - val_loss: 0.4925\n",
      "Epoch 37/100\n",
      "6091/6091 [==============================] - 1s 111us/sample - loss: 0.4260 - val_loss: 0.4908\n",
      "Epoch 38/100\n",
      "6091/6091 [==============================] - 1s 108us/sample - loss: 0.4229 - val_loss: 0.4893\n",
      "Epoch 39/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.4199 - val_loss: 0.4878\n",
      "Epoch 40/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.4170 - val_loss: 0.4864\n",
      "Epoch 41/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.4142 - val_loss: 0.4850\n",
      "Epoch 42/100\n",
      "6091/6091 [==============================] - 1s 110us/sample - loss: 0.4115 - val_loss: 0.4837\n",
      "Epoch 43/100\n",
      "6091/6091 [==============================] - 1s 111us/sample - loss: 0.4088 - val_loss: 0.4825\n",
      "Epoch 44/100\n",
      "6091/6091 [==============================] - 1s 110us/sample - loss: 0.4062 - val_loss: 0.4814\n",
      "Epoch 45/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.4037 - val_loss: 0.4803\n",
      "Epoch 46/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.4012 - val_loss: 0.4793\n",
      "Epoch 47/100\n",
      "6091/6091 [==============================] - 1s 113us/sample - loss: 0.3988 - val_loss: 0.4783\n",
      "Epoch 48/100\n",
      "6091/6091 [==============================] - 1s 111us/sample - loss: 0.3965 - val_loss: 0.4774\n",
      "Epoch 49/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.3942 - val_loss: 0.4765\n",
      "Epoch 50/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.3920 - val_loss: 0.4757\n",
      "Epoch 51/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.3898 - val_loss: 0.4750\n",
      "Epoch 52/100\n",
      "6091/6091 [==============================] - 1s 111us/sample - loss: 0.3877 - val_loss: 0.4743\n",
      "Epoch 53/100\n",
      "6091/6091 [==============================] - 1s 110us/sample - loss: 0.3856 - val_loss: 0.4736\n",
      "Epoch 54/100\n",
      "6091/6091 [==============================] - 1s 127us/sample - loss: 0.3836 - val_loss: 0.4730\n",
      "Epoch 55/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.3816 - val_loss: 0.4724\n",
      "Epoch 56/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.3797 - val_loss: 0.4718\n",
      "Epoch 57/100\n",
      "6091/6091 [==============================] - 1s 109us/sample - loss: 0.3778 - val_loss: 0.4713\n",
      "Epoch 58/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6091/6091 [==============================] - 1s 104us/sample - loss: 0.3759 - val_loss: 0.4708\n",
      "Epoch 59/100\n",
      "6091/6091 [==============================] - 1s 103us/sample - loss: 0.3741 - val_loss: 0.4704\n",
      "Epoch 60/100\n",
      "6091/6091 [==============================] - 1s 101us/sample - loss: 0.3723 - val_loss: 0.4700\n",
      "Epoch 61/100\n",
      "6091/6091 [==============================] - 1s 103us/sample - loss: 0.3706 - val_loss: 0.4696\n",
      "Epoch 62/100\n",
      "6091/6091 [==============================] - 1s 112us/sample - loss: 0.3689 - val_loss: 0.4692\n",
      "Epoch 63/100\n",
      "6091/6091 [==============================] - 1s 100us/sample - loss: 0.3672 - val_loss: 0.4689\n",
      "Epoch 64/100\n",
      "6091/6091 [==============================] - 1s 104us/sample - loss: 0.3655 - val_loss: 0.4686\n",
      "Epoch 65/100\n",
      "6091/6091 [==============================] - 1s 104us/sample - loss: 0.3639 - val_loss: 0.4683\n",
      "Epoch 66/100\n",
      "6091/6091 [==============================] - 1s 101us/sample - loss: 0.3623 - val_loss: 0.4680\n",
      "Epoch 67/100\n",
      "6091/6091 [==============================] - 1s 103us/sample - loss: 0.3608 - val_loss: 0.4678\n",
      "Epoch 68/100\n",
      "6091/6091 [==============================] - 1s 102us/sample - loss: 0.3592 - val_loss: 0.4676\n",
      "Epoch 69/100\n",
      "6091/6091 [==============================] - 1s 100us/sample - loss: 0.3577 - val_loss: 0.4673\n",
      "Epoch 70/100\n",
      "6091/6091 [==============================] - 1s 118us/sample - loss: 0.3563 - val_loss: 0.4672\n",
      "Epoch 71/100\n",
      "6091/6091 [==============================] - 1s 128us/sample - loss: 0.3548 - val_loss: 0.4671\n",
      "Epoch 72/100\n",
      "6091/6091 [==============================] - 1s 124us/sample - loss: 0.3534 - val_loss: 0.4669\n",
      "Epoch 73/100\n",
      "6091/6091 [==============================] - 1s 126us/sample - loss: 0.3520 - val_loss: 0.4668\n",
      "Epoch 74/100\n",
      "6091/6091 [==============================] - 1s 126us/sample - loss: 0.3506 - val_loss: 0.4667\n",
      "Epoch 75/100\n",
      "6091/6091 [==============================] - 1s 122us/sample - loss: 0.3493 - val_loss: 0.4667\n",
      "Epoch 76/100\n",
      "6091/6091 [==============================] - 1s 111us/sample - loss: 0.3479 - val_loss: 0.4666\n",
      "Epoch 77/100\n",
      "6091/6091 [==============================] - 1s 99us/sample - loss: 0.3466 - val_loss: 0.4666\n",
      "Epoch 78/100\n",
      "6091/6091 [==============================] - 1s 99us/sample - loss: 0.3453 - val_loss: 0.4666\n",
      "Epoch 79/100\n",
      "6091/6091 [==============================] - 1s 102us/sample - loss: 0.3441 - val_loss: 0.4665\n",
      "Epoch 80/100\n",
      "6091/6091 [==============================] - 1s 111us/sample - loss: 0.3428 - val_loss: 0.4666\n",
      "Predicting...\n",
      "(Took 54.931 sec)\n",
      "Combined confusion matrix:\n",
      "[[3837.  505.]\n",
      " [1054. 2217.]]\n",
      "(Overall, took 284.850 sec)\n",
      "Accuracy: 79.52% +/- 0.93%\n",
      "Precision for positive class: 78.46% +/- 2.01%\n",
      "Precision for negative class: 81.46% +/- 1.69%\n",
      "Recall for positive class: 88.37% +/- 1.24%\n",
      "Recall for negative class: 67.80% +/- 2.51%\n",
      "F for positive class: 83.11% +/- 0.97%\n",
      "F for negative class: 73.98% +/- 1.14%\n",
      "Mean F score: 78.54% +/- 0.90%\n"
     ]
    }
   ],
   "source": [
    "# Bigrams, transformed\n",
    "# Normalized\n",
    "all_metrics(cross_validate_logistic(dftrain_bi_min_norm, 100))\n",
    "# TF-IDF\n",
    "all_metrics(cross_validate_logistic(dftrain_bi_min_tfidf, 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other logistic regression models were tried, without significant improvement.\n",
    "Again, raising the document frequency threshold decreased scores.\n",
    "Regularization (both L1 and L2 were tried) did not seem to improve scores at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No prep, raw counts\n",
      "Training...\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/100\n",
      "6090/6090 [==============================] - 2s 263us/sample - loss: 0.6479 - val_loss: 0.6052\n",
      "Epoch 2/100\n",
      "6090/6090 [==============================] - 1s 135us/sample - loss: 0.5776 - val_loss: 0.5608\n",
      "Epoch 3/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.5337 - val_loss: 0.5324\n",
      "Epoch 4/100\n",
      "6090/6090 [==============================] - 1s 132us/sample - loss: 0.5017 - val_loss: 0.5132\n",
      "Epoch 5/100\n",
      "6090/6090 [==============================] - 1s 148us/sample - loss: 0.4769 - val_loss: 0.4984\n",
      "Epoch 6/100\n",
      "6090/6090 [==============================] - 1s 162us/sample - loss: 0.4563 - val_loss: 0.4871\n",
      "Epoch 7/100\n",
      "6090/6090 [==============================] - 1s 142us/sample - loss: 0.4387 - val_loss: 0.4785\n",
      "Epoch 8/100\n",
      "6090/6090 [==============================] - 1s 136us/sample - loss: 0.4235 - val_loss: 0.4714\n",
      "Epoch 9/100\n",
      "6090/6090 [==============================] - 1s 136us/sample - loss: 0.4099 - val_loss: 0.4655\n",
      "Epoch 10/100\n",
      "6090/6090 [==============================] - 1s 165us/sample - loss: 0.3978 - val_loss: 0.4608\n",
      "Epoch 11/100\n",
      "6090/6090 [==============================] - 1s 132us/sample - loss: 0.3868 - val_loss: 0.4571\n",
      "Epoch 12/100\n",
      "6090/6090 [==============================] - 1s 168us/sample - loss: 0.3769 - val_loss: 0.4538\n",
      "Epoch 13/100\n",
      "6090/6090 [==============================] - 1s 130us/sample - loss: 0.3676 - val_loss: 0.4509\n",
      "Epoch 14/100\n",
      "6090/6090 [==============================] - 1s 132us/sample - loss: 0.3591 - val_loss: 0.4485\n",
      "Epoch 15/100\n",
      "6090/6090 [==============================] - 1s 130us/sample - loss: 0.3513 - val_loss: 0.4467\n",
      "Epoch 16/100\n",
      "6090/6090 [==============================] - 1s 147us/sample - loss: 0.3438 - val_loss: 0.4452\n",
      "Epoch 17/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.3367 - val_loss: 0.4438\n",
      "Epoch 18/100\n",
      "6090/6090 [==============================] - 1s 128us/sample - loss: 0.3303 - val_loss: 0.4431\n",
      "Epoch 19/100\n",
      "6090/6090 [==============================] - 1s 129us/sample - loss: 0.3242 - val_loss: 0.4421\n",
      "Epoch 20/100\n",
      "6090/6090 [==============================] - 1s 132us/sample - loss: 0.3181 - val_loss: 0.4415\n",
      "Epoch 21/100\n",
      "6090/6090 [==============================] - 1s 135us/sample - loss: 0.3126 - val_loss: 0.4412\n",
      "Epoch 22/100\n",
      "6090/6090 [==============================] - 1s 181us/sample - loss: 0.3071 - val_loss: 0.4410\n",
      "Epoch 23/100\n",
      "6090/6090 [==============================] - 1s 134us/sample - loss: 0.3021 - val_loss: 0.4409\n",
      "Epoch 24/100\n",
      "6090/6090 [==============================] - 1s 140us/sample - loss: 0.2972 - val_loss: 0.4410\n",
      "Predicting...\n",
      "[[787  95]\n",
      " [197 444]]\n",
      "Accuracy: 80.83%\n",
      "Precision: 79.98%\n",
      "Recall: 89.23%\n",
      "F: 84.35\n",
      "(Took 23.070 sec)\n",
      "No prep, normalized\n",
      "Training...\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/100\n",
      "6090/6090 [==============================] - 1s 233us/sample - loss: 0.6859 - val_loss: 0.6782\n",
      "Epoch 2/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.6739 - val_loss: 0.6681\n",
      "Epoch 3/100\n",
      "6090/6090 [==============================] - 1s 135us/sample - loss: 0.6647 - val_loss: 0.6602\n",
      "Epoch 4/100\n",
      "6090/6090 [==============================] - 1s 142us/sample - loss: 0.6568 - val_loss: 0.6532\n",
      "Epoch 5/100\n",
      "6090/6090 [==============================] - 1s 142us/sample - loss: 0.6495 - val_loss: 0.6469\n",
      "Epoch 6/100\n",
      "6090/6090 [==============================] - 1s 144us/sample - loss: 0.6425 - val_loss: 0.6408\n",
      "Epoch 7/100\n",
      "6090/6090 [==============================] - 1s 148us/sample - loss: 0.6358 - val_loss: 0.6350\n",
      "Epoch 8/100\n",
      "6090/6090 [==============================] - 1s 140us/sample - loss: 0.6292 - val_loss: 0.6295\n",
      "Epoch 9/100\n",
      "6090/6090 [==============================] - 1s 142us/sample - loss: 0.6230 - val_loss: 0.6241\n",
      "Epoch 10/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.6169 - val_loss: 0.6190\n",
      "Epoch 11/100\n",
      "6090/6090 [==============================] - 1s 140us/sample - loss: 0.6110 - val_loss: 0.6141\n",
      "Epoch 12/100\n",
      "6090/6090 [==============================] - 1s 141us/sample - loss: 0.6053 - val_loss: 0.6093\n",
      "Epoch 13/100\n",
      "6090/6090 [==============================] - 1s 146us/sample - loss: 0.5998 - val_loss: 0.6047\n",
      "Epoch 14/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.5945 - val_loss: 0.6003\n",
      "Epoch 15/100\n",
      "6090/6090 [==============================] - 1s 153us/sample - loss: 0.5893 - val_loss: 0.5961\n",
      "Epoch 16/100\n",
      "6090/6090 [==============================] - 1s 143us/sample - loss: 0.5843 - val_loss: 0.5919\n",
      "Epoch 17/100\n",
      "6090/6090 [==============================] - 1s 147us/sample - loss: 0.5794 - val_loss: 0.5879\n",
      "Epoch 18/100\n",
      "6090/6090 [==============================] - 1s 149us/sample - loss: 0.5747 - val_loss: 0.5840\n",
      "Epoch 19/100\n",
      "6090/6090 [==============================] - 1s 143us/sample - loss: 0.5701 - val_loss: 0.5802\n",
      "Epoch 20/100\n",
      "6090/6090 [==============================] - 1s 141us/sample - loss: 0.5656 - val_loss: 0.5767\n",
      "Epoch 21/100\n",
      "6090/6090 [==============================] - 1s 150us/sample - loss: 0.5613 - val_loss: 0.5731\n",
      "Epoch 22/100\n",
      "6090/6090 [==============================] - 1s 159us/sample - loss: 0.5571 - val_loss: 0.5698\n",
      "Epoch 23/100\n",
      "6090/6090 [==============================] - 1s 142us/sample - loss: 0.5530 - val_loss: 0.5666\n",
      "Epoch 24/100\n",
      "6090/6090 [==============================] - 1s 141us/sample - loss: 0.5490 - val_loss: 0.5634\n",
      "Epoch 25/100\n",
      "6090/6090 [==============================] - 1s 144us/sample - loss: 0.5451 - val_loss: 0.5604\n",
      "Epoch 26/100\n",
      "6090/6090 [==============================] - 1s 152us/sample - loss: 0.5413 - val_loss: 0.5574\n",
      "Epoch 27/100\n",
      "6090/6090 [==============================] - 1s 157us/sample - loss: 0.5376 - val_loss: 0.5545\n",
      "Epoch 28/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.5340 - val_loss: 0.5518\n",
      "Epoch 29/100\n",
      "6090/6090 [==============================] - 1s 142us/sample - loss: 0.5306 - val_loss: 0.5491\n",
      "Epoch 30/100\n",
      "6090/6090 [==============================] - 1s 141us/sample - loss: 0.5271 - val_loss: 0.5465\n",
      "Epoch 31/100\n",
      "6090/6090 [==============================] - 1s 141us/sample - loss: 0.5238 - val_loss: 0.5439\n",
      "Epoch 32/100\n",
      "6090/6090 [==============================] - 1s 142us/sample - loss: 0.5206 - val_loss: 0.5415\n",
      "Epoch 33/100\n",
      "6090/6090 [==============================] - 1s 143us/sample - loss: 0.5174 - val_loss: 0.5391\n",
      "Epoch 34/100\n",
      "6090/6090 [==============================] - 1s 142us/sample - loss: 0.5143 - val_loss: 0.5368\n",
      "Epoch 35/100\n",
      "6090/6090 [==============================] - 1s 140us/sample - loss: 0.5113 - val_loss: 0.5346\n",
      "Epoch 36/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.5084 - val_loss: 0.5325\n",
      "Epoch 37/100\n",
      "6090/6090 [==============================] - 1s 154us/sample - loss: 0.5055 - val_loss: 0.5303\n",
      "Epoch 38/100\n",
      "6090/6090 [==============================] - 1s 150us/sample - loss: 0.5027 - val_loss: 0.5283\n",
      "Epoch 39/100\n",
      "6090/6090 [==============================] - 1s 155us/sample - loss: 0.5000 - val_loss: 0.5263\n",
      "Epoch 40/100\n",
      "6090/6090 [==============================] - 1s 187us/sample - loss: 0.4973 - val_loss: 0.5244\n",
      "Epoch 41/100\n",
      "6090/6090 [==============================] - 1s 148us/sample - loss: 0.4946 - val_loss: 0.5225\n",
      "Epoch 42/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.4920 - val_loss: 0.5208\n",
      "Epoch 43/100\n",
      "6090/6090 [==============================] - 1s 154us/sample - loss: 0.4895 - val_loss: 0.5190\n",
      "Epoch 44/100\n",
      "6090/6090 [==============================] - 1s 171us/sample - loss: 0.4871 - val_loss: 0.5173\n",
      "Epoch 45/100\n",
      "6090/6090 [==============================] - 1s 162us/sample - loss: 0.4846 - val_loss: 0.5156\n",
      "Epoch 46/100\n",
      "6090/6090 [==============================] - 1s 165us/sample - loss: 0.4822 - val_loss: 0.5140\n",
      "Epoch 47/100\n",
      "6090/6090 [==============================] - 1s 166us/sample - loss: 0.4799 - val_loss: 0.5124\n",
      "Epoch 48/100\n",
      "6090/6090 [==============================] - 1s 148us/sample - loss: 0.4776 - val_loss: 0.5109\n",
      "Epoch 49/100\n",
      "6090/6090 [==============================] - 1s 151us/sample - loss: 0.4754 - val_loss: 0.5094\n",
      "Epoch 50/100\n",
      "6090/6090 [==============================] - 1s 192us/sample - loss: 0.4732 - val_loss: 0.5079\n",
      "Epoch 51/100\n",
      "6090/6090 [==============================] - 1s 210us/sample - loss: 0.4710 - val_loss: 0.5065\n",
      "Epoch 52/100\n",
      "6090/6090 [==============================] - 1s 149us/sample - loss: 0.4689 - val_loss: 0.5052\n",
      "Epoch 53/100\n",
      "6090/6090 [==============================] - 1s 147us/sample - loss: 0.4668 - val_loss: 0.5038\n",
      "Epoch 54/100\n",
      "6090/6090 [==============================] - 1s 153us/sample - loss: 0.4647 - val_loss: 0.5025\n",
      "Epoch 55/100\n",
      "6090/6090 [==============================] - 1s 148us/sample - loss: 0.4628 - val_loss: 0.5012\n",
      "Epoch 56/100\n",
      "6090/6090 [==============================] - 1s 149us/sample - loss: 0.4608 - val_loss: 0.5000\n",
      "Epoch 57/100\n",
      "6090/6090 [==============================] - 1s 184us/sample - loss: 0.4588 - val_loss: 0.4988\n",
      "Epoch 58/100\n",
      "6090/6090 [==============================] - 1s 147us/sample - loss: 0.4569 - val_loss: 0.4976\n",
      "Epoch 59/100\n",
      "6090/6090 [==============================] - 1s 146us/sample - loss: 0.4550 - val_loss: 0.4964\n",
      "Epoch 60/100\n",
      "6090/6090 [==============================] - 1s 143us/sample - loss: 0.4531 - val_loss: 0.4953\n",
      "Epoch 61/100\n",
      "6090/6090 [==============================] - 1s 148us/sample - loss: 0.4513 - val_loss: 0.4942\n",
      "Epoch 62/100\n",
      "6090/6090 [==============================] - 1s 179us/sample - loss: 0.4495 - val_loss: 0.4932\n",
      "Epoch 63/100\n",
      "6090/6090 [==============================] - 1s 159us/sample - loss: 0.4477 - val_loss: 0.4921\n",
      "Epoch 64/100\n",
      "6090/6090 [==============================] - 1s 145us/sample - loss: 0.4460 - val_loss: 0.4911\n",
      "Epoch 65/100\n",
      "6090/6090 [==============================] - 1s 142us/sample - loss: 0.4443 - val_loss: 0.4901\n",
      "Epoch 66/100\n",
      "6090/6090 [==============================] - 1s 154us/sample - loss: 0.4426 - val_loss: 0.4892\n",
      "Epoch 67/100\n",
      "6090/6090 [==============================] - 1s 147us/sample - loss: 0.4409 - val_loss: 0.4882\n",
      "Epoch 68/100\n",
      "6090/6090 [==============================] - 1s 147us/sample - loss: 0.4393 - val_loss: 0.4873\n",
      "Epoch 69/100\n",
      "6090/6090 [==============================] - 1s 146us/sample - loss: 0.4376 - val_loss: 0.4864\n",
      "Epoch 70/100\n",
      "6090/6090 [==============================] - 1s 148us/sample - loss: 0.4360 - val_loss: 0.4855\n",
      "Epoch 71/100\n",
      "6090/6090 [==============================] - 1s 144us/sample - loss: 0.4344 - val_loss: 0.4847\n",
      "Epoch 72/100\n",
      "6090/6090 [==============================] - 1s 142us/sample - loss: 0.4329 - val_loss: 0.4838\n",
      "Epoch 73/100\n",
      "6090/6090 [==============================] - 1s 144us/sample - loss: 0.4314 - val_loss: 0.4830\n",
      "Epoch 74/100\n",
      "6090/6090 [==============================] - 1s 143us/sample - loss: 0.4299 - val_loss: 0.4822\n",
      "Epoch 75/100\n",
      "6090/6090 [==============================] - 1s 140us/sample - loss: 0.4284 - val_loss: 0.4814\n",
      "Epoch 76/100\n",
      "6090/6090 [==============================] - 1s 145us/sample - loss: 0.4269 - val_loss: 0.4807\n",
      "Epoch 77/100\n",
      "6090/6090 [==============================] - 1s 151us/sample - loss: 0.4254 - val_loss: 0.4799\n",
      "Epoch 78/100\n",
      "6090/6090 [==============================] - 1s 150us/sample - loss: 0.4240 - val_loss: 0.4792\n",
      "Epoch 79/100\n",
      "6090/6090 [==============================] - 1s 145us/sample - loss: 0.4226 - val_loss: 0.4785\n",
      "Epoch 80/100\n",
      "6090/6090 [==============================] - 1s 141us/sample - loss: 0.4212 - val_loss: 0.4778\n",
      "Epoch 81/100\n",
      "6090/6090 [==============================] - 1s 145us/sample - loss: 0.4198 - val_loss: 0.4771\n",
      "Epoch 82/100\n",
      "6090/6090 [==============================] - 1s 149us/sample - loss: 0.4184 - val_loss: 0.4765\n",
      "Epoch 83/100\n",
      "6090/6090 [==============================] - 1s 149us/sample - loss: 0.4171 - val_loss: 0.4758\n",
      "Epoch 84/100\n",
      "6090/6090 [==============================] - 1s 142us/sample - loss: 0.4157 - val_loss: 0.4752\n",
      "Epoch 85/100\n",
      "6090/6090 [==============================] - 1s 177us/sample - loss: 0.4144 - val_loss: 0.4745\n",
      "Epoch 86/100\n",
      "6090/6090 [==============================] - 1s 167us/sample - loss: 0.4131 - val_loss: 0.4739\n",
      "Epoch 87/100\n",
      "6090/6090 [==============================] - 1s 167us/sample - loss: 0.4118 - val_loss: 0.4733\n",
      "Epoch 88/100\n",
      "6090/6090 [==============================] - 1s 185us/sample - loss: 0.4105 - val_loss: 0.4727\n",
      "Epoch 89/100\n",
      "6090/6090 [==============================] - 1s 150us/sample - loss: 0.4092 - val_loss: 0.4721\n",
      "Epoch 90/100\n",
      "6090/6090 [==============================] - 1s 143us/sample - loss: 0.4080 - val_loss: 0.4716\n",
      "Epoch 91/100\n",
      "6090/6090 [==============================] - 1s 146us/sample - loss: 0.4068 - val_loss: 0.4710\n",
      "Epoch 92/100\n",
      "6090/6090 [==============================] - 1s 150us/sample - loss: 0.4055 - val_loss: 0.4705\n",
      "Epoch 93/100\n",
      "6090/6090 [==============================] - 1s 151us/sample - loss: 0.4043 - val_loss: 0.4700\n",
      "Epoch 94/100\n",
      "6090/6090 [==============================] - 1s 153us/sample - loss: 0.4031 - val_loss: 0.4695\n",
      "Epoch 95/100\n",
      "6090/6090 [==============================] - 1s 149us/sample - loss: 0.4019 - val_loss: 0.4689\n",
      "Epoch 96/100\n",
      "6090/6090 [==============================] - 1s 155us/sample - loss: 0.4008 - val_loss: 0.4685\n",
      "Epoch 97/100\n",
      "6090/6090 [==============================] - 1s 144us/sample - loss: 0.3996 - val_loss: 0.4680\n",
      "Epoch 98/100\n",
      "6090/6090 [==============================] - 1s 153us/sample - loss: 0.3985 - val_loss: 0.4675\n",
      "Epoch 99/100\n",
      "6090/6090 [==============================] - 1s 149us/sample - loss: 0.3973 - val_loss: 0.4670\n",
      "Epoch 100/100\n",
      "6090/6090 [==============================] - 1s 147us/sample - loss: 0.3962 - val_loss: 0.4666\n",
      "Predicting...\n",
      "[[789  93]\n",
      " [217 424]]\n",
      "Accuracy: 79.65%\n",
      "Precision: 78.43%\n",
      "Recall: 89.46%\n",
      "F: 83.58\n",
      "(Took 94.116 sec)\n",
      "No prep, TF-IDF\n",
      "Training...\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/100\n",
      "6090/6090 [==============================] - 2s 261us/sample - loss: 0.6834 - val_loss: 0.6728\n",
      "Epoch 2/100\n",
      "6090/6090 [==============================] - 1s 171us/sample - loss: 0.6646 - val_loss: 0.6582\n",
      "Epoch 3/100\n",
      "6090/6090 [==============================] - 1s 172us/sample - loss: 0.6491 - val_loss: 0.6458\n",
      "Epoch 4/100\n",
      "6090/6090 [==============================] - 1s 167us/sample - loss: 0.6352 - val_loss: 0.6348\n",
      "Epoch 5/100\n",
      "6090/6090 [==============================] - 1s 167us/sample - loss: 0.6223 - val_loss: 0.6248\n",
      "Epoch 6/100\n",
      "6090/6090 [==============================] - 1s 210us/sample - loss: 0.6102 - val_loss: 0.6154\n",
      "Epoch 7/100\n",
      "6090/6090 [==============================] - 1s 186us/sample - loss: 0.5986 - val_loss: 0.6064\n",
      "Epoch 8/100\n",
      "6090/6090 [==============================] - 1s 180us/sample - loss: 0.5876 - val_loss: 0.5982\n",
      "Epoch 9/100\n",
      "6090/6090 [==============================] - 1s 162us/sample - loss: 0.5771 - val_loss: 0.5904\n",
      "Epoch 10/100\n",
      "6090/6090 [==============================] - 1s 184us/sample - loss: 0.5671 - val_loss: 0.5830\n",
      "Epoch 11/100\n",
      "6090/6090 [==============================] - 1s 189us/sample - loss: 0.5576 - val_loss: 0.5759\n",
      "Epoch 12/100\n",
      "6090/6090 [==============================] - 1s 161us/sample - loss: 0.5485 - val_loss: 0.5693\n",
      "Epoch 13/100\n",
      "6090/6090 [==============================] - 1s 144us/sample - loss: 0.5398 - val_loss: 0.5630\n",
      "Epoch 14/100\n",
      "6090/6090 [==============================] - 1s 152us/sample - loss: 0.5316 - val_loss: 0.5571\n",
      "Epoch 15/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.5236 - val_loss: 0.5515\n",
      "Epoch 16/100\n",
      "6090/6090 [==============================] - 1s 154us/sample - loss: 0.5161 - val_loss: 0.5461\n",
      "Epoch 17/100\n",
      "6090/6090 [==============================] - 1s 160us/sample - loss: 0.5088 - val_loss: 0.5411\n",
      "Epoch 18/100\n",
      "6090/6090 [==============================] - 1s 168us/sample - loss: 0.5018 - val_loss: 0.5362\n",
      "Epoch 19/100\n",
      "6090/6090 [==============================] - 1s 180us/sample - loss: 0.4951 - val_loss: 0.5317\n",
      "Epoch 20/100\n",
      "6090/6090 [==============================] - 1s 177us/sample - loss: 0.4887 - val_loss: 0.5274\n",
      "Epoch 21/100\n",
      "6090/6090 [==============================] - 1s 172us/sample - loss: 0.4825 - val_loss: 0.5233\n",
      "Epoch 22/100\n",
      "6090/6090 [==============================] - 1s 155us/sample - loss: 0.4765 - val_loss: 0.5194\n",
      "Epoch 23/100\n",
      "6090/6090 [==============================] - 1s 185us/sample - loss: 0.4708 - val_loss: 0.5157\n",
      "Epoch 24/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6090/6090 [==============================] - 1s 153us/sample - loss: 0.4653 - val_loss: 0.5123\n",
      "Epoch 25/100\n",
      "6090/6090 [==============================] - 1s 173us/sample - loss: 0.4600 - val_loss: 0.5089\n",
      "Epoch 26/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.4549 - val_loss: 0.5058\n",
      "Epoch 27/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.4500 - val_loss: 0.5028\n",
      "Epoch 28/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.4452 - val_loss: 0.5000\n",
      "Epoch 29/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.4406 - val_loss: 0.4973\n",
      "Epoch 30/100\n",
      "6090/6090 [==============================] - 1s 157us/sample - loss: 0.4361 - val_loss: 0.4947\n",
      "Epoch 31/100\n",
      "6090/6090 [==============================] - 1s 148us/sample - loss: 0.4318 - val_loss: 0.4923\n",
      "Epoch 32/100\n",
      "6090/6090 [==============================] - 1s 151us/sample - loss: 0.4275 - val_loss: 0.4900\n",
      "Epoch 33/100\n",
      "6090/6090 [==============================] - 1s 171us/sample - loss: 0.4235 - val_loss: 0.4879\n",
      "Epoch 34/100\n",
      "6090/6090 [==============================] - 1s 180us/sample - loss: 0.4196 - val_loss: 0.4858\n",
      "Epoch 35/100\n",
      "6090/6090 [==============================] - 1s 140us/sample - loss: 0.4157 - val_loss: 0.4838\n",
      "Epoch 36/100\n",
      "6090/6090 [==============================] - 1s 145us/sample - loss: 0.4120 - val_loss: 0.4819\n",
      "Epoch 37/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.4084 - val_loss: 0.4802\n",
      "Epoch 38/100\n",
      "6090/6090 [==============================] - 1s 135us/sample - loss: 0.4049 - val_loss: 0.4784\n",
      "Epoch 39/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.4015 - val_loss: 0.4768\n",
      "Epoch 40/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.3982 - val_loss: 0.4753\n",
      "Epoch 41/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.3949 - val_loss: 0.4738\n",
      "Epoch 42/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.3918 - val_loss: 0.4724\n",
      "Epoch 43/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.3887 - val_loss: 0.4711\n",
      "Epoch 44/100\n",
      "6090/6090 [==============================] - 1s 135us/sample - loss: 0.3857 - val_loss: 0.4699\n",
      "Epoch 45/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.3828 - val_loss: 0.4687\n",
      "Epoch 46/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.3800 - val_loss: 0.4676\n",
      "Epoch 47/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.3772 - val_loss: 0.4666\n",
      "Epoch 48/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.3745 - val_loss: 0.4656\n",
      "Epoch 49/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.3718 - val_loss: 0.4646\n",
      "Epoch 50/100\n",
      "6090/6090 [==============================] - 1s 136us/sample - loss: 0.3692 - val_loss: 0.4638\n",
      "Epoch 51/100\n",
      "6090/6090 [==============================] - 1s 140us/sample - loss: 0.3667 - val_loss: 0.4629\n",
      "Epoch 52/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.3642 - val_loss: 0.4621\n",
      "Epoch 53/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.3618 - val_loss: 0.4614\n",
      "Epoch 54/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.3594 - val_loss: 0.4606\n",
      "Epoch 55/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.3570 - val_loss: 0.4599\n",
      "Epoch 56/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.3548 - val_loss: 0.4593\n",
      "Epoch 57/100\n",
      "6090/6090 [==============================] - 1s 135us/sample - loss: 0.3525 - val_loss: 0.4588\n",
      "Epoch 58/100\n",
      "6090/6090 [==============================] - 1s 140us/sample - loss: 0.3503 - val_loss: 0.4582\n",
      "Epoch 59/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.3482 - val_loss: 0.4577\n",
      "Epoch 60/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.3461 - val_loss: 0.4572\n",
      "Epoch 61/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.3440 - val_loss: 0.4567\n",
      "Epoch 62/100\n",
      "6090/6090 [==============================] - 1s 140us/sample - loss: 0.3420 - val_loss: 0.4563\n",
      "Epoch 63/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.3399 - val_loss: 0.4559\n",
      "Epoch 64/100\n",
      "6090/6090 [==============================] - 1s 135us/sample - loss: 0.3380 - val_loss: 0.4555\n",
      "Epoch 65/100\n",
      "6090/6090 [==============================] - 1s 135us/sample - loss: 0.3361 - val_loss: 0.4552\n",
      "Epoch 66/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.3342 - val_loss: 0.4549\n",
      "Epoch 67/100\n",
      "6090/6090 [==============================] - 1s 140us/sample - loss: 0.3323 - val_loss: 0.4546\n",
      "Epoch 68/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.3305 - val_loss: 0.4543\n",
      "Epoch 69/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.3287 - val_loss: 0.4541\n",
      "Epoch 70/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.3269 - val_loss: 0.4538\n",
      "Epoch 71/100\n",
      "6090/6090 [==============================] - 1s 140us/sample - loss: 0.3252 - val_loss: 0.4537\n",
      "Epoch 72/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.3235 - val_loss: 0.4535\n",
      "Epoch 73/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.3218 - val_loss: 0.4534\n",
      "Epoch 74/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.3201 - val_loss: 0.4532\n",
      "Epoch 75/100\n",
      "6090/6090 [==============================] - 1s 140us/sample - loss: 0.3185 - val_loss: 0.4531\n",
      "Epoch 76/100\n",
      "6090/6090 [==============================] - 1s 136us/sample - loss: 0.3169 - val_loss: 0.4530\n",
      "Epoch 77/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.3153 - val_loss: 0.4529\n",
      "Epoch 78/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.3137 - val_loss: 0.4528\n",
      "Epoch 79/100\n",
      "6090/6090 [==============================] - 1s 138us/sample - loss: 0.3122 - val_loss: 0.4528\n",
      "Epoch 80/100\n",
      "6090/6090 [==============================] - 1s 142us/sample - loss: 0.3107 - val_loss: 0.4527\n",
      "Epoch 81/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.3092 - val_loss: 0.4527\n",
      "Epoch 82/100\n",
      "6090/6090 [==============================] - 1s 134us/sample - loss: 0.3077 - val_loss: 0.4527\n",
      "Predicting...\n",
      "[[789  93]\n",
      " [209 432]]\n",
      "Accuracy: 80.17%\n",
      "Precision: 79.06%\n",
      "Recall: 89.46%\n",
      "F: 83.94\n",
      "(Took 76.731 sec)\n",
      "Preprocessed, raw counts\n",
      "Training...\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/100\n",
      "6090/6090 [==============================] - 1s 155us/sample - loss: 0.6715 - val_loss: 0.6502\n",
      "Epoch 2/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.6239 - val_loss: 0.6148\n",
      "Epoch 3/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.5864 - val_loss: 0.5870\n",
      "Epoch 4/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.5563 - val_loss: 0.5649\n",
      "Epoch 5/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.5314 - val_loss: 0.5469\n",
      "Epoch 6/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.5106 - val_loss: 0.5324\n",
      "Epoch 7/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.4930 - val_loss: 0.5202\n",
      "Epoch 8/100\n",
      "6090/6090 [==============================] - 1s 116us/sample - loss: 0.4778 - val_loss: 0.5103\n",
      "Epoch 9/100\n",
      "6090/6090 [==============================] - 1s 126us/sample - loss: 0.4645 - val_loss: 0.5017\n",
      "Epoch 10/100\n",
      "6090/6090 [==============================] - 1s 124us/sample - loss: 0.4528 - val_loss: 0.4945\n",
      "Epoch 11/100\n",
      "6090/6090 [==============================] - 1s 118us/sample - loss: 0.4423 - val_loss: 0.4883\n",
      "Epoch 12/100\n",
      "6090/6090 [==============================] - 1s 117us/sample - loss: 0.4329 - val_loss: 0.4829\n",
      "Epoch 13/100\n",
      "6090/6090 [==============================] - 1s 132us/sample - loss: 0.4243 - val_loss: 0.4784\n",
      "Epoch 14/100\n",
      "6090/6090 [==============================] - 1s 123us/sample - loss: 0.4165 - val_loss: 0.4744\n",
      "Epoch 15/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4094 - val_loss: 0.4709\n",
      "Epoch 16/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4028 - val_loss: 0.4681\n",
      "Epoch 17/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.3966 - val_loss: 0.4656\n",
      "Epoch 18/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.3909 - val_loss: 0.4635\n",
      "Epoch 19/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.3855 - val_loss: 0.4617\n",
      "Epoch 20/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.3805 - val_loss: 0.4601\n",
      "Epoch 21/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.3758 - val_loss: 0.4587\n",
      "Epoch 22/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.3713 - val_loss: 0.4577\n",
      "Epoch 23/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.3671 - val_loss: 0.4567\n",
      "Epoch 24/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.3631 - val_loss: 0.4560\n",
      "Epoch 25/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.3593 - val_loss: 0.4554\n",
      "Epoch 26/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.3556 - val_loss: 0.4549\n",
      "Epoch 27/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.3521 - val_loss: 0.4546\n",
      "Epoch 28/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.3488 - val_loss: 0.4544\n",
      "Epoch 29/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.3455 - val_loss: 0.4544\n",
      "Epoch 30/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.3425 - val_loss: 0.4544\n",
      "Predicting...\n",
      "[[780 102]\n",
      " [207 434]]\n",
      "Accuracy: 79.71%\n",
      "Precision: 79.03%\n",
      "Recall: 88.44%\n",
      "F: 83.47\n",
      "(Took 21.801 sec)\n",
      "Preprocessed, normalized\n",
      "Training...\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/100\n",
      "6090/6090 [==============================] - 1s 151us/sample - loss: 0.6866 - val_loss: 0.6799\n",
      "Epoch 2/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.6746 - val_loss: 0.6699\n",
      "Epoch 3/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.6649 - val_loss: 0.6621\n",
      "Epoch 4/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.6565 - val_loss: 0.6552\n",
      "Epoch 5/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.6486 - val_loss: 0.6488\n",
      "Epoch 6/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.6411 - val_loss: 0.6428\n",
      "Epoch 7/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.6339 - val_loss: 0.6370\n",
      "Epoch 8/100\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.6269 - val_loss: 0.6316\n",
      "Epoch 9/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.6201 - val_loss: 0.6263\n",
      "Epoch 10/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.6135 - val_loss: 0.6212\n",
      "Epoch 11/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.6071 - val_loss: 0.6162\n",
      "Epoch 12/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.6008 - val_loss: 0.6114\n",
      "Epoch 13/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.5948 - val_loss: 0.6068\n",
      "Epoch 14/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.5889 - val_loss: 0.6024\n",
      "Epoch 15/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.5831 - val_loss: 0.5980\n",
      "Epoch 16/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.5775 - val_loss: 0.5938\n",
      "Epoch 17/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.5721 - val_loss: 0.5898\n",
      "Epoch 18/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.5668 - val_loss: 0.5858\n",
      "Epoch 19/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.5616 - val_loss: 0.5821\n",
      "Epoch 20/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.5566 - val_loss: 0.5784\n",
      "Epoch 21/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.5517 - val_loss: 0.5749\n",
      "Epoch 22/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.5470 - val_loss: 0.5715\n",
      "Epoch 23/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.5423 - val_loss: 0.5681\n",
      "Epoch 24/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.5378 - val_loss: 0.5650\n",
      "Epoch 25/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.5334 - val_loss: 0.5619\n",
      "Epoch 26/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.5291 - val_loss: 0.5588\n",
      "Epoch 27/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.5249 - val_loss: 0.5560\n",
      "Epoch 28/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.5208 - val_loss: 0.5532\n",
      "Epoch 29/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.5168 - val_loss: 0.5505\n",
      "Epoch 30/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.5129 - val_loss: 0.5479\n",
      "Epoch 31/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.5091 - val_loss: 0.5454\n",
      "Epoch 32/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.5054 - val_loss: 0.5429\n",
      "Epoch 33/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.5019 - val_loss: 0.5405\n",
      "Epoch 34/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4983 - val_loss: 0.5383\n",
      "Epoch 35/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4948 - val_loss: 0.5361\n",
      "Epoch 36/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.4915 - val_loss: 0.5340\n",
      "Epoch 37/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.4882 - val_loss: 0.5319\n",
      "Epoch 38/100\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.4850 - val_loss: 0.5299\n",
      "Epoch 39/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.4820 - val_loss: 0.5280\n",
      "Epoch 40/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.4789 - val_loss: 0.5262\n",
      "Epoch 41/100\n",
      "6090/6090 [==============================] - 1s 118us/sample - loss: 0.4759 - val_loss: 0.5244\n",
      "Epoch 42/100\n",
      "6090/6090 [==============================] - 1s 116us/sample - loss: 0.4730 - val_loss: 0.5227\n",
      "Epoch 43/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.4702 - val_loss: 0.5210\n",
      "Epoch 44/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.4674 - val_loss: 0.5194\n",
      "Epoch 45/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.4647 - val_loss: 0.5179\n",
      "Epoch 46/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.4620 - val_loss: 0.5164\n",
      "Epoch 47/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.4594 - val_loss: 0.5150\n",
      "Epoch 48/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.4569 - val_loss: 0.5136\n",
      "Epoch 49/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.4544 - val_loss: 0.5122\n",
      "Epoch 50/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.4520 - val_loss: 0.5109\n",
      "Epoch 51/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.4496 - val_loss: 0.5097\n",
      "Epoch 52/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.4473 - val_loss: 0.5086\n",
      "Epoch 53/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4450 - val_loss: 0.5074\n",
      "Epoch 54/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.4427 - val_loss: 0.5063\n",
      "Epoch 55/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4406 - val_loss: 0.5052\n",
      "Epoch 56/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4384 - val_loss: 0.5042\n",
      "Epoch 57/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4363 - val_loss: 0.5032\n",
      "Epoch 58/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.4343 - val_loss: 0.5023\n",
      "Epoch 59/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.4323 - val_loss: 0.5013\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.4303 - val_loss: 0.5004\n",
      "Epoch 61/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4284 - val_loss: 0.4996\n",
      "Epoch 62/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.4264 - val_loss: 0.4988\n",
      "Epoch 63/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4246 - val_loss: 0.4980\n",
      "Epoch 64/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4228 - val_loss: 0.4973\n",
      "Epoch 65/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4210 - val_loss: 0.4965\n",
      "Epoch 66/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4192 - val_loss: 0.4958\n",
      "Epoch 67/100\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.4175 - val_loss: 0.4952\n",
      "Epoch 68/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4158 - val_loss: 0.4945\n",
      "Epoch 69/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4141 - val_loss: 0.4939\n",
      "Epoch 70/100\n",
      "6090/6090 [==============================] - 1s 123us/sample - loss: 0.4125 - val_loss: 0.4933\n",
      "Epoch 71/100\n",
      "6090/6090 [==============================] - 1s 122us/sample - loss: 0.4109 - val_loss: 0.4927\n",
      "Epoch 72/100\n",
      "6090/6090 [==============================] - 1s 119us/sample - loss: 0.4093 - val_loss: 0.4922\n",
      "Epoch 73/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.4078 - val_loss: 0.4917\n",
      "Epoch 74/100\n",
      "6090/6090 [==============================] - 1s 116us/sample - loss: 0.4062 - val_loss: 0.4912\n",
      "Epoch 75/100\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.4048 - val_loss: 0.4907\n",
      "Epoch 76/100\n",
      "6090/6090 [==============================] - 1s 116us/sample - loss: 0.4033 - val_loss: 0.4902\n",
      "Epoch 77/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4018 - val_loss: 0.4898\n",
      "Epoch 78/100\n",
      "6090/6090 [==============================] - 1s 116us/sample - loss: 0.4004 - val_loss: 0.4894\n",
      "Epoch 79/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.3990 - val_loss: 0.4890\n",
      "Epoch 80/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.3976 - val_loss: 0.4887\n",
      "Epoch 81/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.3963 - val_loss: 0.4883\n",
      "Epoch 82/100\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.3950 - val_loss: 0.4880\n",
      "Epoch 83/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.3936 - val_loss: 0.4877\n",
      "Epoch 84/100\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.3924 - val_loss: 0.4874\n",
      "Epoch 85/100\n",
      "6090/6090 [==============================] - 1s 120us/sample - loss: 0.3911 - val_loss: 0.4871\n",
      "Epoch 86/100\n",
      "6090/6090 [==============================] - 1s 151us/sample - loss: 0.3898 - val_loss: 0.4868\n",
      "Epoch 87/100\n",
      "6090/6090 [==============================] - 1s 164us/sample - loss: 0.3886 - val_loss: 0.4866\n",
      "Epoch 88/100\n",
      "6090/6090 [==============================] - 1s 117us/sample - loss: 0.3874 - val_loss: 0.4863\n",
      "Epoch 89/100\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.3862 - val_loss: 0.4861\n",
      "Epoch 90/100\n",
      "6090/6090 [==============================] - 1s 126us/sample - loss: 0.3850 - val_loss: 0.4859\n",
      "Epoch 91/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.3839 - val_loss: 0.4857\n",
      "Epoch 92/100\n",
      "6090/6090 [==============================] - 1s 121us/sample - loss: 0.3827 - val_loss: 0.4856\n",
      "Epoch 93/100\n",
      "6090/6090 [==============================] - 1s 120us/sample - loss: 0.3816 - val_loss: 0.4854\n",
      "Epoch 94/100\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.3805 - val_loss: 0.4852\n",
      "Epoch 95/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.3794 - val_loss: 0.4851\n",
      "Epoch 96/100\n",
      "6090/6090 [==============================] - 1s 133us/sample - loss: 0.3783 - val_loss: 0.4849\n",
      "Epoch 97/100\n",
      "6090/6090 [==============================] - 1s 144us/sample - loss: 0.3772 - val_loss: 0.4848\n",
      "Epoch 98/100\n",
      "6090/6090 [==============================] - 1s 126us/sample - loss: 0.3762 - val_loss: 0.4847\n",
      "Epoch 99/100\n",
      "6090/6090 [==============================] - 1s 137us/sample - loss: 0.3752 - val_loss: 0.4845\n",
      "Epoch 100/100\n",
      "6090/6090 [==============================] - 1s 123us/sample - loss: 0.3741 - val_loss: 0.4844\n",
      "Predicting...\n",
      "[[761 121]\n",
      " [204 437]]\n",
      "Accuracy: 78.66%\n",
      "Precision: 78.86%\n",
      "Recall: 86.28%\n",
      "F: 82.40\n",
      "(Took 73.108 sec)\n",
      "Preprocessed, TF-IDF\n",
      "Training...\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/100\n",
      "6090/6090 [==============================] - 1s 222us/sample - loss: 0.6829 - val_loss: 0.6714\n",
      "Epoch 2/100\n",
      "6090/6090 [==============================] - 1s 122us/sample - loss: 0.6601 - val_loss: 0.6540\n",
      "Epoch 3/100\n",
      "6090/6090 [==============================] - 1s 121us/sample - loss: 0.6408 - val_loss: 0.6392\n",
      "Epoch 4/100\n",
      "6090/6090 [==============================] - 1s 122us/sample - loss: 0.6235 - val_loss: 0.6262\n",
      "Epoch 5/100\n",
      "6090/6090 [==============================] - 1s 121us/sample - loss: 0.6077 - val_loss: 0.6145\n",
      "Epoch 6/100\n",
      "6090/6090 [==============================] - 1s 116us/sample - loss: 0.5930 - val_loss: 0.6038\n",
      "Epoch 7/100\n",
      "6090/6090 [==============================] - 1s 131us/sample - loss: 0.5791 - val_loss: 0.5938\n",
      "Epoch 8/100\n",
      "6090/6090 [==============================] - 1s 135us/sample - loss: 0.5662 - val_loss: 0.5847\n",
      "Epoch 9/100\n",
      "6090/6090 [==============================] - 1s 143us/sample - loss: 0.5541 - val_loss: 0.5763\n",
      "Epoch 10/100\n",
      "6090/6090 [==============================] - 1s 125us/sample - loss: 0.5428 - val_loss: 0.5685\n",
      "Epoch 11/100\n",
      "6090/6090 [==============================] - 1s 141us/sample - loss: 0.5320 - val_loss: 0.5613\n",
      "Epoch 12/100\n",
      "6090/6090 [==============================] - 1s 127us/sample - loss: 0.5219 - val_loss: 0.5546\n",
      "Epoch 13/100\n",
      "6090/6090 [==============================] - 1s 124us/sample - loss: 0.5124 - val_loss: 0.5486\n",
      "Epoch 14/100\n",
      "6090/6090 [==============================] - 1s 130us/sample - loss: 0.5035 - val_loss: 0.5428\n",
      "Epoch 15/100\n",
      "6090/6090 [==============================] - 1s 133us/sample - loss: 0.4950 - val_loss: 0.5375\n",
      "Epoch 16/100\n",
      "6090/6090 [==============================] - 1s 135us/sample - loss: 0.4869 - val_loss: 0.5327\n",
      "Epoch 17/100\n",
      "6090/6090 [==============================] - 1s 129us/sample - loss: 0.4794 - val_loss: 0.5282\n",
      "Epoch 18/100\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.4722 - val_loss: 0.5242\n",
      "Epoch 19/100\n",
      "6090/6090 [==============================] - 1s 126us/sample - loss: 0.4653 - val_loss: 0.5203\n",
      "Epoch 20/100\n",
      "6090/6090 [==============================] - 1s 159us/sample - loss: 0.4588 - val_loss: 0.5168\n",
      "Epoch 21/100\n",
      "6090/6090 [==============================] - 1s 160us/sample - loss: 0.4526 - val_loss: 0.5136\n",
      "Epoch 22/100\n",
      "6090/6090 [==============================] - 1s 136us/sample - loss: 0.4467 - val_loss: 0.5106\n",
      "Epoch 23/100\n",
      "6090/6090 [==============================] - 1s 120us/sample - loss: 0.4412 - val_loss: 0.5078\n",
      "Epoch 24/100\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.4358 - val_loss: 0.5053\n",
      "Epoch 25/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.4307 - val_loss: 0.5030\n",
      "Epoch 26/100\n",
      "6090/6090 [==============================] - 1s 118us/sample - loss: 0.4258 - val_loss: 0.5009\n",
      "Epoch 27/100\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4211 - val_loss: 0.4990\n",
      "Epoch 28/100\n",
      "6090/6090 [==============================] - 1s 117us/sample - loss: 0.4166 - val_loss: 0.4971\n",
      "Epoch 29/100\n",
      "6090/6090 [==============================] - 1s 116us/sample - loss: 0.4123 - val_loss: 0.4955\n",
      "Epoch 30/100\n",
      "6090/6090 [==============================] - 1s 124us/sample - loss: 0.4082 - val_loss: 0.4941\n",
      "Epoch 31/100\n",
      "6090/6090 [==============================] - 1s 116us/sample - loss: 0.4042 - val_loss: 0.4927\n",
      "Epoch 32/100\n",
      "6090/6090 [==============================] - 1s 119us/sample - loss: 0.4004 - val_loss: 0.4916\n",
      "Epoch 33/100\n",
      "6090/6090 [==============================] - 1s 121us/sample - loss: 0.3968 - val_loss: 0.4905\n",
      "Epoch 34/100\n",
      "6090/6090 [==============================] - 1s 118us/sample - loss: 0.3933 - val_loss: 0.4897\n",
      "Epoch 35/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.3899 - val_loss: 0.4888\n",
      "Epoch 36/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.3866 - val_loss: 0.4881\n",
      "Epoch 37/100\n",
      "6090/6090 [==============================] - 1s 111us/sample - loss: 0.3835 - val_loss: 0.4874\n",
      "Epoch 38/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.3804 - val_loss: 0.4869\n",
      "Epoch 39/100\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.3775 - val_loss: 0.4865\n",
      "Epoch 40/100\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.3747 - val_loss: 0.4861\n",
      "Epoch 41/100\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.3719 - val_loss: 0.4858\n",
      "Epoch 42/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.3693 - val_loss: 0.4855\n",
      "Epoch 43/100\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.3667 - val_loss: 0.4853\n",
      "Epoch 44/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.3642 - val_loss: 0.4852\n",
      "Epoch 45/100\n",
      "6090/6090 [==============================] - 1s 112us/sample - loss: 0.3617 - val_loss: 0.4852\n",
      "Epoch 46/100\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.3594 - val_loss: 0.4852\n",
      "Predicting...\n",
      "[[753 129]\n",
      " [206 435]]\n",
      "Accuracy: 78.00%\n",
      "Precision: 78.52%\n",
      "Recall: 85.37%\n",
      "F: 81.80\n",
      "(Took 35.979 sec)\n",
      "Done with Logistic models\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# No preprocessing\n",
    "# Raw counts\n",
    "print('No prep, raw counts')\n",
    "run_logistic(dftrain_minimal_noprep, 0.2, 100)\n",
    "# Normalized\n",
    "print('No prep, normalized')\n",
    "run_logistic(dftrain_min_norm_noprep, 0.2, 100)\n",
    "# TF-IDF\n",
    "print('No prep, TF-IDF')\n",
    "run_logistic(dftrain_min_tfidf_noprep, 0.2, 100)\n",
    "# With preprocessing\n",
    "# Raw counts\n",
    "print('Preprocessed, raw counts')\n",
    "run_logistic(dftrain_minimal, 0.2, 100)\n",
    "# Normalized\n",
    "print('Preprocessed, normalized')\n",
    "run_logistic(dftrain_min_norm, 0.2, 100)\n",
    "# TF-IDF\n",
    "print('Preprocessed, TF-IDF')\n",
    "run_logistic(dftrain_min_tfidf, 0.2, 100)\n",
    "print('Done with Logistic models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Train on 7536 samples, validate on 77 samples\n",
      "Epoch 1/100\n",
      "7536/7536 [==============================] - 1s 131us/sample - loss: 0.6654 - val_loss: 0.6375\n",
      "Epoch 2/100\n",
      "7536/7536 [==============================] - 1s 91us/sample - loss: 0.6114 - val_loss: 0.5993\n",
      "Epoch 3/100\n",
      "7536/7536 [==============================] - 1s 95us/sample - loss: 0.5705 - val_loss: 0.5703\n",
      "Epoch 4/100\n",
      "7536/7536 [==============================] - 1s 111us/sample - loss: 0.5391 - val_loss: 0.5487\n",
      "Epoch 5/100\n",
      "7536/7536 [==============================] - 1s 110us/sample - loss: 0.5141 - val_loss: 0.5320\n",
      "Epoch 6/100\n",
      "7536/7536 [==============================] - 1s 93us/sample - loss: 0.4936 - val_loss: 0.5184\n",
      "Epoch 7/100\n",
      "7536/7536 [==============================] - 1s 102us/sample - loss: 0.4766 - val_loss: 0.5079\n",
      "Epoch 8/100\n",
      "7536/7536 [==============================] - 1s 101us/sample - loss: 0.4621 - val_loss: 0.4997\n",
      "Epoch 9/100\n",
      "7536/7536 [==============================] - 1s 102us/sample - loss: 0.4496 - val_loss: 0.4914\n",
      "Epoch 10/100\n",
      "7536/7536 [==============================] - 1s 100us/sample - loss: 0.4388 - val_loss: 0.4863\n",
      "Epoch 11/100\n",
      "7536/7536 [==============================] - 1s 103us/sample - loss: 0.4292 - val_loss: 0.4817\n",
      "Epoch 12/100\n",
      "7536/7536 [==============================] - 1s 89us/sample - loss: 0.4207 - val_loss: 0.4772\n",
      "Epoch 13/100\n",
      "7536/7536 [==============================] - 1s 92us/sample - loss: 0.4130 - val_loss: 0.4743\n",
      "Epoch 14/100\n",
      "7536/7536 [==============================] - 1s 89us/sample - loss: 0.4060 - val_loss: 0.4716\n",
      "Epoch 15/100\n",
      "7536/7536 [==============================] - 1s 102us/sample - loss: 0.3996 - val_loss: 0.4702\n",
      "Epoch 16/100\n",
      "7536/7536 [==============================] - 1s 89us/sample - loss: 0.3938 - val_loss: 0.4674\n",
      "Epoch 17/100\n",
      "7536/7536 [==============================] - 1s 104us/sample - loss: 0.3883 - val_loss: 0.4662\n",
      "Epoch 18/100\n",
      "7536/7536 [==============================] - 1s 88us/sample - loss: 0.3834 - val_loss: 0.4649\n",
      "Epoch 19/100\n",
      "7536/7536 [==============================] - 1s 89us/sample - loss: 0.3787 - val_loss: 0.4639\n",
      "Epoch 20/100\n",
      "7536/7536 [==============================] - 1s 96us/sample - loss: 0.3743 - val_loss: 0.4632\n",
      "Epoch 21/100\n",
      "7536/7536 [==============================] - 1s 97us/sample - loss: 0.3702 - val_loss: 0.4624\n",
      "Epoch 22/100\n",
      "7536/7536 [==============================] - 1s 91us/sample - loss: 0.3663 - val_loss: 0.4619\n",
      "Epoch 23/100\n",
      "7536/7536 [==============================] - 1s 99us/sample - loss: 0.3627 - val_loss: 0.4607\n",
      "Epoch 24/100\n",
      "7536/7536 [==============================] - 1s 91us/sample - loss: 0.3592 - val_loss: 0.4604\n",
      "Epoch 25/100\n",
      "7536/7536 [==============================] - 1s 94us/sample - loss: 0.3559 - val_loss: 0.4610\n",
      "Predicting...\n",
      "[[46  4]\n",
      " [ 7 20]]\n",
      "Accuracy: 85.71%\n",
      "Precision: 86.79%\n",
      "Recall: 92.00%\n",
      "F: 89.32\n",
      "(Took 19.123 sec)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test to see feature importances\n",
    "nn_test = run_logistic(dftrain_minimal, 0.01, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x1152 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Regularization may not help that much\n",
    "weights = nn_test.get_weights()[0]\n",
    "\n",
    "features = {dftrain_minimal.columns[i]:weights[i][0] for i in range(len(weights))}\n",
    "top_features = sorted(features.items(), key=lambda x: x[1], reverse=True)[0:100]\n",
    "topfeatures = pd.DataFrame(columns = ['feature', 'importance'])\n",
    "topfeatures['feature'] = [top_features[i][0] for i in range(100)]\n",
    "topfeatures['importance'] = [top_features[i][1] for i in range(100)]\n",
    "\n",
    "plt.figure(figsize=(16,16))\n",
    "plt.barh([i for i in range(20)], topfeatures['importance'][0:20], align='center', alpha=0.5)\n",
    "plt.yticks([i for i in range(20)], topfeatures['feature'][0:20])\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Top feature importances')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural networks were additional layers did not seem to produce any improvements. Several different numbers of hidden layers, and number of nodes on each hidden layer, were tried. The activation functions were varied between sigmoid and relu without any improvements. Adding dropout helped to avoid overfitting to some extent, but the resulting models were about the same as logistic regression. Better to go with the simpler model, if there isn't a significant improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_neuralNet(dataset, split, epochs, batch_size=32, patience=5):\n",
    "    t0 = time.time()\n",
    "    np.random.seed(1)\n",
    "    train, test = train_test_split(dataset, test_size=split)\n",
    "    train_x = np.array(train.drop('_target', axis=1))\n",
    "    train_y = np.array(train['_target'])\n",
    "    test_x = np.array(test.drop('_target', axis=1))\n",
    "    test_y = np.array(test['_target'])\n",
    "    print('Training...')\n",
    "    nn = Sequential()\n",
    "    nn.add(Dense(16, activation='sigmoid', input_shape=(train_x.shape[1],)))\n",
    "    nn.add(Dropout(0.5))\n",
    "    nn.add(Dense(1, activation='sigmoid'))\n",
    "    nn.compile(optimizer='adam', loss='binary_crossentropy', metrics=[])\n",
    "    es = EarlyStopping(monitor='val_loss', patience=patience)\n",
    "    h = nn.fit(train_x, train_y, batch_size = batch_size, epochs=epochs, validation_data=(test_x, test_y), callbacks=[es])\n",
    "    plt.plot(h.history['loss'])\n",
    "    plt.plot(h.history['val_loss'])\n",
    "    print('Predicting...')\n",
    "    preds = 1*(nn.predict(test_x) > 0.5)\n",
    "    cm = confusion_matrix(test_y, preds)\n",
    "    print(cm)\n",
    "    tp, fn, fp, tn = cm.ravel()\n",
    "    accuracy = (tp+tn)/(tp+fn+fp+tn)\n",
    "    precision = tp/(tp+fp)\n",
    "    recall = tp/(tp+fn)\n",
    "    F = 2*precision*recall/(precision+recall)\n",
    "    print('Accuracy: {:.2f}%\\nPrecision: {:.2f}%\\nRecall: {:.2f}%\\nF: {:.2f}'.format(100*accuracy, 100*precision, 100*recall, 100*F))\n",
    "    t1 = time.time()\n",
    "    print('(Took {:.3f} sec)'.format(t1-t0))\n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No preprocessing, raw counts\n",
      "Training...\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/50\n",
      "6090/6090 [==============================] - 1s 160us/sample - loss: 0.7309 - val_loss: 0.6493\n",
      "Epoch 2/50\n",
      "6090/6090 [==============================] - 1s 116us/sample - loss: 0.6786 - val_loss: 0.6253\n",
      "Epoch 3/50\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.6486 - val_loss: 0.6032\n",
      "Epoch 4/50\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.6168 - val_loss: 0.5814\n",
      "Epoch 5/50\n",
      "6090/6090 [==============================] - 1s 120us/sample - loss: 0.5842 - val_loss: 0.5584\n",
      "Epoch 6/50\n",
      "6090/6090 [==============================] - 1s 118us/sample - loss: 0.5690 - val_loss: 0.5365\n",
      "Epoch 7/50\n",
      "6090/6090 [==============================] - 1s 110us/sample - loss: 0.5522 - val_loss: 0.5194\n",
      "Epoch 8/50\n",
      "6090/6090 [==============================] - 1s 105us/sample - loss: 0.5210 - val_loss: 0.5017\n",
      "Epoch 9/50\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.5066 - val_loss: 0.4893\n",
      "Epoch 10/50\n",
      "6090/6090 [==============================] - 1s 115us/sample - loss: 0.4916 - val_loss: 0.4787\n",
      "Epoch 11/50\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4774 - val_loss: 0.4715\n",
      "Epoch 12/50\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4681 - val_loss: 0.4650\n",
      "Epoch 13/50\n",
      "6090/6090 [==============================] - 1s 107us/sample - loss: 0.4554 - val_loss: 0.4603\n",
      "Epoch 14/50\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.4405 - val_loss: 0.4566\n",
      "Epoch 15/50\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.4294 - val_loss: 0.4538\n",
      "Epoch 16/50\n",
      "6090/6090 [==============================] - 1s 118us/sample - loss: 0.4275 - val_loss: 0.4518\n",
      "Epoch 17/50\n",
      "6090/6090 [==============================] - 1s 109us/sample - loss: 0.4208 - val_loss: 0.4505\n",
      "Epoch 18/50\n",
      "6090/6090 [==============================] - 1s 106us/sample - loss: 0.4113 - val_loss: 0.4499\n",
      "Epoch 19/50\n",
      "6090/6090 [==============================] - 1s 108us/sample - loss: 0.4088 - val_loss: 0.4498\n",
      "Epoch 20/50\n",
      "6090/6090 [==============================] - 1s 113us/sample - loss: 0.4003 - val_loss: 0.4499\n",
      "Epoch 21/50\n",
      "6090/6090 [==============================] - 1s 114us/sample - loss: 0.3957 - val_loss: 0.4501\n",
      "Predicting...\n",
      "[[775 107]\n",
      " [199 442]]\n",
      "Accuracy: 79.91%\n",
      "Precision: 79.57%\n",
      "Recall: 87.87%\n",
      "F: 83.51\n",
      "(Took 15.238 sec)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.engine.sequential.Sequential at 0x22bba3c9128>"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Neural net models\n",
    "# Only with preprocessing, otherwise there are too many weights\n",
    "# Raw counts\n",
    "print('No preprocessing, raw counts')\n",
    "run_neuralNet(dftrain_minimal, 0.2, 50, patience=2)\n",
    "# Normalized\n",
    "#print('Preprocessed, normalized')\n",
    "#run_neuralNet(dftrain_min_norm, 0.2, 20)\n",
    "# TF-IDF\n",
    "#print('Preprocessed, TF-IDF')\n",
    "#run_neuralNet(dftrain_min_tfidf, 0.2, 20)\n",
    "#print('Done with Neural Net models')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the best models were trained on the full train data, and used to make predictions which were submitted to Kaggle. The best model submission ended up being the simple logistic regression model with no preprocessing (other than removing words that only occur in one document and that don't occur in both the train and test data). The final F score was 0.8098. This is significantly lower than the cross-validated result, and that indicates that the testing data is somewhat different in distribution than the training data. This can be expected, given the amount of variation there is in natural language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame(test['id'], columns=['id', 'target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_random_forest_predictions(trees, train, test):\n",
    "    print('Training...')\n",
    "    rft = RandomForestClassifier(n_estimators=trees)\n",
    "    rft.fit(train.drop('_target', axis=1), train['_target'])\n",
    "    print('Predicting...')\n",
    "    preds = rft.predict(test)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Predicting...\n"
     ]
    }
   ],
   "source": [
    "submission['target'] = make_random_forest_predictions(500, dftrain_minimal_noprep, dftest_minimal_noprep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission_rf_noprep.csv', index=False)\n",
    "# Score: 79.55%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Predicting...\n"
     ]
    }
   ],
   "source": [
    "submission['target'] = make_random_forest_predictions(500, dftrain_min_tfidf, dftest_min_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission_rf_prep.csv', index=False)\n",
    "# Score: 78.732%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_logistic_predictions(epochs, batch_size, train, test):\n",
    "    print('Training...')\n",
    "    nn = Sequential()\n",
    "    nn.add(Dense(1, activation='sigmoid', input_shape=(train.drop('_target', axis=1).shape[1],)))\n",
    "    nn.compile(optimizer='adam', loss='binary_crossentropy', metrics=[])\n",
    "    nn.fit(np.array(train.drop('_target', axis=1)), np.array(train['_target']), batch_size = batch_size, epochs=epochs)\n",
    "    print('Predicting...')\n",
    "    preds = 1*(nn.predict(np.array(test)) > 0.5)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Train on 7613 samples\n",
      "Epoch 1/23\n",
      "7613/7613 [==============================] - 1s 172us/sample - loss: 0.6369\n",
      "Epoch 2/23\n",
      "7613/7613 [==============================] - 1s 129us/sample - loss: 0.5614\n",
      "Epoch 3/23\n",
      "7613/7613 [==============================] - 1s 121us/sample - loss: 0.5168\n",
      "Epoch 4/23\n",
      "7613/7613 [==============================] - 1s 116us/sample - loss: 0.4853\n",
      "Epoch 5/23\n",
      "7613/7613 [==============================] - 1s 107us/sample - loss: 0.4610\n",
      "Epoch 6/23\n",
      "7613/7613 [==============================] - 1s 104us/sample - loss: 0.4410\n",
      "Epoch 7/23\n",
      "7613/7613 [==============================] - 1s 107us/sample - loss: 0.4244\n",
      "Epoch 8/23\n",
      "7613/7613 [==============================] - 1s 114us/sample - loss: 0.4100\n",
      "Epoch 9/23\n",
      "7613/7613 [==============================] - 1s 108us/sample - loss: 0.3972\n",
      "Epoch 10/23\n",
      "7613/7613 [==============================] - 1s 109us/sample - loss: 0.3858\n",
      "Epoch 11/23\n",
      "7613/7613 [==============================] - 1s 110us/sample - loss: 0.3756\n",
      "Epoch 12/23\n",
      "7613/7613 [==============================] - 1s 110us/sample - loss: 0.3663\n",
      "Epoch 13/23\n",
      "7613/7613 [==============================] - 1s 111us/sample - loss: 0.3576\n",
      "Epoch 14/23\n",
      "7613/7613 [==============================] - 1s 115us/sample - loss: 0.3497\n",
      "Epoch 15/23\n",
      "7613/7613 [==============================] - 1s 111us/sample - loss: 0.3423\n",
      "Epoch 16/23\n",
      "7613/7613 [==============================] - 1s 107us/sample - loss: 0.3354\n",
      "Epoch 17/23\n",
      "7613/7613 [==============================] - 1s 114us/sample - loss: 0.3290\n",
      "Epoch 18/23\n",
      "7613/7613 [==============================] - 1s 113us/sample - loss: 0.3230\n",
      "Epoch 19/23\n",
      "7613/7613 [==============================] - 1s 111us/sample - loss: 0.3171\n",
      "Epoch 20/23\n",
      "7613/7613 [==============================] - 1s 109us/sample - loss: 0.3117\n",
      "Epoch 21/23\n",
      "7613/7613 [==============================] - 1s 110us/sample - loss: 0.3066\n",
      "Epoch 22/23\n",
      "7613/7613 [==============================] - 1s 110us/sample - loss: 0.3017\n",
      "Epoch 23/23\n",
      "7613/7613 [==============================] - 1s 111us/sample - loss: 0.2970\n",
      "Predicting...\n"
     ]
    }
   ],
   "source": [
    "submission['target'] = make_logistic_predictions(23, 32, dftrain_minimal_noprep, dftest_minimal_noprep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission_lg_noprep.csv', index=False)\n",
    "# Score: 80.98%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Train on 7613 samples\n",
      "Epoch 1/46\n",
      "7613/7613 [==============================] - 1s 135us/sample - loss: 0.6800\n",
      "Epoch 2/46\n",
      "7613/7613 [==============================] - 1s 93us/sample - loss: 0.6537\n",
      "Epoch 3/46\n",
      "7613/7613 [==============================] - 1s 92us/sample - loss: 0.6320\n",
      "Epoch 4/46\n",
      "7613/7613 [==============================] - 1s 95us/sample - loss: 0.6128\n",
      "Epoch 5/46\n",
      "7613/7613 [==============================] - 1s 98us/sample - loss: 0.5952\n",
      "Epoch 6/46\n",
      "7613/7613 [==============================] - 1s 85us/sample - loss: 0.5791\n",
      "Epoch 7/46\n",
      "7613/7613 [==============================] - 1s 78us/sample - loss: 0.5643\n",
      "Epoch 8/46\n",
      "7613/7613 [==============================] - 1s 91us/sample - loss: 0.5506\n",
      "Epoch 9/46\n",
      "7613/7613 [==============================] - 1s 109us/sample - loss: 0.5379\n",
      "Epoch 10/46\n",
      "7613/7613 [==============================] - 1s 123us/sample - loss: 0.5261\n",
      "Epoch 11/46\n",
      "7613/7613 [==============================] - 1s 91us/sample - loss: 0.5153\n",
      "Epoch 12/46\n",
      "7613/7613 [==============================] - 1s 80us/sample - loss: 0.5051\n",
      "Epoch 13/46\n",
      "7613/7613 [==============================] - 1s 84us/sample - loss: 0.4957\n",
      "Epoch 14/46\n",
      "7613/7613 [==============================] - 1s 96us/sample - loss: 0.4869\n",
      "Epoch 15/46\n",
      "7613/7613 [==============================] - 1s 76us/sample - loss: 0.4787\n",
      "Epoch 16/46\n",
      "7613/7613 [==============================] - 1s 78us/sample - loss: 0.4710\n",
      "Epoch 17/46\n",
      "7613/7613 [==============================] - ETA: 0s - loss: 0.464 - 1s 78us/sample - loss: 0.4638\n",
      "Epoch 18/46\n",
      "7613/7613 [==============================] - 1s 81us/sample - loss: 0.4570\n",
      "Epoch 19/46\n",
      "7613/7613 [==============================] - 1s 80us/sample - loss: 0.4506\n",
      "Epoch 20/46\n",
      "7613/7613 [==============================] - 1s 83us/sample - loss: 0.4447\n",
      "Epoch 21/46\n",
      "7613/7613 [==============================] - 1s 78us/sample - loss: 0.4390\n",
      "Epoch 22/46\n",
      "7613/7613 [==============================] - 1s 79us/sample - loss: 0.4337\n",
      "Epoch 23/46\n",
      "7613/7613 [==============================] - 1s 84us/sample - loss: 0.4286\n",
      "Epoch 24/46\n",
      "7613/7613 [==============================] - 1s 74us/sample - loss: 0.4238\n",
      "Epoch 25/46\n",
      "7613/7613 [==============================] - 1s 102us/sample - loss: 0.4192\n",
      "Epoch 26/46\n",
      "7613/7613 [==============================] - 1s 81us/sample - loss: 0.4149\n",
      "Epoch 27/46\n",
      "7613/7613 [==============================] - 1s 95us/sample - loss: 0.4108\n",
      "Epoch 28/46\n",
      "7613/7613 [==============================] - 1s 92us/sample - loss: 0.4069\n",
      "Epoch 29/46\n",
      "7613/7613 [==============================] - 1s 83us/sample - loss: 0.4031\n",
      "Epoch 30/46\n",
      "7613/7613 [==============================] - 1s 83us/sample - loss: 0.3996\n",
      "Epoch 31/46\n",
      "7613/7613 [==============================] - 1s 76us/sample - loss: 0.3962\n",
      "Epoch 32/46\n",
      "7613/7613 [==============================] - 1s 78us/sample - loss: 0.3929\n",
      "Epoch 33/46\n",
      "7613/7613 [==============================] - 1s 77us/sample - loss: 0.3898\n",
      "Epoch 34/46\n",
      "7613/7613 [==============================] - 1s 76us/sample - loss: 0.3868\n",
      "Epoch 35/46\n",
      "7613/7613 [==============================] - 1s 70us/sample - loss: 0.3840\n",
      "Epoch 36/46\n",
      "7613/7613 [==============================] - 1s 73us/sample - loss: 0.3812\n",
      "Epoch 37/46\n",
      "7613/7613 [==============================] - 1s 69us/sample - loss: 0.3785\n",
      "Epoch 38/46\n",
      "7613/7613 [==============================] - 1s 79us/sample - loss: 0.3760\n",
      "Epoch 39/46\n",
      "7613/7613 [==============================] - 1s 72us/sample - loss: 0.3735\n",
      "Epoch 40/46\n",
      "7613/7613 [==============================] - 1s 74us/sample - loss: 0.3711\n",
      "Epoch 41/46\n",
      "7613/7613 [==============================] - 1s 76us/sample - loss: 0.3689\n",
      "Epoch 42/46\n",
      "7613/7613 [==============================] - 1s 71us/sample - loss: 0.3667\n",
      "Epoch 43/46\n",
      "7613/7613 [==============================] - 1s 73us/sample - loss: 0.3645\n",
      "Epoch 44/46\n",
      "7613/7613 [==============================] - 1s 70us/sample - loss: 0.3625\n",
      "Epoch 45/46\n",
      "7613/7613 [==============================] - 1s 72us/sample - loss: 0.3605\n",
      "Epoch 46/46\n",
      "7613/7613 [==============================] - 1s 71us/sample - loss: 0.3585\n",
      "Predicting...\n"
     ]
    }
   ],
   "source": [
    "submission['target'] = make_logistic_predictions(46, 32, dftrain_min_tfidf, dftest_min_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission_lg_prep.csv', index=False)\n",
    "# Score: 78.22%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression, with bigrams, no preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Train on 7613 samples\n",
      "Epoch 1/27\n",
      "7613/7613 [==============================] - 1s 131us/sample - loss: 0.6371\n",
      "Epoch 2/27\n",
      "7613/7613 [==============================] - 1s 89us/sample - loss: 0.5661\n",
      "Epoch 3/27\n",
      "7613/7613 [==============================] - 1s 87us/sample - loss: 0.5247\n",
      "Epoch 4/27\n",
      "7613/7613 [==============================] - 1s 86us/sample - loss: 0.4959\n",
      "Epoch 5/27\n",
      "7613/7613 [==============================] - 1s 92us/sample - loss: 0.4739\n",
      "Epoch 6/27\n",
      "7613/7613 [==============================] - 1s 92us/sample - loss: 0.4563\n",
      "Epoch 7/27\n",
      "7613/7613 [==============================] - 1s 95us/sample - loss: 0.4414\n",
      "Epoch 8/27\n",
      "7613/7613 [==============================] - 1s 94us/sample - loss: 0.4288\n",
      "Epoch 9/27\n",
      "7613/7613 [==============================] - 1s 92us/sample - loss: 0.4177\n",
      "Epoch 10/27\n",
      "7613/7613 [==============================] - 1s 94us/sample - loss: 0.4080\n",
      "Epoch 11/27\n",
      "7613/7613 [==============================] - 1s 98us/sample - loss: 0.3992\n",
      "Epoch 12/27\n",
      "7613/7613 [==============================] - 1s 93us/sample - loss: 0.3912\n",
      "Epoch 13/27\n",
      "7613/7613 [==============================] - 1s 95us/sample - loss: 0.3840\n",
      "Epoch 14/27\n",
      "7613/7613 [==============================] - 1s 91us/sample - loss: 0.3773\n",
      "Epoch 15/27\n",
      "7613/7613 [==============================] - 1s 95us/sample - loss: 0.3712\n",
      "Epoch 16/27\n",
      "7613/7613 [==============================] - 1s 91us/sample - loss: 0.3655\n",
      "Epoch 17/27\n",
      "7613/7613 [==============================] - 1s 92us/sample - loss: 0.3602\n",
      "Epoch 18/27\n",
      "7613/7613 [==============================] - 1s 94us/sample - loss: 0.3552\n",
      "Epoch 19/27\n",
      "7613/7613 [==============================] - 1s 92us/sample - loss: 0.3505\n",
      "Epoch 20/27\n",
      "7613/7613 [==============================] - 1s 96us/sample - loss: 0.3461\n",
      "Epoch 21/27\n",
      "7613/7613 [==============================] - 1s 97us/sample - loss: 0.3420\n",
      "Epoch 22/27\n",
      "7613/7613 [==============================] - 1s 106us/sample - loss: 0.3380\n",
      "Epoch 23/27\n",
      "7613/7613 [==============================] - 1s 99us/sample - loss: 0.3342\n",
      "Epoch 24/27\n",
      "7613/7613 [==============================] - 1s 92us/sample - loss: 0.3307\n",
      "Epoch 25/27\n",
      "7613/7613 [==============================] - 1s 90us/sample - loss: 0.3273\n",
      "Epoch 26/27\n",
      "7613/7613 [==============================] - 1s 87us/sample - loss: 0.3239\n",
      "Epoch 27/27\n",
      "7613/7613 [==============================] - 1s 96us/sample - loss: 0.3208\n",
      "Predicting...\n"
     ]
    }
   ],
   "source": [
    "submission['target'] = make_logistic_predictions(27, 32, dftrain_bi_minimal, dftest_bi_minimal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission_lg_bigrams.csv', index=False)\n",
    "# Score: 79.55%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A final consideration was made to ensemble together a set of logistic regression models, with some boosting of incorrectly predicted samples. First, I did this without the document frequency threshold by accident, and the result was actually higher than when I used a document frequency threshold of 1 like I usually did. However, both final results were lower in score than the plain logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_x, train_y, batch_size, epochs):\n",
    "    nn = Sequential()\n",
    "    nn.add(Dense(1, activation='sigmoid', input_shape=(train_x.shape[1],)))\n",
    "    nn.compile(optimizer='adam', loss='binary_crossentropy', metrics=[])\n",
    "    nn.fit(train_x, train_y, batch_size = batch_size, epochs=epochs)\n",
    "    return nn\n",
    "\n",
    "def train_ensemble(n, batch_size, epochs, data_x, data_y):\n",
    "    #with tf.device('/GPU:0'):\n",
    "    models = []\n",
    "    total = data_x.shape[0]\n",
    "    all_inds = np.arange(data_x.shape[0])\n",
    "    dist = np.repeat(1, data_x.shape[0])\n",
    "    for i in range(n):\n",
    "        train_x = []\n",
    "        train_y = []\n",
    "        if i == 0:\n",
    "            # First run uses all samples\n",
    "            train_x = data_x\n",
    "            train_y = data_y\n",
    "        else:\n",
    "            # Next runs weight samples more if they came up as errors\n",
    "            inds = np.random.choice(all_inds, data_x.shape[0], replace=False, p=dist/total)\n",
    "            train_x = data_x[inds]\n",
    "            train_y = data_y[inds]\n",
    "        print('Training model', i)\n",
    "        # Todo: simple model for debug...\n",
    "        model = train_model(train_x, train_y, batch_size, epochs)\n",
    "        print('Making predictions')\n",
    "        raw_pred = model.predict(train_x) # Batch size has to match for this to work, apparently.\n",
    "        #print(raw_pred[0:10])\n",
    "        pred = np.argmax(raw_pred, axis=1)\n",
    "        #print(pred[0:10])\n",
    "        errors = np.not_equal(pred, train_y)\n",
    "        #print(train_y[0:10])\n",
    "        #print('Model {0} accuracy is {1:.3f}'.format(i, 100*(1-np.sum(errors)/train_y.shape[0])))\n",
    "        dist += errors\n",
    "        total += sum(errors)\n",
    "        models.append(model)\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model 0\n",
      "Train on 7613 samples\n",
      "Epoch 1/23\n",
      "7613/7613 [==============================] - 1s 126us/sample - loss: 0.6635\n",
      "Epoch 2/23\n",
      "7613/7613 [==============================] - 1s 88us/sample - loss: 0.6094\n",
      "Epoch 3/23\n",
      "7613/7613 [==============================] - 1s 90us/sample - loss: 0.5689\n",
      "Epoch 4/23\n",
      "7613/7613 [==============================] - 1s 87us/sample - loss: 0.5376\n",
      "Epoch 5/23\n",
      "7613/7613 [==============================] - 1s 81us/sample - loss: 0.5127\n",
      "Epoch 6/23\n",
      "7613/7613 [==============================] - 1s 87us/sample - loss: 0.4924\n",
      "Epoch 7/23\n",
      "7613/7613 [==============================] - 1s 87us/sample - loss: 0.4757\n",
      "Epoch 8/23\n",
      "7613/7613 [==============================] - 1s 85us/sample - loss: 0.4613\n",
      "Epoch 9/23\n",
      "7613/7613 [==============================] - 1s 86us/sample - loss: 0.4489\n",
      "Epoch 10/23\n",
      "7613/7613 [==============================] - 1s 87us/sample - loss: 0.4382\n",
      "Epoch 11/23\n",
      "7613/7613 [==============================] - 1s 95us/sample - loss: 0.4287\n",
      "Epoch 12/23\n",
      "7613/7613 [==============================] - 1s 89us/sample - loss: 0.4203\n",
      "Epoch 13/23\n",
      "7613/7613 [==============================] - 1s 81us/sample - loss: 0.4126\n",
      "Epoch 14/23\n",
      "7613/7613 [==============================] - 1s 83us/sample - loss: 0.4057\n",
      "Epoch 15/23\n",
      "7613/7613 [==============================] - 1s 85us/sample - loss: 0.3994\n",
      "Epoch 16/23\n",
      "7613/7613 [==============================] - 1s 86us/sample - loss: 0.3936\n",
      "Epoch 17/23\n",
      "7613/7613 [==============================] - 1s 86us/sample - loss: 0.3882\n",
      "Epoch 18/23\n",
      "7613/7613 [==============================] - 1s 87us/sample - loss: 0.3833\n",
      "Epoch 19/23\n",
      "7613/7613 [==============================] - 1s 88us/sample - loss: 0.3786\n",
      "Epoch 20/23\n",
      "7613/7613 [==============================] - 1s 87us/sample - loss: 0.3743\n",
      "Epoch 21/23\n",
      "7613/7613 [==============================] - 1s 88us/sample - loss: 0.3702\n",
      "Epoch 22/23\n",
      "7613/7613 [==============================] - 1s 89us/sample - loss: 0.3664\n",
      "Epoch 23/23\n",
      "7613/7613 [==============================] - 1s 98us/sample - loss: 0.3627\n",
      "Making predictions\n",
      "Training model 1\n",
      "Train on 7613 samples\n",
      "Epoch 1/23\n",
      "7613/7613 [==============================] - 1s 128us/sample - loss: 0.6647\n",
      "Epoch 2/23\n",
      "7613/7613 [==============================] - 1s 89us/sample - loss: 0.6099\n",
      "Epoch 3/23\n",
      "7613/7613 [==============================] - 1s 97us/sample - loss: 0.5694\n",
      "Epoch 4/23\n",
      "7613/7613 [==============================] - 1s 85us/sample - loss: 0.5381\n",
      "Epoch 5/23\n",
      "7613/7613 [==============================] - 1s 88us/sample - loss: 0.5133\n",
      "Epoch 6/23\n",
      "7613/7613 [==============================] - 1s 92us/sample - loss: 0.4930\n",
      "Epoch 7/23\n",
      "7613/7613 [==============================] - 1s 93us/sample - loss: 0.4761\n",
      "Epoch 8/23\n",
      "7613/7613 [==============================] - 1s 94us/sample - loss: 0.4618\n",
      "Epoch 9/23\n",
      "7613/7613 [==============================] - 1s 94us/sample - loss: 0.4495\n",
      "Epoch 10/23\n",
      "7613/7613 [==============================] - 1s 103us/sample - loss: 0.4387\n",
      "Epoch 11/23\n",
      "7613/7613 [==============================] - 1s 91us/sample - loss: 0.4292\n",
      "Epoch 12/23\n",
      "7613/7613 [==============================] - 1s 98us/sample - loss: 0.4207\n",
      "Epoch 13/23\n",
      "7613/7613 [==============================] - 1s 94us/sample - loss: 0.4130\n",
      "Epoch 14/23\n",
      "7613/7613 [==============================] - 1s 92us/sample - loss: 0.4061\n",
      "Epoch 15/23\n",
      "7613/7613 [==============================] - 1s 105us/sample - loss: 0.3997\n",
      "Epoch 16/23\n",
      "7613/7613 [==============================] - 1s 103us/sample - loss: 0.3939\n",
      "Epoch 17/23\n",
      "7613/7613 [==============================] - 1s 91us/sample - loss: 0.3886\n",
      "Epoch 18/23\n",
      "7613/7613 [==============================] - 1s 88us/sample - loss: 0.3836\n",
      "Epoch 19/23\n",
      "7613/7613 [==============================] - 1s 96us/sample - loss: 0.3789\n",
      "Epoch 20/23\n",
      "7613/7613 [==============================] - 1s 100us/sample - loss: 0.3746\n",
      "Epoch 21/23\n",
      "7613/7613 [==============================] - 1s 85us/sample - loss: 0.3705\n",
      "Epoch 22/23\n",
      "7613/7613 [==============================] - 1s 83us/sample - loss: 0.3666\n",
      "Epoch 23/23\n",
      "7613/7613 [==============================] - 1s 82us/sample - loss: 0.3630\n",
      "Making predictions\n",
      "Training model 2\n",
      "Train on 7613 samples\n",
      "Epoch 1/23\n",
      "7613/7613 [==============================] - 1s 108us/sample - loss: 0.6665\n",
      "Epoch 2/23\n",
      "7613/7613 [==============================] - 1s 87us/sample - loss: 0.6118\n",
      "Epoch 3/23\n",
      "7613/7613 [==============================] - 1s 91us/sample - loss: 0.5711\n",
      "Epoch 4/23\n",
      "7613/7613 [==============================] - 1s 86us/sample - loss: 0.5396\n",
      "Epoch 5/23\n",
      "7613/7613 [==============================] - 1s 95us/sample - loss: 0.5144\n",
      "Epoch 6/23\n",
      "7613/7613 [==============================] - 1s 84us/sample - loss: 0.4940\n",
      "Epoch 7/23\n",
      "7613/7613 [==============================] - 1s 85us/sample - loss: 0.4770\n",
      "Epoch 8/23\n",
      "7613/7613 [==============================] - 1s 98us/sample - loss: 0.4626\n",
      "Epoch 9/23\n",
      "7613/7613 [==============================] - 1s 105us/sample - loss: 0.4501\n",
      "Epoch 10/23\n",
      "7613/7613 [==============================] - 1s 96us/sample - loss: 0.4393\n",
      "Epoch 11/23\n",
      "7613/7613 [==============================] - 1s 103us/sample - loss: 0.4297\n",
      "Epoch 12/23\n",
      "7613/7613 [==============================] - 1s 79us/sample - loss: 0.4212\n",
      "Epoch 13/23\n",
      "7613/7613 [==============================] - 1s 79us/sample - loss: 0.4135\n",
      "Epoch 14/23\n",
      "7613/7613 [==============================] - 1s 74us/sample - loss: 0.4065\n",
      "Epoch 15/23\n",
      "7613/7613 [==============================] - 1s 86us/sample - loss: 0.4001\n",
      "Epoch 16/23\n",
      "7613/7613 [==============================] - 1s 81us/sample - loss: 0.3943\n",
      "Epoch 17/23\n",
      "7613/7613 [==============================] - 1s 77us/sample - loss: 0.3889\n",
      "Epoch 18/23\n",
      "7613/7613 [==============================] - 1s 76us/sample - loss: 0.3839\n",
      "Epoch 19/23\n",
      "7613/7613 [==============================] - 1s 90us/sample - loss: 0.3793\n",
      "Epoch 20/23\n",
      "7613/7613 [==============================] - 1s 80us/sample - loss: 0.3749\n",
      "Epoch 21/23\n",
      "7613/7613 [==============================] - 1s 84us/sample - loss: 0.3707\n",
      "Epoch 22/23\n",
      "7613/7613 [==============================] - 1s 122us/sample - loss: 0.3668\n",
      "Epoch 23/23\n",
      "7613/7613 [==============================] - 1s 120us/sample - loss: 0.3632\n",
      "Making predictions\n",
      "Training model 3\n",
      "Train on 7613 samples\n",
      "Epoch 1/23\n",
      "7613/7613 [==============================] - 1s 123us/sample - loss: 0.6642\n",
      "Epoch 2/23\n",
      "7613/7613 [==============================] - 1s 86us/sample - loss: 0.6098\n",
      "Epoch 3/23\n",
      "7613/7613 [==============================] - 1s 89us/sample - loss: 0.5693\n",
      "Epoch 4/23\n",
      "7613/7613 [==============================] - 1s 84us/sample - loss: 0.5381\n",
      "Epoch 5/23\n",
      "7613/7613 [==============================] - 1s 82us/sample - loss: 0.5133\n",
      "Epoch 6/23\n",
      "7613/7613 [==============================] - 1s 77us/sample - loss: 0.4930\n",
      "Epoch 7/23\n",
      "7613/7613 [==============================] - 1s 81us/sample - loss: 0.4761\n",
      "Epoch 8/23\n",
      "7613/7613 [==============================] - 1s 76us/sample - loss: 0.4618\n",
      "Epoch 9/23\n",
      "7613/7613 [==============================] - 1s 88us/sample - loss: 0.4495\n",
      "Epoch 10/23\n",
      "7613/7613 [==============================] - 1s 80us/sample - loss: 0.4388\n",
      "Epoch 11/23\n",
      "7613/7613 [==============================] - 1s 80us/sample - loss: 0.4292\n",
      "Epoch 12/23\n",
      "7613/7613 [==============================] - 1s 92us/sample - loss: 0.4207\n",
      "Epoch 13/23\n",
      "7613/7613 [==============================] - 1s 89us/sample - loss: 0.4131\n",
      "Epoch 14/23\n",
      "7613/7613 [==============================] - 1s 87us/sample - loss: 0.4062\n",
      "Epoch 15/23\n",
      "7613/7613 [==============================] - 1s 79us/sample - loss: 0.3998\n",
      "Epoch 16/23\n",
      "7613/7613 [==============================] - 1s 77us/sample - loss: 0.3940\n",
      "Epoch 17/23\n",
      "7613/7613 [==============================] - 1s 78us/sample - loss: 0.3886\n",
      "Epoch 18/23\n",
      "7613/7613 [==============================] - 1s 75us/sample - loss: 0.3836\n",
      "Epoch 19/23\n",
      "7613/7613 [==============================] - 1s 75us/sample - loss: 0.3790\n",
      "Epoch 20/23\n",
      "7613/7613 [==============================] - 1s 91us/sample - loss: 0.3746\n",
      "Epoch 21/23\n",
      "7613/7613 [==============================] - 1s 100us/sample - loss: 0.3705\n",
      "Epoch 22/23\n",
      "7613/7613 [==============================] - 1s 92us/sample - loss: 0.3667\n",
      "Epoch 23/23\n",
      "7613/7613 [==============================] - 1s 94us/sample - loss: 0.3631\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making predictions\n",
      "Training model 4\n",
      "Train on 7613 samples\n",
      "Epoch 1/23\n",
      "7613/7613 [==============================] - 1s 108us/sample - loss: 0.6661\n",
      "Epoch 2/23\n",
      "7613/7613 [==============================] - 1s 75us/sample - loss: 0.6115\n",
      "Epoch 3/23\n",
      "7613/7613 [==============================] - 1s 77us/sample - loss: 0.5706\n",
      "Epoch 4/23\n",
      "7613/7613 [==============================] - 1s 77us/sample - loss: 0.5391\n",
      "Epoch 5/23\n",
      "7613/7613 [==============================] - 1s 75us/sample - loss: 0.5141\n",
      "Epoch 6/23\n",
      "7613/7613 [==============================] - 1s 74us/sample - loss: 0.4937\n",
      "Epoch 7/23\n",
      "7613/7613 [==============================] - 1s 74us/sample - loss: 0.4767\n",
      "Epoch 8/23\n",
      "7613/7613 [==============================] - 1s 87us/sample - loss: 0.4623\n",
      "Epoch 9/23\n",
      "7613/7613 [==============================] - 1s 82us/sample - loss: 0.4499\n",
      "Epoch 10/23\n",
      "7613/7613 [==============================] - 1s 73us/sample - loss: 0.4391\n",
      "Epoch 11/23\n",
      "7613/7613 [==============================] - 1s 73us/sample - loss: 0.4295\n",
      "Epoch 12/23\n",
      "7613/7613 [==============================] - 1s 74us/sample - loss: 0.4209\n",
      "Epoch 13/23\n",
      "7613/7613 [==============================] - 1s 74us/sample - loss: 0.4133\n",
      "Epoch 14/23\n",
      "7613/7613 [==============================] - 1s 73us/sample - loss: 0.4063\n",
      "Epoch 15/23\n",
      "7613/7613 [==============================] - 1s 74us/sample - loss: 0.3999\n",
      "Epoch 16/23\n",
      "7613/7613 [==============================] - 1s 72us/sample - loss: 0.3941\n",
      "Epoch 17/23\n",
      "7613/7613 [==============================] - 1s 71us/sample - loss: 0.3887\n",
      "Epoch 18/23\n",
      "7613/7613 [==============================] - 1s 72us/sample - loss: 0.3837\n",
      "Epoch 19/23\n",
      "7613/7613 [==============================] - 1s 71us/sample - loss: 0.3790\n",
      "Epoch 20/23\n",
      "7613/7613 [==============================] - 1s 76us/sample - loss: 0.3747\n",
      "Epoch 21/23\n",
      "7613/7613 [==============================] - 1s 71us/sample - loss: 0.3706\n",
      "Epoch 22/23\n",
      "7613/7613 [==============================] - 1s 73us/sample - loss: 0.3667\n",
      "Epoch 23/23\n",
      "7613/7613 [==============================] - 1s 72us/sample - loss: 0.3630\n",
      "Making predictions\n",
      "Training model 5\n",
      "Train on 7613 samples\n",
      "Epoch 1/23\n",
      "7613/7613 [==============================] - 1s 100us/sample - loss: 0.6646\n",
      "Epoch 2/23\n",
      "7613/7613 [==============================] - 1s 72us/sample - loss: 0.6103\n",
      "Epoch 3/23\n",
      "7613/7613 [==============================] - 1s 73us/sample - loss: 0.5696\n",
      "Epoch 4/23\n",
      "7613/7613 [==============================] - 1s 73us/sample - loss: 0.5383\n",
      "Epoch 5/23\n",
      "7613/7613 [==============================] - 1s 73us/sample - loss: 0.5134\n",
      "Epoch 6/23\n",
      "7613/7613 [==============================] - 1s 76us/sample - loss: 0.4931\n",
      "Epoch 7/23\n",
      "7613/7613 [==============================] - 1s 76us/sample - loss: 0.4762\n",
      "Epoch 8/23\n",
      "7613/7613 [==============================] - 1s 73us/sample - loss: 0.4618\n",
      "Epoch 9/23\n",
      "7613/7613 [==============================] - 1s 81us/sample - loss: 0.4495\n",
      "Epoch 10/23\n",
      "7613/7613 [==============================] - 1s 81us/sample - loss: 0.4387\n",
      "Epoch 11/23\n",
      "7613/7613 [==============================] - 1s 80us/sample - loss: 0.4292\n",
      "Epoch 12/23\n",
      "7613/7613 [==============================] - 1s 81us/sample - loss: 0.4207\n",
      "Epoch 13/23\n",
      "7613/7613 [==============================] - 1s 85us/sample - loss: 0.4131\n",
      "Epoch 14/23\n",
      "7613/7613 [==============================] - 1s 80us/sample - loss: 0.4061\n",
      "Epoch 15/23\n",
      "7613/7613 [==============================] - 1s 86us/sample - loss: 0.3998\n",
      "Epoch 16/23\n",
      "7613/7613 [==============================] - 1s 82us/sample - loss: 0.3940\n",
      "Epoch 17/23\n",
      "7613/7613 [==============================] - 1s 80us/sample - loss: 0.3886\n",
      "Epoch 18/23\n",
      "7613/7613 [==============================] - 1s 74us/sample - loss: 0.3836\n",
      "Epoch 19/23\n",
      "7613/7613 [==============================] - 1s 86us/sample - loss: 0.3789\n",
      "Epoch 20/23\n",
      "7613/7613 [==============================] - 1s 81us/sample - loss: 0.3746\n",
      "Epoch 21/23\n",
      "7613/7613 [==============================] - 1s 77us/sample - loss: 0.3705\n",
      "Epoch 22/23\n",
      "7613/7613 [==============================] - 1s 78us/sample - loss: 0.3667\n",
      "Epoch 23/23\n",
      "7613/7613 [==============================] - 1s 83us/sample - loss: 0.3630\n",
      "Making predictions\n",
      "Training model 6\n",
      "Train on 7613 samples\n",
      "Epoch 1/23\n",
      "7613/7613 [==============================] - 1s 103us/sample - loss: 0.6646\n",
      "Epoch 2/23\n",
      "7613/7613 [==============================] - 1s 78us/sample - loss: 0.6102\n",
      "Epoch 3/23\n",
      "7613/7613 [==============================] - 1s 77us/sample - loss: 0.5696\n",
      "Epoch 4/23\n",
      "7613/7613 [==============================] - 1s 76us/sample - loss: 0.5383\n",
      "Epoch 5/23\n",
      "7613/7613 [==============================] - 1s 77us/sample - loss: 0.5135\n",
      "Epoch 6/23\n",
      "7613/7613 [==============================] - 1s 74us/sample - loss: 0.4932\n",
      "Epoch 7/23\n",
      "7613/7613 [==============================] - 1s 74us/sample - loss: 0.4764\n",
      "Epoch 8/23\n",
      "7613/7613 [==============================] - 1s 75us/sample - loss: 0.4620\n",
      "Epoch 9/23\n",
      "7613/7613 [==============================] - 1s 83us/sample - loss: 0.4497\n",
      "Epoch 10/23\n",
      "7613/7613 [==============================] - 1s 95us/sample - loss: 0.4389\n",
      "Epoch 11/23\n",
      "7613/7613 [==============================] - 1s 78us/sample - loss: 0.4294\n",
      "Epoch 12/23\n",
      "7613/7613 [==============================] - 1s 79us/sample - loss: 0.4209\n",
      "Epoch 13/23\n",
      "7613/7613 [==============================] - 1s 77us/sample - loss: 0.4133\n",
      "Epoch 14/23\n",
      "7613/7613 [==============================] - 1s 76us/sample - loss: 0.4063\n",
      "Epoch 15/23\n",
      "7613/7613 [==============================] - 1s 79us/sample - loss: 0.3999\n",
      "Epoch 16/23\n",
      "7613/7613 [==============================] - 1s 88us/sample - loss: 0.3941\n",
      "Epoch 17/23\n",
      "7613/7613 [==============================] - 1s 85us/sample - loss: 0.3887\n",
      "Epoch 18/23\n",
      "7613/7613 [==============================] - 1s 82us/sample - loss: 0.3838\n",
      "Epoch 19/23\n",
      "7613/7613 [==============================] - 1s 80us/sample - loss: 0.3791\n",
      "Epoch 20/23\n",
      "7613/7613 [==============================] - 1s 83us/sample - loss: 0.3747\n",
      "Epoch 21/23\n",
      "7613/7613 [==============================] - 1s 79us/sample - loss: 0.3706\n",
      "Epoch 22/23\n",
      "7613/7613 [==============================] - 1s 76us/sample - loss: 0.3668\n",
      "Epoch 23/23\n",
      "7613/7613 [==============================] - 1s 78us/sample - loss: 0.3632\n",
      "Making predictions\n",
      "Training model 7\n",
      "Train on 7613 samples\n",
      "Epoch 1/23\n",
      "7613/7613 [==============================] - 1s 107us/sample - loss: 0.6645\n",
      "Epoch 2/23\n",
      "7613/7613 [==============================] - 1s 76us/sample - loss: 0.6100\n",
      "Epoch 3/23\n",
      "7613/7613 [==============================] - 1s 75us/sample - loss: 0.5693\n",
      "Epoch 4/23\n",
      "7613/7613 [==============================] - 1s 76us/sample - loss: 0.5379\n",
      "Epoch 5/23\n",
      "7613/7613 [==============================] - 1s 75us/sample - loss: 0.5131\n",
      "Epoch 6/23\n",
      "7613/7613 [==============================] - 1s 76us/sample - loss: 0.4928\n",
      "Epoch 7/23\n",
      "7613/7613 [==============================] - 1s 76us/sample - loss: 0.4759\n",
      "Epoch 8/23\n",
      "7613/7613 [==============================] - 1s 74us/sample - loss: 0.4616\n",
      "Epoch 9/23\n",
      "7613/7613 [==============================] - 1s 75us/sample - loss: 0.4493\n",
      "Epoch 10/23\n",
      "7613/7613 [==============================] - 1s 75us/sample - loss: 0.4385\n",
      "Epoch 11/23\n",
      "7613/7613 [==============================] - 1s 75us/sample - loss: 0.4290\n",
      "Epoch 12/23\n",
      "7613/7613 [==============================] - 1s 79us/sample - loss: 0.4206\n",
      "Epoch 13/23\n",
      "7613/7613 [==============================] - 1s 76us/sample - loss: 0.4129\n",
      "Epoch 14/23\n",
      "7613/7613 [==============================] - 1s 76us/sample - loss: 0.4060\n",
      "Epoch 15/23\n",
      "7613/7613 [==============================] - 1s 75us/sample - loss: 0.3997\n",
      "Epoch 16/23\n",
      "7613/7613 [==============================] - 1s 75us/sample - loss: 0.3939\n",
      "Epoch 17/23\n",
      "7613/7613 [==============================] - 1s 76us/sample - loss: 0.3885\n",
      "Epoch 18/23\n",
      "7613/7613 [==============================] - 1s 77us/sample - loss: 0.3835\n",
      "Epoch 19/23\n",
      "7613/7613 [==============================] - 1s 78us/sample - loss: 0.3789\n",
      "Epoch 20/23\n",
      "7613/7613 [==============================] - 1s 79us/sample - loss: 0.3745\n",
      "Epoch 21/23\n",
      "7613/7613 [==============================] - 1s 79us/sample - loss: 0.3704\n",
      "Epoch 22/23\n",
      "7613/7613 [==============================] - 1s 77us/sample - loss: 0.3666\n",
      "Epoch 23/23\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7613/7613 [==============================] - 1s 78us/sample - loss: 0.3630\n",
      "Making predictions\n",
      "Training model 8\n",
      "Train on 7613 samples\n",
      "Epoch 1/23\n",
      "7613/7613 [==============================] - 1s 106us/sample - loss: 0.6653\n",
      "Epoch 2/23\n",
      "7613/7613 [==============================] - 1s 78us/sample - loss: 0.6105\n",
      "Epoch 3/23\n",
      "7613/7613 [==============================] - 1s 77us/sample - loss: 0.5696\n",
      "Epoch 4/23\n",
      "7613/7613 [==============================] - 1s 78us/sample - loss: 0.5381\n",
      "Epoch 5/23\n",
      "7613/7613 [==============================] - 1s 85us/sample - loss: 0.5132\n",
      "Epoch 6/23\n",
      "7613/7613 [==============================] - 1s 77us/sample - loss: 0.4929\n",
      "Epoch 7/23\n",
      "7613/7613 [==============================] - 1s 75us/sample - loss: 0.4760\n",
      "Epoch 8/23\n",
      "7613/7613 [==============================] - 1s 77us/sample - loss: 0.4617\n",
      "Epoch 9/23\n",
      "7613/7613 [==============================] - 1s 76us/sample - loss: 0.4493\n",
      "Epoch 10/23\n",
      "7613/7613 [==============================] - 1s 87us/sample - loss: 0.4385\n",
      "Epoch 11/23\n",
      "7613/7613 [==============================] - 1s 97us/sample - loss: 0.4290\n",
      "Epoch 12/23\n",
      "7613/7613 [==============================] - 1s 77us/sample - loss: 0.4205\n",
      "Epoch 13/23\n",
      "7613/7613 [==============================] - 1s 77us/sample - loss: 0.4128\n",
      "Epoch 14/23\n",
      "7613/7613 [==============================] - 1s 77us/sample - loss: 0.4059\n",
      "Epoch 15/23\n",
      "7613/7613 [==============================] - 1s 76us/sample - loss: 0.3996\n",
      "Epoch 16/23\n",
      "7613/7613 [==============================] - 1s 86us/sample - loss: 0.3938\n",
      "Epoch 17/23\n",
      "7613/7613 [==============================] - 1s 75us/sample - loss: 0.3884\n",
      "Epoch 18/23\n",
      "7613/7613 [==============================] - 1s 81us/sample - loss: 0.3834\n",
      "Epoch 19/23\n",
      "7613/7613 [==============================] - 1s 81us/sample - loss: 0.3787\n",
      "Epoch 20/23\n",
      "7613/7613 [==============================] - 1s 88us/sample - loss: 0.3744\n",
      "Epoch 21/23\n",
      "7613/7613 [==============================] - 1s 75us/sample - loss: 0.3703\n",
      "Epoch 22/23\n",
      "7613/7613 [==============================] - 1s 77us/sample - loss: 0.3665\n",
      "Epoch 23/23\n",
      "7613/7613 [==============================] - 1s 78us/sample - loss: 0.3629\n",
      "Making predictions\n",
      "Training model 9\n",
      "Train on 7613 samples\n",
      "Epoch 1/23\n",
      "7613/7613 [==============================] - 1s 107us/sample - loss: 0.6657\n",
      "Epoch 2/23\n",
      "7613/7613 [==============================] - 1s 79us/sample - loss: 0.6110\n",
      "Epoch 3/23\n",
      "7613/7613 [==============================] - 1s 79us/sample - loss: 0.5703\n",
      "Epoch 4/23\n",
      "7613/7613 [==============================] - 1s 77us/sample - loss: 0.5389\n",
      "Epoch 5/23\n",
      "7613/7613 [==============================] - 1s 82us/sample - loss: 0.5139\n",
      "Epoch 6/23\n",
      "7613/7613 [==============================] - 1s 79us/sample - loss: 0.4935\n",
      "Epoch 7/23\n",
      "7613/7613 [==============================] - 1s 79us/sample - loss: 0.4766\n",
      "Epoch 8/23\n",
      "7613/7613 [==============================] - 1s 80us/sample - loss: 0.4622\n",
      "Epoch 9/23\n",
      "7613/7613 [==============================] - 1s 77us/sample - loss: 0.4497\n",
      "Epoch 10/23\n",
      "7613/7613 [==============================] - 1s 79us/sample - loss: 0.4390\n",
      "Epoch 11/23\n",
      "7613/7613 [==============================] - 1s 84us/sample - loss: 0.4294\n",
      "Epoch 12/23\n",
      "7613/7613 [==============================] - 1s 81us/sample - loss: 0.4209\n",
      "Epoch 13/23\n",
      "7613/7613 [==============================] - 1s 79us/sample - loss: 0.4132\n",
      "Epoch 14/23\n",
      "7613/7613 [==============================] - 1s 77us/sample - loss: 0.4063\n",
      "Epoch 15/23\n",
      "7613/7613 [==============================] - 1s 77us/sample - loss: 0.3999\n",
      "Epoch 16/23\n",
      "7613/7613 [==============================] - 1s 75us/sample - loss: 0.3941\n",
      "Epoch 17/23\n",
      "7613/7613 [==============================] - 1s 76us/sample - loss: 0.3887\n",
      "Epoch 18/23\n",
      "7613/7613 [==============================] - 1s 80us/sample - loss: 0.3837\n",
      "Epoch 19/23\n",
      "7613/7613 [==============================] - 1s 77us/sample - loss: 0.3790\n",
      "Epoch 20/23\n",
      "7613/7613 [==============================] - 1s 76us/sample - loss: 0.3747\n",
      "Epoch 21/23\n",
      "7613/7613 [==============================] - 1s 76us/sample - loss: 0.3706\n",
      "Epoch 22/23\n",
      "7613/7613 [==============================] - 1s 80us/sample - loss: 0.3667\n",
      "Epoch 23/23\n",
      "7613/7613 [==============================] - 1s 76us/sample - loss: 0.3630\n",
      "Making predictions\n",
      "Training model 10\n",
      "Train on 7613 samples\n",
      "Epoch 1/23\n",
      "7613/7613 [==============================] - 1s 119us/sample - loss: 0.6662\n",
      "Epoch 2/23\n",
      "7613/7613 [==============================] - 1s 87us/sample - loss: 0.6117\n",
      "Epoch 3/23\n",
      "7613/7613 [==============================] - 1s 81us/sample - loss: 0.5707\n",
      "Epoch 4/23\n",
      "7613/7613 [==============================] - 1s 88us/sample - loss: 0.5391\n",
      "Epoch 5/23\n",
      "7613/7613 [==============================] - 1s 103us/sample - loss: 0.5140\n",
      "Epoch 6/23\n",
      "7613/7613 [==============================] - 1s 94us/sample - loss: 0.4935\n",
      "Epoch 7/23\n",
      "7613/7613 [==============================] - 1s 84us/sample - loss: 0.4765\n",
      "Epoch 8/23\n",
      "7613/7613 [==============================] - 1s 76us/sample - loss: 0.4621\n",
      "Epoch 9/23\n",
      "7613/7613 [==============================] - 1s 75us/sample - loss: 0.4496\n",
      "Epoch 10/23\n",
      "7613/7613 [==============================] - 1s 77us/sample - loss: 0.4388\n",
      "Epoch 11/23\n",
      "7613/7613 [==============================] - 1s 90us/sample - loss: 0.4292\n",
      "Epoch 12/23\n",
      "7613/7613 [==============================] - 1s 83us/sample - loss: 0.4206\n",
      "Epoch 13/23\n",
      "7613/7613 [==============================] - 1s 87us/sample - loss: 0.4130\n",
      "Epoch 14/23\n",
      "7613/7613 [==============================] - 1s 79us/sample - loss: 0.4060\n",
      "Epoch 15/23\n",
      "7613/7613 [==============================] - 1s 80us/sample - loss: 0.3997\n",
      "Epoch 16/23\n",
      "7613/7613 [==============================] - 1s 81us/sample - loss: 0.3939\n",
      "Epoch 17/23\n",
      "7613/7613 [==============================] - 1s 78us/sample - loss: 0.3884\n",
      "Epoch 18/23\n",
      "7613/7613 [==============================] - 1s 79us/sample - loss: 0.3835\n",
      "Epoch 19/23\n",
      "7613/7613 [==============================] - 1s 80us/sample - loss: 0.3788\n",
      "Epoch 20/23\n",
      "7613/7613 [==============================] - 1s 79us/sample - loss: 0.3745\n",
      "Epoch 21/23\n",
      "7613/7613 [==============================] - 1s 77us/sample - loss: 0.3703\n",
      "Epoch 22/23\n",
      "7613/7613 [==============================] - 1s 76us/sample - loss: 0.3665\n",
      "Epoch 23/23\n",
      "7613/7613 [==============================] - 1s 76us/sample - loss: 0.3629\n",
      "Making predictions\n",
      "Training model 11\n",
      "Train on 7613 samples\n",
      "Epoch 1/23\n",
      "7613/7613 [==============================] - 1s 108us/sample - loss: 0.6657\n",
      "Epoch 2/23\n",
      "7613/7613 [==============================] - 1s 85us/sample - loss: 0.6106\n",
      "Epoch 3/23\n",
      "7613/7613 [==============================] - 1s 80us/sample - loss: 0.56970s - lo\n",
      "Epoch 4/23\n",
      "7613/7613 [==============================] - 1s 90us/sample - loss: 0.5383\n",
      "Epoch 5/23\n",
      "7613/7613 [==============================] - 1s 83us/sample - loss: 0.5133\n",
      "Epoch 6/23\n",
      "7613/7613 [==============================] - 1s 83us/sample - loss: 0.49300s - l\n",
      "Epoch 7/23\n",
      "7613/7613 [==============================] - 1s 84us/sample - loss: 0.4762\n",
      "Epoch 8/23\n",
      "7613/7613 [==============================] - 1s 79us/sample - loss: 0.4618\n",
      "Epoch 9/23\n",
      "7613/7613 [==============================] - 1s 81us/sample - loss: 0.4494\n",
      "Epoch 10/23\n",
      "7613/7613 [==============================] - 1s 79us/sample - loss: 0.4387\n",
      "Epoch 11/23\n",
      "7613/7613 [==============================] - 1s 78us/sample - loss: 0.4291\n",
      "Epoch 12/23\n",
      "7613/7613 [==============================] - 1s 78us/sample - loss: 0.4206\n",
      "Epoch 13/23\n",
      "7613/7613 [==============================] - 1s 79us/sample - loss: 0.4130\n",
      "Epoch 14/23\n",
      "7613/7613 [==============================] - 1s 79us/sample - loss: 0.4061\n",
      "Epoch 15/23\n",
      "7613/7613 [==============================] - 1s 80us/sample - loss: 0.3997\n",
      "Epoch 16/23\n",
      "7613/7613 [==============================] - 1s 80us/sample - loss: 0.3939\n",
      "Epoch 17/23\n",
      "7613/7613 [==============================] - 1s 81us/sample - loss: 0.3885\n",
      "Epoch 18/23\n",
      "7613/7613 [==============================] - 1s 79us/sample - loss: 0.3835\n",
      "Epoch 19/23\n",
      "7613/7613 [==============================] - 1s 78us/sample - loss: 0.3789\n",
      "Epoch 20/23\n",
      "7613/7613 [==============================] - 1s 79us/sample - loss: 0.3745\n",
      "Epoch 21/23\n",
      "7613/7613 [==============================] - 1s 79us/sample - loss: 0.3705\n",
      "Epoch 22/23\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7613/7613 [==============================] - 1s 76us/sample - loss: 0.3666\n",
      "Epoch 23/23\n",
      "7613/7613 [==============================] - 1s 79us/sample - loss: 0.3629\n",
      "Making predictions\n",
      "Training model 12\n",
      "Train on 7613 samples\n",
      "Epoch 1/23\n",
      "7613/7613 [==============================] - 1s 114us/sample - loss: 0.6657\n",
      "Epoch 2/23\n",
      "7613/7613 [==============================] - 1s 82us/sample - loss: 0.6111\n",
      "Epoch 3/23\n",
      "7613/7613 [==============================] - 1s 78us/sample - loss: 0.5704\n",
      "Epoch 4/23\n",
      "7613/7613 [==============================] - 1s 78us/sample - loss: 0.5389\n",
      "Epoch 5/23\n",
      "7613/7613 [==============================] - 1s 82us/sample - loss: 0.5139\n",
      "Epoch 6/23\n",
      "7613/7613 [==============================] - 1s 77us/sample - loss: 0.4935\n",
      "Epoch 7/23\n",
      "7613/7613 [==============================] - 1s 82us/sample - loss: 0.4765\n",
      "Epoch 8/23\n",
      "7613/7613 [==============================] - 1s 76us/sample - loss: 0.4621\n",
      "Epoch 9/23\n",
      "7613/7613 [==============================] - 1s 79us/sample - loss: 0.4497\n",
      "Epoch 10/23\n",
      "7613/7613 [==============================] - 1s 79us/sample - loss: 0.4389\n",
      "Epoch 11/23\n",
      "7613/7613 [==============================] - 1s 91us/sample - loss: 0.4294\n",
      "Epoch 12/23\n",
      "7613/7613 [==============================] - 1s 84us/sample - loss: 0.4208\n",
      "Epoch 13/23\n",
      "7613/7613 [==============================] - 1s 86us/sample - loss: 0.4131\n",
      "Epoch 14/23\n",
      "7613/7613 [==============================] - 1s 79us/sample - loss: 0.4062\n",
      "Epoch 15/23\n",
      "7613/7613 [==============================] - 1s 79us/sample - loss: 0.3999\n",
      "Epoch 16/23\n",
      "7613/7613 [==============================] - 1s 79us/sample - loss: 0.3940\n",
      "Epoch 17/23\n",
      "7613/7613 [==============================] - 1s 77us/sample - loss: 0.38870s - los\n",
      "Epoch 18/23\n",
      "7613/7613 [==============================] - 1s 87us/sample - loss: 0.3836\n",
      "Epoch 19/23\n",
      "7613/7613 [==============================] - 1s 76us/sample - loss: 0.3790\n",
      "Epoch 20/23\n",
      "7613/7613 [==============================] - 1s 80us/sample - loss: 0.3746\n",
      "Epoch 21/23\n",
      "7613/7613 [==============================] - 1s 80us/sample - loss: 0.3705\n",
      "Epoch 22/23\n",
      "7613/7613 [==============================] - 1s 78us/sample - loss: 0.3667\n",
      "Epoch 23/23\n",
      "7613/7613 [==============================] - 1s 77us/sample - loss: 0.3630\n",
      "Making predictions\n",
      "Training model 13\n",
      "Train on 7613 samples\n",
      "Epoch 1/23\n",
      "7613/7613 [==============================] - 1s 108us/sample - loss: 0.6647\n",
      "Epoch 2/23\n",
      "7613/7613 [==============================] - 1s 85us/sample - loss: 0.6103\n",
      "Epoch 3/23\n",
      "7613/7613 [==============================] - 1s 79us/sample - loss: 0.5696\n",
      "Epoch 4/23\n",
      "7613/7613 [==============================] - 1s 79us/sample - loss: 0.5383\n",
      "Epoch 5/23\n",
      "7613/7613 [==============================] - 1s 78us/sample - loss: 0.5134\n",
      "Epoch 6/23\n",
      "7613/7613 [==============================] - 1s 83us/sample - loss: 0.4931\n",
      "Epoch 7/23\n",
      "7613/7613 [==============================] - 1s 80us/sample - loss: 0.4762\n",
      "Epoch 8/23\n",
      "7613/7613 [==============================] - 1s 76us/sample - loss: 0.4618\n",
      "Epoch 9/23\n",
      "7613/7613 [==============================] - 1s 77us/sample - loss: 0.4494\n",
      "Epoch 10/23\n",
      "7613/7613 [==============================] - 1s 76us/sample - loss: 0.4386\n",
      "Epoch 11/23\n",
      "7613/7613 [==============================] - 1s 87us/sample - loss: 0.4291\n",
      "Epoch 12/23\n",
      "7613/7613 [==============================] - 1s 80us/sample - loss: 0.4206\n",
      "Epoch 13/23\n",
      "7613/7613 [==============================] - 1s 83us/sample - loss: 0.4130\n",
      "Epoch 14/23\n",
      "7613/7613 [==============================] - 1s 78us/sample - loss: 0.4060\n",
      "Epoch 15/23\n",
      "7613/7613 [==============================] - 1s 78us/sample - loss: 0.3997\n",
      "Epoch 16/23\n",
      "7613/7613 [==============================] - 1s 77us/sample - loss: 0.3939\n",
      "Epoch 17/23\n",
      "7613/7613 [==============================] - 1s 77us/sample - loss: 0.3885\n",
      "Epoch 18/23\n",
      "7613/7613 [==============================] - 1s 77us/sample - loss: 0.3835\n",
      "Epoch 19/23\n",
      "7613/7613 [==============================] - 1s 80us/sample - loss: 0.3789\n",
      "Epoch 20/23\n",
      "7613/7613 [==============================] - 1s 81us/sample - loss: 0.3745\n",
      "Epoch 21/23\n",
      "7613/7613 [==============================] - 1s 80us/sample - loss: 0.3704\n",
      "Epoch 22/23\n",
      "7613/7613 [==============================] - 1s 78us/sample - loss: 0.3666\n",
      "Epoch 23/23\n",
      "7613/7613 [==============================] - 1s 76us/sample - loss: 0.3629\n",
      "Making predictions\n",
      "Training model 14\n",
      "Train on 7613 samples\n",
      "Epoch 1/23\n",
      "7613/7613 [==============================] - 1s 109us/sample - loss: 0.6655\n",
      "Epoch 2/23\n",
      "7613/7613 [==============================] - 1s 76us/sample - loss: 0.6107\n",
      "Epoch 3/23\n",
      "7613/7613 [==============================] - 1s 77us/sample - loss: 0.5699\n",
      "Epoch 4/23\n",
      "7613/7613 [==============================] - 1s 77us/sample - loss: 0.5385\n",
      "Epoch 5/23\n",
      "7613/7613 [==============================] - 1s 76us/sample - loss: 0.5135\n",
      "Epoch 6/23\n",
      "7613/7613 [==============================] - 1s 82us/sample - loss: 0.4932\n",
      "Epoch 7/23\n",
      "7613/7613 [==============================] - 1s 77us/sample - loss: 0.4762\n",
      "Epoch 8/23\n",
      "7613/7613 [==============================] - 1s 77us/sample - loss: 0.4618\n",
      "Epoch 9/23\n",
      "7613/7613 [==============================] - 1s 92us/sample - loss: 0.4495\n",
      "Epoch 10/23\n",
      "7613/7613 [==============================] - 1s 87us/sample - loss: 0.4387\n",
      "Epoch 11/23\n",
      "7613/7613 [==============================] - 1s 103us/sample - loss: 0.4292\n",
      "Epoch 12/23\n",
      "7613/7613 [==============================] - 1s 92us/sample - loss: 0.4207\n",
      "Epoch 13/23\n",
      "7613/7613 [==============================] - 1s 88us/sample - loss: 0.4130\n",
      "Epoch 14/23\n",
      "7613/7613 [==============================] - 1s 80us/sample - loss: 0.4061\n",
      "Epoch 15/23\n",
      "7613/7613 [==============================] - 1s 81us/sample - loss: 0.3997\n",
      "Epoch 16/23\n",
      "7613/7613 [==============================] - 1s 83us/sample - loss: 0.3939\n",
      "Epoch 17/23\n",
      "7613/7613 [==============================] - 1s 80us/sample - loss: 0.3886\n",
      "Epoch 18/23\n",
      "7613/7613 [==============================] - 1s 81us/sample - loss: 0.3836\n",
      "Epoch 19/23\n",
      "7613/7613 [==============================] - 1s 81us/sample - loss: 0.3789\n",
      "Epoch 20/23\n",
      "7613/7613 [==============================] - 1s 81us/sample - loss: 0.3745\n",
      "Epoch 21/23\n",
      "7613/7613 [==============================] - 1s 79us/sample - loss: 0.3705\n",
      "Epoch 22/23\n",
      "7613/7613 [==============================] - 1s 80us/sample - loss: 0.3666\n",
      "Epoch 23/23\n",
      "7613/7613 [==============================] - 1s 77us/sample - loss: 0.3630\n",
      "Making predictions\n",
      "Training model 15\n",
      "Train on 7613 samples\n",
      "Epoch 1/23\n",
      "7613/7613 [==============================] - 1s 104us/sample - loss: 0.6655\n",
      "Epoch 2/23\n",
      "7613/7613 [==============================] - 1s 79us/sample - loss: 0.6106\n",
      "Epoch 3/23\n",
      "7613/7613 [==============================] - 1s 76us/sample - loss: 0.5698\n",
      "Epoch 4/23\n",
      "7613/7613 [==============================] - 1s 77us/sample - loss: 0.5383\n",
      "Epoch 5/23\n",
      "7613/7613 [==============================] - 1s 76us/sample - loss: 0.5134\n",
      "Epoch 6/23\n",
      "7613/7613 [==============================] - 1s 76us/sample - loss: 0.4930\n",
      "Epoch 7/23\n",
      "7613/7613 [==============================] - 1s 79us/sample - loss: 0.4761\n",
      "Epoch 8/23\n",
      "7613/7613 [==============================] - 1s 79us/sample - loss: 0.46170s - l\n",
      "Epoch 9/23\n",
      "7613/7613 [==============================] - 1s 77us/sample - loss: 0.4493\n",
      "Epoch 10/23\n",
      "7613/7613 [==============================] - 1s 76us/sample - loss: 0.4385\n",
      "Epoch 11/23\n",
      "7613/7613 [==============================] - 1s 83us/sample - loss: 0.4290\n",
      "Epoch 12/23\n",
      "7613/7613 [==============================] - 1s 84us/sample - loss: 0.4205\n",
      "Epoch 13/23\n",
      "7613/7613 [==============================] - 1s 84us/sample - loss: 0.4128\n",
      "Epoch 14/23\n",
      "7613/7613 [==============================] - 1s 87us/sample - loss: 0.4059\n",
      "Epoch 15/23\n",
      "7613/7613 [==============================] - 1s 80us/sample - loss: 0.3996\n",
      "Epoch 16/23\n",
      "7613/7613 [==============================] - 1s 82us/sample - loss: 0.3937\n",
      "Epoch 17/23\n",
      "7613/7613 [==============================] - 1s 83us/sample - loss: 0.3884\n",
      "Epoch 18/23\n",
      "7613/7613 [==============================] - 1s 77us/sample - loss: 0.3834\n",
      "Epoch 19/23\n",
      "7613/7613 [==============================] - 1s 77us/sample - loss: 0.3787\n",
      "Epoch 20/23\n",
      "7613/7613 [==============================] - 1s 79us/sample - loss: 0.3744\n",
      "Epoch 21/23\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7613/7613 [==============================] - 1s 79us/sample - loss: 0.3703\n",
      "Epoch 22/23\n",
      "7613/7613 [==============================] - 1s 78us/sample - loss: 0.3664\n",
      "Epoch 23/23\n",
      "7613/7613 [==============================] - 1s 76us/sample - loss: 0.3628\n",
      "Making predictions\n",
      "Training model 16\n",
      "Train on 7613 samples\n",
      "Epoch 1/23\n",
      "7613/7613 [==============================] - 1s 107us/sample - loss: 0.6645\n",
      "Epoch 2/23\n",
      "7613/7613 [==============================] - 1s 83us/sample - loss: 0.6098\n",
      "Epoch 3/23\n",
      "7613/7613 [==============================] - 1s 81us/sample - loss: 0.5690\n",
      "Epoch 4/23\n",
      "7613/7613 [==============================] - 1s 81us/sample - loss: 0.5376\n",
      "Epoch 5/23\n",
      "7613/7613 [==============================] - 1s 79us/sample - loss: 0.5128\n",
      "Epoch 6/23\n",
      "7613/7613 [==============================] - 1s 79us/sample - loss: 0.4925\n",
      "Epoch 7/23\n",
      "7613/7613 [==============================] - 1s 81us/sample - loss: 0.4756\n",
      "Epoch 8/23\n",
      "7613/7613 [==============================] - 1s 85us/sample - loss: 0.4612\n",
      "Epoch 9/23\n",
      "7613/7613 [==============================] - 1s 78us/sample - loss: 0.4489\n",
      "Epoch 10/23\n",
      "7613/7613 [==============================] - 1s 91us/sample - loss: 0.4382\n",
      "Epoch 11/23\n",
      "7613/7613 [==============================] - 1s 93us/sample - loss: 0.4287\n",
      "Epoch 12/23\n",
      "7613/7613 [==============================] - 1s 88us/sample - loss: 0.4202\n",
      "Epoch 13/23\n",
      "7613/7613 [==============================] - 1s 86us/sample - loss: 0.4126\n",
      "Epoch 14/23\n",
      "7613/7613 [==============================] - 1s 84us/sample - loss: 0.4057\n",
      "Epoch 15/23\n",
      "7613/7613 [==============================] - 1s 80us/sample - loss: 0.3994\n",
      "Epoch 16/23\n",
      "7613/7613 [==============================] - 1s 80us/sample - loss: 0.3936\n",
      "Epoch 17/23\n",
      "7613/7613 [==============================] - 1s 79us/sample - loss: 0.3882\n",
      "Epoch 18/23\n",
      "7613/7613 [==============================] - 1s 84us/sample - loss: 0.3833\n",
      "Epoch 19/23\n",
      "7613/7613 [==============================] - 1s 81us/sample - loss: 0.3786\n",
      "Epoch 20/23\n",
      "7613/7613 [==============================] - 1s 87us/sample - loss: 0.3743\n",
      "Epoch 21/23\n",
      "7613/7613 [==============================] - 1s 83us/sample - loss: 0.3702\n",
      "Epoch 22/23\n",
      "7613/7613 [==============================] - 1s 80us/sample - loss: 0.3664\n",
      "Epoch 23/23\n",
      "7613/7613 [==============================] - 1s 88us/sample - loss: 0.3628\n",
      "Making predictions\n",
      "Training model 17\n",
      "Train on 7613 samples\n",
      "Epoch 1/23\n",
      "7613/7613 [==============================] - 1s 111us/sample - loss: 0.6646\n",
      "Epoch 2/23\n",
      "7613/7613 [==============================] - 1s 90us/sample - loss: 0.6103\n",
      "Epoch 3/23\n",
      "7613/7613 [==============================] - 1s 81us/sample - loss: 0.5698\n",
      "Epoch 4/23\n",
      "7613/7613 [==============================] - 1s 80us/sample - loss: 0.5384\n",
      "Epoch 5/23\n",
      "7613/7613 [==============================] - 1s 89us/sample - loss: 0.5135\n",
      "Epoch 6/23\n",
      "7613/7613 [==============================] - 1s 83us/sample - loss: 0.4932\n",
      "Epoch 7/23\n",
      "7613/7613 [==============================] - 1s 81us/sample - loss: 0.4762\n",
      "Epoch 8/23\n",
      "7613/7613 [==============================] - 1s 82us/sample - loss: 0.4619\n",
      "Epoch 9/23\n",
      "7613/7613 [==============================] - 1s 80us/sample - loss: 0.4495\n",
      "Epoch 10/23\n",
      "7613/7613 [==============================] - 1s 90us/sample - loss: 0.4387\n",
      "Epoch 11/23\n",
      "7613/7613 [==============================] - 1s 86us/sample - loss: 0.4292\n",
      "Epoch 12/23\n",
      "7613/7613 [==============================] - 1s 81us/sample - loss: 0.4208\n",
      "Epoch 13/23\n",
      "7613/7613 [==============================] - 1s 96us/sample - loss: 0.4130\n",
      "Epoch 14/23\n",
      "7613/7613 [==============================] - 1s 86us/sample - loss: 0.4061\n",
      "Epoch 15/23\n",
      "7613/7613 [==============================] - 1s 78us/sample - loss: 0.3998\n",
      "Epoch 16/23\n",
      "7613/7613 [==============================] - 1s 84us/sample - loss: 0.3940\n",
      "Epoch 17/23\n",
      "7613/7613 [==============================] - 1s 81us/sample - loss: 0.3886\n",
      "Epoch 18/23\n",
      "7613/7613 [==============================] - 1s 87us/sample - loss: 0.3836\n",
      "Epoch 19/23\n",
      "7613/7613 [==============================] - 1s 79us/sample - loss: 0.3789\n",
      "Epoch 20/23\n",
      "7613/7613 [==============================] - 1s 77us/sample - loss: 0.3746\n",
      "Epoch 21/23\n",
      "7613/7613 [==============================] - 1s 84us/sample - loss: 0.3705\n",
      "Epoch 22/23\n",
      "7613/7613 [==============================] - 1s 79us/sample - loss: 0.3666\n",
      "Epoch 23/23\n",
      "7613/7613 [==============================] - 1s 78us/sample - loss: 0.3630\n",
      "Making predictions\n",
      "Training model 18\n",
      "Train on 7613 samples\n",
      "Epoch 1/23\n",
      "7613/7613 [==============================] - 1s 106us/sample - loss: 0.6655\n",
      "Epoch 2/23\n",
      "7613/7613 [==============================] - 1s 79us/sample - loss: 0.6107\n",
      "Epoch 3/23\n",
      "7613/7613 [==============================] - 1s 83us/sample - loss: 0.5697\n",
      "Epoch 4/23\n",
      "7613/7613 [==============================] - 1s 85us/sample - loss: 0.5383\n",
      "Epoch 5/23\n",
      "7613/7613 [==============================] - 1s 86us/sample - loss: 0.5132\n",
      "Epoch 6/23\n",
      "7613/7613 [==============================] - 1s 81us/sample - loss: 0.4929\n",
      "Epoch 7/23\n",
      "7613/7613 [==============================] - 1s 79us/sample - loss: 0.4760\n",
      "Epoch 8/23\n",
      "7613/7613 [==============================] - 1s 80us/sample - loss: 0.4616\n",
      "Epoch 9/23\n",
      "7613/7613 [==============================] - 1s 78us/sample - loss: 0.4492\n",
      "Epoch 10/23\n",
      "7613/7613 [==============================] - 1s 104us/sample - loss: 0.4384\n",
      "Epoch 11/23\n",
      "7613/7613 [==============================] - 1s 79us/sample - loss: 0.4289\n",
      "Epoch 12/23\n",
      "7613/7613 [==============================] - 1s 79us/sample - loss: 0.4204\n",
      "Epoch 13/23\n",
      "7613/7613 [==============================] - 1s 78us/sample - loss: 0.4128\n",
      "Epoch 14/23\n",
      "7613/7613 [==============================] - 1s 80us/sample - loss: 0.4058\n",
      "Epoch 15/23\n",
      "7613/7613 [==============================] - 1s 81us/sample - loss: 0.3995\n",
      "Epoch 16/23\n",
      "7613/7613 [==============================] - 1s 78us/sample - loss: 0.3937\n",
      "Epoch 17/23\n",
      "7613/7613 [==============================] - 1s 94us/sample - loss: 0.3883\n",
      "Epoch 18/23\n",
      "7613/7613 [==============================] - 1s 91us/sample - loss: 0.3834\n",
      "Epoch 19/23\n",
      "7613/7613 [==============================] - 1s 87us/sample - loss: 0.3787\n",
      "Epoch 20/23\n",
      "7613/7613 [==============================] - 1s 86us/sample - loss: 0.3744\n",
      "Epoch 21/23\n",
      "7613/7613 [==============================] - 1s 89us/sample - loss: 0.3703\n",
      "Epoch 22/23\n",
      "7613/7613 [==============================] - 1s 83us/sample - loss: 0.3665\n",
      "Epoch 23/23\n",
      "7613/7613 [==============================] - 1s 82us/sample - loss: 0.3628\n",
      "Making predictions\n",
      "Training model 19\n",
      "Train on 7613 samples\n",
      "Epoch 1/23\n",
      "7613/7613 [==============================] - 1s 111us/sample - loss: 0.6661\n",
      "Epoch 2/23\n",
      "7613/7613 [==============================] - 1s 81us/sample - loss: 0.6118\n",
      "Epoch 3/23\n",
      "7613/7613 [==============================] - 1s 79us/sample - loss: 0.5709\n",
      "Epoch 4/23\n",
      "7613/7613 [==============================] - 1s 78us/sample - loss: 0.5394\n",
      "Epoch 5/23\n",
      "7613/7613 [==============================] - 1s 83us/sample - loss: 0.5143\n",
      "Epoch 6/23\n",
      "7613/7613 [==============================] - 1s 78us/sample - loss: 0.4938\n",
      "Epoch 7/23\n",
      "7613/7613 [==============================] - 1s 80us/sample - loss: 0.4768\n",
      "Epoch 8/23\n",
      "7613/7613 [==============================] - 1s 81us/sample - loss: 0.4624\n",
      "Epoch 9/23\n",
      "7613/7613 [==============================] - 1s 78us/sample - loss: 0.4499\n",
      "Epoch 10/23\n",
      "7613/7613 [==============================] - 1s 88us/sample - loss: 0.4390\n",
      "Epoch 11/23\n",
      "7613/7613 [==============================] - 1s 79us/sample - loss: 0.4295\n",
      "Epoch 12/23\n",
      "7613/7613 [==============================] - 1s 79us/sample - loss: 0.4209\n",
      "Epoch 13/23\n",
      "7613/7613 [==============================] - 1s 81us/sample - loss: 0.4133\n",
      "Epoch 14/23\n",
      "7613/7613 [==============================] - 1s 79us/sample - loss: 0.4063\n",
      "Epoch 15/23\n",
      "7613/7613 [==============================] - 1s 81us/sample - loss: 0.3999\n",
      "Epoch 16/23\n",
      "7613/7613 [==============================] - 1s 82us/sample - loss: 0.3941\n",
      "Epoch 17/23\n",
      "7613/7613 [==============================] - 1s 81us/sample - loss: 0.3887\n",
      "Epoch 18/23\n",
      "7613/7613 [==============================] - 1s 90us/sample - loss: 0.3837\n",
      "Epoch 19/23\n",
      "7613/7613 [==============================] - 1s 90us/sample - loss: 0.3791\n",
      "Epoch 20/23\n",
      "7613/7613 [==============================] - 1s 84us/sample - loss: 0.3747\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/23\n",
      "7613/7613 [==============================] - 1s 83us/sample - loss: 0.3706\n",
      "Epoch 22/23\n",
      "7613/7613 [==============================] - 1s 83us/sample - loss: 0.3667\n",
      "Epoch 23/23\n",
      "7613/7613 [==============================] - 1s 85us/sample - loss: 0.3631\n",
      "Making predictions\n",
      "Training model 20\n",
      "Train on 7613 samples\n",
      "Epoch 1/23\n",
      "7613/7613 [==============================] - 1s 111us/sample - loss: 0.6631\n",
      "Epoch 2/23\n",
      "7613/7613 [==============================] - 1s 81us/sample - loss: 0.6089\n",
      "Epoch 3/23\n",
      "7613/7613 [==============================] - 1s 81us/sample - loss: 0.5684\n",
      "Epoch 4/23\n",
      "7613/7613 [==============================] - 1s 91us/sample - loss: 0.5372\n",
      "Epoch 5/23\n",
      "7613/7613 [==============================] - 1s 77us/sample - loss: 0.5124\n",
      "Epoch 6/23\n",
      "7613/7613 [==============================] - 1s 81us/sample - loss: 0.4923\n",
      "Epoch 7/23\n",
      "7613/7613 [==============================] - 1s 80us/sample - loss: 0.4754\n",
      "Epoch 8/23\n",
      "7613/7613 [==============================] - 1s 85us/sample - loss: 0.4612\n",
      "Epoch 9/23\n",
      "7613/7613 [==============================] - 1s 92us/sample - loss: 0.4488\n",
      "Epoch 10/23\n",
      "7613/7613 [==============================] - 1s 81us/sample - loss: 0.4381\n",
      "Epoch 11/23\n",
      "7613/7613 [==============================] - 1s 80us/sample - loss: 0.4286\n",
      "Epoch 12/23\n",
      "7613/7613 [==============================] - 1s 78us/sample - loss: 0.4201\n",
      "Epoch 13/23\n",
      "7613/7613 [==============================] - 1s 78us/sample - loss: 0.4126\n",
      "Epoch 14/23\n",
      "7613/7613 [==============================] - 1s 79us/sample - loss: 0.4056\n",
      "Epoch 15/23\n",
      "7613/7613 [==============================] - 1s 78us/sample - loss: 0.3994\n",
      "Epoch 16/23\n",
      "7613/7613 [==============================] - 1s 78us/sample - loss: 0.3935\n",
      "Epoch 17/23\n",
      "7613/7613 [==============================] - 1s 81us/sample - loss: 0.3882\n",
      "Epoch 18/23\n",
      "7613/7613 [==============================] - 1s 78us/sample - loss: 0.3832\n",
      "Epoch 19/23\n",
      "7613/7613 [==============================] - 1s 83us/sample - loss: 0.3786\n",
      "Epoch 20/23\n",
      "7613/7613 [==============================] - 1s 90us/sample - loss: 0.3742\n",
      "Epoch 21/23\n",
      "7613/7613 [==============================] - 1s 103us/sample - loss: 0.3702\n",
      "Epoch 22/23\n",
      "7613/7613 [==============================] - 1s 81us/sample - loss: 0.3663\n",
      "Epoch 23/23\n",
      "7613/7613 [==============================] - 1s 78us/sample - loss: 0.3627\n",
      "Making predictions\n",
      "Training model 21\n",
      "Train on 7613 samples\n",
      "Epoch 1/23\n",
      "7613/7613 [==============================] - 1s 106us/sample - loss: 0.6664\n",
      "Epoch 2/23\n",
      "7613/7613 [==============================] - 1s 78us/sample - loss: 0.6114\n",
      "Epoch 3/23\n",
      "7613/7613 [==============================] - 1s 77us/sample - loss: 0.5705\n",
      "Epoch 4/23\n",
      "7613/7613 [==============================] - 1s 86us/sample - loss: 0.5390\n",
      "Epoch 5/23\n",
      "7613/7613 [==============================] - 1s 87us/sample - loss: 0.5140\n",
      "Epoch 6/23\n",
      "7613/7613 [==============================] - 1s 79us/sample - loss: 0.4936\n",
      "Epoch 7/23\n",
      "7613/7613 [==============================] - 1s 78us/sample - loss: 0.4766\n",
      "Epoch 8/23\n",
      "7613/7613 [==============================] - 1s 94us/sample - loss: 0.4622\n",
      "Epoch 9/23\n",
      "7613/7613 [==============================] - 1s 83us/sample - loss: 0.4498\n",
      "Epoch 10/23\n",
      "7613/7613 [==============================] - 1s 85us/sample - loss: 0.4389\n",
      "Epoch 11/23\n",
      "7613/7613 [==============================] - 1s 79us/sample - loss: 0.4294\n",
      "Epoch 12/23\n",
      "7613/7613 [==============================] - 1s 79us/sample - loss: 0.4209\n",
      "Epoch 13/23\n",
      "7613/7613 [==============================] - 1s 77us/sample - loss: 0.4132\n",
      "Epoch 14/23\n",
      "7613/7613 [==============================] - 1s 86us/sample - loss: 0.4063\n",
      "Epoch 15/23\n",
      "7613/7613 [==============================] - 1s 80us/sample - loss: 0.3999\n",
      "Epoch 16/23\n",
      "7613/7613 [==============================] - 1s 80us/sample - loss: 0.3941\n",
      "Epoch 17/23\n",
      "7613/7613 [==============================] - 1s 79us/sample - loss: 0.3887\n",
      "Epoch 18/23\n",
      "7613/7613 [==============================] - 1s 79us/sample - loss: 0.3837\n",
      "Epoch 19/23\n",
      "7613/7613 [==============================] - 1s 84us/sample - loss: 0.3790\n",
      "Epoch 20/23\n",
      "7613/7613 [==============================] - 1s 81us/sample - loss: 0.3747\n",
      "Epoch 21/23\n",
      "7613/7613 [==============================] - 1s 80us/sample - loss: 0.3706\n",
      "Epoch 22/23\n",
      "7613/7613 [==============================] - 1s 79us/sample - loss: 0.3667\n",
      "Epoch 23/23\n",
      "7613/7613 [==============================] - 1s 86us/sample - loss: 0.3631\n",
      "Making predictions\n",
      "Training model 22\n",
      "Train on 7613 samples\n",
      "Epoch 1/23\n",
      "7613/7613 [==============================] - 1s 107us/sample - loss: 0.6682\n",
      "Epoch 2/23\n",
      "7613/7613 [==============================] - 1s 79us/sample - loss: 0.6128\n",
      "Epoch 3/23\n",
      "7613/7613 [==============================] - 1s 80us/sample - loss: 0.5715\n",
      "Epoch 4/23\n",
      "7613/7613 [==============================] - 1s 80us/sample - loss: 0.5396\n",
      "Epoch 5/23\n",
      "7613/7613 [==============================] - 1s 81us/sample - loss: 0.5143\n",
      "Epoch 6/23\n",
      "7613/7613 [==============================] - 1s 80us/sample - loss: 0.4938\n",
      "Epoch 7/23\n",
      "7613/7613 [==============================] - 1s 82us/sample - loss: 0.4767\n",
      "Epoch 8/23\n",
      "7613/7613 [==============================] - 1s 100us/sample - loss: 0.4623\n",
      "Epoch 9/23\n",
      "7613/7613 [==============================] - 1s 81us/sample - loss: 0.4498\n",
      "Epoch 10/23\n",
      "7613/7613 [==============================] - 1s 79us/sample - loss: 0.4389\n",
      "Epoch 11/23\n",
      "7613/7613 [==============================] - 1s 78us/sample - loss: 0.4294\n",
      "Epoch 12/23\n",
      "7613/7613 [==============================] - 1s 78us/sample - loss: 0.4208\n",
      "Epoch 13/23\n",
      "7613/7613 [==============================] - 1s 80us/sample - loss: 0.4131\n",
      "Epoch 14/23\n",
      "7613/7613 [==============================] - 1s 85us/sample - loss: 0.4062\n",
      "Epoch 15/23\n",
      "7613/7613 [==============================] - 1s 89us/sample - loss: 0.3998\n",
      "Epoch 16/23\n",
      "7613/7613 [==============================] - 1s 84us/sample - loss: 0.3940\n",
      "Epoch 17/23\n",
      "7613/7613 [==============================] - ETA: 0s - loss: 0.388 - 1s 78us/sample - loss: 0.3886\n",
      "Epoch 18/23\n",
      "7613/7613 [==============================] - 1s 80us/sample - loss: 0.3836\n",
      "Epoch 19/23\n",
      "7613/7613 [==============================] - 1s 82us/sample - loss: 0.3790\n",
      "Epoch 20/23\n",
      "7613/7613 [==============================] - 1s 86us/sample - loss: 0.3746\n",
      "Epoch 21/23\n",
      "7613/7613 [==============================] - 1s 87us/sample - loss: 0.3704\n",
      "Epoch 22/23\n",
      "7613/7613 [==============================] - 1s 80us/sample - loss: 0.3666\n",
      "Epoch 23/23\n",
      "7613/7613 [==============================] - 1s 82us/sample - loss: 0.3630\n",
      "Making predictions\n",
      "Training model 23\n",
      "Train on 7613 samples\n",
      "Epoch 1/23\n",
      "7613/7613 [==============================] - 1s 123us/sample - loss: 0.6636\n",
      "Epoch 2/23\n",
      "7613/7613 [==============================] - 1s 84us/sample - loss: 0.6091\n",
      "Epoch 3/23\n",
      "7613/7613 [==============================] - 1s 98us/sample - loss: 0.5685\n",
      "Epoch 4/23\n",
      "7613/7613 [==============================] - 1s 85us/sample - loss: 0.5372\n",
      "Epoch 5/23\n",
      "7613/7613 [==============================] - 1s 87us/sample - loss: 0.5125\n",
      "Epoch 6/23\n",
      "7613/7613 [==============================] - 1s 83us/sample - loss: 0.4922\n",
      "Epoch 7/23\n",
      "7613/7613 [==============================] - 1s 86us/sample - loss: 0.4754\n",
      "Epoch 8/23\n",
      "7613/7613 [==============================] - 1s 79us/sample - loss: 0.4611\n",
      "Epoch 9/23\n",
      "7613/7613 [==============================] - 1s 81us/sample - loss: 0.4488\n",
      "Epoch 10/23\n",
      "7613/7613 [==============================] - 1s 79us/sample - loss: 0.4380\n",
      "Epoch 11/23\n",
      "7613/7613 [==============================] - 1s 78us/sample - loss: 0.4286\n",
      "Epoch 12/23\n",
      "7613/7613 [==============================] - 1s 79us/sample - loss: 0.4202\n",
      "Epoch 13/23\n",
      "7613/7613 [==============================] - 1s 80us/sample - loss: 0.4125\n",
      "Epoch 14/23\n",
      "7613/7613 [==============================] - 1s 80us/sample - loss: 0.4056\n",
      "Epoch 15/23\n",
      "7613/7613 [==============================] - 1s 80us/sample - loss: 0.3994\n",
      "Epoch 16/23\n",
      "7613/7613 [==============================] - 1s 88us/sample - loss: 0.3936\n",
      "Epoch 17/23\n",
      "7613/7613 [==============================] - 1s 78us/sample - loss: 0.3882\n",
      "Epoch 18/23\n",
      "7613/7613 [==============================] - 1s 77us/sample - loss: 0.3833\n",
      "Epoch 19/23\n",
      "7613/7613 [==============================] - 1s 84us/sample - loss: 0.3786\n",
      "Epoch 20/23\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7613/7613 [==============================] - 1s 78us/sample - loss: 0.3743\n",
      "Epoch 21/23\n",
      "7613/7613 [==============================] - 1s 77us/sample - loss: 0.3702\n",
      "Epoch 22/23\n",
      "7613/7613 [==============================] - 1s 78us/sample - loss: 0.3664\n",
      "Epoch 23/23\n",
      "7613/7613 [==============================] - 1s 77us/sample - loss: 0.3628\n",
      "Making predictions\n",
      "Training model 24\n",
      "Train on 7613 samples\n",
      "Epoch 1/23\n",
      "7613/7613 [==============================] - 1s 107us/sample - loss: 0.6643\n",
      "Epoch 2/23\n",
      "7613/7613 [==============================] - 1s 91us/sample - loss: 0.6097\n",
      "Epoch 3/23\n",
      "7613/7613 [==============================] - 1s 85us/sample - loss: 0.5691\n",
      "Epoch 4/23\n",
      "7613/7613 [==============================] - 1s 84us/sample - loss: 0.5378\n",
      "Epoch 5/23\n",
      "7613/7613 [==============================] - 1s 87us/sample - loss: 0.5129\n",
      "Epoch 6/23\n",
      "7613/7613 [==============================] - 1s 91us/sample - loss: 0.4927\n",
      "Epoch 7/23\n",
      "7613/7613 [==============================] - 1s 96us/sample - loss: 0.4758\n",
      "Epoch 8/23\n",
      "7613/7613 [==============================] - 1s 83us/sample - loss: 0.4615\n",
      "Epoch 9/23\n",
      "7613/7613 [==============================] - 1s 80us/sample - loss: 0.4492\n",
      "Epoch 10/23\n",
      "7613/7613 [==============================] - 1s 80us/sample - loss: 0.4384\n",
      "Epoch 11/23\n",
      "7613/7613 [==============================] - 1s 80us/sample - loss: 0.4289\n",
      "Epoch 12/23\n",
      "7613/7613 [==============================] - 1s 82us/sample - loss: 0.4204\n",
      "Epoch 13/23\n",
      "7613/7613 [==============================] - 1s 82us/sample - loss: 0.4129\n",
      "Epoch 14/23\n",
      "7613/7613 [==============================] - 1s 81us/sample - loss: 0.4059\n",
      "Epoch 15/23\n",
      "7613/7613 [==============================] - 1s 81us/sample - loss: 0.3996\n",
      "Epoch 16/23\n",
      "7613/7613 [==============================] - 1s 85us/sample - loss: 0.3938\n",
      "Epoch 17/23\n",
      "7613/7613 [==============================] - 1s 80us/sample - loss: 0.3884\n",
      "Epoch 18/23\n",
      "7613/7613 [==============================] - 1s 95us/sample - loss: 0.3834\n",
      "Epoch 19/23\n",
      "7613/7613 [==============================] - 1s 81us/sample - loss: 0.3787\n",
      "Epoch 20/23\n",
      "7613/7613 [==============================] - 1s 83us/sample - loss: 0.3744\n",
      "Epoch 21/23\n",
      "7613/7613 [==============================] - 1s 83us/sample - loss: 0.3703\n",
      "Epoch 22/23\n",
      "7613/7613 [==============================] - 1s 83us/sample - loss: 0.3665\n",
      "Epoch 23/23\n",
      "7613/7613 [==============================] - 1s 81us/sample - loss: 0.3628\n",
      "Making predictions\n"
     ]
    }
   ],
   "source": [
    "models = train_ensemble(25, 32, 23, np.array(dftrain_minimal.drop('_target', axis=1)), np.array(dftrain_minimal['_target']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making prediction 0\n",
      "Making prediction 1\n",
      "Making prediction 2\n",
      "Making prediction 3\n",
      "Making prediction 4\n",
      "Making prediction 5\n",
      "Making prediction 6\n",
      "Making prediction 7\n",
      "Making prediction 8\n",
      "Making prediction 9\n",
      "Making prediction 10\n",
      "Making prediction 11\n",
      "Making prediction 12\n",
      "Making prediction 13\n",
      "Making prediction 14\n",
      "Making prediction 15\n",
      "Making prediction 16\n",
      "Making prediction 17\n",
      "Making prediction 18\n",
      "Making prediction 19\n",
      "Making prediction 20\n",
      "Making prediction 21\n",
      "Making prediction 22\n",
      "Making prediction 23\n",
      "Making prediction 24\n",
      "Prediction complete\n"
     ]
    }
   ],
   "source": [
    "# Make test predictions\n",
    "raw_preds = np.zeros((dftest_minimal.shape[0],1))\n",
    "for i in range(len(models)):\n",
    "    print('Making prediction', i)\n",
    "    raw_preds += models[i].predict(dftest_minimal)\n",
    "raw_preds /= 25\n",
    "preds = 1*(raw_preds >= 0.5)\n",
    "print('Prediction complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['target'] = preds\n",
    "submission.to_csv('submission_lg_ensemble.csv', index=False)\n",
    "# Score: 80.572%, then 79.243%, ugh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
